"In this paper, we focus on the problem of unsupervised image-sentence matching. Existing research explores to utilize document-level structural information to sample positive and negative instances for model training. Although the approach achieves positive results, it introduces a sampling bias and fails to distinguish instances with high semantic similarity. To alleviate the bias, we propose a new sampling strategy to select additional intra-document image-sentence pairs as positive or negative samples. Furthermore, to recognize the complex pattern in intra-document samples, we propose a Transformer based model to capture fine-grained features and implicitly construct a graph for each document, where concepts in a document are introduced to bridge the representation learning of images and sentences in the context of a document. Experimental results show the effectiveness of our approach to alleviate the bias and learn well-aligned multimodal representations.",0
"This paper concerns itself with the issue of unsupervised image-sentence matching. Previous research has explored the utilization of structural information at the document level to generate positive and negative instances for model training. While this approach yields positive results, it has a sampling bias and cannot differentiate between instances with high semantic similarity. To address this bias, we suggest a new sampling strategy that selects additional intra-document image-sentence pairs as positive or negative samples. Additionally, we propose a Transformer-based model for recognizing complex patterns in intra-document samples, capturing fine-grained features, and implicitly constructing a graph for each document. This graph introduces concepts in a document to bridge the representation learning of images and sentences in the context of a document. We conducted experiments that demonstrate the effectiveness of our approach in mitigating bias and learning well-aligned multimodal representations.",1
"Engineering simulations for analysis of structural and fluid systems require information of contacts between various 3-D surfaces of the geometry to accurately model the physics between them. In machine learning applications, 3-D surfaces are most suitably represented with point clouds or meshes and learning representations of interacting geometries form point-based representations is challenging. The objective of this work is to introduce a machine learning algorithm, ActivationNet, that can learn from point clouds or meshes of interacting 3-D surfaces and predict the quality of contact between these surfaces. The ActivationNet generates activation states from point-based representation of surfaces using a multi-dimensional binning approach. The activation states are further used to contact quality between surfaces using deep neural networks. The performance of our model is demonstrated using several experiments, including tests on interacting surfaces extracted from engineering geometries. In all the experiments presented in this paper, the contact quality predictions of ActivationNet agree well with the expectations.",0
"To accurately model the physics between various 3-D surfaces in structural and fluid systems, engineering simulations require information on their contacts. However, machine learning applications face challenges in representing interacting geometries from point-based representations of 3-D surfaces, which are commonly represented as point clouds or meshes. The aim of this study is to introduce ActivationNet, a machine learning algorithm that predicts contact quality between interacting 3-D surfaces by generating activation states from point-based representations using a multi-dimensional binning approach. Deep neural networks are then employed to analyze the activation states and predict contact quality. To evaluate the model's performance, experiments were conducted on interacting surfaces extracted from engineering geometries, and the results showed that the ActivationNet predictions aligned well with expectations.",1
"Estimating an individual's potential response to interventions from observational data is of high practical relevance for many domains, such as healthcare, public policy or economics. In this setting, it is often the case that combinations of interventions may be applied simultaneously, for example, multiple prescriptions in healthcare or different fiscal and monetary measures in economics. However, existing methods for counterfactual inference are limited to settings in which actions are not used simultaneously. Here, we present Neural Counterfactual Relation Estimation (NCoRE), a new method for learning counterfactual representations in the combination treatment setting that explicitly models cross-treatment interactions. NCoRE is based on a novel branched conditional neural representation that includes learnt treatment interaction modulators to infer the potential causal generative process underlying the combination of multiple treatments. Our experiments show that NCoRE significantly outperforms existing state-of-the-art methods for counterfactual treatment effect estimation that do not account for the effects of combining multiple treatments across several synthetic, semi-synthetic and real-world benchmarks.",0
"The practical significance of estimating an individual's potential reaction to interventions using observational data is high in various fields, including healthcare, public policy, and economics. However, in these domains, it is common to apply combinations of interventions simultaneously; for instance, multiple prescriptions in healthcare or different fiscal and monetary measures in economics. Unfortunately, existing methods for counterfactual inference are inadequate for settings where actions are used simultaneously. To address this limitation, we introduce Neural Counterfactual Relation Estimation (NCoRE), a new approach for learning counterfactual representations in the combination treatment setting that explicitly models cross-treatment interactions. NCoRE uses a novel branched conditional neural representation that includes learnt treatment interaction modulators to infer the potential causal generative process underlying the combination of multiple treatments. Our experiments demonstrate that NCoRE outperforms existing state-of-the-art methods for counterfactual treatment effect estimation that do not account for the effects of combining multiple treatments across several synthetic, semi-synthetic, and real-world benchmarks.",1
"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",0
"Over the past decade, deep supervised learning has achieved significant success, but its reliance on manual labels and susceptibility to attacks has prompted researchers to seek a better solution. Self-supervised learning has gained popularity as an alternative due to its impressive performance in representation learning and its ability to benefit a wide range of downstream tasks without requiring external supervision. This survey delves into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. Existing empirical methods are comprehensively reviewed and categorized into three main objectives: generative, contrastive, and generative-contrastive (adversarial). The survey also explores related theoretical analysis work to provide further insights into the workings of self-supervised learning. Finally, the survey briefly discusses open problems and future directions for self-supervised learning, and an outline slide is provided.",1
"We present Self-Classifier -- a novel self-supervised end-to-end classification neural network. Self-Classifier learns labels and representations simultaneously in a single-stage end-to-end manner by optimizing for same-class prediction of two augmented views of the same sample. To guarantee non-degenerate solutions (i.e., solutions where all labels are assigned to the same class), a uniform prior is asserted on the labels. We show mathematically that unlike the regular cross-entropy loss, our approach avoids such solutions. Self-Classifier is simple to implement and is scalable to practically unlimited amounts of data. Unlike other unsupervised classification approaches, it does not require any form of pre-training or the use of expectation maximization algorithms, pseudo-labelling or external clustering. Unlike other contrastive learning representation learning approaches, it does not require a memory bank or a second network. Despite its relative simplicity, our approach achieves comparable results to state-of-the-art performance with ImageNet, CIFAR10 and CIFAR100 for its two objectives: unsupervised classification and unsupervised representation learning. Furthermore, it is the first unsupervised end-to-end classification network to perform well on the large-scale ImageNet dataset. Code will be made available.",0
"Introducing the Self-Classifier, a ground-breaking self-supervised neural network for end-to-end classification. The Self-Classifier simultaneously learns labels and representations in one stage by optimizing two augmented views of the same sample for same-class prediction. To ensure solutions are non-degenerate and avoid assigning all labels to one class, a uniform prior is applied to the labels. Unlike cross-entropy loss, this approach guarantees avoidance of degenerate solutions. The Self-Classifier is straightforward to implement and scalable to vast amounts of data. It surpasses other unsupervised classification methods as it does not require pre-training, expectation maximization algorithms, pseudo-labelling, external clustering or a memory bank or second network. Despite its simplicity, it delivers comparable results to state-of-the-art performance with ImageNet, CIFAR10 and CIFAR100 for unsupervised classification and representation learning. Additionally, it is the first unsupervised end-to-end classification network to deliver excellent results on the large-scale ImageNet dataset. Code will be accessible.",1
"We introduce HarperValleyBank, a free, public domain spoken dialog corpus. The data simulate simple consumer banking interactions, containing about 23 hours of audio from 1,446 human-human conversations between 59 unique speakers. We selected intents and utterance templates to allow realistic variation while controlling overall task complexity and limiting vocabulary size to about 700 unique words. We provide audio data along with transcripts and annotations for speaker identity, caller intent, dialog actions, and emotional valence. The data size and domain specificity makes for quick transcription experiments with modern end-to-end neural approaches. Further, we provide baselines for representation learning, adapting recent work to embed waveforms for downstream prediction tasks. Our experiments show that tasks using our annotations are sensitive to both the model choice and corpus size.",0
"HarperValleyBank is a spoken dialog corpus that is available to the public for free. The dataset consists of approximately 23 hours of audio containing 1,446 human-human conversations between 59 unique speakers. We carefully selected intents and utterance templates to ensure realistic variation while controlling task complexity and limiting the vocabulary size to about 700 unique words. Transcripts and annotations for speaker identity, caller intent, dialog actions, and emotional valence accompany the audio data. Due to the dataset's size and domain specificity, it is ideal for quick transcription experiments using modern end-to-end neural approaches. We also provide baselines for representation learning, which have been adapted to embed waveforms for downstream prediction tasks. Our experiments indicate that the model choice and corpus size significantly impact the performance of tasks using our annotations.",1
"Momentum Contrast (MoCo) achieves great success for unsupervised visual representation. However, there are a lot of supervised and semi-supervised datasets, which are already labeled. To fully utilize the label annotations, we propose Unified Momentum Contrast (UniMoCo), which extends MoCo to support arbitrary ratios of labeled data and unlabeled data training. Compared with MoCo, UniMoCo has two modifications as follows: (1) Different from a single positive pair in MoCo, we maintain multiple positive pairs on-the-fly by comparing the query label to a label queue. (2) We propose a Unified Contrastive(UniCon) loss to support an arbitrary number of positives and negatives in a unified pair-wise optimization perspective. Our UniCon is more reasonable and powerful than the supervised contrastive loss in theory and practice. In our experiments, we pre-train multiple UniMoCo models with different ratios of ImageNet labels and evaluate the performance on various downstream tasks. Experiment results show that UniMoCo generalizes well for unsupervised, semi-supervised and supervised visual representation learning.",0
"Unsupervised visual representation has achieved significant success through the use of Momentum Contrast (MoCo). However, there are already a large number of labeled supervised and semi-supervised datasets available. To make full use of these annotations, we propose Unified Momentum Contrast (UniMoCo), which extends MoCo to support training with arbitrary ratios of labeled and unlabeled data. UniMoCo includes two modifications compared to MoCo. First, we maintain multiple positive pairs on-the-fly by comparing the query label to a label queue. Second, we introduce a Unified Contrastive (UniCon) loss that supports an arbitrary number of positives and negatives in a unified pair-wise optimization perspective. UniCon is more reasonable and powerful than the supervised contrastive loss in both theory and practice. Our experiments involved pre-training multiple UniMoCo models with various ratios of ImageNet labels and evaluating their performance on a range of downstream tasks. The results indicate that UniMoCo generalizes well across unsupervised, semi-supervised, and supervised visual representation learning.",1
"We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept ""disentangled representations"" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability - which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.",0
"We offer two theoretical contributions to the field of disentanglement learning. Firstly, we provide clear definitions of disentangled representations, which are used in both supervised and unsupervised methods. We identify three dimensions of these representations - informativeness, separability, and interpretability - that can be explicitly quantified using information-theoretic concepts. This allows us to explain the behavior of several well-known disentanglement learning models. Secondly, we propose robust metrics for measuring informativeness, separability, and interpretability, which enable a fair comparison of different disentanglement learning methods. Through a range of experiments, we demonstrate that our metrics accurately capture the representations learned by different methods and align with qualitative (visual) results. We also uncover new properties of VAE-based methods and provide interpretations of these findings using our formulation. Our contributions are significant and have the potential to inspire the development of more theoretically motivated models for disentanglement learning.",1
"Learning 3D representations that generalize well to arbitrarily oriented inputs is a challenge of practical importance in applications varying from computer vision to physics and chemistry. We propose a novel multi-resolution convolutional architecture for learning over concentric spherical feature maps, of which the single sphere representation is a special case. Our hierarchical architecture is based on alternatively learning to incorporate both intra-sphere and inter-sphere information. We show the applicability of our method for two different types of 3D inputs, mesh objects, which can be regularly sampled, and point clouds, which are irregularly distributed. We also propose an efficient mapping of point clouds to concentric spherical images, thereby bridging spherical convolutions on grids with general point clouds. We demonstrate the effectiveness of our approach in improving state-of-the-art performance on 3D classification tasks with rotated data.",0
"Developing 3D representations that can be applied to inputs with any orientation is a significant challenge with practical implications across a range of fields, including computer vision, physics, and chemistry. Our novel multi-resolution convolutional architecture is designed to learn over concentric spherical feature maps, including the special case of a single sphere representation. Our hierarchical structure involves learning to incorporate intra-sphere and inter-sphere information alternatively. Our approach is suitable for two distinct types of 3D inputs: mesh objects that can be regularly sampled, and point clouds that are irregularly distributed. We also propose an efficient method for mapping point clouds to concentric spherical images, allowing us to bridge spherical convolutions on grids with general point clouds. We demonstrate the effectiveness of our approach by improving the performance of state-of-the-art 3D classification tasks involving rotated data.",1
"Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.",0
"Training reinforcement learning methods on a limited number of environments often results in policies that do not apply to new environments. To enhance generalization, we integrate the sequential structure inherent in reinforcement learning into the representation learning process. This approach differs from recent methods that do not explicitly utilize this structure. Our technique involves a policy similarity metric (PSM) that measures the similarity between states based on their behavioral characteristics. PSM assigns high similarity scores to states with similar optimal policies in both the present and future. To embed any state similarity metric, we present a contrastive representation learning method which we use with PSM to generate policy similarity embeddings (PSEs). We provide evidence that PSEs enhance generalization across various benchmarks, such as LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.",1
"The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-time Crop & Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.",0
"The success of image representations acquired through self-supervised learning heavily relies on the type of data augmentations applied in the learning process. Although recent studies have extended these approaches from still images to videos and discovered that utilizing both audio and video signals leads to significant improvements, they did not observe the same level of effectiveness with spatial augmentations, like cropping, which are critical for still images. This research aims to enhance these methods specifically for spatio-temporal aspects of videos. Firstly, we demonstrate that spatial augmentations, such as cropping, can also be useful for videos, but their previous applications were not efficient enough to function well at scale. Therefore, we introduce Feature Crop, a technique that can simulate these augmentations more efficiently in feature space. Secondly, we show that utilizing transformer-based attention instead of average pooling can considerably boost performance and is well-suited for processing feature crops. Combining both of these improvements, we introduce a new method called Space-time Crop & Attend (STiCA) that achieves state-of-the-art results across various video-representation learning benchmarks. Specifically, we attain new highest-accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 by pre-training on Kinetics-400.",1
"Deep neural networks trained with standard cross-entropy loss memorize noisy labels, which degrades their performance. Most research to mitigate this memorization proposes new robust classification loss functions. Conversely, we propose a Multi-Objective Interpolation Training (MOIT) approach that jointly exploits contrastive learning and classification to mutually help each other and boost performance against label noise. We show that standard supervised contrastive learning degrades in the presence of label noise and propose an interpolation training strategy to mitigate this behavior. We further propose a novel label noise detection method that exploits the robust feature representations learned via contrastive learning to estimate per-sample soft-labels whose disagreements with the original labels accurately identify noisy samples. This detection allows treating noisy samples as unlabeled and training a classifier in a semi-supervised manner to prevent noise memorization and improve representation learning. We further propose MOIT+, a refinement of MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation studies verify the key components of our method. Experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. Code is available at https://git.io/JI40X.",0
"The performance of deep neural networks can decrease due to the memorization of noisy labels when trained with standard cross-entropy loss. To address this issue, various studies have suggested using new robust classification loss functions. However, we propose a different approach called Multi-Objective Interpolation Training (MOIT), which combines contrastive learning and classification to improve performance against label noise. We found that standard supervised contrastive learning is negatively impacted by label noise, so we developed an interpolation training strategy to alleviate this effect. Additionally, we created a novel label noise detection method that uses the feature representations learned via contrastive learning to estimate per-sample soft-labels and identify noisy samples. This allows us to treat noisy samples as unlabeled and train a classifier in a semi-supervised manner to prevent noise memorization and enhance representation learning. Furthermore, we refined MOIT to create MOIT+ by fine-tuning on detected clean samples. We conducted hyperparameter and ablation studies to confirm the critical components of our approach. Our experiments on synthetic and real-world noise benchmarks revealed that MOIT/MOIT+ achieved state-of-the-art results. The code is available at https://git.io/JI40X.",1
"Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix.",0
"Although contrastive representation learning has been successful in learning representations from unlabeled data, progress in vision fields has heavily relied on carefully designed data augmentations based on domain knowledge. Thus, we introduce i-Mix, a domain-agnostic regularization technique that enhances contrastive representation learning. By treating contrastive learning as training a non-parametric classifier and assigning a unique virtual class to each data in a batch, we mix data instances in both the input and virtual label spaces to generate more augmented data during training. Our experiments demonstrate that i-Mix consistently enhances the quality of learned representations across various domains, such as image, speech, and tabular data. Furthermore, we have confirmed its regularization effect through thorough ablation studies on model and dataset sizes. The code for i-Mix can be accessed at https://github.com/kibok90/imix.",1
"We present ALADIN (All Layer AdaIN); a novel architecture for searching images based on the similarity of their artistic style. Representation learning is critical to visual search, where distance in the learned search embedding reflects image similarity. Learning an embedding that discriminates fine-grained variations in style is hard, due to the difficulty of defining and labelling style. ALADIN takes a weakly supervised approach to learning a representation for fine-grained style similarity of digital artworks, leveraging BAM-FG, a novel large-scale dataset of user generated content groupings gathered from the web. ALADIN sets a new state of the art accuracy for style-based visual search over both coarse labelled style data (BAM) and BAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style groupings also contributed by this work.",0
"Introducing ALADIN (All Layer AdaIN); a fresh architecture for exploring images based on their artistic style similarity. Visual search relies heavily on representation learning, where image similarity is reflected by the distance in the learned search embedding. However, it is challenging to define and label style, making it difficult to learn an embedding that discriminates fine-grained variations in style. To address this issue, ALADIN adopts a weakly supervised approach to learning a representation for fine-grained style similarity in digital artworks. This is achieved by leveraging BAM-FG, a novel large-scale dataset of user-generated content groupings collected from the web. ALADIN achieves a new state-of-the-art accuracy for style-based visual search across both coarse labeled style data (BAM) and BAM-FG, which is a new dataset of 2.62 million images and 310,000 fine-grained style groupings contributed by this work.",1
"Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This ""fusion"" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.",0
"Various data augmentation techniques are being reconsidered as a way to prevent discriminator overfitting in Generative Adversarial Networks (GANs). However, it is uncertain which augmentations can actually enhance GANs and how to broaden the range of augmentations during training. This paper introduces a novel method, ContraD, which merges a recent contrastive representation learning scheme into the GAN discriminator. This enables the discriminators to use more robust augmentations without increasing their training instability, leading to more effective prevention of discriminator overfitting in GANs. Additionally, the contrastive learning itself benefits from this GAN training, maintaining discriminative features between real and fake samples. The experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques, while still maintaining highly discriminative features in the discriminator. Moreover, our unsupervised GANs can generate many conditional generative models via a simple latent sampling that leverages the learned features of ContraD. The code is available at https://github.com/jh-jeong/ContraD.",1
"A standard pipeline of current face recognition frameworks consists of four individual steps: locating a face with a rough bounding box and several fiducial landmarks, aligning the face image using a pre-defined template, extracting representations and comparing. Among them, face detection, landmark detection and representation learning have long been studied and a lot of works have been proposed. As an essential step with a significant impact on recognition performance, the alignment step has attracted little attention. In this paper, we first explore and highlight the effects of different alignment templates on face recognition. Then, for the first time, we try to search for the optimal template automatically. We construct a well-defined searching space by decomposing the template searching into the crop size and vertical shift, and propose an efficient method Face Alignment Policy Search (FAPS). Besides, a well-designed benchmark is proposed to evaluate the searched policy. Experiments on our proposed benchmark validate the effectiveness of our method to improve face recognition performance.",0
"The current face recognition frameworks follow a standard pipeline that includes four steps: identifying a face with a rough bounding box and fiducial landmarks, aligning the face image with a pre-defined template, extracting representations, and comparing. While face detection, landmark detection, and representation learning have received considerable attention and research, the alignment step has been largely overlooked, despite its significant impact on recognition performance. This paper addresses this gap by exploring the effects of different alignment templates on face recognition and introducing an automated method, Face Alignment Policy Search (FAPS), to search for the optimal template. FAPS decomposes the template searching into crop size and vertical shift and proposes a well-defined searching space. A benchmark is also proposed to evaluate the searched policy, and experiments on this benchmark confirm the effectiveness of the proposed method in improving face recognition performance.",1
"We construct an unsupervised learning model that achieves nonlinear disentanglement of underlying factors of variation in naturalistic videos. Previous work suggests that representations can be disentangled if all but a few factors in the environment stay constant at any point in time. As a result, algorithms proposed for this problem have only been tested on carefully constructed datasets with this exact property, leaving it unclear whether they will transfer to natural scenes. Here we provide evidence that objects in segmented natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. We leverage this finding and present SlowVAE, a model for unsupervised representation learning that uses a sparse prior on temporally adjacent observations to disentangle generative factors without any assumptions on the number of changing factors. We provide a proof of identifiability and show that the model reliably learns disentangled representations on several established benchmark datasets, often surpassing the current state-of-the-art. We additionally demonstrate transferability towards video datasets with natural dynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks for guiding disentanglement research towards more natural data domains.",0
"Our team has created an unsupervised learning model that can achieve nonlinear disentanglement of underlying factors in naturalistic videos. Past research has shown that disentangling representations is possible when only a few factors change in the environment at any given time. However, previous algorithms have only been tested on carefully crafted datasets with this specific property, making it uncertain whether they will work on natural scenes. Our research has shown that objects in segmented natural movies undergo transitions that are usually small in magnitude but occasionally have significant jumps, which is characteristic of a temporally sparse distribution. We have used this discovery to develop SlowVAE, an unsupervised representation learning model that employs a sparse prior on temporally adjacent observations to disentangle generative factors without presuming the number of changing factors. We have provided a proof of identifiability and demonstrated that the model can learn disentangled representations that often surpass the current state-of-the-art on several established benchmark datasets. Additionally, we have demonstrated the model's transferability towards video datasets with natural dynamics, namely Natural Sprites and KITTI Masks, which we have contributed as benchmarks for guiding disentanglement research towards more natural data domains.",1
"The promising performance of Deep Neural Networks (DNNs) in text classification, has attracted researchers to use them for fraud review detection. However, the lack of trusted labeled data has limited the performance of the current solutions in detecting fraud reviews. The Generative Adversarial Network (GAN) as a semi-supervised method has demonstrated to be effective for data augmentation purposes. The state-of-the-art solutions utilize GANs to overcome the data scarcity problem. However, they fail to incorporate the behavioral clues in fraud generation. Additionally, state-of-the-art approaches overlook the possible bot-generated reviews in the dataset. Finally, they also suffer from a common limitation in scalability and stability of the GAN, slowing down the training procedure. In this work, we propose ScoreGAN for fraud review detection that makes use of both review text and review rating scores in the generation and detection process. Scores are incorporated through Information Gain Maximization (IGM) into the loss function for three reasons. One is to generate score-correlated reviews based on the scores given to the generator. Second, the generated reviews are employed to train the discriminator, so the discriminator can correctly label the possible bot-generated reviews through joint representations learned from the concatenation of GLobal Vector for Word representation (GLoVe) extracted from the text and the score. Finally, it can be used to improve the stability and scalability of the GAN. Results show that the proposed framework outperformed the existing state-of-the-art framework, namely FakeGAN, in terms of AP by 7\%, and 5\% on the Yelp and TripAdvisor datasets, respectively.",0
"Researchers have turned to using Deep Neural Networks (DNNs) for fraud review detection due to their promising performance in text classification. However, their effectiveness has been limited by the scarcity of reliable labeled data. To address this issue, the use of Generative Adversarial Networks (GANs) for data augmentation has been effective, but current approaches neglect behavioral clues in fraud generation and fail to identify bot-generated reviews. Additionally, scalability and stability issues in the GAN training process have been problematic. To address these issues, we propose ScoreGAN, which incorporates review rating scores through Information Gain Maximization (IGM) into the loss function. This approach generates score-correlated reviews to train the discriminator, allowing for bot-generated reviews to be correctly labeled. Furthermore, it improves the stability and scalability of the GAN. In experiments, ScoreGAN outperformed the state-of-the-art FakeGAN by 7% and 5% on Yelp and TripAdvisor datasets, respectively.",1
"Deeply-learned planning methods are often based on learning representations that are optimized for unrelated tasks. For example, they might be trained on reconstructing the environment. These representations are then combined with predictor functions for simulating rollouts to navigate the environment. We find this principle of learning representations unsatisfying and propose to learn them such that they are directly optimized for the task at hand: to be maximally predictable for the predictor function. This results in representations that are by design optimal for the downstream task of planning, where the learned predictor function is used as a forward model.   To this end, we propose a new way of jointly learning this representation along with the prediction function, a system we dub Latent Representation Prediction Network (LARP). The prediction function is used as a forward model for search on a graph in a viewpoint-matching task and the representation learned to maximize predictability is found to outperform a pre-trained representation. Our approach is shown to be more sample-efficient than standard reinforcement learning methods and our learned representation transfers successfully to dissimilar objects.",0
"Planning methods that are deeply-learned often rely on representations that are optimized for tasks that are unrelated. These representations are usually trained on reconstructing the environment and are combined with predictor functions to simulate rollouts for navigating the environment. However, this approach of learning representations is unsatisfactory. Therefore, we suggest that representations should be directly optimized for the task at hand, which is to be maximally predictable for the predictor function. This leads to representations that are ideal for the downstream planning task, where the learned predictor function acts as a forward model. We propose a new way of jointly learning the representation and prediction function, which is called the Latent Representation Prediction Network (LARP). In a viewpoint-matching task, the prediction function is used as a forward model for search on a graph, and the representation learned to maximize predictability is found to perform better than a pre-trained representation. Our approach is more efficient in terms of sample size than standard reinforcement learning methods, and our learned representation transfers successfully to dissimilar objects.",1
"Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the representation's induced treatment group distance, and encourage sharing of information between treatment groups. We extend these algorithms to simultaneously learn a weighted representation to further reduce treatment group distances. Finally, an experimental evaluation on real and synthetic data shows the value of our proposed representation architecture and regularization scheme.",0
"Machine learning is an appealing solution for practitioners in various fields, including healthcare, economics, and education, who seek to enhance their decision-making abilities. The rising cost and impracticality of performing experiments have drawn attention to the challenge of evaluating decisions based on non-experimental observational data, which is the focus of this work. Specifically, we examine the estimation of individual-level causal effects, such as how a single patient responds to different medications, based on recorded contexts, decisions, and outcomes. We establish generalization bounds on the error in estimated effects using distance measures between groups receiving different treatments and allowing for sample re-weighting. Our bound is tightly constrained under certain conditions and is related to results for unsupervised domain adaptation. Based on our theoretical findings, we design representation learning algorithms that minimize the bound by regulating the treatment group distance induced by the representation and promoting information sharing between treatment groups. We extend these algorithms to learn a weighted representation that further reduces treatment group distances. Finally, we conduct an experimental assessment of our proposed representation architecture and regularization scheme using both real and synthetic data, demonstrating their value.",1
"Attempt to fully discover the temporal diversity and chronological characteristics for self-supervised video representation learning, this work takes advantage of the temporal dependencies within videos and further proposes a novel self-supervised method named Temporal Contrastive Graph Learning (TCGL). In contrast to the existing methods that ignore modeling elaborate temporal dependencies, our TCGL roots in a hybrid graph contrastive learning strategy to jointly regard the inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. To model multi-scale temporal dependencies, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter- snippet temporal contrastive graphs. By randomly removing edges and masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL can generate different correlated graph views. Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different views. To adaptively learn the global context representation and recalibrate the channel-wise features, we introduce an adaptive video snippet order prediction module, which leverages the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.",0
"In an effort to fully explore the temporal diversity and chronological characteristics of self-supervised video representation learning, this study has utilized the temporal dependencies within videos and introduced a novel self-supervised method called Temporal Contrastive Graph Learning (TCGL). Unlike existing methods that neglect the modeling of intricate temporal dependencies, TCGL employs a hybrid graph contrastive learning approach that considers both inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. TCGL integrates prior knowledge about frame and snippet orders into graph structures, namely intra-/inter-snippet temporal contrastive graphs, to model multi-scale temporal dependencies. By randomly removing edges and masking nodes of these graphs, TCGL can generate various correlated graph views, which are then used by specific contrastive learning modules to maximize the agreement between nodes in different views. Furthermore, an adaptive video snippet order prediction module is introduced to learn the global context representation and recalibrate the channel-wise features by leveraging the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of TCGL over state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.",1
"MoCo is effective for unsupervised image representation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the temporal feature representations of MoCo from two perspectives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations regardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a temporally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning. This degradation is reflected via temporal decay to attend the input sample to recent keys in the queue. As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empowering the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video representation learning method.",0
"The effectiveness of MoCo in unsupervised image representation learning has been established. However, in this paper, we propose VideoMoCo as a means of unsupervised video representation learning. We aim to enhance the temporal feature representations of MoCo in two ways. Firstly, we introduce a generator that removes some frames from the input video sequence, and the discriminator is trained to encode similar feature representations regardless of these removals. Through adaptive frame dropout during adversarial learning iterations, we train a temporally robust encoder. Secondly, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. This decay reflects the degradation of the representation ability of the keys as the momentum encoder updates after keys enqueue. By attending the input sample to recent keys in the queue, we adapt MoCo to learn video representations without requiring pretext tasks. VideoMoCo improves MoCo temporally through contrastive learning by enhancing the temporal robustness of the encoder and modeling the temporal decay of the keys. Our experiments on benchmark datasets such as UCF101 and HMDB51 demonstrate that VideoMoCo is a state-of-the-art video representation learning method.",1
"This paper presents SPICE, a Semantic Pseudo-labeling framework for Image ClustEring. Instead of using indirect loss functions required by the recently proposed methods, SPICE generates pseudo-labels via self-learning and directly uses the pseudo-label-based classification loss to train a deep clustering network. The basic idea of SPICE is to synergize the discrepancy among semantic clusters, the similarity among instance samples, and the semantic consistency of local samples in an embedding space to optimize the clustering network in a semantically-driven paradigm. Specifically, a semantic-similarity-based pseudo-labeling algorithm is first proposed to train a clustering network through unsupervised representation learning. Given the initial clustering results, a local semantic consistency principle is used to select a set of reliably labeled samples, and a semi-pseudo-labeling algorithm is adapted for performance boosting. Extensive experiments demonstrate that SPICE clearly outperforms the state-of-the-art methods on six common benchmark datasets including STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. On average, our SPICE method improves the current best results by about 10% in terms of adjusted rand index, normalized mutual information, and clustering accuracy.",0
"The Semantic Pseudo-labeling framework for Image Clustering (SPICE) is the focus of this paper. Unlike other methods that require indirect loss functions, SPICE uses self-learning to generate pseudo-labels and directly trains a deep clustering network based on the pseudo-label-based classification loss. The goal of SPICE is to optimize the clustering network using a semantically-driven paradigm that considers the discrepancy among semantic clusters, the similarity among instance samples, and the semantic consistency of local samples in an embedding space. The paper proposes a semantic-similarity-based pseudo-labeling algorithm to train a clustering network using unsupervised representation learning. After the initial clustering results are obtained, a local semantic consistency principle is applied to select a set of reliably labeled samples, and a semi-pseudo-labeling algorithm is used to boost performance. The paper presents extensive experiments that show SPICE outperforms state-of-the-art methods on six benchmark datasets, including STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. On average, SPICE improves the current best results by approximately 10% in terms of adjusted rand index, normalized mutual information, and clustering accuracy.",1
"Successor-style representations have many advantages for reinforcement learning: for example, they can help an agent generalize from past experience to new goals, and they have been proposed as explanations of behavioral and neural data from human and animal learners. They also form a natural bridge between model-based and model-free RL methods: like the former they make predictions about future experiences, and like the latter they allow efficient prediction of total discounted rewards. However, successor-style representations are not optimized to generalize across policies: typically, we maintain a limited-length list of policies, and share information among them by representation learning or GPI. Successor-style representations also typically make no provision for gathering information or reasoning about latent variables. To address these limitations, we bring together ideas from predictive state representations, belief space value iteration, successor features, and convex analysis: we develop a new, general successor-style representation, together with a Bellman equation that connects multiple sources of information within this representation, including different latent states, policies, and reward functions. The new representation is highly expressive: for example, it lets us efficiently read off an optimal policy for a new reward function, or a policy that imitates a new demonstration. For this paper, we focus on exact computation of the new representation in small, known environments, since even this restricted setting offers plenty of interesting questions. Our implementation does not scale to large, unknown environments -- nor would we expect it to, since it generalizes POMDP value iteration, which is difficult to scale. However, we believe that future work will allow us to extend our ideas to approximate reasoning in large, unknown environments.",0
"Successor-style representations are advantageous in reinforcement learning as they enable an agent to generalize from past experiences to new goals, and have been suggested as explanations for the behavior and neural data of human and animal learners. These representations provide a natural connection between model-based and model-free RL methods, allowing predictions of future experiences and efficient prediction of total discounted rewards. However, they are not optimized for policy generalization, and typically do not consider latent variables. To address these limitations, we combine ideas from various approaches to develop a new successor-style representation that connects multiple sources of information, including latent states, policies, and reward functions. This new representation is highly expressive, allowing for the efficient derivation of optimal policies for new reward functions or imitating new demonstrations. Our focus is on exact computation of this representation in small, known environments, but we anticipate future work will extend these ideas to approximate reasoning in larger, unknown environments.",1
"In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.",0
"Our work introduces a novel generative model that can automatically separate global and local image representations without supervision. We achieve this by incorporating a generative flow into the VAE framework to model the decoder. Our model utilizes the variational auto-encoding framework to learn a low-dimensional vector of latent variables that captures global image information. This vector is then used as a conditional input to a flow-based invertible decoder, with architecture inspired by style transfer literature. Our model is effective in terms of density estimation, image generation, and unsupervised representation learning, as demonstrated by experimental results on standard image benchmarks. Notably, our work shows that a generative model with a likelihood-based objective can learn decoupled representations without explicit supervision, using only architectural inductive biases. The code for our model is available at https://github.com/XuezheMax/wolf.",1
"We study unsupervised video representation learning that seeks to learn both motion and appearance features from unlabeled video only, which can be reused for downstream tasks such as action recognition. This task, however, is extremely challenging due to 1) the highly complex spatial-temporal information in videos; and 2) the lack of labeled data for training. Unlike the representation learning for static images, it is difficult to construct a suitable self-supervised task to well model both motion and appearance features. More recently, several attempts have been made to learn video representation through video playback speed prediction. However, it is non-trivial to obtain precise speed labels for the videos. More critically, the learnt models may tend to focus on motion pattern and thus may not learn appearance features well. In this paper, we observe that the relative playback speed is more consistent with motion pattern, and thus provide more effective and stable supervision for representation learning. Therefore, we propose a new way to perceive the playback speed and exploit the relative speed between two video clips as labels. In this way, we are able to well perceive speed and learn better motion features. Moreover, to ensure the learning of appearance features, we further propose an appearance-focused task, where we enforce the model to perceive the appearance difference between two video clips. We show that optimizing the two tasks jointly consistently improves the performance on two downstream tasks, namely action recognition and video retrieval. Remarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy without the use of labeled data for pre-training, which outperforms the ImageNet supervised pre-trained model. Code and pre-trained models can be found at https://github.com/PeihaoChen/RSPNet.",0
"Our focus is on unsupervised video representation learning, which aims to learn motion and appearance features from unlabeled video for use in downstream tasks like action recognition. However, this task is challenging due to the complex spatial-temporal information in videos and the lack of labeled data for training. Unlike static image representation learning, constructing a self-supervised task that models both motion and appearance features is difficult. Although some attempts have been made to learn video representation through video playback speed prediction, obtaining precise speed labels is not straightforward, and the models may focus on motion pattern rather than appearance features. We propose a new approach that exploits the relative playback speed between two video clips as labels to learn better motion features and enforce an appearance-focused task to learn appearance features. Jointly optimizing the two tasks improves performance on downstream tasks like action recognition and video retrieval. Our method achieves 93.7% accuracy for action recognition on the UCF101 dataset without using labeled data for pre-training, outperforming the ImageNet supervised pre-trained model. Our code and pre-trained models can be found at https://github.com/PeihaoChen/RSPNet.",1
"Developing an agent in reinforcement learning (RL) that is capable of performing complex control tasks directly from high-dimensional observation such as raw pixels is yet a challenge as efforts are made towards improving sample efficiency and generalization. This paper considers a learning framework for Curiosity Contrastive Forward Dynamics Model (CCFDM) in achieving a more sample-efficient RL based directly on raw pixels. CCFDM incorporates a forward dynamics model (FDM) and performs contrastive learning to train its deep convolutional neural network-based image encoder (IE) to extract conducive spatial and temporal information for achieving a more sample efficiency for RL. In addition, during training, CCFDM provides intrinsic rewards, produced based on FDM prediction error, encourages the curiosity of the RL agent to improve exploration. The diverge and less-repetitive observations provide by both our exploration strategy and data augmentation available in contrastive learning improve not only the sample efficiency but also the generalization. Performance of existing model-free RL methods such as Soft Actor-Critic built on top of CCFDM outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.",0
"Efforts to enhance sample efficiency and generalization in reinforcement learning (RL) by developing an RL agent capable of performing complex control tasks directly from high-dimensional observation like raw pixels is still a challenge. This study focuses on a learning framework for Curiosity Contrastive Forward Dynamics Model (CCFDM) that aims to achieve a more sample-efficient RL based on raw pixels. CCFDM involves contrastive learning and a forward dynamics model (FDM) to train a deep convolutional neural network-based image encoder (IE) to extract suitable spatial and temporal information for enhancing sample efficiency in RL. During training, CCFDM provides intrinsic rewards based on FDM prediction error to encourage RL agent curiosity and improve exploration. The combination of our exploration strategy and data augmentation available in contrastive learning results in more diverse and less repetitive observations, improving both sample efficiency and generalization. CCFDM, along with model-free RL methods such as Soft Actor-Critic, outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.",1
"Deep learning methods have made significant progress in ship detection in synthetic aperture radar (SAR) images. The pretraining technique is usually adopted to support deep neural networks-based SAR ship detectors due to the scarce labeled SAR images. However, directly leveraging ImageNet pretraining is hardly to obtain a good ship detector because of different imaging perspective and geometry. In this paper, to resolve the problem of inconsistent imaging perspective between ImageNet and earth observations, we propose an optical ship detector (OSD) pretraining technique, which transfers the characteristics of ships in earth observations to SAR images from a large-scale aerial image dataset. On the other hand, to handle the problem of different imaging geometry between optical and SAR images, we propose an optical-SAR matching (OSM) pretraining technique, which transfers plentiful texture features from optical images to SAR images by common representation learning on the optical-SAR matching task. Finally, observing that the OSD pretraining based SAR ship detector has a better recall on sea area while the OSM pretraining based SAR ship detector can reduce false alarms on land area, we combine the predictions of the two detectors through weighted boxes fusion to further improve detection results. Extensive experiments on four SAR ship detection datasets and two representative CNN-based detection benchmarks are conducted to show the effectiveness and complementarity of the two proposed detectors, and the state-of-the-art performance of the combination of the two detectors. The proposed method won the sixth place of ship detection in SAR images in 2020 Gaofen challenge.",0
"Significant progress has been made in ship detection in synthetic aperture radar (SAR) images through the use of deep learning methods. Due to the limited availability of labeled SAR images, pretraining techniques are commonly adopted to support deep neural networks-based SAR ship detectors. However, leveraging ImageNet pretraining directly is insufficient in obtaining a good ship detector because of differences in imaging perspective and geometry. To address this inconsistency, an optical ship detector (OSD) pretraining technique was proposed in this study, which transfers ship characteristics in earth observations to SAR images from a large-scale aerial image dataset. Additionally, to handle the differences in imaging geometry between optical and SAR images, an optical-SAR matching (OSM) pretraining technique was proposed, which transfers texture features from optical images to SAR images through common representation learning on the optical-SAR matching task. Combining the predictions of the OSD and OSM pretraining-based SAR ship detectors through weighted boxes fusion further improves detection results. Extensive experiments conducted on four SAR ship detection datasets and two CNN-based detection benchmarks demonstrate the effectiveness and complementarity of the two proposed detectors and the state-of-the-art performance of their combination. The proposed method achieved sixth place in ship detection in SAR images in the 2020 Gaofen challenge.",1
"Few-shot image classification consists of two consecutive learning processes: 1) In the meta-learning stage, the model acquires a knowledge base from a set of training classes. 2) During meta-testing, the acquired knowledge is used to recognize unseen classes from very few examples. Inspired by the compositional representation of objects in humans, we train a neural network architecture that explicitly represents objects as a set of parts and their spatial composition. In particular, during meta-learning, we train a knowledge base that consists of a dictionary of part representations and a dictionary of part activation maps that encode common spatial activation patterns of parts. The elements of both dictionaries are shared among the training classes. During meta-testing, the representation of unseen classes is learned using the part representations and the part activation maps from the knowledge base. Finally, an attention mechanism is used to strengthen those parts that are most important for each category. We demonstrate the value of our compositional learning framework for a few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where we achieve state-of-the-art performance.",0
"The process of few-shot image classification involves two stages. Firstly, in the meta-learning stage, the model acquires knowledge from a set of training classes. Secondly, during meta-testing, this knowledge is used to identify new classes with limited examples. Our approach is inspired by the way humans represent objects compositionally. We have developed a neural network architecture that explicitly represents objects as a set of parts and their spatial composition. During meta-learning, we train a knowledge base comprising a dictionary of part representations and activation maps that encode common spatial activation patterns. The dictionaries are shared among training classes. During meta-testing, we use part representations and activation maps from the knowledge base to represent unseen classes. An attention mechanism is employed to strengthen parts that are most important for each category. Our compositional learning framework has achieved state-of-the-art performance in few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100.",1
"Molecules have seemed like a natural fit to deep learning's tendency to handle a complex structure through representation learning, given enough data. However, this often continuous representation is not natural for understanding chemical space as a domain and is particular to samples and their differences. We focus on exploring a natural structure for representing chemical space as a structured domain: embedding drug-like chemical space into an enumerable hypergraph based on scaffold classes linked through an inclusion operator. This paper shows how molecules form classes of scaffolds, how scaffolds relate to each in a hypergraph, and how this structure of scaffolds is natural for drug discovery workflows such as predicting properties and optimizing molecular structures. We compare the assumptions and utility of various embeddings of molecules, such as their respective induced distance metrics, their extendibility to represent chemical space as a structured domain, and the consequences of utilizing the structure for learning tasks.",0
"The complex nature of molecules makes them a natural candidate for deep learning's representation learning approach, which requires abundant data. However, the continuous representation often used in deep learning is not ideal for understanding chemical space as a domain, as it fails to account for differences among samples. Our focus is on developing a natural structure to represent chemical space as a structured domain. Specifically, we propose embedding drug-like chemical space into an enumerable hypergraph based on scaffold classes, which are linked through an inclusion operator. Our paper demonstrates how molecules can be classified into scaffolds and how scaffolds can be related to each other in a hypergraph. This scaffold structure is highly relevant for drug discovery workflows, such as predicting properties and optimizing molecular structures. We compare different embeddings of molecules, including their induced distance metrics and their ability to represent chemical space as a structured domain. Finally, we examine the implications of utilizing this structure for learning tasks.",1
"Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable. We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution. We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance.",0
"The attainment of meaningful representations that can disentangle the fundamental structure of data generation is regarded as highly significant in the field of machine learning. Although disentangled representations have proven to be beneficial for various tasks such as impartial categorization and abstract reasoning, their viability and practicality in real-world scenarios are still uncertain. To address this, we present a fresh high-resolution dataset featuring 1 million simulated images and more than 1,800 labeled real-life images of the identical configuration. Unlike prior works, this new dataset displays interrelationships, a complex fundamental structure, and allows for the assessment of transferability to unseen simulated and real-world situations, where the encoder remains in or out of distribution. We suggest novel architectures to facilitate disentangled representation learning for practical high-resolution contexts and carry out a comprehensive empirical analysis of disentangled representations using this dataset. Our findings indicate that disentanglement is a reliable predictor of out-of-distribution (OOD) task performance.",1
"The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compared with base models. Codes are available at https://github.com/ssui-liu/learnable-embed-sizes-for-RecSys.",0
"Deep learning recommendation models commonly use embedding-based representation learning to convert raw sparse features into dense vectors. However, the traditional method of assigning a uniform size to all features has two problems. Firstly, it results in a large embedding table, which leads to high memory usage. Secondly, it may cause over-fitting for features that do not require a large representation capacity. Previous attempts to solve this issue have resulted in a significant drop in recommendation performance or expensive training costs. In this paper, we introduce a new technique called PEP (Plug-in Embedding Pruning), which reduces the size of the embedding table without impacting recommendation accuracy. PEP prunes embedding parameters based on adaptively learned pruning thresholds, creating a mixed-dimension embedding-scheme for each feature. PEP is a general framework that can be used with various recommendation models, and experiments show that it can efficiently reduce the number of embedding parameters while improving performance. PEP only adds an additional 20-30% computation time compared to base models, and the codes are available at https://github.com/ssui-liu/learnable-embed-sizes-for-RecSys.",1
"We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map (""what is where?"") from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length x width x feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations build by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour(""Find chair"") or answering questions about the space (""How many chairs did you see in the house?""). Project page: https://vincentcartillier.github.io/smnet.html.",0
"Our focus is on studying the process of semantic mapping, where an embodied agent, such as a robot or an AI assistant, is tasked with creating an allocentric top-down semantic map of a new environment based on egocentric observations from an RGB-D camera with a known pose. To achieve this, we introduce SemanticMapNet (SMNet), which comprises of an Egocentric Visual Encoder, a Feature Projector, a Spatial Memory Tensor, and a Map Decoder. By combining projective camera geometry and neural representation learning, SMNet outperforms competitive baselines by a significant margin on the Matterport3D dataset. Additionally, we demonstrate the use of SMNet's neural episodic memories and spatio-semantic allocentric representations for subsequent tasks such as navigating to objects or answering questions about the space. More information can be found on our project page at https://vincentcartillier.github.io/smnet.html.",1
"Real-world data usually have high dimensionality and it is important to mitigate the curse of dimensionality. High-dimensional data are usually in a coherent structure and make the data in relatively small true degrees of freedom. There are global and local dimensionality reduction methods to alleviate the problem. Most of existing methods for local dimensionality reduction obtain an embedding with the eigenvalue or singular value decomposition, where the computational complexities are very high for a large amount of data. Here we propose a novel local nonlinear approach named Vec2vec for general purpose dimensionality reduction, which generalizes recent advancements in embedding representation learning of words to dimensionality reduction of matrices. It obtains the nonlinear embedding using a neural network with only one hidden layer to reduce the computational complexity. To train the neural network, we build the neighborhood similarity graph of a matrix and define the context of data points by exploiting the random walk properties. Experiments demenstrate that Vec2vec is more efficient than several state-of-the-art local dimensionality reduction methods in a large number of high-dimensional data. Extensive experiments of data classification and clustering on eight real datasets show that Vec2vec is better than several classical dimensionality reduction methods in the statistical hypothesis test, and it is competitive with recently developed state-of-the-art UMAP.",0
"The curse of dimensionality is a challenge when dealing with real-world data, which often has a high number of dimensions. This can result in a coherent structure that limits the true degrees of freedom. To alleviate this problem, there are global and local dimensionality reduction methods available. However, existing local methods that use eigenvalue or singular value decomposition can be computationally complex for large amounts of data. To address this, we propose a new approach called Vec2vec, which is a local nonlinear method that uses a neural network with only one hidden layer to reduce computational complexity. We train the network by building a neighborhood similarity graph and defining the context of data points using random walk properties. Our experiments show that Vec2vec is more efficient than other local methods for high-dimensional data and is statistically better than classical dimensionality reduction methods in data classification and clustering on real datasets. It is also competitive with state-of-the-art UMAP.",1
"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.",0
"Recently, self-supervised representation learning using contrastive learning has gained renewed attention and has led to exceptional performance in unsupervised training of deep image models. Compared to traditional contrastive losses such as triplet, max-margin, and N-pairs loss, modern batch contrastive methods have either subsumed or significantly outperformed them. This study aims to expand self-supervised batch contrastive techniques to the fully-supervised environment, where label information can be effectively utilized. By drawing together clusters of points belonging to the same category and simultaneously pushing apart clusters of samples from different categories in the embedding space, we present two versions of supervised contrastive loss (SupCon) and identify the best-performing formulation of the loss. On the ImageNet database, our ResNet-200 model yields a top-1 accuracy of 81.4%, which is 0.8% higher than the best recorded performance for this architecture. Furthermore, our method consistently outperforms cross-entropy on other datasets and two ResNet models, while also providing advantages in terms of natural corruption robustness and hyperparameter stability. Our loss function is straightforward to implement, and TensorFlow code is available at https://t.ly/supcon.",1
"Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.",0
"The distribution of semantically meaningful information in perceptual signals is often uneven, with speech signals containing numerous silences and varying pronunciation speeds. To address this, we introduce SlowAEs for unsupervised learning of high-level variable-rate discrete representations of sequences, particularly in speech. These SlowAEs create event-based representations that automatically adjust to the density of salient information in the input signals, while maintaining accurate signal reconstruction. We also develop RLTs for event-based representation modelling, which we use to construct language models in the speech domain. These models generate coherent and grammatically correct utterances and continuations.",1
"Recent studies on computer vision mainly focus on natural images that express real-world scenes. They achieve outstanding performance on diverse tasks such as visual question answering. Diagram is a special form of visual expression that frequently appears in the education field and is of great significance for learners to understand multimodal knowledge. Current research on diagrams preliminarily focuses on natural disciplines such as Biology and Geography, whose expressions are still similar to natural images. Another type of diagrams such as from Computer Science is composed of graphics containing complex topologies and relations, and research on this type of diagrams is still blank. The main challenges of graphic diagrams understanding are the rarity of data and the confusion of semantics, which are mainly reflected in the diversity of expressions. In this paper, we construct a novel dataset of graphic diagrams named Computer Science Diagrams (CSDia). It contains more than 1,200 diagrams and exhaustive annotations of objects and relations. Considering the visual noises caused by the various expressions in diagrams, we introduce the topology of diagrams to parse topological structure. After that, we propose Diagram Parsing Net (DPN) to represent the diagram from three branches: topology, visual feature, and text, and apply the model to the diagram classification task to evaluate the ability of diagrams understanding. The results show the effectiveness of the proposed DPN on diagrams understanding.",0
"Current research in computer vision primarily focuses on natural images depicting real-world scenes, resulting in exceptional performance on various tasks, including visual question answering. However, diagrams are a unique form of visual expression that plays a crucial role in helping learners understand multimodal knowledge, especially in fields such as Biology and Geography, where the expressions are similar to natural images. On the other hand, Computer Science diagrams are composed of graphics with complex topologies and relationships, making research on this type of diagrams challenging due to the scarcity of data and semantic confusion. To address this gap, we introduce the Computer Science Diagrams (CSDia) dataset, which includes over 1,200 diagrams and detailed annotations of objects and relationships. We also propose the Diagram Parsing Net (DPN), a model that represents diagrams through three branches: topology, visual features, and text, to classify diagrams effectively. Our results demonstrate the efficacy of the DPN in understanding diagrams, particularly in dealing with the diverse expressions and topological structures present in Computer Science diagrams.",1
"In comparison to classical shallow representation learning techniques, deep neural networks have achieved superior performance in nearly every application benchmark. But despite their clear empirical advantages, it is still not well understood what makes them so effective. To approach this question, we introduce deep frame approximation, a unifying framework for representation learning with structured overcomplete frames. While exact inference requires iterative optimization, it may be approximated by the operations of a feed-forward deep neural network. We then indirectly analyze how model capacity relates to the frame structure induced by architectural hyperparameters such as depth, width, and skip connections. We quantify these structural differences with the deep frame potential, a data-independent measure of coherence linked to representation uniqueness and stability. As a criterion for model selection, we show correlation with generalization error on a variety of common deep network architectures such as ResNets and DenseNets. We also demonstrate how recurrent networks implementing iterative optimization algorithms achieve performance comparable to their feed-forward approximations. This connection to the established theory of overcomplete representations suggests promising new directions for principled deep network architecture design with less reliance on ad-hoc engineering.",0
"Deep neural networks have outperformed classical shallow representation learning techniques in almost all application benchmarks. Despite their empirical advantages, it is not well understood why they are so effective. To address this question, a unifying framework called deep frame approximation is introduced for representation learning with structured overcomplete frames. Although exact inference requires iterative optimization, a feed-forward deep neural network can approximate it. The relationship between model capacity and the frame structure induced by architectural hyperparameters, such as depth, width, and skip connections, is indirectly analyzed. The deep frame potential is used to quantify these structural differences, which is a data-independent measure of coherence linked to representation uniqueness and stability. The deep frame potential is also correlated with generalization error on various common deep network architectures such as ResNets and DenseNets, making it a criterion for model selection. Additionally, recurrent networks implementing iterative optimization algorithms have shown comparable performance to their feed-forward approximations. This connection to the established theory of overcomplete representations suggests promising new directions for principled deep network architecture design with less reliance on ad-hoc engineering.",1
"Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://github.com/hrlblab/SimTriple.",0
"Modern self-supervised learning relies heavily on contrastive learning, which has been hindered by the need for significant computational resources, such as at least 8 GPUs or 32 TPU cores, to accommodate for large-scale negative samples or momentum. However, recent advancements such as the SimSiam approach have addressed these limitations through stop-gradient without momentum encoders. In the field of medical image analysis, multiple instances can be obtained from the same patient or tissue, and the proposed SimTriplet approach takes advantage of this by maximizing both intra-sample and inter-sample similarities via triplets from positive pairs. This method does not require negative samples and employs recent mix precision training to advance training with only a single GPU with 16GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning and 2.13% better performance compared with SimSiam. The proposed SimTriplet method can achieve decent performance using only 1% labeled data and the code and data are available at https://github.com/hrlblab/SimTriple.",1
"Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of contrastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The first task directly applies contrastive learning at the pixel level. We additionally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are complementary to instance-level contrastive methods. These results demonstrate the strong potential of defining pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is available at \url{https://github.com/zdaxie/PixPro}.",0
"Unsupervised visual representation learning using contrastive learning methods has achieved impressive transfer performance. However, current methods are limited to instance-level pretext tasks, resulting in sub-optimal representations for dense pixel prediction downstream tasks. To address this, our paper proposes pixel-level pretext tasks for learning dense feature representations. The first task applies contrastive learning directly at the pixel level, while the second task, pixel-to-propagation consistency, produces even better results, surpassing state-of-the-art approaches by a significant margin. These tasks achieve improved performance when transferred to Pascal VOC object detection, COCO object detection, and Cityscapes semantic segmentation. Furthermore, the pixel-level pretext tasks are effective for pre-training both regular and head networks, and complement instance-level contrastive methods. Our results demonstrate the potential of pixel-level pretext tasks and suggest a new approach to unsupervised visual representation learning. The code is available at \url{https://github.com/zdaxie/PixPro}.",1
"Visual Attention Prediction (VAP) is a significant and imperative issue in the field of computer vision. Most of existing VAP methods are based on deep learning. However, they do not fully take advantage of the low-level contrast features while generating the visual attention map. In this paper, a novel VAP method is proposed to generate visual attention map via bio-inspired representation learning. The bio-inspired representation learning combines both low-level contrast and high-level semantic features simultaneously, which are developed by the fact that human eye is sensitive to the patches with high contrast and objects with high semantics. The proposed method is composed of three main steps: 1) feature extraction, 2) bio-inspired representation learning and 3) visual attention map generation. Firstly, the high-level semantic feature is extracted from the refined VGG16, while the low-level contrast feature is extracted by the proposed contrast feature extraction block in a deep network. Secondly, during bio-inspired representation learning, both the extracted low-level contrast and high-level semantic features are combined by the designed densely connected block, which is proposed to concatenate various features scale by scale. Finally, the weighted-fusion layer is exploited to generate the ultimate visual attention map based on the obtained representations after bio-inspired representation learning. Extensive experiments are performed to demonstrate the effectiveness of the proposed method.",0
"Computer vision researchers have identified Visual Attention Prediction (VAP) as a crucial issue, with most existing VAP methods relying on deep learning. However, these methods do not fully utilize low-level contrast features when generating visual attention maps. To address this limitation, this paper introduces a novel VAP method that leverages bio-inspired representation learning to generate attention maps. This approach combines low-level contrast and high-level semantic features, taking into account the human eye's sensitivity to patches with high contrast and objects with high semantics. The proposed method involves three main steps: feature extraction, bio-inspired representation learning, and visual attention map generation. Specifically, the method extracts high-level semantic features from a refined VGG16 model and low-level contrast features using a new contrast feature extraction block in a deep network. The bio-inspired representation learning step combines these features using a densely connected block that concatenates features at various scales. Finally, a weighted-fusion layer generates the visual attention map based on the obtained representations. The proposed method is evaluated through extensive experiments, which demonstrate its effectiveness.",1
"Nearest Neighbor Search (NNS) is a central task in knowledge representation, learning, and reasoning. There is vast literature on efficient algorithms for constructing data structures and performing exact and approximate NNS. This paper studies NNS under Uncertainty (NNSU). Specifically, consider the setting in which an NNS algorithm has access only to a stochastic distance oracle that provides a noisy, unbiased estimate of the distance between any pair of points, rather than the exact distance. This models many situations of practical importance, including NNS based on human similarity judgements, physical measurements, or fast, randomized approximations to exact distances. A naive approach to NNSU could employ any standard NNS algorithm and repeatedly query and average results from the stochastic oracle (to reduce noise) whenever it needs a pairwise distance. The problem is that a sufficient number of repeated queries is unknown in advance; e.g., a point maybe distant from all but one other point (crude distance estimates suffice) or it may be close to a large number of other points (accurate estimates are necessary). This paper shows how ideas from cover trees and multi-armed bandits can be leveraged to develop an NNSU algorithm that has optimal dependence on the dataset size and the (unknown)geometry of the dataset.",0
"The task of Nearest Neighbor Search (NNS) plays a crucial role in knowledge representation, learning, and reasoning. There is an extensive body of literature on constructing efficient algorithms for data structures and performing exact and approximate NNS. This paper investigates NNS under Uncertainty (NNSU). It focuses on a scenario where an NNS algorithm can only access a stochastic distance oracle that offers a noisy, unbiased estimate of the distance between any pair of points instead of the exact distance. This situation models many practical scenarios, including NNS based on human similarity judgments, physical measurements, or fast, randomized approximations to exact distances. A straightforward approach to NNSU would require any standard NNS algorithm to query and average results from the stochastic oracle repeatedly whenever it needs a pairwise distance to reduce noise. However, it is unknown in advance how many repeated queries are sufficient. For instance, a point may be distant from all but one other point (crude distance estimates suffice), or it may be close to a large number of other points (accurate estimates are necessary). This paper demonstrates how ideas from cover trees and multi-armed bandits can be employed to develop an NNSU algorithm that has an optimal dependence on the dataset size and the (unknown) geometry of the dataset.",1
"Unsupervised representation learning achieves promising performances in pre-training representations for object detectors. However, previous approaches are mainly designed for image-level classification, leading to suboptimal detection performance. To bridge the performance gap, this work proposes a simple yet effective representation learning method for object detection, named patch re-identification (Re-ID), which can be treated as a contrastive pretext task to learn location-discriminative representation unsupervisedly, possessing appealing advantages compared to its counterparts. Firstly, unlike fully-supervised person Re-ID that matches a human identity in different camera views, patch Re-ID treats an important patch as a pseudo identity and contrastively learns its correspondence in two different image views, where the pseudo identity has different translations and transformations, enabling to learn discriminative features for object detection. Secondly, patch Re-ID is performed in Deeply Unsupervised manner to learn multi-level representations, appealing to object detection. Thirdly, extensive experiments show that our method significantly outperforms its counterparts on COCO in all settings, such as different training iterations and data percentages. For example, Mask R-CNN initialized with our representation surpasses MoCo v2 and even its fully-supervised counterparts in all setups of training iterations (e.g. 2.1 and 1.1 mAP improvement compared to MoCo v2 in 12k and 90k iterations respectively). Code will be released at https://github.com/dingjiansw101/DUPR.",0
"Although unsupervised representation learning has shown promising results in pre-training object detectors, the previous approaches were primarily created for image-level classification, resulting in suboptimal detection performance. To address this performance gap, this study suggests a simple yet effective representation learning method called patch re-identification (Re-ID) for object detection. This method can be viewed as a contrastive pretext task to learn location-discriminative representation unsupervisedly, which offers several benefits in comparison to its counterparts. Firstly, patch Re-ID considers an essential patch as a pseudo identity and learns its correspondence in two different image views contrastively, where the pseudo identity has varied translations and transformations. This enables the method to develop discriminative features for object detection. Secondly, patch Re-ID is executed in a Deeply Unsupervised manner to learn multi-level representations. This makes it ideal for object detection. Finally, numerous experiments have demonstrated that our approach significantly outperforms its counterparts on COCO in all settings, such as different training iterations and data percentages. For instance, Mask R-CNN initialized with our representation outperforms MoCo v2 and even its fully-supervised counterparts in all setups of training iterations. (For instance, 2.1 and 1.1 mAP improvement compared to MoCo v2 in 12k and 90k iterations, respectively). Code will be available at https://github.com/dingjiansw101/DUPR.",1
"We propose PermaKey, a novel approach to representation learning based on object keypoints. It leverages the predictability of local image regions from spatial neighborhoods to identify salient regions that correspond to object parts, which are then converted to keypoints. Unlike prior approaches, it utilizes predictability to discover object keypoints, an intrinsic property of objects. This ensures that it does not overly bias keypoints to focus on characteristics that are not unique to objects, such as movement, shape, colour etc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints corresponding to the most salient object parts and is robust to certain visual distractors. Further, on downstream RL tasks in the Atari domain we demonstrate how agents equipped with our keypoints outperform those using competing alternatives, even on challenging environments with moving backgrounds or distractor objects.",0
"Our proposed approach, PermaKey, utilizes object keypoints for representation learning. It identifies salient regions corresponding to object parts by leveraging the predictability of local image regions from spatial neighborhoods, which are then converted to keypoints. Unlike previous methods, PermaKey utilizes predictability to discover intrinsic object keypoints, ensuring that they are not biased towards characteristics that are not unique to objects, such as movement, shape, or color. We prove the effectiveness of our approach on Atari, where it learns keypoints representing the most salient object parts and remains robust even in the presence of visual distractors. Moreover, we demonstrate how agents equipped with our keypoints outperform those using competing alternatives in downstream RL tasks, even in challenging environments with moving backgrounds or distractor objects.",1
"With the high requirements of automation in the era of Industry 4.0, anomaly detection plays an increasingly important role in higher safety and reliability in the production and manufacturing industry. Recently, autoencoders have been widely used as a backend algorithm for anomaly detection. Different techniques have been developed to improve the anomaly detection performance of autoencoders. Nonetheless, little attention has been paid to the latent representations learned by autoencoders. In this paper, we propose a novel selection-and-weighting-based anomaly detection framework called SWAD. In particular, the learned latent representations are individually selected and weighted. Experiments on both benchmark and real-world datasets have shown the effectiveness and superiority of SWAD. On the benchmark datasets, the SWAD framework has reached comparable or even better performance than the state-of-the-art approaches.",0
"In the age of Industry 4.0, automation requirements have increased significantly, making anomaly detection crucial for ensuring higher safety and reliability in manufacturing. Autoencoders have become a popular backend algorithm for detecting anomalies, and several techniques have been developed to improve their performance. However, little attention has been paid to the latent representations learned by autoencoders. This paper proposes a novel anomaly detection framework called SWAD that utilizes a selection-and-weighting-based approach to individually select and weight learned latent representations. The effectiveness and superiority of SWAD have been demonstrated through experiments on both benchmark and real-world datasets. The results have shown that SWAD performs comparably or even better than state-of-the-art approaches on benchmark datasets.",1
"Graph Neural Networks (GNNs) have achieved tremendous success in various real-world applications due to their strong ability in graph representation learning. GNNs explore the graph structure and node features by aggregating and transforming information within node neighborhoods. However, through theoretical and empirical analysis, we reveal that the aggregation process of GNNs tends to destroy node similarity in the original feature space. There are many scenarios where node similarity plays a crucial role. Thus, it has motivated the proposed framework SimP-GCN that can effectively and efficiently preserve node similarity while exploiting graph structure. Specifically, to balance information from graph structure and node features, we propose a feature similarity preserving aggregation which adaptively integrates graph structure and node features. Furthermore, we employ self-supervised learning to explicitly capture the complex feature similarity and dissimilarity relations between nodes. We validate the effectiveness of SimP-GCN on seven benchmark datasets including three assortative and four disassorative graphs. The results demonstrate that SimP-GCN outperforms representative baselines. Further probe shows various advantages of the proposed framework. The implementation of SimP-GCN is available at \url{https://github.com/ChandlerBang/SimP-GCN}.",0
"Due to their exceptional ability in graph representation learning, Graph Neural Networks (GNNs) have achieved remarkable success in a range of real-world applications. GNNs explore the graph structure and node features by aggregating and transforming information within node neighborhoods. However, our theoretical and empirical analysis has revealed that the aggregation process of GNNs has a tendency to disrupt node similarity in the original feature space. This is problematic as node similarity plays a crucial role in many scenarios. Consequently, we have developed the SimP-GCN framework, which effectively and efficiently preserves node similarity while exploiting graph structure. To balance information from graph structure and node features, we propose a feature similarity preserving aggregation that adaptively integrates graph structure and node features. Additionally, we use self-supervised learning to explicitly capture the complex feature similarity and dissimilarity relations between nodes. We have validated the effectiveness of SimP-GCN on seven benchmark datasets, including three assortative and four disassorative graphs, and the results show that SimP-GCN outperforms representative baselines. Furthermore, the proposed framework has various advantages, and its implementation is available at \url{https://github.com/ChandlerBang/SimP-GCN}.",1
"To address the challenging task of instance-aware human part parsing, a new bottom-up regime is proposed to learn category-level human semantic segmentation as well as multi-person pose estimation in a joint and end-to-end manner. It is a compact, efficient and powerful framework that exploits structural information over different human granularities and eases the difficulty of person partitioning. Specifically, a dense-to-sparse projection field, which allows explicitly associating dense human semantics with sparse keypoints, is learnt and progressively improved over the network feature pyramid for robustness. Then, the difficult pixel grouping problem is cast as an easier, multi-person joint assembling task. By formulating joint association as maximum-weight bipartite matching, a differentiable solution is developed to exploit projected gradient descent and Dykstra's cyclic projection algorithm. This makes our method end-to-end trainable and allows back-propagating the grouping error to directly supervise multi-granularity human representation learning. This is distinguished from current bottom-up human parsers or pose estimators which require sophisticated post-processing or heuristic greedy algorithms. Experiments on three instance-aware human parsing datasets show that our model outperforms other bottom-up alternatives with much more efficient inference.",0
"A new approach has been proposed to tackle the complex task of instance-aware human part parsing. This approach utilizes a bottom-up regime that enables category-level human semantic segmentation and multi-person pose estimation to be learned in a joint and end-to-end manner. The framework is both compact and efficient, leveraging structural information across various human granularities to alleviate person partitioning challenges. The model uses a dense-to-sparse projection field that enables the association of dense human semantics with sparse keypoints, progressively improving over the network feature pyramid for enhanced robustness. Pixel grouping is simplified by casting the problem as a multi-person joint assembling task, with joint association being formulated as maximum-weight bipartite matching. This allows for differentiable solutions using projected gradient descent and Dykstra's cyclic projection algorithm, making the method end-to-end trainable. Our model outperforms other bottom-up alternatives in terms of efficiency and accuracy, without requiring complex post-processing or heuristic greedy algorithms. Experiments on three instance-aware human parsing datasets demonstrate the effectiveness of this approach.",1
"Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ""muscly-supervised"" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.",0
"Computer vision has long sought after learning effective representations of visual data that can be applied to various tasks. Commonly, representation learning approaches rely solely on visual data like images or videos. However, this paper delves into a new method by incorporating human interaction and attention cues to determine if better representations can be achieved in comparison to visual-only representations. To conduct this study, a dataset was collected that recorded human interactions including body movements and gaze in their daily lives. The results of the study show that our ""muscly-supervised"" representation, which encodes interaction and attention cues, is more effective than the visual-only state-of-the-art method MoCo (He et al.,2020) in various target tasks such as scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics), and walkable surface estimation (affordance). Our code and dataset can be obtained at: https://github.com/ehsanik/muscleTorch.",1
"Patient no-shows is a major burden for health centers leading to loss of revenue, increased waiting time and deteriorated health outcome. Developing machine learning (ML) models for the prediction of no -shows could help addressing this important issue. It is crucial to consider fair ML models for no-show prediction in order to ensure equality of opportunity in accessing healthcare services. In this wo rk, we are interested in developing deep learning models for no-show prediction based on tabular data while ensuring fairness properties. Our baseline model, TabNet, uses on attentive feature transforme rs and has shown promising results for tabular data. We propose Fair-TabNet based on representation learning that disentangles predictive from sensitive components. The model is trained to jointly min imize loss functions on no-shows and sensitive variables while ensuring that the sensitive and prediction representations are orthogonal. In the experimental analysis, we used a hospital dataset of 210, 000 appointments collected in 2019. Our preliminary results show that the proposed Fair-TabNet improves the predictive, fairness performance and convergence speed over TabNet for the task of appointment no-show prediction. The comparison with the state-of-the art models for tabular data shows promising results and could be further improved by a better tuning of hyper-parameters.",0
"Health centers face significant challenges due to patient no-shows, such as decreased revenue, longer wait times, and poorer health outcomes. One solution to this problem is to develop machine learning models that can predict no-shows. However, it's important to ensure that such models are fair, in order to promote equal access to healthcare services. In this study, we focus on developing deep learning models for no-show prediction using tabular data, while maintaining fairness. Our model, Fair-TabNet, is based on representation learning that separates predictive and sensitive components. By minimizing loss functions on both no-shows and sensitive variables, and ensuring that the representations are orthogonal, we achieve better performance, fairness, and convergence speed compared to TabNet. We test our model on a hospital dataset of 210,000 appointments from 2019, and show promising results. With further tuning of hyper-parameters, our model could provide even better performance than current state-of-the-art models for tabular data.",1
"An increasing number of machine learning tasks deal with learning representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention). In this paper, we propose a geometrically-interpretable framework for learning representations from set-structured data, which is rooted in the optimal mass transportation problem. In particular, we treat elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn from set-structured data effectively. We evaluate our proposed framework on multiple supervised and unsupervised set learning tasks and demonstrate its superiority over state-of-the-art set representation learning approaches.",0
"There is a growing trend towards using machine learning to learn representations from data that is set-structured. To solve these problems, a combination of permutation-equivariant modules (such as self-attention or feed-forward neural networks) and permutation-invariant modules (e.g. global average pooling or multi-head attention pooling) is required. In this article, we introduce a framework for learning representations from set-structured data that is based on the optimal mass transportation problem and has a geometric interpretation. Our approach involves treating set elements as samples from a probability measure and using an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn effectively. We evaluate our framework on a range of supervised and unsupervised set learning tasks and demonstrate its superiority over other set representation learning approaches.",1
"The technology of dynamic map fusion among networked vehicles has been developed to enlarge sensing ranges and improve sensing accuracies for individual vehicles. This paper proposes a federated learning (FL) based dynamic map fusion framework to achieve high map quality despite unknown numbers of objects in fields of view (FoVs), various sensing and model uncertainties, and missing data labels for online learning. The novelty of this work is threefold: (1) developing a three-stage fusion scheme to predict the number of objects effectively and to fuse multiple local maps with fidelity scores; (2) developing an FL algorithm which fine-tunes feature models (i.e., representation learning networks for feature extraction) distributively by aggregating model parameters; (3) developing a knowledge distillation method to generate FL training labels when data labels are unavailable. The proposed framework is implemented in the Car Learning to Act (CARLA) simulation platform. Extensive experimental results are provided to verify the superior performance and robustness of the developed map fusion and FL schemes.",0
"To enhance the sensing ranges and accuracies of individual vehicles, dynamic map fusion technology has been created for networked vehicles. This study introduces a federated learning (FL) based dynamic map fusion framework that aims to achieve high-quality maps despite various uncertainties, such as unknown numbers of objects in fields of view (FoVs), model uncertainties, and missing data labels for online learning. The study proposes a novel three-stage fusion scheme for predicting the number of objects effectively and fusing multiple local maps with fidelity scores. Additionally, an FL algorithm is developed that fine-tunes feature models distributively by aggregating model parameters. Finally, a knowledge distillation method is introduced to generate FL training labels when data labels are unavailable. This framework is implemented in the Car Learning to Act (CARLA) simulation platform, and extensive experimental results demonstrate the superior performance and robustness of the developed map fusion and FL schemes.",1
"Convolutional neural networks have enabled major progress in addressing pixel-level prediction tasks such as semantic segmentation, depth estimation, surface normal prediction, and so on, benefiting from their powerful capabilities in visual representation learning. Typically, state-of-the-art models integrates attention mechanisms for improved deep feature representations. Recently, some works have demonstrated the significance of learning and combining both spatial- and channel-wise attentions for deep feature refinement. In this paper, we aim at effectively boosting previous approaches and propose a unified deep framework to jointly learn both spatial attention maps and channel attention vectors in a principled manner so as to structure the resulting attention tensors and model interactions between these two types of attentions. Specifically, we integrate the estimation and the interaction of the attentions within a probabilistic representation learning framework, leading to Variational STructured Attention networks (VISTA-Net). We implement the inference rules within the neural network, thus allowing for end-to-end learning of the probabilistic and the CNN front-end parameters. As demonstrated by our extensive empirical evaluation on six large-scale datasets for dense visual prediction, VISTA-Net outperforms the state-of-the-art in multiple continuous and discrete prediction tasks, thus confirming the benefit of the proposed approach in joint structured spatial-channel attention estimation for deep representation learning. The code is available at https://github.com/ygjwd12345/VISTA-Net.",0
"The use of convolutional neural networks has greatly advanced pixel-level prediction tasks such as depth estimation, semantic segmentation, and surface normal prediction due to their exceptional capacity for visual representation learning. Modern models often integrate attention mechanisms to enhance deep feature representations. Recently, it has been demonstrated that combining spatial and channel-wise attentions can further refine deep features. In this study, we propose a unified deep framework that jointly learns spatial attention maps and channel attention vectors to structure resulting attention tensors and model interactions between these two types of attentions. We integrate the estimation and interaction of attentions within a probabilistic representation learning framework, leading to Variational STructured Attention networks (VISTA-Net). We implemented the inference rules within the neural network, enabling end-to-end learning of the probabilistic and CNN front-end parameters. Our empirical evaluation on six large-scale datasets for dense visual prediction demonstrates that VISTA-Net outperforms the state-of-the-art in various continuous and discrete prediction tasks, confirming the efficacy of our approach in joint structured spatial-channel attention estimation for deep representation learning. The code for VISTA-Net is available at https://github.com/ygjwd12345/VISTA-Net.",1
"Learning reliable motion representation between consecutive frames, such as optical flow, has proven to have great promotion to video understanding. However, the TV-L1 method, an effective optical flow solver, is time-consuming and expensive in storage for caching the extracted optical flow. To fill the gap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced with an embedded lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues from adjacent frames in a coarse-to-fine manner and focuses on small displacement for each level by extracting pyramid of feature and warping one to the other according to the estimated flow of the last level. Due to the lack of labeled motion for action datasets, we constrain the flow prediction with multi-scale photometric consistency and edge-aware smoothness. Compared with state-of-the-art unsupervised motion representation learning methods, our model achieves better accuracy while maintaining efficiency, which is competitive with some supervised or more complicated approaches.",0
"The use of optical flow as a reliable motion representation between frames has been shown to greatly enhance video understanding. However, the TV-L1 method, which is an effective optical flow solver, is both time-consuming and requires a lot of storage to cache the extracted optical flow. To address this issue, we propose UF-TSN, a new end-to-end approach for action recognition that incorporates a lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues from adjacent frames in a coarse-to-fine manner, focusing on small displacement for each level by extracting a pyramid of features and warping them according to the estimated flow of the previous level. Since action datasets often lack labeled motion information, we constrain flow prediction using multi-scale photometric consistency and edge-aware smoothness. Our model achieves high accuracy while maintaining efficiency, which is competitive with some supervised or more complex methods, when compared to current unsupervised motion representation learning techniques.",1
"Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Unsupervised person ReID attracts a lot of attention recently, due to it works without intensive manual annotation and thus shows great potential of adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively sampled negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic dictionaries, among which the global and local memory banks are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state-of-the-arts.",0
"The objective of Person re-identification (ReID) is to locate the same person in different camera images. The unsupervised method of Person ReID has recently gained a lot of attention as it does not require manual annotation and has the potential to adapt to new conditions. Representation learning is crucial in unsupervised Person ReID. In this study, we suggest an innovative selective contrastive learning framework for unsupervised feature learning. Unlike traditional contrastive learning strategies, we use multiple positives and adaptively sampled negatives to define the contrastive loss, allowing for a stronger identity discriminative representation. Additionally, we combine global and local features to construct three dynamic dictionaries, where the global and local memory banks compute pairwise similarity, and the mixture memory bank defines the contrastive loss. Our experimental results demonstrate that our method outperforms the state-of-the-art methods in unsupervised Person ReID.",1
"Whilst contrastive learning has achieved remarkable success in self-supervised representation learning, its potential for deep clustering remains unknown. This is due to its fundamental limitation that the instance discrimination strategy it takes is not class sensitive and hence unable to reason about the underlying decision boundaries between semantic concepts or classes. In this work, we solve this problem by introducing a novel variant called Semantic Contrastive Learning (SCL). It explores the characteristics of both conventional contrastive learning and deep clustering by imposing distance-based cluster structures on unlabelled training data and also introducing a discriminative contrastive loss formulation. For explicitly modelling class boundaries on-the-fly, we further formulate a clustering consistency condition on the two different predictions given by visual similarities and semantic decision boundaries. By advancing implicit representation learning towards explicit understandings of visual semantics, SCL can amplify jointly the strengths of contrastive learning and deep clustering in a unified approach. Extensive experiments show that the proposed model outperforms the state-of-the-art deep clustering methods on six challenging object recognition benchmarks, especially on finer-grained and larger datasets.",0
"Although contrastive learning has been successful in self-supervised representation learning, it has not been fully explored for deep clustering due to its limitation in being unable to reason about decision boundaries between semantic concepts or classes. To address this, we introduce Semantic Contrastive Learning (SCL), which combines distance-based cluster structures and a discriminative contrastive loss formulation. Additionally, we formulate a clustering consistency condition to explicitly model class boundaries. SCL enhances implicit representation learning by incorporating visual semantics, resulting in superior performance compared to state-of-the-art deep clustering methods on six object recognition benchmarks, particularly on larger and finer-grained datasets.",1
"The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information (across image and text modalities). In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset (https://github.com/google-research-datasets/wit) to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example.",0
"The advancements made possible by deep representation learning and pre-training techniques have resulted in significant performance improvements in various NLP, IR, and Vision tasks. Multimodal modeling methods aim to utilize large visio-linguistic datasets to acquire complementary information from image and text modalities. To aid in multimodal, multilingual learning, we present the Wikipedia-based Image Text (WIT) Dataset (https://github.com/google-research-datasets/wit), which contains 37.6 million image-text examples featuring 11.5 million distinct images and covering 108 Wikipedia languages. WIT's size enables it to be utilized as a pretraining dataset for multimodal models, as demonstrated by its application to image-text retrieval tasks. WIT has several key advantages: it is the largest multimodal dataset by 3x, it is massively multilingual, it covers a more diverse array of concepts and entities than previous datasets, and it presents a challenging real-world test set, as evidenced by our empirical study of an image-text retrieval task.",1
"Balanced representation learning methods have been applied successfully to counterfactual inference from observational data. However, approaches that account for survival outcomes are relatively limited. Survival data are frequently encountered across diverse medical applications, i.e., drug development, risk profiling, and clinical trials, and such data are also relevant in fields like manufacturing (e.g., for equipment monitoring). When the outcome of interest is a time-to-event, special precautions for handling censored events need to be taken, as ignoring censored outcomes may lead to biased estimates. We propose a theoretically grounded unified framework for counterfactual inference applicable to survival outcomes. Further, we formulate a nonparametric hazard ratio metric for evaluating average and individualized treatment effects. Experimental results on real-world and semi-synthetic datasets, the latter of which we introduce, demonstrate that the proposed approach significantly outperforms competitive alternatives in both survival-outcome prediction and treatment-effect estimation.",0
"Although balanced representation learning methods have been successful in countering factual inference from observational data, their ability to account for survival outcomes is limited. Survival data are prevalent in various medical applications and fields like manufacturing. When the outcome of interest is time-to-event, ignoring censored outcomes can lead to biased estimates, making it crucial to handle them carefully. To address this, we propose a theoretically grounded unified framework for counterfactual inference applicable to survival outcomes, along with a nonparametric hazard ratio metric to evaluate average and individualized treatment effects. Our experimental results on real-world and semi-synthetic datasets, which we introduce, demonstrate that our approach significantly outperforms competitive alternatives in both survival-outcome prediction and treatment-effect estimation.",1
"Most multilayer least squares (LS)-based neural networks are structured with two separate stages: unsupervised feature encoding and supervised pattern classification. Once the unsupervised learning is finished, the latent encoding would be fixed without supervised fine-tuning. However, in complex tasks such as handling the ImageNet dataset, there are often many more clues that can be directly encoded, while the unsupervised learning, by definition cannot know exactly what is useful for a certain task. This serves as the motivation to retrain the latent space representations to learn some clues that unsupervised learning has not yet learned. In particular, the error matrix from the output layer is pulled back to each hidden layer, and the parameters of the hidden layer are recalculated with Moore-Penrose (MP) inverse for more generalized representations. In this paper, a recomputation-based multilayer network using MP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the performance of RML-MP is then proposed. The experimental results with varying training samples (from 3 K to 1.8 M) show that the proposed models provide better generalization performance than most representation learning algorithms.",0
"Typically, multilayer least squares (LS)-based neural networks consist of two stages: unsupervised feature encoding and supervised pattern classification. The latent encoding obtained from unsupervised learning is fixed and not fine-tuned in most cases. However, for complex tasks, such as handling the ImageNet dataset, there may be additional information that can be directly encoded, but unsupervised learning cannot determine its usefulness for a specific task. This has led to the need to retrain the latent space representations in order to learn additional clues. To accomplish this, the error matrix from the output layer is propagated back to each hidden layer, and the parameters are recalculated using the Moore-Penrose (MP) inverse for more generalized representations. This paper presents the development of a recomputation-based multilayer network using MP inverse (RML-MP), along with a sparse RML-MP (SRML-MP) model to improve its performance. Experimental results, with varying numbers of training samples, demonstrate that these models offer superior generalization performance compared to most representation learning algorithms.",1
"Understanding relationships between feature variables is one important way humans use to make decisions. However, state-of-the-art deep learning studies either focus on task-agnostic statistical dependency learning or do not model explicit feature dependencies during prediction. We propose a deep neural network framework, dGAP, to learn neural dependency Graph and optimize structure-Aware target Prediction simultaneously. dGAP trains towards a structure self-supervision loss and a target prediction loss jointly. Our method leads to an interpretable model that can disentangle sparse feature relationships, informing the user how relevant dependencies impact the target task. We empirically evaluate dGAP on multiple simulated and real datasets. dGAP is not only more accurate, but can also recover correct dependency structure.",0
"Humans typically rely on understanding the relationships between feature variables to make decisions. However, current deep learning studies tend to either focus on statistical dependency learning that is not task-specific, or do not account for explicit feature dependencies during prediction. To address this issue, we propose a deep neural network framework called dGAP, which can simultaneously learn a neural dependency graph and optimize structure-aware target prediction. By training towards a joint structure self-supervision loss and target prediction loss, our method produces an interpretable model that can identify sparse feature relationships and explain how relevant dependencies impact the target task. Empirical evaluations on multiple simulated and real datasets demonstrate that dGAP is not only more accurate, but can also recover the correct dependency structure.",1
"In this work, we present FFB6D, a Full Flow Bidirectional fusion network designed for 6D pose estimation from a single RGBD image. Our key insight is that appearance information in the RGB image and geometry information from the depth image are two complementary data sources, and it still remains unknown how to fully leverage them. Towards this end, we propose FFB6D, which learns to combine appearance and geometry information for representation learning as well as output representation selection. Specifically, at the representation learning stage, we build bidirectional fusion modules in the full flow of the two networks, where fusion is applied to each encoding and decoding layer. In this way, the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover, at the output representation stage, we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects, which simplifies keypoint localization for precise pose estimation. Experimental results show that our method outperforms the state-of-the-art by large margins on several benchmarks. Code and video are available at \url{https://github.com/ethnhe/FFB6D.git}.",0
"FFB6D is a fusion network that has been developed for 6D pose estimation from a single RGBD image. The network is designed to combine appearance information from the RGB image with geometry information from the depth image. Our research has shown that these two data sources complement each other, but it is not yet clear how best to utilize them. FFB6D has been designed to learn how to combine these two sources of information for representation learning and output representation selection. By building bidirectional fusion modules in the full flow of the two networks, FFB6D can leverage local and global complementary information from both sources to obtain better representations. Additionally, we have designed a 3D keypoints selection algorithm that considers the texture and geometry information of objects, simplifying keypoint localization for precise pose estimation. Our experimental results demonstrate that FFB6D outperforms the state-of-the-art by significant margins on several benchmarks. Code and video are available at \url{https://github.com/ethnhe/FFB6D.git}.",1
"Semi-supervised learning has been an effective paradigm for leveraging unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a new semi-supervised learning method that unifies dominant approaches and addresses their limitations. CoMatch jointly learns two representations of the training data, their class probabilities and low-dimensional embeddings. The two representations interact with each other to jointly evolve. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels, whereas the pseudo-labels regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves state-of-the-art performance on multiple datasets. It achieves substantial accuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1% labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch by 12.6%. Furthermore, CoMatch achieves better representation learning performance on downstream tasks, outperforming both supervised learning and self-supervised learning. Code and pre-trained models are available at https://github.com/salesforce/CoMatch.",0
"To reduce the dependence on labeled data, semi-supervised learning has been an effective approach that utilizes unlabeled data. We propose CoMatch, a novel approach to semi-supervised learning that overcomes the limitations of dominant methods by jointly learning two representations of training data - their class probabilities and low-dimensional embeddings. The two representations are intertwined to evolve together, with the embeddings imposing a smoothness constraint on the class probabilities to enhance the pseudo-labels. The pseudo-labels, in turn, regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves exceptional results on multiple datasets, including significant accuracy improvements on the label-sparse CIFAR-10 and STL-10. In ImageNet with only 1% labels, CoMatch outperforms FixMatch by 12.6%, achieving a top-1 accuracy of 66.0%. Furthermore, CoMatch surpasses both supervised and self-supervised learning in representation learning performance on downstream tasks. Code and pre-trained models are available at https://github.com/salesforce/CoMatch.",1
"Sequential matching using hand-crafted heuristics has been standard practice in route-based place recognition for enhancing pairwise similarity results for nearly a decade. However, precision-recall performance of these algorithms dramatically degrades when searching on short temporal window (TW) lengths, while demanding high compute and storage costs on large robotic datasets for autonomous navigation research. Here, influenced by biological systems that robustly navigate spacetime scales even without vision, we develop a joint visual and positional representation learning technique, via a sequential process, and design a learning-based CNN+LSTM architecture, trainable via backpropagation through time, for viewpoint- and appearance-invariant place recognition. Our approach, Sequential Place Learning (SPL), is based on a CNN function that visually encodes an environment from a single traversal, thus reducing storage capacity, while an LSTM temporally fuses each visual embedding with corresponding positional data -- obtained from any source of motion estimation -- for direct sequential inference. Contrary to classical two-stage pipelines, e.g., match-then-temporally-filter, our network directly eliminates false-positive rates while jointly learning sequence matching from a single monocular image sequence, even using short TWs. Hence, we demonstrate that our model outperforms 15 classical methods while setting new state-of-the-art performance standards on 4 challenging benchmark datasets, where one of them can be considered solved with recall rates of 100% at 100% precision, correctly matching all places under extreme sunlight-darkness changes. In addition, we show that SPL can be up to 70x faster to deploy than classical methods on a 729 km route comprising 35,768 consecutive frames. Extensive experiments demonstrate the... Baseline code available at https://github.com/mchancan/deepseqslam",0
"For almost a decade, route-based place recognition has relied on sequential matching using hand-crafted heuristics to enhance pairwise similarity results. However, these algorithms have limitations when searching on short temporal window lengths and require high compute and storage costs on large robotic datasets for autonomous navigation research. Inspired by biological systems that navigate spacetime scales without vision, we have developed Sequential Place Learning (SPL), a joint visual and positional representation learning technique using a CNN+LSTM architecture. SPL reduces storage capacity by visually encoding an environment from a single traversal and fusing each visual embedding with corresponding positional data for direct sequential inference. Our network eliminates false-positive rates and jointly learns sequence matching from a single monocular image sequence, even using short temporal windows. SPL outperforms 15 classical methods and sets new state-of-the-art performance standards on 4 challenging benchmark datasets. It can also be up to 70x faster to deploy than classical methods on a 729 km route comprising 35,768 consecutive frames. Extensive experiments demonstrate the efficacy of SPL. The baseline code is available at https://github.com/mchancan/deepseqslam.",1
"Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.",0
"The audio-visual scene aware dialog (AVSD) task involves engaging in a question-answer dialog with a human about the audio-visual content of an input video, its associated audio, and a brief caption. This task is a complex challenge that requires learning and reasoning across multiple modalities, and its progress could have a significant impact on various human-machine interaction applications. To address this task, we propose a multi-modal shuffled Transformer reasoning framework that controls semantics and uses a sequence of Transformer modules, each producing representations based on the input question. Our proposed Transformer variant incorporates a shuffling scheme on its multi-head outputs, providing better regularization. To capture fine-grained visual information, we introduce a dynamic scene graph representation learning pipeline comprising an intra-frame reasoning layer creating spatio-semantic graph representations for each frame and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end, and we present experiments on the AVSD dataset, including answer generation and selection tasks. Our results show that our approach achieves state-of-the-art performance on all evaluation metrics.",1
"This article aims to study the topological invariant properties encoded in node graph representational embeddings by utilizing tools available in persistent homology. Specifically, given a node embedding representation algorithm, we consider the case when these embeddings are real-valued. By viewing these embeddings as scalar functions on a domain of interest, we can utilize the tools available in persistent homology to study the topological information encoded in these representations. Our construction effectively defines a unique persistence-based graph descriptor, on both the graph and node levels, for every node representation algorithm. To demonstrate the effectiveness of the proposed method, we study the topological descriptors induced by DeepWalk, Node2Vec and Diff2Vec.",0
"The objective of this article is to examine the topological invariant properties that are contained in node graph representations using persistent homology tools. Specifically, we investigate the scenario where the node embeddings are expressed as real numbers. By considering these embeddings as scalar functions on a particular domain, we can utilize persistent homology tools to examine the topological information encoded in these representations. Our approach establishes a distinct graph descriptor based on persistence for each node representation algorithm on both the graph and node levels. To demonstrate the efficacy of this approach, we examine the topological descriptors generated by DeepWalk, Node2Vec, and Diff2Vec.",1
"Variational AutoEncoders (VAEs) are powerful generative models that merge elements from statistics and information theory with the flexibility offered by deep neural networks to efficiently solve the generation problem for high dimensional data. The key insight of VAEs is to learn the latent distribution of data in such a way that new meaningful samples can be generated from it. This approach led to tremendous research and variations in the architectural design of VAEs, nourishing the recent field of research known as unsupervised representation learning. In this article, we provide a comparative evaluation of some of the most successful, recent variations of VAEs. We particularly focus the analysis on the energetic efficiency of the different models, in the spirit of the so called Green AI, aiming both to reduce the carbon footprint and the financial cost of generative techniques. For each architecture we provide its mathematical formulation, the ideas underlying its design, a detailed model description, a running implementation and quantitative results.",0
"Variational AutoEncoders (VAEs) are generative models that efficiently tackle the generation problem for high dimensional data by merging elements from statistics and information theory with the flexibility of deep neural networks. VAEs learn the latent distribution of data to generate new and meaningful samples. As a result, the field of unsupervised representation learning has seen tremendous research and variations in VAE architectural design. This article compares recent successful variations of VAEs, focusing on their energetic efficiency in line with Green AI principles. The aim is to reduce both the carbon footprint and financial cost of generative techniques. The evaluation includes mathematical formulation, design ideas, model descriptions, running implementations, and quantitative results for each architecture.",1
"Event-based cameras are dynamic vision sensors that can provide asynchronous measurements of changes in per-pixel brightness at a microsecond level. This makes them significantly faster than conventional frame-based cameras, and an appealing choice for high-speed navigation. While an interesting sensor modality, this asynchronous data poses a challenge for common machine learning techniques. In this paper, we present an event variational autoencoder for unsupervised representation learning from asynchronous event camera data. We show that it is feasible to learn compact representations from spatiotemporal event data to encode the context. Furthermore, we show that such pretrained representations can be beneficial for navigation, allowing for usage in reinforcement learning instead of end-to-end reward driven perception. We validate this framework of learning visuomotor policies by applying it to an obstacle avoidance scenario in simulation. We show that representations learnt from event data enable training fast control policies that can adapt to different control capacities, and demonstrate a higher degree of robustness than end-to-end learning from event images.",0
"Dynamic vision sensors known as event-based cameras are capable of providing microsecond-level asynchronous measurements of changes in per-pixel brightness. They are faster than conventional frame-based cameras, making them an attractive option for high-speed navigation. However, their asynchronous data presents a challenge for traditional machine learning techniques. This paper introduces an event variational autoencoder that allows for unsupervised representation learning from asynchronous event camera data. The study shows that it is possible to learn compact representations from spatiotemporal event data to encode context. Furthermore, pretrained representations can be useful for navigation, allowing for their use in reinforcement learning instead of end-to-end reward-driven perception. By applying this framework to an obstacle avoidance scenario in simulation, the study validates the learning of visuomotor policies. The research demonstrates that representations learned from event data enable the training of fast control policies that can adapt to different control capacities, and exhibit greater robustness than end-to-end learning from event images.",1
"Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. However, it remains notoriously challenging to train and inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because as the graph size grows, the sheer number of node features and the large adjacency matrix can easily explode the required memory and data movements. To tackle the aforementioned challenge, we explore the possibility of drawing lottery tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency matrix yet are capable of achieving accuracy comparable to or even better than their corresponding full graphs. Specifically, we for the first time discover the existence of graph early-bird (GEB) tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of such GEB tickets. Furthermore, we develop a generic efficient GCN training framework dubbed GEBT that can significantly boost the efficiency of GCN training by (1) drawing joint early-bird tickets between the GCN graphs and models and (2) enabling simultaneously sparsifying both GCN graphs and models, paving the way for training and inferencing large GCN graphs to handle real-world graph datasets. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2% ~ 85.6% and 84.6% ~ 87.5% savings of GCN training and inference costs while leading to a comparable or even better accuracy as compared to state-of-the-art methods. Code available at https://github.com/RICE-EIC/GEBT",0
"The latest deep learning model for representation learning on graphs is Graph Convolutional Networks (GCNs). However, it is challenging to train and infer GCNs over large graph datasets, limiting their application to real-world graphs and hindering the exploration of more sophisticated GCN graphs. As the graph size increases, the number of node features and the large adjacency matrix can cause memory and data movement issues. To address this challenge, the possibility of drawing lottery tickets when sparsifying GCN graphs is explored by identifying subgraphs that can significantly reduce the adjacency matrix while achieving comparable accuracy to their corresponding full graphs. Graph early-bird (GEB) tickets are discovered at the early stage of sparsifying GCN graphs, and a simple yet effective detector is proposed to identify them. A generic efficient GCN training framework, GEBT, is developed to draw joint early-bird tickets between GCN graphs and models and simultaneously sparsify both, enabling training and inferencing of large GCN graphs for real-world graph datasets. Experiments on various GCN models and datasets validate the GEB finding and the effectiveness of GEBT code, which is available at https://github.com/RICE-EIC/GEBT.",1
"3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learn multi-view feature representations without involving labeled data. To this end, we propose a novel self-supervised paradigm to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views. Specifically, we perform a 3D transformation on a 3D object, and obtain multiple views before and after the transformation via projection. Then, we self-train a representation to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposed MV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks, and show the generalization to real-world datasets.",0
"Learning to represent 3D objects is a critical challenge in computer vision that enables inferring about the 3D world. Recent progress in deep learning has proven its effectiveness in recognizing 3D objects, with view-based techniques having the best performance so far. Nonetheless, existing methods for feature learning of multiple views mostly rely on supervised learning, which requires large amounts of labeled data and can be costly. In contrast, self-supervised learning aims to learn multi-view feature representations without the need for labeled data. Here, we introduce a new self-supervised approach for learning Multi-View Transformation Equivariant Representations (MV-TER), which explores the equivariant transformations of a 3D object and its projected multiple views. Our method involves performing a 3D transformation on a 3D object, capturing multiple views before and after the transformation via projection, and self-training a representation to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Our experimental results show that MV-TER outperforms state-of-the-art view-based approaches in 3D object classification and retrieval tasks, and generalizes well to real-world datasets.",1
"We propose Deep Autoencoding Predictive Components (DAPC) -- a self-supervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data.",0
"Our proposed method, known as DAPC or Deep Autoencoding Predictive Components, is a technique for learning representations of sequence data through self-supervised learning. Our approach is based on the belief that the most valuable representations of sequence data will display a straightforward structure in the latent space. We encourage this structure by maximizing the predictive information of latent feature sequences, which we define as the mutual information between past and future windows at each time step. Unlike the mutual information lower bound used in contrastive learning, our predictive information estimate is exact when assuming a Gaussian distribution and does not require negative sampling. To prevent the degeneracy of the latent space resulting from powerful encoders and retain the useful information from inputs, we use a challenging masked reconstruction loss to regulate the learning of predictive information. In our experiments, we show that our method can extract the latent space of noisy dynamical systems, identify predictive features for forecasting tasks, and enhance automatic speech recognition when pretraining the encoder with vast amounts of unlabeled data.",1
"We introduce a notion of usable information contained in the representation learned by a deep network, and use it to study how optimal representations for the task emerge during training. We show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations for the task. In the process of arriving at a minimal sufficient representation, we find that the content of the representation changes dynamically during training. In particular, we find that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded. In addition, we evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. We show these effects on both perceptual decision-making tasks inspired by neuroscience literature, as well as on standard image classification tasks.",0
"Our study focuses on how deep networks learn optimal representations for a task and introduces the concept of usable information contained in these representations. We demonstrate that Stochastic Gradient Descent with a high learning-rate and small batch size provides implicit regularization that aids in the learning of minimal sufficient representations. Throughout the training process, the content of the representation changes dynamically, and we observe the encoding of semantically meaningful but ultimately irrelevant information in the early stages of training, which is later discarded. We also investigate the impact of perturbing the initial stages of training on the learning dynamics and resulting representations. Our analysis includes both perceptual decision-making tasks inspired by neuroscience literature and standard image classification tasks.",1
"As an effective tool for two-dimensional data analysis, two-dimensional canonical correlation analysis (2DCCA) is not only capable of preserving the intrinsic structural information of original two-dimensional (2D) data, but also reduces the computational complexity effectively. However, due to the unsupervised nature, 2DCCA is incapable of extracting sufficient discriminatory representations, resulting in an unsatisfying performance. In this letter, we propose a complete discriminative tensor representation learning (CDTRL) method based on linear correlation analysis for analyzing 2D signals (e.g. images). This letter shows that the introduction of the complete discriminatory tensor representation strategy provides an effective vehicle for revealing, and extracting the discriminant representations across the 2D data sets, leading to improved results. Experimental results show that the proposed CDTRL outperforms state-of-the-art methods on the evaluated data sets.",0
"Two-dimensional canonical correlation analysis (2DCCA) is a useful tool for analyzing two-dimensional data, as it preserves the original structure and reduces computational complexity. However, it has limitations in extracting sufficient discriminatory representations, which affects its performance. To address this issue, we propose a discriminative tensor representation learning (CDTRL) method based on linear correlation analysis for 2D signals like images. Our approach introduces a complete discriminatory tensor representation strategy that effectively reveals and extracts discriminant representations across 2D data sets, resulting in improved performance. Our experiments show that CDTRL outperforms state-of-the-art methods on evaluated data sets.",1
"Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of function approximators based on polynomial expansions. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning. The source code is available at \url{https://github.com/grigorisg9gr/polynomial_nets}.",0
"Currently, for both generative and discriminative learning in computer vision and machine learning, Deep Convolutional Neural Networks (DCNNs) are the preferred method. This success can be attributed to the careful selection of building blocks such as residual blocks, rectifiers, and sophisticated normalization schemes. A new class of function approximators based on polynomial expansions called $\Pi$-Nets is proposed in this paper. These are polynomial neural networks, where the output is a high-order polynomial of the input. The unknown parameters, represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. Three tensor decompositions are introduced that significantly reduce the number of parameters, and they can be efficiently implemented by hierarchical neural networks. Empirical evidence shows that $\Pi$-Nets are very expressive and can even produce good results without non-linear activation functions in various tasks, such as images, graphs, and audio. When combined with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks: image generation, face verification, and 3D mesh representation learning. The source code is available at \url{https://github.com/grigorisg9gr/polynomial_nets}.",1
"A complete representation of 3D objects requires characterizing the space of deformations in an interpretable manner, from articulations of a single instance to changes in shape across categories. In this work, we improve on a prior generative model of geometric disentanglement for 3D shapes, wherein the space of object geometry is factorized into rigid orientation, non-rigid pose, and intrinsic shape. The resulting model can be trained from raw 3D shapes, without correspondences, labels, or even rigid alignment, using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space. Our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space. The geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object. Furthermore, it enables tasks like pose transfer and pose-aware retrieval without requiring supervision. We evaluate our model on its generative modelling, representation learning, and disentanglement performance, showing improved rotation invariance and intrinsic-extrinsic factorization quality over the prior model.",0
"To fully represent 3D objects, it is important to describe the range of deformations in a way that is easy to understand, from changes in shape across different categories to the articulations of a single instance. This study builds upon a previous generative model of geometric disentanglement for 3D shapes by breaking down the space of object geometry into rigid orientation, non-rigid pose, and intrinsic shape. The improved model can be trained using classical spectral geometry and probabilistic disentanglement without the need for correspondences, labels, or rigid alignment. Our enhancements include better management of rotational invariance and the use of a diffeomorphic flow network to connect latent and spectral space. The structured latent space provides an understandable representation of an object's deformation space and enables pose transfer and pose-aware retrieval without supervision. Our model shows improved performance in generative modelling, representation learning, and disentanglement compared to the prior model, with better rotation invariance and intrinsic-extrinsic factorization quality.",1
"Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes -- a crucial component in CL -- remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.",0
"In recent times, unsupervised graph representation learning has seen success through contrastive learning (CL). Initially, most graph CL methods involve stochastic augmentation of the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the success of graph CL, the design of graph augmentation schemes, which is a vital component of CL, has been less explored. The argument is that data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbations on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, such as uniformly dropping edges and shuffling features, leading to suboptimal performance. This study proposes a new graph contrastive representation learning method with adaptive augmentation that integrates various priors for topological and semantic aspects of the graph. The augmentation schemes are designed based on node centrality measures to highlight important connective structures on the topology level. On the node attribute level, node features are corrupted by adding more noise to unimportant node features to enforce the model to recognize underlying semantic information. The proposed method was tested through extensive experiments on node classification using a variety of real-world datasets. The results indicate that the proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, validating the effectiveness of the proposed contrastive framework with adaptive augmentation.",1
"Accurately identifying different representations of the same real-world entity is an integral part of data cleaning and many methods have been proposed to accomplish it. The challenges of this entity resolution task that demand so much research attention are often rooted in the task-specificity and user-dependence of the process. Adopting deep learning techniques has the potential to lessen these challenges. In this paper, we set out to devise an entity resolution method that builds on the robustness conferred by deep autoencoders to reduce human-involvement costs. Specifically, we reduce the cost of training deep entity resolution models by performing unsupervised representation learning. This unveils a transferability property of the resulting model that can further reduce the cost of applying the approach to new datasets by means of transfer learning. Finally, we reduce the cost of labelling training data through an active learning approach that builds on the properties conferred by the use of deep autoencoders. Empirical evaluation confirms the accomplishment of our cost-reduction desideratum while achieving comparable effectiveness with state-of-the-art alternatives.",0
"The task of accurately identifying various representations of a real-world entity is crucial in data cleaning. Many methods have been proposed to achieve this, but the challenges associated with entity resolution demand significant research attention. These challenges are often linked to the task's specificity and dependence on users. The use of deep learning techniques has the potential to alleviate these challenges. In this study, we propose an entity resolution method that leverages the robustness of deep autoencoders to minimize human involvement costs. We achieve this by implementing unsupervised representation learning, which uncovers the transferability of the resulting model and reduces the cost of applying the approach to new datasets through transfer learning. Additionally, we reduce the cost of labeling training data using an active learning approach based on the properties of deep autoencoders. Our empirical evaluation shows that our method accomplishes cost reduction objectives while maintaining effectiveness comparable to state-of-the-art alternatives.",1
"We consider representation learning from 3D graphs in which each node is associated with a spatial position in 3D. This is an under explored area of research, and a principled framework is currently lacking. In this work, we propose a generic framework, known as the 3D graph network (3DGN), to provide a unified interface at different levels of granularity for 3D graphs. Built on 3DGN, we propose the spherical message passing (SMP) as a novel and specific scheme for realizing the 3DGN framework in the spherical coordinate system (SCS). We conduct formal analyses and show that the relative location of each node in 3D graphs is uniquely defined in the SMP scheme. Thus, our SMP represents a complete and accurate architecture for learning from 3D graphs in the SCS. We derive physically-based representations of geometric information and propose the SphereNet for learning representations of 3D graphs. We show that existing 3D deep models can be viewed as special cases of the SphereNet. Experimental results demonstrate that the use of complete and accurate 3D information in 3DGN and SphereNet leads to significant performance improvements in prediction tasks.",0
"The study of representation learning from 3D graphs that assign a spatial position to each node is an area of research that has not yet been fully explored and lacks a solid framework. To address this, we introduce a universal framework called the 3D graph network (3DGN) that offers a consistent interface for 3D graphs at different levels of granularity. Using 3DGN, we propose the novel spherical message passing (SMP) scheme, which employs the spherical coordinate system (SCS) to achieve a complete and precise architecture for learning from 3D graphs. We demonstrate that SMP accurately defines the relative location of each node in 3D graphs, enabling us to derive physically-based geometric representations and introduce SphereNet for learning 3D graph representations. We show that existing 3D deep models are special cases of SphereNet and that the use of 3DGN and SphereNet's complete and precise 3D information leads to significant improvements in prediction tasks.",1
"In supervised learning, smoothing label or prediction distribution in neural network training has been proven useful in preventing the model from being over-confident, and is crucial for learning more robust visual representations. This observation motivates us to explore ways to make predictions flattened in unsupervised learning. Considering that human-annotated labels are not adopted in unsupervised learning, we introduce a straightforward approach to perturb input image space in order to soften the output prediction space indirectly, meanwhile, assigning new label values in the unsupervised frameworks accordingly. Despite its conceptual simplicity, we show empirically that with the simple solution -- Unsupervised image mixtures (Un-Mix), we can learn more robust visual representations from the transformed input. Extensive experiments are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, etc. Our proposed image mixture and label assignment strategy can obtain consistent improvement by 1~3% following exactly the same hyperparameters and training procedures of the base methods.",0
"The usefulness of smoothing labels or prediction distributions in neural network training has been established in preventing over-confidence and developing more reliable visual representations in supervised learning. This observation has prompted us to explore ways of flattening predictions in unsupervised learning, which lacks human-annotated labels. To address this challenge, we propose a simple method of perturbing the input image space to indirectly soften the output prediction space and assign new label values in unsupervised frameworks. Empirical evidence demonstrates that our proposed strategy, Unsupervised Image Mixtures (Un-Mix), can yield more robust visual representations from transformed inputs. Our experiments on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and standard ImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, etc. consistently improve performance by 1~3% using the same hyperparameters and training procedures as the base methods.",1
"A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed from the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.",0
"Research indicates that embodied gameplay is crucial for developing the neural flexibility necessary for creative problem solving, decision making, and socialization, as observed in various animal species, including turtles and ravens. However, little is known about the effect of embodied gameplay on artificial agents, as most studies have produced agents proficient in abstract games that offer limited insight into the benefits of embodied play. Hide-and-seek games, played universally, provide an excellent opportunity to study the impact of embodied gameplay on representation learning in perspective taking, secret keeping, and false belief understanding. In this study, our adversarial reinforcement learning agents playing Cache, a hide-and-seek variant, in a high fidelity, interactive environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Our agents' representations are developed through interaction and play, enhanced by intentionality and memory, moving closer to biologically motivated learning strategies. These findings offer a model for studying the development of vision through interaction and an experimental framework for assessing what artificial agents learn, demonstrating the value of experiential, interactive representation learning over static datasets.",1
"Variational autoencoders (VAEs) are essential tools in end-to-end representation learning. However, the sequential text generation common pitfall with VAEs is that the model tends to ignore latent variables with a strong auto-regressive decoder. In this paper, we propose a principled approach to alleviate this issue by applying a discretized bottleneck to enforce an implicit latent feature matching in a more compact latent space. We impose a shared discrete latent space where each input is learned to choose a combination of latent atoms as a regularized latent representation. Our model endows a promising capability to model underlying semantics of discrete sequences and thus provide more interpretative latent structures. Empirically, we demonstrate our model's efficiency and effectiveness on a broad range of tasks, including language modeling, unaligned text style transfer, dialog response generation, and neural machine translation.",0
"VAEs are crucial in representation learning from end-to-end. However, VAEs commonly suffer from a flaw in sequential text generation where the model disregards latent variables with a strong auto-regressive decoder. This paper presents a systematic approach to address this issue by implementing a discretized bottleneck that enforces implicit latent feature matching in a more condensed latent space. We establish a shared discrete latent space where every input learns to select a combination of latent atoms as a regulated latent representation. Our model has the potential to model the underlying semantics of discrete sequences, resulting in more interpretable latent structures. Our model's efficiency and effectiveness are demonstrated empirically across various tasks, such as language modeling, unaligned text style transfer, dialog response generation, and neural machine translation.",1
"Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.   Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.   We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",0
"Recommender models are crucial in modern search and recommendation systems as they help in identifying the most relevant items from vast catalogs. These models typically learn a joint embedding space for queries and items through neural networks using user feedback data. However, due to the power-law distribution, feedback data for long-tail items is extremely sparse. To address this label sparsity problem, we propose a multi-task self-supervised learning framework that improves item representation learning and serves as additional regularization for better generalization. Additionally, we introduce a novel data augmentation method that utilizes feature correlations. Our evaluation of the framework using real-world datasets demonstrates its effectiveness in regularization and superior performance over state-of-the-art techniques. We have also implemented the proposed techniques in a web-scale commercial app-to-app recommendation system with significant improvements in business metrics. Our online results confirm our hypothesis that the framework improves model performance on slices that lack supervision.",1
"This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language",0
"Instead of describing a working system, this paper proposes a single representation idea that combines advancements from various groups to create an imaginary system called GLOM. These advancements include transformers, neural fields, contrastive representation learning, distillation, and capsules. The objective of GLOM is to answer the question of how a neural network with a fixed architecture can parse an image into a part-whole hierarchy with distinct structures for each image. The proposed approach involves using identical vectors to represent nodes in the parse tree. If successful, GLOM could significantly enhance the interpretability of the representations generated by transformer-like systems in language or vision applications.",1
"A key to causal inference with observational data is achieving balance in predictive features associated with each treatment type. Recent literature has explored representation learning to achieve this goal. In this work, we discuss the pitfalls of these strategies - such as a steep trade-off between achieving balance and predictive power - and present a remedy via the integration of balancing weights in causal learning. Specifically, we theoretically link balance to the quality of propensity estimation, emphasize the importance of identifying a proper target population, and elaborate on the complementary roles of feature balancing and weight adjustments. Using these concepts, we then develop an algorithm for flexible, scalable and accurate estimation of causal effects. Finally, we show how the learned weighted representations may serve to facilitate alternative causal learning procedures with appealing statistical features. We conduct an extensive set of experiments on both synthetic examples and standard benchmarks, and report encouraging results relative to state-of-the-art baselines.",0
"Achieving balance in predictive features associated with each treatment type is crucial for causal inference with observational data. Representation learning has been explored in recent literature as a means to achieve this objective. However, there are pitfalls to these strategies, including a trade-off between achieving balance and predictive power. To address this issue, we propose the integration of balancing weights in causal learning. We establish a theoretical link between balance and the quality of propensity estimation, emphasize the significance of identifying an appropriate target population, and elaborate on the roles of feature balancing and weight adjustments. We develop an algorithm for flexible, scalable, and accurate estimation of causal effects based on these concepts. Furthermore, we demonstrate how the learned weighted representations can facilitate alternative causal learning procedures with appealing statistical features. We conduct extensive experiments on synthetic examples and standard benchmarks and report promising results in comparison to state-of-the-art baselines.",1
"Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.",0
"Over the past few years, product embeddings have been extensively researched and have become a crucial component of many machine learning applications in e-commerce. Despite their practical success, there is a lack of understanding regarding the theoretical basis of their effectiveness. Unlike natural language processing (NLP), which relies on domain-specific features that are not transferable to e-commerce, downstream tasks for product embeddings focus on different aspects. In this study, we take an e-commerce-centric approach to product embeddings and provide a comprehensive theoretical perspective from both representation learning and learning theory. Our research shows that product embeddings produced by the commonly used skip-gram negative sampling algorithm and its variations are effective in dimension reduction regarding a crucial measure of product relatedness. The generalization performance in downstream machine learning tasks is determined by the alignment between the embeddings and the product relatedness measure. Based on our theoretical findings, we conduct exploratory experiments that support our insights into the effectiveness of product embeddings.",1
"Automated monitoring and analysis of passenger movement in safety-critical parts of transport infrastructures represent a relevant visual surveillance task. Recent breakthroughs in visual representation learning and spatial sensing opened up new possibilities for detecting and tracking humans and objects within a 3D spatial context. This paper proposes a flexible analysis scheme and a thorough evaluation of various processing pipelines to detect and track humans on a ground plane, calibrated automatically via stereo depth and pedestrian detection. We consider multiple combinations within a set of RGB- and depth-based detection and tracking modalities. We exploit the modular concepts of Meshroom [2] and demonstrate its use as a generic vision processing pipeline and scalable evaluation framework. Furthermore, we introduce a novel open RGB-D railway platform dataset with annotations to support research activities in automated RGB-D surveillance. We present quantitative results for multiple object detection and tracking for various algorithmic combinations on our dataset. Results indicate that the combined use of depth-based spatial information and learned representations yields substantially enhanced detection and tracking accuracies. As demonstrated, these enhancements are especially pronounced in adverse situations when occlusions and objects not captured by learned representations are present.",0
"The task of monitoring and analyzing passenger movement in critical parts of transport infrastructures is important for safety. With new developments in visual representation learning and spatial sensing, it is now possible to detect and track humans and objects in a 3D spatial context. In this study, we propose a flexible analysis scheme and evaluate various processing pipelines to detect and track humans on a ground plane using stereo depth and pedestrian detection. We explore multiple combinations of RGB- and depth-based detection and tracking modalities using the modular concept of Meshroom. We also introduce a novel open RGB-D railway platform dataset with annotations to support automated RGB-D surveillance research. Our quantitative results show that using depth-based spatial information and learned representations significantly improves detection and tracking accuracies, particularly in situations with occlusions and unrepresented objects.",1
"Representation learning approaches require a massive amount of discriminative training data, which is unavailable in many scenarios, such as healthcare, smart city, education, etc. In practice, people refer to crowdsourcing to get annotated labels. However, due to issues like data privacy, budget limitation, shortage of domain-specific annotators, the number of crowdsourced labels is still very limited. Moreover, because of annotators' diverse expertises, crowdsourced labels are often inconsistent. Thus, directly applying existing supervised representation learning (SRL) algorithms may easily get the overfitting problem and yield suboptimal solutions. In this paper, we propose \emph{NeuCrowd}, a unified framework for SRL from crowdsourced labels. The proposed framework (1) creates a sufficient number of high-quality \emph{n}-tuplet training samples by utilizing safety-aware sampling and robust anchor generation; and (2) automatically learns a neural sampling network that adaptively learns to select effective samples for SRL networks. The proposed framework is evaluated on both one synthetic and three real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at \url{https://github.com/crowd-data-mining/NeuCrowd}.",0
"Many representation learning methods require a large amount of discriminative training data, which may not be available in various scenarios such as healthcare, smart city, and education. To obtain annotated labels, crowdsourcing is often used, but due to issues such as data privacy, budget constraints, and a shortage of domain-specific annotators, the number of crowdsourced labels is limited and inconsistent. This can lead to overfitting and suboptimal solutions when using existing supervised representation learning algorithms. To address these issues, this paper proposes the use of a unified framework called NeuCrowd for supervised representation learning from crowdsourced labels. The framework creates high-quality training samples and adapts to select effective samples for SRL networks. The proposed approach outperforms state-of-the-art baselines in terms of prediction accuracy and AUC on synthetic and real-world datasets. The code is publicly available for reproducible results.",1
"Tissue microarray (TMA) images have emerged as an important high-throughput tool for cancer study and the validation of biomarkers. Efforts have been dedicated to further improve the accuracy of TACOMA, a cutting-edge automatic scoring algorithm for TMA images. One major advance is due to deepTacoma, an algorithm that incorporates suitable deep representations of a group nature. Inspired by the recent advance in semi-supervised learning and deep learning, we propose mfTacoma to learn alternative deep representations in the context of TMA image scoring. In particular, mfTacoma learns the low-dimensional manifolds, a common latent structure in high dimensional data. Deep representation learning and manifold learning typically requires large data. By encoding deep representation of the manifolds as regularizing features, mfTacoma effectively leverages the manifold information that is potentially crude due to small data. Our experiments show that deep features by manifolds outperforms two alternatives -- deep features by linear manifolds with principal component analysis or by leveraging the group property.",0
"The use of Tissue Microarray (TMA) images has become a vital tool in cancer research and the validation of biomarkers. To enhance the accuracy of the TACOMA automatic scoring algorithm, researchers have developed deepTacoma by integrating deep representations of group nature. In light of recent advances in semi-supervised and deep learning, we present mfTacoma, which utilizes manifold learning to learn alternative deep representations for TMA image scoring. By encoding deep representations of the manifolds as regularizing features, mfTacoma is able to effectively leverage the manifold information despite limited data. Our experiments demonstrate that the deep features obtained from manifolds outperform two other alternatives, namely deep features by linear manifolds using principal component analysis or by utilizing the group property.",1
"The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.",0
"Initially, machine learning and graphical causality were distinct fields that evolved independently. Nevertheless, there has been a recent emergence of cross-fertilization and growing interest in each field to benefit from the advancements of the other. In this study, we examine fundamental causal inference concepts and connect them to pivotal open challenges in machine learning, such as transfer and generalization. This enables us to assess the role of causality in contemporary machine learning research. Conversely, we highlight that most causality studies assume the presence of causal variables, which prompts the need for causal representation learning - the identification of high-level causal variables from low-level observations. Lastly, we identify the implications of causality for machine learning and suggest essential research areas at the intersection of both communities.",1
"Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. In low data regime, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks.",0
"In recent times, there has been an emergence of auxiliary tasks aimed at hastening representation learning and enhancing efficiency in deep reinforcement learning (RL). However, these tasks fail to acknowledge the characteristics of RL problems and are not supervised. To address this, we have introduced a fresh auxiliary task that utilizes returns - the most crucial feedback signals in RL - to compel the learned representations to differentiate between state-action pairs with varying returns. Our auxiliary loss has been theoretically validated to facilitate the learning of representations that capture the structure of a new state-action abstraction form, which aggregates state-action pairs with similar return distributions. In situations of limited data, our method surpasses strong baselines in challenging tasks in Atari games and the DeepMind Control suite. Furthermore, when combined with existing auxiliary tasks, our method yields even better performance.",1
"To deal with various datasets over different complexity, this paper presents an self-adaptive learning model that combines the proposed Dynamic Connected Neural Decision Networks (DNDN) and a new pruning method--Dynamic Soft Pruning (DSP). DNDN is a combination of random forests and deep neural networks that enjoys both the advantages of strong classification capability of tree-like structure and representation learning capability of network structure. Based on Deep Neural Decision Forests (DNDF), this paper adopts an end-to-end training approach by representing the classification distribution with multiple randomly initialized softmax layers, which further allows an ensemble of multiple random forests attached to layers of neural network with different depth. We also propose a soft pruning method DSP to reduce the redundant connections of the network adaptively to avoid over-fitting simple dataset. The model demonstrates no performance loss compared with unpruned models and even higher robustness over different data and feature distribution. Extensive experiments on different datasets demonstrate the superiority of the proposed model over other popular algorithms in solving classification tasks.",0
This paper introduces a self-adaptive learning model that uses Dynamic Connected Neural Decision Networks (DNDN) and Dynamic Soft Pruning (DSP) to handle various datasets of different complexities. DNDN combines the strengths of random forests and deep neural networks to provide strong classification capability and representation learning capability. The model is based on Deep Neural Decision Forests (DNDF) and uses an end-to-end training approach with multiple randomly initialized softmax layers to represent the classification distribution. This allows for an ensemble of multiple random forests attached to layers of neural network with different depth. The proposed soft pruning method DSP reduces redundant connections of the network adaptively to avoid over-fitting on simple datasets. The model performs equally well or better than unpruned models and is more robust across different data and feature distributions. Extensive experiments show that the proposed model outperforms popular algorithms in solving classification tasks.,1
"Electronic Health Records (EHR) are high-dimensional data with implicit connections among thousands of medical concepts. These connections, for instance, the co-occurrence of diseases and lab-disease correlations can be informative when only a subset of these variables is documented by the clinician. A feasible approach to improving the representation learning of EHR data is to associate relevant medical concepts and utilize these connections. Existing medical ontologies can be the reference for EHR structures, but they place numerous constraints on the data source. Recent progress on graph neural networks (GNN) enables end-to-end learning of topological structures for non-grid or non-sequential data. However, there are problems to be addressed on how to learn the medical graph adaptively and how to understand the effect of the medical graph on representation learning. In this paper, we propose a variationally regularized encoder-decoder graph network that achieves more robustness in graph structure learning by regularizing node representations. Our model outperforms the existing graph and non-graph based methods in various EHR predictive tasks based on both public data and real-world clinical data. Besides the improvements in empirical experiment performances, we provide an interpretation of the effect of variational regularization compared to standard graph neural network, using singular value analysis.",0
"The data found in Electronic Health Records (EHR) is complex and involves connections between thousands of medical concepts. These connections, such as correlations between diseases and lab results, can provide useful information even when only a small subset of variables is recorded by clinicians. One way to improve the representation of EHR data is to associate relevant medical concepts and use these connections. While existing medical ontologies can provide a reference for EHR structures, they also place constraints on the data source. Recently, advances in graph neural networks (GNN) have enabled end-to-end learning of topological structures for non-grid or non-sequential data. However, there are still challenges to overcome in terms of adaptively learning the medical graph and understanding its impact on representation learning. This paper proposes a variationally regularized encoder-decoder graph network that achieves more robustness in graph structure learning by regularizing node representations. The model shows improved performance in various EHR predictive tasks using both public and real-world clinical data. Additionally, singular value analysis is used to interpret the effect of variational regularization compared to standard graph neural network.",1
"Self-supervised tasks have been utilized to build useful representations that can be used in downstream tasks when the annotation is unavailable. In this paper, we introduce a self-supervised video representation learning method based on the multi-transformation classification to efficiently classify human actions. Self-supervised learning on various transformations not only provides richer contextual information but also enables the visual representation more robust to the transforms. The spatio-temporal representation of the video is learned in a self-supervised manner by classifying seven different transformations i.e. rotation, clip inversion, permutation, split, join transformation, color switch, frame replacement, noise addition. First, seven different video transformations are applied to video clips. Then the 3D convolutional neural networks are utilized to extract features for clips and these features are processed to classify the pseudo-labels. We use the learned models in pretext tasks as the pre-trained models and fine-tune them to recognize human actions in the downstream task. We have conducted the experiments on UCF101 and HMDB51 datasets together with C3D and 3D Resnet-18 as backbone networks. The experimental results have shown that our proposed framework is outperformed other SOTA self-supervised action recognition approaches. The code will be made publicly available.",0
"The aim of self-supervised tasks is to create valuable representations that can be applied in tasks where annotation is unavailable. This study presents a self-supervised video representation learning approach that employs multi-transformation classification to efficiently classify human actions. The use of various transformations in self-supervised learning offers richer contextual information and enhances the robustness of visual representation to different transforms. The spatio-temporal representation of the video is learned through the classification of seven different transformations, including rotation, clip inversion, permutation, split, join transformation, color switch, frame replacement, and noise addition. The proposed method involves applying seven different video transformations to video clips and extracting features for clips using 3D convolutional neural networks. These features are then processed to classify the pseudo-labels. The resulting pre-trained models are used in pretext tasks and fine-tuned to recognize human actions in the downstream task. The study used C3D and 3D Resnet-18 as backbone networks and conducted experiments on UCF101 and HMDB51 datasets. The results showed that the proposed framework outperformed other state-of-the-art self-supervised action recognition approaches. The code for this study will be made publicly available.",1
"Classical machine learning implicitly assumes that labels of the training data are sampled from a clean distribution, which can be too restrictive for real-world scenarios. However, statistical-learning-based methods may not train deep learning models robustly with these noisy labels. Therefore, it is urgent to design Label-Noise Representation Learning (LNRL) methods for robustly training deep models with noisy labels. To fully understand LNRL, we conduct a survey study. We first clarify a formal definition for LNRL from the perspective of machine learning. Then, via the lens of learning theory and empirical study, we figure out why noisy labels affect deep models' performance. Based on the theoretical guidance, we categorize different LNRL methods into three directions. Under this unified taxonomy, we provide a thorough discussion of the pros and cons of different categories. More importantly, we summarize the essential components of robust LNRL, which can spark new directions. Lastly, we propose possible research directions within LNRL, such as new datasets, instance-dependent LNRL, and adversarial LNRL. We also envision potential directions beyond LNRL, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise and demonstration-noise.",0
"The traditional approach to machine learning assumes that training data is derived from a clean distribution, which is often not the case in real-world scenarios. While statistical learning methods may not be able to train deep learning models effectively with noisy labels, Label-Noise Representation Learning (LNRL) techniques are urgently needed to overcome this limitation. To explore LNRL, we conducted a survey study, beginning with a formal definition and examining how noisy labels impact deep models via learning theory and empirical research. We categorized LNRL methods into three directions and discussed their pros and cons, identifying essential components of robust LNRL and highlighting potential research directions such as instance-dependent LNRL and adversarial LNRL. We also envision potential directions beyond LNRL, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise, and demonstration-noise.",1
"Molecular machine learning bears promise for efficient molecule property prediction and drug discovery. However, due to the limited labeled data and the giant chemical space, machine learning models trained via supervised learning perform poorly in generalization. This greatly limits the applications of machine learning methods for molecular design and discovery. In this work, we present MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks (GNNs), a self-supervised learning framework for large unlabeled molecule datasets. Specifically, we first build a molecular graph, where each node represents an atom and each edge represents a chemical bond. A GNN is then used to encode the molecule graph. We propose three novel molecule graph augmentations: atom masking, bond deletion, and subgraph removal. A contrastive estimator is utilized to maximize the agreement of different graph augmentations from the same molecule. Experiments show that molecule representations learned by MolCLR can be transferred to multiple downstream molecular property prediction tasks. Our method thus achieves state-of-the-art performance on many challenging datasets. We also prove the efficiency of our proposed molecule graph augmentations on supervised molecular classification tasks.",0
"The potential of molecular machine learning for the prediction of molecule properties and drug discovery is great. However, due to limited labeled data and a vast chemical space, supervised learning-based machine learning models do not generalize well, which hinders their practical application for molecular design and discovery. In this study, we introduce MolCLR: a self-supervised learning framework based on Molecular Contrastive Learning of Representations via Graph Neural Networks (GNNs) for large unlabeled molecule datasets. To begin, we create a molecular graph where each atom and chemical bond is represented by a node and edge, respectively. The molecule graph is then encoded using a GNN. We propose three novel molecule graph augmentations: atom masking, bond deletion, and subgraph removal. A contrastive estimator is used to maximize the agreement between different graph augmentations from the same molecule. Our experiments demonstrate that the molecule representations learned by MolCLR can be applied to multiple downstream molecular property prediction tasks, achieving state-of-the-art performance on challenging datasets. Additionally, we prove the efficacy of our proposed molecule graph augmentations on supervised molecular classification tasks.",1
"The collection and sharing of individuals' data has become commonplace in many industries. Local differential privacy (LDP) is a rigorous approach to preserving data privacy even from a database administrator, unlike the more standard central differential privacy. To achieve LDP, one traditionally adds noise directly to each data dimension, but for high-dimensional data the level of noise required for sufficient anonymization all but entirely destroys the data's utility. In this paper, we introduce a novel LDP mechanism that leverages representation learning to overcome the prohibitive noise requirements of direct methods. We demonstrate that, rather than simply estimating aggregate statistics of the privatized data as is the norm in LDP applications, our method enables the training of performant machine learning models. Unique applications of our approach include private novel-class classification and the augmentation of clean datasets with additional privatized features. Methods that rely on central differential privacy are not applicable to such tasks. Our approach achieves significant performance gains on these tasks relative to state-of-the-art LDP benchmarks that noise data directly.",0
"The practice of collecting and sharing personal data has become widespread across various industries. Local differential privacy (LDP) is a meticulous method of safeguarding data privacy, even from an administrator, in contrast to the more typical central differential privacy. LDP is typically achieved by introducing noise directly to each data dimension, but this approach is ineffective for high-dimensional data, as adding sufficient noise to achieve anonymity renders the data unusable. This study presents a new LDP technique that utilizes representation learning to overcome the noise requirements of direct methods. Unlike conventional LDP applications that only estimate aggregate statistics of anonymized data, our method enables the training of robust machine learning models, allowing for private novel-class classification and the expansion of clean datasets with additional anonymized features. These tasks are not feasible with central differential privacy methods. Our novel approach outperforms existing LDP benchmarks that apply direct noise to data.",1
"Generative modeling has recently seen many exciting developments with the advent of deep generative architectures such as Variational Auto-Encoders (VAE) or Generative Adversarial Networks (GAN). The ability to draw synthetic i.i.d. observations with the same joint probability distribution as a given dataset has a wide range of applications including representation learning, compression or imputation. It appears that it also has many applications in privacy preserving data analysis, especially when used in conjunction with differential privacy techniques. This paper focuses on synthetic data generation models with privacy preserving applications in mind. It introduces a novel architecture, the Composable Generative Model (CGM) that is state-of-the-art in tabular data generation. Any conditional generative model can be used as a sub-component of the CGM, including CGMs themselves, allowing the generation of numerical, categorical data as well as images, text, or time series. The CGM has been evaluated on 13 datasets (6 standard datasets and 7 simulated) and compared to 14 recent generative models. It beats the state of the art in tabular data generation by a significant margin.",0
"Recently, there have been several exciting developments in generative modeling, thanks to new deep generative architectures like Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN). These models can create synthetic data that resembles a given dataset's joint probability distribution, which has numerous applications, including representation learning, compression, and imputation. Moreover, these models show great promise in privacy-preserving data analysis, especially when combined with differential privacy techniques. This study focuses on generating synthetic data models with privacy preservation in mind and introduces the Composable Generative Model (CGM), which is currently the best tabular data generation model. Any conditional generative model, including CGMs, can be used as a sub-component of the CGM, allowing it to generate numerical, categorical data, images, text, and time series. The CGM has been tested on 13 datasets, including six standard and seven simulated datasets, and it outperforms 14 recent generative models in tabular data generation by a significant margin.",1
"Corporate credit rating reflects the level of corporate credit and plays a crucial role in modern financial risk control. But real-world credit rating data usually shows long-tail distributions, which means heavy class imbalanced problem challenging the corporate credit rating system greatly. To tackle that, inspried by the recent advances of pre-train techniques in self-supervised representation learning, we propose a novel framework named Contrastive Pre-training for Corporate Credit Rating (CP4CCR), which utilizes the self-surpervision for getting over class imbalance. Specifically, we propose to, in the first phase, exert constrastive self-superivised pre-training without label information, which want to learn a better class-agnostic initialization. During this phase, two self-supervised task are developed within CP4CCR: (i) Feature Masking (FM) and (ii) Feature Swapping(FS). In the second phase, we can train any standard corporate redit rating model initialized by the pre-trained network. Extensive experiments conducted on the Chinese public-listed corporate rating dataset, prove that CP4CCR can improve the performance of standard corporate credit rating models, especially for class with few samples.",0
"The corporate credit rating plays a vital role in managing financial risks by gauging the level of corporate credit. However, real-world credit rating data often exhibits long-tail distributions, posing a significant challenge to the corporate credit rating system due to the imbalanced class problem. To address this issue, we have proposed a novel framework called Contrastive Pre-training for Corporate Credit Rating (CP4CCR), inspired by recent advances in pre-train techniques in self-supervised representation learning. CP4CCR uses self-supervision to overcome class imbalance. In the first phase, we leverage contrastive self-supervised pre-training without label information to obtain a better class-agnostic initialization. Two self-supervised tasks, Feature Masking (FM) and Feature Swapping (FS), are developed within CP4CCR during this phase. In the second phase, any standard corporate credit rating model can be trained, initialized by the pre-trained network. Our experiments on the Chinese public-listed corporate rating dataset demonstrate that CP4CCR can significantly improve the performance of standard corporate credit rating models, particularly for classes with limited samples.",1
"Knowledge Graphs (KG) are gaining increasing attention in both academia and industry. Despite their diverse benefits, recent research have identified social and cultural biases embedded in the representations learned from KGs. Such biases can have detrimental consequences on different population and minority groups as applications of KG begin to intersect and interact with social spheres. This paper aims at identifying and mitigating such biases in Knowledge Graph (KG) embeddings. As a first step, we explore popularity bias -- the relationship between node popularity and link prediction accuracy. In case of node2vec graph embeddings, we find that prediction accuracy of the embedding is negatively correlated with the degree of the node. However, in case of knowledge-graph embeddings (KGE), we observe an opposite trend. As a second step, we explore gender bias in KGE, and a careful examination of popular KGE algorithms suggest that sensitive attribute like the gender of a person can be predicted from the embedding. This implies that such biases in popular KGs is captured by the structural properties of the embedding. As a preliminary solution to debiasing KGs, we introduce a novel framework to filter out the sensitive attribute information from the KG embeddings, which we call FAN (Filtering Adversarial Network). We also suggest the applicability of FAN for debiasing other network embeddings which could be explored in future work.",0
"The use of Knowledge Graphs (KG) is becoming increasingly popular in both academic and industrial settings. However, recent studies have revealed that these representations can contain social and cultural biases. These biases can have negative impacts on minority groups as KGs begin to intersect with social spheres. This study aims to identify and address such biases in KG embeddings. The first step is to examine popularity bias, where greater node popularity is associated with lower link prediction accuracy. The results show that this is true for node2vec graph embeddings, but the opposite is observed for knowledge-graph embeddings (KGE). The second step is to investigate gender bias in KGE, which can predict sensitive attributes such as a person's gender. To mitigate such biases, a new framework called Filtering Adversarial Network (FAN) is proposed to remove sensitive attribute information from KG embeddings. This solution can also be applied to other network embeddings in future research.",1
"Recently, there has been a rising surge of momentum for deep representation learning in hyperbolic spaces due to theirhigh capacity of modeling data like knowledge graphs or synonym hierarchies, possessing hierarchical structure. We refer to the model as hyperbolic deep neural network in this paper. Such a hyperbolic neural architecture potentially leads to drastically compact model withmuch more physical interpretability than its counterpart in Euclidean space. To stimulate future research, this paper presents acoherent and comprehensive review of the literature around the neural components in the construction of hyperbolic deep neuralnetworks, as well as the generalization of the leading deep approaches to the Hyperbolic space. It also presents current applicationsaround various machine learning tasks on several publicly available datasets, together with insightful observations and identifying openquestions and promising future directions.",0
"In recent times, there has been a significant increase in interest towards deep representation learning in hyperbolic spaces. This is mainly due to their ability to model data such as knowledge graphs and synonym hierarchies that have a hierarchical structure. In this paper, we term this model as the hyperbolic deep neural network. Compared to its Euclidean space counterpart, this type of neural architecture can result in a more compact model, which is easier to interpret physically. The purpose of this paper is to provide a comprehensive review of the literature on the neural components involved in constructing hyperbolic deep neural networks. Additionally, this paper discusses the generalization of leading deep learning approaches to the Hyperbolic space, current applications in various machine learning tasks on publicly available datasets, and identifies open questions and promising future directions to stimulate further research.",1
"Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, ""fully-connected layers with Quaternions"" (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily $1/n$ learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.",0
"Representation learning in hypercomplex space has shown promising results in recent studies. One example is the use of ""fully-connected layers with Quaternions,"" which replace real-valued matrix multiplications with Hamilton products of Quaternions. This method achieves similar performance in various applications while reducing the number of learnable parameters by 1/4. However, the disadvantage is that hypercomplex space is limited to a few predefined dimensions (4D, 8D, and 16D), which can restrict model flexibility. To address this issue, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of predefined dimensions. This approach not only subsumes the Hamilton product but also enables models to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility and using arbitrarily 1/n learnable parameters compared to fully-connected layer counterparts. Experiments on natural language inference, machine translation, text style transfer, and subject-verb agreement using LSTM and Transformer models demonstrate the effectiveness and flexibility of our proposed approach.",1
"Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a significant impact on the performance. In this paper, we exploit the fact that the underlying Markov decision process (MDP) represents a graph, which enables us to incorporate the topological information for effective state representation learning.   Motivated by the recent success of node representations for several graph analytical tasks we specifically investigate the capability of node representation learning methods to effectively encode the topology of the underlying MDP in Deep RL. To this end we perform a comparative analysis of several models chosen from 4 different classes of representation learning algorithms for policy learning in grid-world navigation tasks, which are representative of a large class of RL problems. We find that all embedding methods outperform the commonly used matrix representation of grid-world environments in all of the studied cases. Moreoever, graph convolution based methods are outperformed by simpler random walk based methods and graph linear autoencoders.",0
"The success of deep RL approaches relies heavily on the deep neural network's ability to create beneficial internal representations, but these approaches have a drawback of high sample-complexity. A good input representation at the outset can greatly affect performance. This paper takes advantage of the fact that the underlying Markov decision process (MDP) is a graph, which allows us to incorporate topological information for effective state representation learning. Node representations have been successful for various graph analytical tasks, motivating us to investigate their capability to encode the topology of the MDP in Deep RL. We compare several models from four different classes of representation learning algorithms for policy learning in grid-world navigation tasks, representative of many RL problems. We find that embedding methods outperform the commonly used matrix representation of grid-world environments in all cases studied. Additionally, simpler random walk-based methods and graph linear autoencoders outperform graph convolution-based methods.",1
"Market Basket Analysis (MBA) is a popular technique to identify associations between products, which is crucial for business decision making. Previous studies typically adopt conventional frequent itemset mining algorithms to perform MBA. However, they generally fail to uncover rarely occurring associations among the products at their most granular level. Also, they have limited ability to capture temporal dynamics in associations between products. Hence, we propose OMBA, a novel representation learning technique for Online Market Basket Analysis. OMBA jointly learns representations for products and users such that they preserve the temporal dynamics of product-to-product and user-to-product associations. Subsequently, OMBA proposes a scalable yet effective online method to generate products' associations using their representations. Our extensive experiments on three real-world datasets show that OMBA outperforms state-of-the-art methods by as much as 21%, while emphasizing rarely occurring strong associations and effectively capturing temporal changes in associations.",0
"Market Basket Analysis (MBA) is a widely used method in business decision-making to discover connections between products. However, traditional frequent itemset mining algorithms used in previous studies have limitations in uncovering rare associations at a granular level and capturing temporal dynamics between products. Therefore, we introduce OMBA, a novel representation learning technique for Online Market Basket Analysis, which learns representations for products and users to preserve temporal associations and proposes a scalable online method for generating product associations. Our experiments on three real-world datasets demonstrate that OMBA outperforms existing methods by up to 21%, highlighting rare strong associations and effectively capturing temporal changes in associations.",1
"COVID-19 classification using chest Computed Tomography (CT) has been found pragmatically useful by several studies. Due to the lack of annotated samples, these studies recommend transfer learning and explore the choices of pre-trained models and data augmentation. However, it is still unknown if there are better strategies than vanilla transfer learning for more accurate COVID-19 classification with limited CT data. This paper provides an affirmative answer, devising a novel `model' augmentation technique that allows a considerable performance boost to transfer learning for the task. Our method systematically reduces the distributional shift between the source and target domains and considers augmenting deep learning with complementary representation learning techniques. We establish the efficacy of our method with publicly available datasets and models, along with identifying contrasting observations in the previous studies.",0
"Several studies have found that using chest Computed Tomography (CT) for COVID-19 classification is practically useful. However, due to the lack of annotated samples, these studies suggest transfer learning and exploring pre-trained models and data augmentation. Nonetheless, it remains unclear if there are better strategies than vanilla transfer learning for accurate COVID-19 classification with limited CT data. This paper proposes a new `model' augmentation technique that significantly enhances transfer learning for the task. Our method systematically reduces the distributional shift between the source and target domains and incorporates complementary representation learning techniques into deep learning. We demonstrate the effectiveness of our approach using publicly available datasets and models, while also identifying contrasting observations from previous studies.",1
"We implement stacked denoising autoencoders, a class of neural networks that are capable of learning powerful representations of high dimensional data. We describe stochastic gradient descent for unsupervised training of autoencoders, as well as a novel genetic algorithm based approach that makes use of gradient information. We analyze the performance of both optimization algorithms and also the representation learning ability of the autoencoder when it is trained on standard image classification datasets.",0
"To learn potent representations of high-dimensional data, we utilize stacked denoising autoencoders, a type of neural network. We expound on the unsupervised training of autoencoders using stochastic gradient descent, and introduce a unique genetic algorithm-based method that utilizes gradient information. Our analysis of the optimization algorithms and the autoencoder's ability to learn representations is based on its training with commonly used image classification datasets.",1
"The success of deep learning in the computer vision and natural language processing communities can be attributed to training of very deep neural networks with millions or billions of parameters which can then be trained with massive amounts of data. However, similar trend has largely eluded training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of 1) wider networks with DenseNet connection, 2) decoupling representation learning from training of RL, 3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.",0
"The triumph of deep learning in the natural language processing and computer vision domains is due to the training of deep neural networks containing millions or billions of parameters, which can then be trained with vast amounts of data. However, the success of deep reinforcement learning (RL) algorithms in this area has been limited as larger networks do not lead to better performance. This is mainly due to instability during training of deep RL agents when utilizing larger networks. This article aims to investigate and tackle the problem of training larger networks for deep RL. Initially, it is demonstrated that increasing network capacity does not improve performance. Subsequently, a new technique is proposed, which incorporates wider networks with DenseNet connections, decouples representation learning from RL training, and utilizes a distributed training method to alleviate overfitting issues. With this three-pronged approach, it is demonstrated that very large networks can be trained, resulting in significant performance improvements. Several ablation studies are presented to confirm the effectiveness of the proposed method and to provide some insight into the reasons for the performance gains. Finally, it is shown that the proposed method surpasses other baseline algorithms on various challenging locomotion tasks.",1
"In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the generalization capacity of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on classic few-shot classification and continual learning benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice of training popular meta-learning methods.",0
"The aim of this paper is to explore the concept of multi-task representation (MTR) learning, which involves utilizing source tasks to develop a representation that simplifies the process of solving a target task. Our analysis focuses on recent developments in MTR theory and how they can provide new perspectives for meta-learning algorithms. We draw attention to a key distinction between gradient-based and metric-based algorithms and offer a theoretical explanation for this difference. Ultimately, we use these insights to enhance the generalization capacity of meta-learning methods through the introduction of a novel spectral-based regularization term. Our experimental studies on few-shot classification and continual learning benchmarks demonstrate the effectiveness of this approach and, to our knowledge, this is the first paper to apply the latest learning bounds of MTR theory to the training of popular meta-learning methods.",1
"The original design of Graph Convolution Network (GCN) couples feature transformation and neighborhood aggregation for node representation learning. Recently, some work shows that coupling is inferior to decoupling, which supports deep graph propagation better and has become the latest paradigm of GCN (e.g., APPNP and SGCN). Despite effectiveness, the working mechanisms of the decoupled GCN are not well understood. In this paper, we explore the decoupled GCN for semi-supervised node classification from a novel and fundamental perspective -- label propagation. We conduct thorough theoretical analyses, proving that the decoupled GCN is essentially the same as the two-step label propagation: first, propagating the known labels along the graph to generate pseudo-labels for the unlabeled nodes, and second, training normal neural network classifiers on the augmented pseudo-labeled data. More interestingly, we reveal the effectiveness of decoupled GCN: going beyond the conventional label propagation, it could automatically assign structure- and model- aware weights to the pseudo-label data. This explains why the decoupled GCN is relatively robust to the structure noise and over-smoothing, but sensitive to the label noise and model initialization. Based on this insight, we propose a new label propagation method named Propagation then Training Adaptively (PTA), which overcomes the flaws of the decoupled GCN with a dynamic and adaptive weighting strategy. Our PTA is simple yet more effective and robust than decoupled GCN. We empirically validate our findings on four benchmark datasets, demonstrating the advantages of our method. The code is available at https://github.com/DongHande/PT_propagation_then_training.",0
"The Graph Convolution Network (GCN) was originally designed to learn node representation through coupling feature transformation and neighborhood aggregation. However, recent studies suggest that decoupling is a better approach, as it supports deep graph propagation and has become the latest paradigm of GCN, such as in APPNP and SGCN. Despite its effectiveness, the mechanism of the decoupled GCN is not well understood. To address this, our paper explores the decoupled GCN for semi-supervised node classification from a novel perspective of label propagation. We conduct thorough theoretical analyses and prove that the decoupled GCN is essentially the same as the two-step label propagation, where known labels propagate along the graph to generate pseudo-labels for unlabeled nodes, and neural network classifiers train on the augmented pseudo-labeled data. Additionally, we reveal the effectiveness of decoupled GCN, which assigns structure- and model-aware weights to pseudo-label data, explaining its robustness to structure noise and over-smoothing but sensitivity to label noise and model initialization. Based on this insight, we propose a new label propagation method called Propagation then Training Adaptively (PTA), which overcomes the flaws of the decoupled GCN with a dynamic and adaptive weighting strategy. Our PTA is simple yet more effective and robust than decoupled GCN, and we demonstrate its advantages on four benchmark datasets. The code for our method is available at https://github.com/DongHande/PT_propagation_then_training.",1
"Clustering performs an essential role in many real world applications, such as market research, pattern recognition, data analysis, and image processing. However, due to the high dimensionality of the input feature values, the data being fed to clustering algorithms usually contains noise and thus could lead to in-accurate clustering results. While traditional dimension reduction and feature selection algorithms could be used to address this problem, the simple heuristic rules used in those algorithms are based on some particular assumptions. When those assumptions does not hold, these algorithms then might not work. In this paper, we propose DAC, Deep Autoencoder-based Clustering, a generalized data-driven framework to learn clustering representations using deep neuron networks. Experiment results show that our approach could effectively boost performance of the K-Means clustering algorithm on a variety types of datasets.",0
"Many applications, including market research, data analysis, pattern recognition, and image processing, rely on clustering. However, the high dimensionality of input feature values often results in noisy data, which can cause inaccurate clustering results. While traditional algorithms for dimension reduction and feature selection can mitigate this issue, they are limited by their reliance on specific assumptions. Our paper presents the Deep Autoencoder-based Clustering (DAC) framework, which uses deep neural networks to learn clustering representations in a data-driven manner. Our experiments demonstrate that DAC can significantly improve the performance of the K-Means clustering algorithm across various datasets.",1
"This paper describes an end-to-end solution for the relationship prediction task in heterogeneous, multi-relational graphs. We particularly address two building blocks in the pipeline, namely heterogeneous graph representation learning and negative sampling. Existing message passing-based graph neural networks use edges either for graph traversal and/or selection of message encoding functions. Ignoring the edge semantics could have severe repercussions on the quality of embeddings, especially when dealing with two nodes having multiple relations. Furthermore, the expressivity of the learned representation depends on the quality of negative samples used during training. Although existing hard negative sampling techniques can identify challenging negative relationships for optimization, new techniques are required to control false negatives during training as false negatives could corrupt the learning process. To address these issues, first, we propose RelGNN -- a message passing-based heterogeneous graph attention model. In particular, RelGNN generates the states of different relations and leverages them along with the node states to weigh the messages. RelGNN also adopts a self-attention mechanism to balance the importance of attribute features and topological features for generating the final entity embeddings. Second, we introduce a parameter-free negative sampling technique -- adaptive self-adversarial (ASA) negative sampling. ASA reduces the false-negative rate by leveraging positive relationships to effectively guide the identification of true negative samples. Our experimental evaluation demonstrates that RelGNN optimized by ASA for relationship prediction improves state-of-the-art performance across established benchmarks as well as on a real industrial dataset.",0
"In this paper, we present a complete solution for predicting relationships in complex, multi-relational graphs. We focus on two important components of the solution: learning representation for heterogeneous graphs and negative sampling. Traditional methods for graph neural networks rely on edges to traverse graphs and encode messages, but this can lead to poor quality embeddings when dealing with nodes with multiple relationships. Moreover, the accuracy of learned representation depends on negative samples used during training, and existing techniques can produce false negatives that corrupt the learning process. To address these issues, we propose RelGNN, which is a message passing-based heterogeneous graph attention model that generates relation states and self-attention mechanisms to balance attribute and topological features. We also introduce a novel negative sampling technique, ASA, which uses positive relationships to guide the identification of true negative samples. Our experimental results show that RelGNN optimized by ASA outperforms existing methods on established benchmarks and real-world datasets.",1
"Learning low-level node embeddings using techniques from network representation learning is useful for solving downstream tasks such as node classification and link prediction. An important consideration in such applications is the robustness of the embedding algorithms against adversarial attacks, which can be examined by performing perturbation on the original network. An efficient perturbation technique can degrade the performance of network embeddings on downstream tasks. In this paper, we study network embedding algorithms from an adversarial point of view and observe the effect of poisoning the network on downstream tasks. We propose VIKING, a supervised network poisoning strategy that outperforms the state-of-the-art poisoning methods by upto 18% on the original network structure. We also extend VIKING to a semi-supervised attack setting and show that it is comparable to its supervised counterpart.",0
"Utilizing techniques from network representation learning to learn low-level node embeddings is advantageous for solving various downstream tasks like node classification and link prediction. However, when employing embedding algorithms in such applications, it is essential to consider their resilience against adversarial attacks. To examine this, perturbation on the primary network can be performed. An effective perturbation technique can decrease the quality of network embeddings for downstream tasks. This paper explores network embedding algorithms from an adversarial viewpoint and analyzes the impact of network poisoning on downstream tasks. The authors present VIKING, a supervised network poisoning approach that surpasses current poisoning methods by up to 18% on the initial network structure. They also extend VIKING to a semi-supervised attack scenario and prove its similarity to its supervised counterpart.",1
"Domain generalization refers to the problem where we aim to train a model on data from a set of source domains so that the model can generalize to unseen target domains. Naively training a model on the aggregate set of data (pooled from all source domains) has been shown to perform suboptimally, since the information learned by that model might be domain-specific and generalize imperfectly to target domains. To tackle this problem, a predominant approach is to find and learn some domain-invariant information in order to use it for the prediction task. In this paper, we propose a theoretically grounded method to learn a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions among domains. We also show how to use generative adversarial networks to learn such domain transformations to implement our method in practice. We demonstrate the effectiveness of our method on several widely used datasets for the domain generalization problem, on all of which we achieve competitive results with state-of-the-art models.",0
"The issue of domain generalization involves training a model using data from various source domains, with the goal of enabling the model to work effectively with unseen target domains. Simply training a model using data pooled from all source domains has been found to be suboptimal, as the information learned may be specific to certain domains and may not translate well to target domains. To address this problem, a commonly used approach is to identify and learn domain-invariant information that can be used for prediction tasks. This paper presents a method for learning a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions between domains, based on theoretical principles. Additionally, we demonstrate how to use generative adversarial networks to learn domain transformations to apply this method practically. We evaluate the effectiveness of our approach on several popular datasets used for the domain generalization problem, and demonstrate competitive results compared to state-of-the-art models.",1
"The low rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments.",0
"The low rank MDP is a significant model for exploring reinforcement learning and representation learning. When a representation is known, various exploration strategies can be model-free. On the other hand, algorithms for the unknown representation require full dynamics modeling, making them model-based. In this study, we introduce the first model-free representation learning algorithms for low rank MDPs. Our algorithmic contribution is a new minimax representation learning objective with variants that have different statistical and computational properties. We combine this representation learning step with an exploration strategy to reward-free state space coverage. These algorithms are sample-efficient and can handle complex environments with general function approximation.",1
"We propose a Healthcare Graph Convolutional Network (HealGCN) to offer disease self-diagnosis service for online users based on Electronic Healthcare Records (EHRs). Two main challenges are focused in this paper for online disease diagnosis: (1) serving cold-start users via graph convolutional networks and (2) handling scarce clinical description via a symptom retrieval system. To this end, we first organize the EHR data into a heterogeneous graph that is capable of modeling complex interactions among users, symptoms and diseases, and tailor the graph representation learning towards disease diagnosis with an inductive learning paradigm. Then, we build a disease self-diagnosis system with a corresponding EHR Graph-based Symptom Retrieval System (GraphRet) that can search and provide a list of relevant alternative symptoms by tracing the predefined meta-paths. GraphRet helps enrich the seed symptom set through the EHR graph when confronting users with scarce descriptions, hence yield better diagnosis accuracy. At last, we validate the superiority of our model on a large-scale EHR dataset.",0
"The Healthcare Graph Convolutional Network (HealGCN) is proposed in this study to enable online users to self-diagnose diseases based on their Electronic Healthcare Records (EHRs). The article focuses on two key challenges associated with online disease diagnosis: (1) using graph convolutional networks to serve cold-start users and (2) handling scarce clinical descriptions through a symptom retrieval system. The EHR data is organized into a heterogeneous graph that models complex interactions between users, symptoms, and diseases. Graph representation learning is tailored towards disease diagnosis using an inductive learning paradigm. A disease self-diagnosis system is then developed using the EHR Graph-based Symptom Retrieval System (GraphRet), which can search for and provide a list of relevant alternative symptoms by tracing predefined meta-paths. GraphRet enriches the seed symptom set through the EHR graph when users have scarce descriptions, resulting in better diagnosis accuracy. Finally, the efficacy of the model is validated on a large-scale EHR dataset.",1
"Contrastive self-supervised learning (CSL) leverages unlabeled data to train models that provide instance-discriminative visual representations uniformly scattered in the feature space. In deployment, the common practice is to directly fine-tune models with the cross-entropy loss, which however may not be an optimal strategy. Although cross-entropy tends to separate inter-class features, the resulted models still have limited capability of reducing intra-class feature scattering that inherits from pre-training, and thus may suffer unsatisfactory performance on downstream tasks. In this paper, we investigate whether applying contrastive learning to fine-tuning would bring further benefits, and analytically find that optimizing the supervised contrastive loss benefits both class-discriminative representation learning and model optimization during fine-tuning. Inspired by these findings, we propose Contrast-regularized tuning (Core-tuning), a novel approach for fine-tuning contrastive self-supervised visual models. Instead of simply adding the contrastive loss to the objective of fine-tuning, Core-tuning also generates hard sample pairs for more effective contrastive learning through a novel feature mixup strategy, as well as improves the generalizability of the model by smoothing the decision boundary via mixed samples. Extensive experiments on image classification and semantic segmentation verify the effectiveness of Core-tuning.",0
"The technique of Contrastive self-supervised learning (CSL) is utilized to develop models that have instance-discriminative visual representations that are evenly spread throughout the feature space, by using unlabeled data. However, in the deployment phase, directly fine-tuning the models with cross-entropy loss may not be the best approach. Although cross-entropy helps separate inter-class features, it does not reduce intra-class feature scattering inherited from pre-training, which can negatively impact downstream tasks. This paper explores the benefits of using contrastive learning during fine-tuning and discovers that optimizing the supervised contrastive loss aids in both class-discriminative representation learning and model optimization during fine-tuning. Based on these findings, the paper introduces Contrast-regularized tuning (Core-tuning), a novel approach for fine-tuning contrastive self-supervised visual models. Core-tuning generates hard sample pairs for more effective contrastive learning through a unique feature mixup strategy and improves model generalizability by smoothing the decision boundary via mixed samples. Extensive experiments on image classification and semantic segmentation confirm the effectiveness of Core-tuning.",1
"Current state-of-the-art self-supervised learning methods for graph neural networks (GNNs) are based on contrastive learning. As such, they heavily depend on the construction of augmentations and negative examples. For example, on the standard PPI benchmark, increasing the number of negative pairs improves performance, thereby requiring computation and memory cost quadratic in the number of nodes to achieve peak performance. Inspired by BYOL, a recently introduced method for self-supervised learning that does not require negative pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph representation method that gets rid of this potentially quadratic bottleneck. BGRL outperforms or matches the previous unsupervised state-of-the-art results on several established benchmark datasets. Moreover, it enables the effective usage of graph attentional (GAT) encoders, allowing us to further improve the state of the art. In particular on the PPI dataset, using GAT as an encoder we achieve state-of-the-art 70.49% Micro-F1, using the linear evaluation protocol. On all other datasets under consideration, our model is competitive with the equivalent supervised GNN results, often exceeding them.",0
"The latest self-supervised learning techniques for graph neural networks (GNNs) rely heavily on contrastive learning and require the creation of augmentations and negative examples. However, this approach presents a quadratic computation and memory cost issue on the standard PPI benchmark. To address this challenge, we introduce Bootstrapped Graph Latents (BGRL), a self-supervised graph representation method inspired by BYOL that eliminates the need for negative pairs. BGRL achieves superior or equivalent results compared to previous unsupervised methods on various benchmark datasets and allows for the effective use of graph attentional (GAT) encoders. Notably, our approach achieves state-of-the-art results on the PPI dataset, reaching a Micro-F1 score of 70.49% using the linear evaluation protocol with GAT as an encoder. Moreover, our model performs competitively with supervised GNN results on all other considered datasets, often surpassing them.",1
"CNN feature spaces can be linearly mapped and consequently are often interchangeable. This equivalence holds across variations in architectures, training datasets, and network tasks. Specifically, we mapped between 10 image-classification CNNs and between 4 facial-recognition CNNs. When image embeddings generated by one CNN are transformed into embeddings corresponding to the feature space of a second CNN trained on the same task, their respective image classification or face verification performance is largely preserved. For CNNs trained to the same classes and sharing a common backend-logit (soft-max) architecture, a linear-mapping may always be calculated directly from the backend layer weights. However, the case of a closed-set analysis with perfect knowledge of classifiers is limiting. Therefore, empirical methods of estimating mappings are presented for both the closed-set image classification task and the open-set task of face recognition. The results presented expose the essentially interchangeable nature of CNNs embeddings for two important and common recognition tasks. The implications are far-reaching, suggesting an underlying commonality between representations learned by networks designed and trained for a common task. One practical implication is that face embeddings from some commonly used CNNs can be compared using these mappings.",0
"CNN feature spaces have the ability to be linearly mapped, making them interchangeable in many cases. This interchangeability applies even when there are variations in network structures, training datasets, and tasks. In our experiment, we mapped 10 image-classification CNNs and 4 facial-recognition CNNs. Our findings revealed that when image embeddings generated by one CNN are transformed into embeddings corresponding to the feature space of a second CNN trained on the same task, their respective image classification or face verification performance is mostly maintained. In cases where CNNs are trained on the same classes and share a common backend-logit architecture, a linear-mapping can be directly calculated from the backend layer weights. However, this approach has limitations when it comes to closed-set analysis with perfect knowledge of classifiers. To address this, we presented empirical methods of estimating mappings for both the closed-set image classification task and the open-set task of face recognition. Our results show that CNNs embeddings are essentially interchangeable for two important and common recognition tasks, suggesting a commonality between representations learned by networks designed and trained for a common task. This finding has practical implications, such as the ability to compare face embeddings from commonly used CNNs using these mappings.",1
"The knowledge that data lies close to a particular submanifold of the ambient Euclidean space may be useful in a number of ways. For instance, one may want to automatically mark any point far away from the submanifold as an outlier, or to use its geodesic distance to measure similarity between points. Classical problems for manifold learning are often posed in a very high dimension, e.g. for spaces of images or spaces of representations of words. Today, with deep representation learning on the rise in areas such as computer vision and natural language processing, many problems of this kind may be transformed into problems of moderately high dimension, typically of the order of hundreds. Motivated by this, we propose a manifold learning technique suitable for moderately high dimension and large datasets. The manifold is learned from the training data in the form of an intersection of quadric hypersurfaces -- simple but expressive objects. At test time, this manifold can be used to introduce an outlier score for arbitrary new points and to improve a given similarity metric by incorporating learned geometric structure into it.",0
"The understanding that data is located in close proximity to a specific submanifold within the larger Euclidean space can be advantageous in multiple ways. For example, one may wish to automatically flag any points that are far from the submanifold as outliers, or use the geodesic distance to quantify the similarity between points. Traditional problems in manifold learning are often presented in very high dimensions, such as for image spaces or word representation spaces. However, with the growing popularity of deep representation learning in fields like computer vision and natural language processing, many of these problems can now be transformed into moderately high-dimensional problems, typically with hundreds of dimensions. In light of this, we offer a manifold learning approach that is suitable for large datasets and moderately high dimensions. The manifold is learned from the training data as an intersection of quadric hypersurfaces, which are both simple and powerful objects. During testing, this manifold can be used to determine an outlier score for new points and enhance a given similarity metric by integrating the learned geometric structure.",1
"As the class size grows, maintaining a balanced dataset across many classes is challenging because the data are long-tailed in nature; it is even impossible when the sample-of-interest co-exists with each other in one collectable unit, e.g., multiple visual instances in one image. Therefore, long-tailed classification is the key to deep learning at scale. However, existing methods are mainly based on re-weighting/re-sampling heuristics that lack a fundamental theory. In this paper, we establish a causal inference framework, which not only unravels the whys of previous methods, but also derives a new principled solution. Specifically, our theory shows that the SGD momentum is essentially a confounder in long-tailed classification. On one hand, it has a harmful causal effect that misleads the tail prediction biased towards the head. On the other hand, its induced mediation also benefits the representation learning and head prediction. Our framework elegantly disentangles the paradoxical effects of the momentum, by pursuing the direct causal effect caused by an input sample. In particular, we use causal intervention in training, and counterfactual reasoning in inference, to remove the ""bad"" while keep the ""good"". We achieve new state-of-the-arts on three long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100, ImageNet-LT for image classification and LVIS for instance segmentation.",0
"Maintaining a balanced dataset becomes increasingly difficult as class sizes grow due to the long-tail nature of the data. This challenge is exacerbated when the sample-of-interest co-exists with others in a collectable unit, such as multiple visual instances in one image. Long-tailed classification is therefore crucial for deep learning at scale. However, current methods rely on re-weighting/re-sampling heuristics without a solid theoretical foundation. In this study, we introduce a causal inference framework that not only explains the rationale behind previous methods but also proposes a new principled solution. Our theory identifies SGD momentum as a confounder in long-tailed classification, with both harmful and beneficial causal effects on tail and head predictions, respectively. By applying causal intervention in training and counterfactual reasoning in inference, our framework disentangles the paradoxical effects of momentum, removing the negative while retaining the positive. Our approach achieves new state-of-the-art results on three long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100, ImageNet-LT for image classification, and LVIS for instance segmentation.",1
"Training a neural network with a large labeled dataset is still a dominant paradigm in computational histopathology. However, obtaining such exhaustive manual annotations is often expensive, laborious, and prone to inter and Intra-observer variability. While recent self-supervised and semi-supervised methods can alleviate this need by learn-ing unsupervised feature representations, they still struggle to generalize well to downstream tasks when the number of labeled instances is small. In this work, we overcome this challenge by leveraging both task-agnostic and task-specific unlabeled data based on two novel strategies: i) a self-supervised pretext task that harnesses the underlying multi-resolution contextual cues in histology whole-slide images to learn a powerful supervisory signal for unsupervised representation learning; ii) a new teacher-student semi-supervised consistency paradigm that learns to effectively transfer the pretrained representations to downstream tasks based on prediction consistency with the task-specific un-labeled data. We carry out extensive validation experiments on three histopathology benchmark datasets across two classification and one regression-based tasks, i.e., tumor metastasis detection, tissue type classification, and tumor cellularity quantification. Under limited-label data, the proposed method yields tangible improvements, which is close or even outperforming other state-of-the-art self-supervised and supervised baselines. Furthermore, we empirically show that the idea of bootstrapping the self-supervised pretrained features is an effective way to improve the task-specific semi-supervised learning on standard benchmarks. Code and pretrained models will be made available at: https://github.com/srinidhiPY/SSL_CR_Histo",0
"The dominant approach in computational histopathology involves training a neural network with a large dataset that has been labeled. However, this process is often costly, time-consuming, and subject to inconsistencies between observers. While self-supervised and semi-supervised methods have been developed to reduce the reliance on labeled data, they struggle to perform well on tasks with limited labeled instances. To address this issue, we have developed two innovative strategies that leverage both task-agnostic and task-specific unlabeled data. The first strategy involves a self-supervised pretext task that uses multi-resolution contextual cues in histology whole-slide images to learn a powerful supervisory signal for unsupervised representation learning. The second strategy is a new teacher-student semi-supervised consistency paradigm that can transfer pretrained representations to downstream tasks based on prediction consistency with task-specific unlabeled data. Our method has been extensively validated on three histopathology benchmark datasets, including tumor metastasis detection, tissue type classification, and tumor cellularity quantification. Our proposed approach outperforms other state-of-the-art self-supervised and supervised baselines when dealing with limited labeled data. We also demonstrate that bootstrapping the self-supervised pretrained features is an effective way to improve task-specific semi-supervised learning on standard benchmarks. Code and pretrained models will be available at: https://github.com/srinidhiPY/SSL_CR_Histo.",1
"We present a novel multiview canonical correlation analysis model based on a variational approach. This is the first nonlinear model that takes into account the available graph-based geometric constraints while being scalable for processing large scale datasets with multiple views. It is based on an autoencoder architecture with graph convolutional neural network layers. We experiment with our approach on classification, clustering, and recommendation tasks on real datasets. The algorithm is competitive with state-of-the-art multiview representation learning techniques.",0
"Our study introduces a new approach to multiview canonical correlation analysis using a variational method. It is the first nonlinear model that accommodates graph-based geometric constraints and can handle the analysis of large datasets with multiple views. The model utilizes an autoencoder architecture with graph convolutional neural network layers. We conducted experiments on actual datasets, applying our approach to classification, clustering, and recommendation tasks. The results show that our algorithm is comparable to other advanced multiview representation learning techniques.",1
"Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA)that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.",0
"Data augmentation is a common technique used to increase the size of datasets by generating synthetic samples in line with the underlying distribution of the data. In order to expand the range of augmentations available, negative data augmentation (NDA) strategies have been explored. NDA intentionally creates out-of-distribution samples, which can provide valuable information about the support of the data distribution and can be used for generative modeling and representation learning. A new GAN training objective has been introduced that incorporates NDA as an additional source of synthetic data for the discriminator. This approach recovers the true data distribution under certain conditions, while also biasing the generator to avoid samples lacking the desired structure. Empirically, models trained with this method have improved image generation and anomaly detection, and incorporating NDA in a contrastive learning framework has led to better performance on image classification, object detection, and action recognition tasks. These findings suggest that knowledge of what does not constitute valid data can serve as a useful form of weak supervision in unsupervised learning.",1
"We propose a method to learn image representations from uncurated videos. We combine a supervised loss from off-the-shelf object detectors and self-supervised losses which naturally arise from the video-shot-frame-object hierarchy present in each video. We report competitive results on 19 transfer learning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8 out-of-distribution-generalization tasks, and discuss the benefits and shortcomings of the proposed approach. In particular, it improves over the baseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution generalization tasks. Finally, we perform several ablation studies and analyze the impact of the pretrained object detector on the performance across this suite of tasks.",0
"Our proposed approach involves acquiring image representations from uncurated videos through a combination of supervised loss from readily available object detectors and self-supervised losses based on the video-shot-frame-object hierarchy. We have achieved impressive results on 19 transfer learning tasks of the Visual Task Adaptation Benchmark (VTAB), as well as 8 out-of-distribution-generalization tasks. We also discuss the advantages and limitations of our technique, which notably outperforms the baseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution generalization tasks. Lastly, we conducted ablation studies to evaluate the impact of the pretrained object detector on the performance of this comprehensive range of tasks.",1
"This paper presents a simple unsupervised visual representation learning method with a pretext task of discriminating all images in a dataset using a parametric, instance-level classifier. The overall framework is a replica of a supervised classification model, where semantic classes (e.g., dog, bird, and ship) are replaced by instance IDs. However, scaling up the classification task from thousands of semantic labels to millions of instance labels brings specific challenges including 1) the large-scale softmax computation; 2) the slow convergence due to the infrequent visiting of instance samples; and 3) the massive number of negative classes that can be noisy. This work presents several novel techniques to handle these difficulties. First, we introduce a hybrid parallel training framework to make large-scale training feasible. Second, we present a raw-feature initialization mechanism for classification weights, which we assume offers a contrastive prior for instance discrimination and can clearly speed up converge in our experiments. Finally, we propose to smooth the labels of a few hardest classes to avoid optimizing over very similar negative pairs. While being conceptually simple, our framework achieves competitive or superior performance compared to state-of-the-art unsupervised approaches, i.e., SimCLR, MoCoV2, and PIC under ImageNet linear evaluation protocol and on several downstream visual tasks, verifying that full instance classification is a strong pretraining technique for many semantic visual tasks.",0
"In this article, a straightforward unsupervised method for visual representation learning is presented, which involves a pretext task of distinguishing all images in a dataset using a parametric, instance-level classifier. The method mimics a supervised classification model, but with instance IDs replacing semantic classes such as dog, bird, and ship. However, when scaling up from thousands of semantic labels to millions of instance labels, there are challenges such as the extensive softmax computation, slow convergence due to infrequent instance visits, and noisy negative classes. To address these issues, several novel techniques are proposed. Firstly, a hybrid parallel training framework is introduced to enable large-scale training. Secondly, a raw-feature initialization mechanism for classification weights is presented, which provides a contrastive prior for instance discrimination and speeds up convergence. Finally, label smoothing is used for a few hardest classes to prevent optimization over very similar negative pairs. Although the framework is conceptually simple, it achieves comparable or superior performance to state-of-the-art unsupervised approaches such as SimCLR, MoCoV2, and PIC under ImageNet linear evaluation protocol and on several downstream visual tasks, demonstrating the effectiveness of full instance classification as a pretraining technique for many semantic visual tasks.",1
"Representation learning for graphs enables the application of standard machine learning algorithms and data analysis tools to graph data. Replacing discrete unordered objects such as graph nodes by real-valued vectors is at the heart of many approaches to learning from graph data. Such vector representations, or embeddings, capture the discrete relationships in the original data by representing nodes as vectors in a high-dimensional space.   In most applications graphs model the relationship between real-life objects and often nodes contain valuable meta-information about the original objects. While being a powerful machine learning tool, embeddings are not able to preserve such node attributes. We address this shortcoming and consider the problem of learning discrete node embeddings such that the coordinates of the node vector representations are graph nodes. This opens the door to designing interpretable machine learning algorithms for graphs as all attributes originally present in the nodes are preserved.   We present a framework for coordinated local graph neighborhood sampling (COLOGNE) such that each node is represented by a fixed number of graph nodes, together with their attributes. Individual samples are coordinated and they preserve the similarity between node neighborhoods. We consider different notions of similarity for which we design scalable algorithms. We show theoretical results for all proposed algorithms. Experiments on benchmark graphs evaluate the quality of the designed embeddings and demonstrate how the proposed embeddings can be used in training interpretable machine learning algorithms for graph data.",0
"Graph representation learning allows for the use of standard machine learning algorithms and data analysis tools on graph data by replacing discrete and unordered graph nodes with real-valued vectors. These vector representations, known as embeddings, capture the discrete relationships in the original data by representing nodes as vectors in a high-dimensional space. However, embeddings do not preserve valuable meta-information about the original objects that nodes represent. To address this issue, we propose the problem of learning discrete node embeddings that preserve the coordinates of the graph nodes, allowing for the design of interpretable machine learning algorithms that retain all attributes originally present in the nodes. Our framework, COLOGNE, coordinates local graph neighborhood sampling such that each node is represented by a fixed number of graph nodes with their attributes intact, preserving the similarity between node neighborhoods. We propose scalable algorithms for different notions of similarity and provide theoretical results for all proposed algorithms. Our experiments on benchmark graphs demonstrate the quality of the designed embeddings and their application in training interpretable machine learning algorithms for graph data.",1
"Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g.message passing methods are run differently from node embedding ones. Despite their apparent differences, all these methods utilize the graph structure, and therefore, their learning can be approximated with stochastic graph traversals. We propose Graph Traversal via Tensor Functionals(GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs. GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly. With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries - with only a handful of lines of code for each method specialization. GTTF and its various GRL implementations are on: https://github.com/isi-usc-edu/gttf.",0
"Various fields, including chemistry and social science, have been impacted by Graph Representation Learning (GRL) methods. However, these methods are typically specialized for specific use-cases, such as running message passing methods differently from node embedding ones. Despite these differences, all GRL methods utilize the graph structure, and therefore, can be approximated with stochastic graph traversals. To address this, we propose Graph Traversal via Tensor Functionals (GTTF), which is a meta-algorithm framework that unifies diverse graph algorithms and enables transparent and efficient scaling to large graphs. GTTF is founded upon a data structure stored as a sparse tensor and a stochastic graph traversal algorithm described using tensor operations. This algorithm is a functional that accepts two functions and can be specialized to obtain various GRL models and objectives by simply changing those two functions. We demonstrate that our algorithm learns in an unbiased fashion and approximates the learning as if the specialized implementations were run directly for a wide class of methods. With only a few lines of code for each method specialization, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries. GTTF and its various GRL implementations are available at https://github.com/isi-usc-edu/gttf.",1
"Data privacy is an increasingly important aspect of many real-world Data sources that contain sensitive information may have immense potential which could be unlocked using the right privacy enhancing transformations, but current methods often fail to produce convincing output. Furthermore, finding the right balance between privacy and utility is often a tricky trade-off. In this work, we propose a novel approach for data privatization, which involves two steps: in the first step, it removes the sensitive information, and in the second step, it replaces this information with an independent random sample. Our method builds on adversarial representation learning which ensures strong privacy by training the model to fool an increasingly strong adversary. While previous methods only aim at obfuscating the sensitive information, we find that adding new random information in its place strengthens the provided privacy and provides better utility at any given level of privacy. The result is an approach that can provide stronger privatization on image data, and yet be preserving both the domain and the utility of the inputs, entirely independent of the downstream task.",0
"In many real-world situations, data privacy is becoming increasingly important. Some data sources contain sensitive information that could be unlocked with the appropriate privacy-enhancing transformations. However, current methods often fail to produce convincing results, and it is challenging to find the right balance between privacy and utility. In this study, we propose a new approach to data privatization that involves two steps. First, the sensitive information is removed, and second, it is replaced with an independent random sample. Our method uses adversarial representation learning to ensure strong privacy by training the model to deceive a powerful adversary continually. While previous methods focus on obscuring the sensitive information, our approach adds new random information to strengthen privacy and improve utility at any given level of privacy. Our method provides stronger privatization on image data while preserving both the domain and the utility of the inputs, completely independent of the downstream task.",1
"Unsupervised representation learning techniques, such as learning word embeddings, have had a significant impact on the field of natural language processing. Similar representation learning techniques have not yet become commonplace in the context of 3D vision. This, despite the fact that the physical 3D spaces have a similar semantic structure to bodies of text: words are surrounded by words that are semantically related, just like objects are surrounded by other objects that are similar in concept and usage.   In this work, we exploit this structure in learning semantically meaningful low dimensional vector representations of objects. We learn these vector representations by mining a dataset of scanned 3D spaces using an unsupervised algorithm. We represent objects as point clouds, a flexible and general representation for 3D data, which we encode into a vector representation. We show that using our method to include context increases the ability of a clustering algorithm to distinguish different semantic classes from each other. Furthermore, we show that our algorithm produces continuous and meaningful object embeddings through interpolation experiments.",0
"Learning word embeddings, an unsupervised representation learning technique, has revolutionized natural language processing. However, similar techniques are not yet widely used in 3D vision, despite the fact that physical 3D spaces have a similar semantic structure to text. Objects in 3D space are surrounded by others that are conceptually and usage-wise similar, just like words in a text. To exploit this structure, we use an unsupervised algorithm to mine a dataset of scanned 3D spaces and learn semantically meaningful low-dimensional vector representations of objects. We represent objects as point clouds, which we encode into vector representations. Our method increases the ability of clustering algorithms to distinguish different semantic classes from each other by including context. Interpolation experiments show that our algorithm produces continuous and meaningful object embeddings.",1
"This paper studies representation learning for multi-task linear bandits and multi-task episodic RL with linear value function approximation. We first consider the setting where we play $M$ linear bandits with dimension $d$ concurrently, and these bandits share a common $k$-dimensional linear representation so that $k\ll d$ and $k \ll M$. We propose a sample-efficient algorithm, MTLR-OFUL, which leverages the shared representation to achieve $\tilde{O}(M\sqrt{dkT} + d\sqrt{kMT} )$ regret, with $T$ being the number of total steps. Our regret significantly improves upon the baseline $\tilde{O}(Md\sqrt{T})$ achieved by solving each task independently. We further develop a lower bound that shows our regret is near-optimal when $d > M$. Furthermore, we extend the algorithm and analysis to multi-task episodic RL with linear value function approximation under low inherent Bellman error \citep{zanette2020learning}. To the best of our knowledge, this is the first theoretical result that characterizes the benefits of multi-task representation learning for exploration in RL with function approximation.",0
"The focus of this research is on representation learning for multi-task linear bandits and multi-task episodic RL using linear value function approximation. The study begins by examining the scenario where $M$ linear bandits are played simultaneously, each with a dimension of $d$, and share a common linear representation of $k$ dimensions, where $k$ is significantly smaller than both $d$ and $M$. To achieve a low regret, we propose the MTLR-OFUL algorithm, which capitalizes on the shared representation and achieves a regret of $\tilde{O}(M\sqrt{dkT} + d\sqrt{kMT} )$, where $T$ is the total number of steps. This regret is much lower than the baseline of $\tilde{O}(Md\sqrt{T})$, which is the regret obtained when each task is solved independently. We also present a lower bound that shows that our regret is nearly optimal when $d > M$. Additionally, we extend the algorithm and analysis to multi-task episodic RL with linear value function approximation under low inherent Bellman error. This research is the first to theoretically characterize the advantages of multi-task representation learning for exploration in RL with function approximation.",1
"Privacy-preserving representation learning (PPRL) aims to learn a data encoding that obfuscates sensitive information and retains target information. We develop the Exclusion-Inclusion Generative Adversarial Network (EIGAN), which generalizes existing adversarial PPRL approaches to account for multiple, potentially overlapping ally and adversary objectives in a dataset. We further extend EIGAN to the case where the data is distributed and cannot be centrally aggregated for training due to privacy constraints. In doing so, we introduce D-EIGAN, the first distributed PPRL method, which decentralizes EIGAN training based on federated learning with fractional parameter sharing. We theoretically analyze the convergence of EIGAN and behavior of adversaries under the optimal EIGAN and D-EIGAN encoders, considering the impact of dependencies among target and sensitive objectives on the encoder performance. Our experiments demonstrate the advantages of EIGAN encodings in terms of accuracy, robustness, and scalability; EIGAN outperforms the previous state-of-the-art in centralized PPRL by a significant margin (47%). The experiments further reveal that D-EIGAN's performance is consistent with that of EIGAN under different node data distributions and is resilient to communication constraints.",0
"The goal of privacy-preserving representation learning (PPRL) is to create a data encoding that conceals sensitive information while preserving target information. Our research introduces the Exclusion-Inclusion Generative Adversarial Network (EIGAN), which improves upon existing adversarial PPRL techniques by accommodating multiple, potentially overlapping ally and adversary objectives within a dataset. We also expand EIGAN to address distributed data, which cannot be centrally aggregated for training due to privacy concerns. This leads us to develop D-EIGAN, the first distributed PPRL method that uses federated learning with fractional parameter sharing. Through theoretical analysis, we examine the convergence of EIGAN and the behavior of adversaries under optimal EIGAN and D-EIGAN encoders, taking into account the impact of dependencies among target and sensitive objectives on encoder performance. Our experiments demonstrate the superior accuracy, robustness, and scalability of EIGAN encodings, which outperform the previous state-of-the-art in centralized PPRL by a significant margin (47%). Additionally, our experiments show that D-EIGAN performs comparably to EIGAN under varying node data distributions and communication constraints.",1
"Missing node attributes is a common problem in real-world graphs. Graph neural networks have been demonstrated powerful in graph representation learning, however, they rely heavily on the completeness of graph information. Few of them consider the incomplete node attributes, which can bring great damage to the performance in practice. In this paper, we propose an innovative node representation learning framework, Wasserstein graph diffusion (WGD), to mitigate the problem. Instead of feature imputation, our method directly learns node representations from the missing-attribute graphs. Specifically, we extend the message passing schema in general graph neural networks to a Wasserstein space derived from the decomposition of attribute matrices. We test WGD in node classification tasks under two settings: missing whole attributes on some nodes and missing only partial attributes on all nodes. In addition, we find WGD is suitable to recover missing values and adapt it to tackle matrix completion problems with graphs of users and items. Experimental results on both tasks demonstrate the superiority of our method.",0
"In real-world graphs, it is common to encounter missing node attributes. While graph neural networks have proven effective in graph representation learning, they heavily rely on complete graph information. Unfortunately, few of them address the issue of incomplete node attributes, which can significantly impair their performance. To address this problem, we introduce a novel node representation learning approach called Wasserstein graph diffusion (WGD). Instead of relying on feature imputation, our method directly learns node representations from incomplete-attribute graphs. We achieve this by extending the message passing schema used in general graph neural networks to a Wasserstein space obtained from the decomposition of attribute matrices. We evaluate WGD in node classification tasks under two settings: missing whole attributes on some nodes and missing partial attributes on all nodes. Additionally, we demonstrate that WGD is capable of recovering missing values and can be applied to matrix completion problems involving graphs of users and items. Our experimental results on both tasks demonstrate the superior performance of our method.",1
"Meta-learning, or learning-to-learn, seeks to design algorithms that can utilize previous experience to rapidly learn new skills or adapt to new environments. Representation learning -- a key tool for performing meta-learning -- learns a data representation that can transfer knowledge across multiple tasks, which is essential in regimes where data is scarce. Despite a recent surge of interest in the practice of meta-learning, the theoretical underpinnings of meta-learning algorithms are lacking, especially in the context of learning transferable representations. In this paper, we focus on the problem of multi-task linear regression -- in which multiple linear regression models share a common, low-dimensional linear representation. Here, we provide provably fast, sample-efficient algorithms to address the dual challenges of (1) learning a common set of features from multiple, related tasks, and (2) transferring this knowledge to new, unseen tasks. Both are central to the general problem of meta-learning. Finally, we complement these results by providing information-theoretic lower bounds on the sample complexity of learning these linear features.",0
"The objective of meta-learning, also known as learning-to-learn, is to create algorithms that can use prior experience to quickly acquire new skills or adapt to new surroundings. Representation learning is a crucial tool for accomplishing meta-learning as it learns a data representation that can transfer knowledge across different tasks, which is vital in situations where there is a scarcity of data. Despite the recent interest in meta-learning, the theoretical foundations of meta-learning algorithms are insufficient, particularly when it comes to learning transferable representations. This study concentrates on the issue of multi-task linear regression, where many linear regression models share a common linear representation that is low-dimensional. We present algorithms that are provably fast and sample-efficient to address the dual challenges of learning a common set of features from multiple associated tasks and transferring this knowledge to new, unseen tasks. Both of these challenges are critical to the general problem of meta-learning. We also provide information-theoretic lower bounds on the sample complexity of learning these linear features to supplement these results.",1
"Disentanglement is a useful property in representation learning which increases the interpretability of generative models such as Variational Auto-Encoders (VAE), Generative Adversarial Models, and their many variants. Typically in such models, an increase in disentanglement performance is traded-off with generation quality. In the context of latent space models, this work presents a representation learning framework that explicitly promotes disentanglement by encouraging orthogonal directions of variations. The proposed objective is the sum of an auto-encoder error term along with a Principal Component Analysis reconstruction error in the feature space. This has an interpretation of a Restricted Kernel Machine with an interconnection matrix on the Stiefel manifold. Our analysis shows that such a construction promotes disentanglement by matching the principal directions in latent space with the directions of orthogonal variation in data space. The training algorithm involves a stochastic optimization method on the Stiefel manifold, which increases only marginally the computing time compared to an analogous VAE. Our theoretical discussion and various experiments show that the proposed model improves over many VAE variants in terms of both generation quality and disentangled representation learning.",0
"Representation learning benefits from the property of disentanglement, which enhances the interpretability of generative models, such as Generative Adversarial Models and Variational Auto-Encoders (VAE), and their variants. However, disentanglement performance is typically inversely related to generation quality in such models. This study proposes a representation learning framework for latent space models that explicitly promotes disentanglement by encouraging orthogonal directions of variations. The objective function combines an auto-encoder error term with a Principal Component Analysis reconstruction error in the feature space and has the interpretation of a Restricted Kernel Machine with an interconnection matrix on the Stiefel manifold. This construction promotes disentanglement by aligning the principal directions in the latent space with orthogonal variations in the data space. The training algorithm uses a stochastic optimization method on the Stiefel manifold, which only slightly increases computing time compared to a similar VAE. Theoretical analysis and experiments demonstrate that the proposed model outperforms many VAE variants concerning both generation quality and disentangled representation learning.",1
"Most artificial neural networks used for object detection and recognition are trained in a fully supervised setup. This is not only very resource consuming as it requires large data sets of labeled examples but also very different from how humans learn. We introduce a setup in which an artificial agent first learns in a simulated world through self-supervised exploration. Following this, the representations learned through interaction with the world can be used to associate semantic concepts such as different types of doors. To do this, we use a method we call fast concept mapping which uses correlated firing patterns of neurons to define and detect semantic concepts. This association works instantaneous with very few labeled examples, similar to what we observe in humans in a phenomenon called fast mapping. Strikingly, this method already identifies objects with as little as one labeled example which highlights the quality of the encoding learned self-supervised through embodiment using curiosity-driven exploration. It therefor presents a feasible strategy for learning concepts without much supervision and shows that through pure interaction with the world meaningful representations of an environment can be learned.",0
"Object detection and recognition through artificial neural networks typically involve a high resource expenditure due to the need for large labeled datasets. Additionally, this approach differs significantly from human learning. Our proposed method involves an artificial agent first learning through self-supervised exploration in a simulated environment. The acquired representations can then be utilized to associate semantic concepts, such as different types of doors, using a technique called fast concept mapping. This method uses correlated firing patterns of neurons to detect and define semantic concepts, similar to the human phenomenon of fast mapping. Impressively, this approach can identify objects with just one labeled example, showcasing the effectiveness of self-supervised learning through curiosity-driven exploration. Our strategy provides a feasible solution for learning concepts without extensive supervision and highlights the ability to learn meaningful representations of an environment through interaction with the world.",1
"The medical field is creating large amount of data that physicians are unable to decipher and use efficiently. Moreover, rule-based expert systems are inefficient in solving complicated medical tasks or for creating insights using big data. Deep learning has emerged as a more accurate and effective technology in a wide range of medical problems such as diagnosis, prediction and intervention. Deep learning is a representation learning method that consists of layers that transform the data non-linearly, thus, revealing hierarchical relationships and structures. In this review we survey deep learning application papers that use structured data, signal and imaging modalities from cardiology. We discuss the advantages and limitations of applying deep learning in cardiology that also apply in medicine in general, while proposing certain directions as the most viable for clinical use.",0
"Physicians are struggling to efficiently use the copious amounts of data being generated in the medical field. Traditional rule-based expert systems are not equipped to handle complex medical tasks or extract insights from big data. However, deep learning technology has emerged as a more precise and potent solution in addressing a range of medical problems, including diagnosis, prediction, and intervention. Deep learning involves layer-based transformation of data, leading to the discovery of hierarchical relationships and structures. This article examines the use of deep learning in cardiology, evaluating its benefits and drawbacks, and offering potential directions for clinical applications that are also relevant to medicine as a whole.",1
"Organ transplantation is often the last resort for treating end-stage illness, but the probability of a successful transplantation depends greatly on compatibility between donors and recipients. Current medical practice relies on coarse rules for donor-recipient matching, but is short of domain knowledge regarding the complex factors underlying organ compatibility. In this paper, we formulate the problem of learning data-driven rules for organ matching using observational data for organ allocations and transplant outcomes. This problem departs from the standard supervised learning setup in that it involves matching the two feature spaces (i.e., donors and recipients), and requires estimating transplant outcomes under counterfactual matches not observed in the data. To address these problems, we propose a model based on representation learning to predict donor-recipient compatibility; our model learns representations that cluster donor features, and applies donor-invariant transformations to recipient features to predict outcomes for a given donor-recipient feature instance. Experiments on semi-synthetic and real-world datasets show that our model outperforms state-of-art allocation methods and policies executed by human experts.",0
"Organ transplantation is often the final option for treating end-stage illness. However, the success of such a procedure depends on the compatibility between the donor and the recipient. Current medical practice uses basic rules for donor-recipient matching but lacks knowledge about the complex factors that influence organ compatibility. This paper proposes a solution to this problem by formulating a data-driven approach to organ matching using observational data. The approach involves matching donors and recipients by estimating transplant outcomes under counterfactual matches not observed in the data. To address these issues, a model based on representation learning is proposed to predict donor-recipient compatibility. This model learns representations that group donor features and applies donor-invariant transformations to recipient features to forecast outcomes for a particular donor-recipient feature instance. The results of experiments on semi-synthetic and real-world datasets demonstrate that this model surpasses current allocation methods and policies implemented by human experts.",1
"For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on ""disentanglement"" measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations, where users interactively modify representations to reconstruct target instances. On synthetic datasets, we find performance on this task much more reliably differentiates entangled and disentangled models than baseline approaches. On a real dataset, we find it differentiates between representation learning methods widely believed but never shown to produce more or less interpretable models. In both cases, we ran small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm that our qualitative and quantitative results agreed.",0
"Many argue that machine learning models must be human-interpretable to be useful in various sociotechnical systems. However, there is no consensus on how to measure interpretability, particularly in representation learning. The focus on ""disentanglement"" measures in interpretability research for synthetic datasets is not grounded in human factors. To address this, we introduce a task to quantify the human-interpretability of generative model representations. Users modify representations to reconstruct target instances, and on synthetic datasets, our task more reliably differentiates between entangled and disentangled models than baseline approaches. We also found that it differentiates representation learning methods that are widely believed but never shown to produce more or less interpretable models on a real dataset. We conducted small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm our qualitative and quantitative results.",1
"The quality of learned features by representation learning determines the performance of learning algorithms and the related application tasks (such as high-dimensional data clustering). As a relatively new paradigm for representation learning, Concept Factorization (CF) has attracted a great deal of interests in the areas of machine learning and data mining for over a decade. Lots of effective CF based methods have been proposed based on different perspectives and properties, but note that it still remains not easy to grasp the essential connections and figure out the underlying explanatory factors from exiting studies. In this paper, we therefore survey the recent advances on CF methodologies and the potential benchmarks by categorizing and summarizing the current methods. Specifically, we first re-view the root CF method, and then explore the advancement of CF-based representation learning ranging from shallow to deep/multilayer cases. We also introduce the potential application areas of CF-based methods. Finally, we point out some future directions for studying the CF-based representation learning. Overall, this survey provides an insightful overview of both theoretical basis and current developments in the field of CF, which can also help the interested researchers to understand the current trends of CF and find the most appropriate CF techniques to deal with particular applications.",0
"The performance of learning algorithms and high-dimensional data clustering tasks depend on the quality of learned features through representation learning. Concept Factorization (CF) has gained significant attention as a relatively new paradigm for representation learning in machine learning and data mining. Despite the numerous effective CF-based methods proposed from different perspectives, grasping the essential connections and explanatory factors remains challenging. This paper surveys recent advancements in CF methodologies and their potential benchmarks, categorizing and summarizing current methods ranging from shallow to deep/multilayer cases. It also explores potential application areas and suggests future research directions. This survey provides an insightful overview of the theoretical basis and current developments in CF, helping researchers understand current trends and choose appropriate CF techniques for specific applications.",1
"Anomaly detection is a challenging problem in machine learning, and is even more so when dealing with instances that are captured in low-level, raw data representations without a well-behaved set of engineered features. The Radial Basis Function Data Descriptor (RBFDD) network is an effective solution for anomaly detection, however, it is a shallow model that does not deal effectively with raw data representations. This paper investigates approaches to modifying the RBFDD network to transform it into a deep one-class classifier suitable for anomaly detection problems with low-level raw data representations. We show that approaches based on transfer learning are not effective and our results suggest that this is because the latent representations learned by generic classification models are not suitable for anomaly detection. Instead we show that an approach that adds multiple convolutional layers before the RBF layer, to form a Deep Radial Basis Function Data Descriptor (D-RBFDD) network, is very effective. This is shown in a set of evaluation experiments using multiple anomaly detection scenarios created from publicly available image classification datasets, and a real-world anomaly detection dataset in which different types of arrhythmia are detected in electrocardiogram (ECG) data. Our experiments show that the D-RBFDD network out-performs state-of-the-art anomaly detection methods including the Deep Support Vector Data Descriptor (Deep SVDD), One-Class SVM, and Isolation Forest on the image datasets, and produces competitive results for the ECG dataset.",0
"Detecting anomalies in machine learning is a difficult task, especially when dealing with raw data representations lacking engineered features. While the Radial Basis Function Data Descriptor (RBFDD) network is a useful solution for anomaly detection, it is limited as a shallow model that struggles with raw data representations. This study explores ways to modify the RBFDD network for deep one-class classification suitable for anomaly detection with low-level raw data representations. Transfer learning approaches are found to be ineffective as the latent representations learned by generic classification models are not suitable for anomaly detection. Instead, the study proposes adding multiple convolutional layers before the RBF layer to create a Deep Radial Basis Function Data Descriptor (D-RBFDD) network, which is highly effective. Evaluation experiments using publicly available image classification datasets and a real-world dataset with electrocardiogram (ECG) data detecting various types of arrhythmia show that the D-RBFDD network outperforms state-of-the-art anomaly detection methods, including the Deep Support Vector Data Descriptor (Deep SVDD), One-Class SVM, and Isolation Forest, on image datasets and produces competitive results for the ECG dataset.",1
"This paper proposes a novel pretext task to address the self-supervised video representation learning problem. Specifically, given an unlabeled video clip, we compute a series of spatio-temporal statistical summaries, such as the spatial location and dominant direction of the largest motion, the spatial location and dominant color of the largest color diversity along the temporal axis, etc. Then a neural network is built and trained to yield the statistical summaries given the video frames as inputs. In order to alleviate the learning difficulty, we employ several spatial partitioning patterns to encode rough spatial locations instead of exact spatial Cartesian coordinates. Our approach is inspired by the observation that human visual system is sensitive to rapidly changing contents in the visual field, and only needs impressions about rough spatial locations to understand the visual contents. To validate the effectiveness of the proposed approach, we conduct extensive experiments with four 3D backbone networks, i.e., C3D, 3D-ResNet, R(2+1)D and S3D-G. The results show that our approach outperforms the existing approaches across these backbone networks on four downstream video analysis tasks including action recognition, video retrieval, dynamic scene recognition, and action similarity labeling. The source code is publicly available at: https://github.com/laura-wang/video_repres_sts.",0
"In this paper, a new method is proposed to tackle the problem of self-supervised video representation learning. The proposed method involves computing various spatio-temporal statistical summaries from an unlabeled video clip, such as the location and dominant direction of the largest motion and the location and dominant color of the largest color diversity over time. A neural network is then built and trained to generate these statistical summaries based on the input video frames. To simplify the learning process, rough spatial locations are encoded using several spatial partitioning patterns instead of exact Cartesian coordinates. The inspiration for this approach comes from the human visual system's sensitivity to rapidly changing visual content and the notion that rough spatial location impressions are sufficient to comprehend the visual content. To demonstrate the effectiveness of this approach, extensive experiments are conducted using four 3D backbone networks and compared to existing approaches on four downstream video analysis tasks (action recognition, video retrieval, dynamic scene recognition, and action similarity labeling). The source code for this approach is publicly available at: https://github.com/laura-wang/video_repres_sts.",1
"Continual learning aims to provide intelligent agents capable of learning multiple tasks sequentially with neural networks. One of its main challenging, catastrophic forgetting, is caused by the neural networks non-optimal ability to learn in non-stationary distributions. In most settings of the current approaches, the agent starts from randomly initialized parameters and is optimized to master the current task regardless of the usefulness of the learned representation for future tasks. Moreover, each of the future tasks uses all the previously learned knowledge although parts of this knowledge might not be helpful for its learning. These cause interference among tasks, especially when the data of previous tasks is not accessible. In this paper, we propose a new method, named Self-Attention Meta-Learner (SAM), which learns a prior knowledge for continual learning that permits learning a sequence of tasks, while avoiding catastrophic forgetting. SAM incorporates an attention mechanism that learns to select the particular relevant representation for each future task. Each task builds a specific representation branch on top of the selected knowledge, avoiding the interference between tasks. We evaluate the proposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the task agnostic inference. We empirically show that we can achieve a better performance than several state-of-the-art methods for continual learning by building on the top of selected representation learned by SAM. We also show the role of the meta-attention mechanism in boosting informative features corresponding to the input data and identifying the correct target in the task agnostic inference. Finally, we demonstrate that popular existing continual learning methods gain a performance boost when they adopt SAM as a starting point.",0
"The goal of continual learning is to equip intelligent agents with neural networks that can learn multiple tasks in sequence. However, a major obstacle is catastrophic forgetting, which occurs when the neural networks struggle to learn in non-stationary distributions. Current approaches typically start with randomly initialized parameters and optimize for the current task without considering the usefulness of the learned representation for future tasks. This leads to interference between tasks and hinders learning, especially when previous task data is inaccessible. To address this issue, we propose a new method called Self-Attention Meta-Learner (SAM) that incorporates an attention mechanism to select relevant representations for each future task. SAM allows for the construction of specific representation branches for each task, thereby avoiding interference between tasks. We evaluate SAM on Split CIFAR-10/100 and Split MNIST benchmarks in task agnostic inference and demonstrate its superior performance compared to existing methods. Furthermore, we show that SAM's meta-attention mechanism boosts informative features and correctly identifies targets in the task agnostic inference. Finally, we demonstrate that existing continual learning methods can benefit from using SAM as a starting point.",1
"This paper presents a novel framework for speech-driven gesture production, applicable to virtual agents to enhance human-computer interaction. Specifically, we extend recent deep-learning-based, data-driven methods for speech-driven gesture generation by incorporating representation learning. Our model takes speech as input and produces gestures as output, in the form of a sequence of 3D coordinates. We provide an analysis of different representations for the input (speech) and the output (motion) of the network by both objective and subjective evaluations. We also analyse the importance of smoothing of the produced motion. Our results indicated that the proposed method improved on our baseline in terms of objective measures. For example, it better captured the motion dynamics and better matched the motion-speed distribution. Moreover, we performed user studies on two different datasets. The studies confirmed that our proposed method is perceived as more natural than the baseline, although the difference in the studies was eliminated by appropriate post-processing: hip-centering and smoothing. We conclude that it is important to take both motion representation and post-processing into account when designing an automatic gesture-production method.",0
"In this article, a new framework for producing gestures based on speech is introduced. This framework can be used to improve human-computer interaction in virtual environments. The framework builds on recent deep-learning-based approaches for generating gestures from speech by including representation learning. The model takes speech as input and generates a sequence of 3D coordinates as output, representing gestures. The paper provides a detailed analysis of different input and output representations, as well as the importance of smoothing the generated motion. The results show that the proposed method outperforms the baseline in terms of objective measures, including better capturing of motion dynamics and matching of motion-speed distribution. User studies were conducted on two datasets, which confirmed that the proposed method produced more natural-looking gestures compared to the baseline. However, this difference was eliminated with appropriate post-processing, such as hip-centering and smoothing. The authors conclude that it is essential to consider both the motion representation and post-processing when designing an automatic gesture-production method.",1
"A steady momentum of innovations and breakthroughs has convincingly pushed the limits of unsupervised image representation learning. Compared to static 2D images, video has one more dimension (time). The inherent supervision existing in such sequential structure offers a fertile ground for building unsupervised learning models. In this paper, we compose a trilogy of exploring the basic and generic supervision in the sequence from spatial, spatiotemporal and sequential perspectives. We materialize the supervisory signals through determining whether a pair of samples is from one frame or from one video, and whether a triplet of samples is in the correct temporal order. We uniquely regard the signals as the foundation in contrastive learning and derive a particular form named Sequence Contrastive Learning (SeCo). SeCo shows superior results under the linear protocol on action recognition (Kinetics), untrimmed activity recognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo demonstrates considerable improvements over recent unsupervised pre-training techniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised ImageNet pre-training in action recognition task on UCF101 and HMDB51, respectively. Source code is available at \url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.",0
"The boundaries of unsupervised image representation learning have been consistently expanded through a series of innovations and breakthroughs. Video, which has an additional dimension of time compared to static 2D images, provides inherent supervision in its sequential structure, making it a fertile ground for building unsupervised learning models. This paper explores the basic and generic supervision in sequences from spatial, spatiotemporal, and sequential perspectives. Supervisory signals are determined by verifying whether a pair of samples is from one frame or one video and whether a triplet of samples is in the correct temporal order. These signals serve as the foundation for contrastive learning, and a particular form called Sequence Contrastive Learning (SeCo) is derived. SeCo outperforms recent unsupervised pre-training techniques and leads the accuracy by 2.96% and 6.47% against fully-supervised ImageNet pre-training in action recognition task on UCF101 and HMDB51, respectively. The source code for SeCo is available at \url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.",1
"Graph representation learning is an important task with applications in various areas such as online social networks, e-commerce networks, WWW, and semantic webs. For unsupervised graph representation learning, many algorithms such as Node2Vec and Graph-SAGE make use of ""negative sampling"" and/or noise contrastive estimation loss. This bears similar ideas to contrastive learning, which ""contrasts"" the node representation similarities of semantically similar (positive) pairs against those of negative pairs. However, despite the success of contrastive learning, we found that directly applying this technique to graph representation learning models (e.g., graph convolutional networks) does not always work. We theoretically analyze the generalization performance and propose a light-weight regularization term that avoids the high scales of node representations' norms and the high variance among them to improve the generalization performance. Our experimental results further validate that this regularization term significantly improves the representation quality across different node similarity definitions and outperforms the state-of-the-art methods.",0
"Learning how to represent graphs is a crucial task for a wide range of applications, including online social networks, e-commerce networks, the World Wide Web, and semantic webs. Many unsupervised graph representation learning algorithms, such as Node2Vec and Graph-SAGE, utilize negative sampling and/or noise contrastive estimation loss. These techniques share similarities with contrastive learning, which contrasts the node representation similarities of semantically similar (positive) pairs against those of negative pairs. However, applying this technique directly to graph representation learning models, such as graph convolutional networks, does not always yield successful results. We have conducted a theoretical analysis of the generalization performance and have proposed a lightweight regularization term that improves the generalization performance by avoiding the high scales of node representations' norms and the high variance among them. Our experimental results have further demonstrated that this regularization term significantly enhances the representation quality across different node similarity definitions and outperforms the state-of-the-art methods.",1
"Prior highly-tuned image parsing models are usually studied in a certain domain with a specific set of semantic labels and can hardly be adapted into other scenarios (e.g., sharing discrepant label granularity) without extensive re-training. Learning a single universal parsing model by unifying label annotations from different domains or at various levels of granularity is a crucial but rarely addressed topic. This poses many fundamental learning challenges, e.g., discovering underlying semantic structures among different label granularity or mining label correlation across relevant tasks. To address these challenges, we propose a graph reasoning and transfer learning framework, named ""Graphonomy"", which incorporates human knowledge and label taxonomy into the intermediate graph representation learning beyond local convolutions. In particular, Graphonomy learns the global and structured semantic coherency in multiple domains via semantic-aware graph reasoning and transfer, enforcing the mutual benefits of the parsing across domains (e.g., different datasets or co-related tasks). The Graphonomy includes two iterated modules: Intra-Graph Reasoning and Inter-Graph Transfer modules. The former extracts the semantic graph in each domain to improve the feature representation learning by propagating information with the graph; the latter exploits the dependencies among the graphs from different domains for bidirectional knowledge transfer. We apply Graphonomy to two relevant but different image understanding research topics: human parsing and panoptic segmentation, and show Graphonomy can handle both of them well via a standard pipeline against current state-of-the-art approaches. Moreover, some extra benefit of our framework is demonstrated, e.g., generating the human parsing at various levels of granularity by unifying annotations across different datasets.",0
"Models for image parsing that have been highly-tuned are typically limited to a specific domain with a set of semantic labels, making it difficult to adapt to other scenarios, such as discrepant label granularity, without significant re-training. Developing a universal parsing model that can accommodate various levels of granularity and domains is an essential but often neglected area of study. This poses several fundamental learning challenges, including identifying underlying semantic structures and mining label correlation across relevant tasks. To address these issues, we introduce the ""Graphonomy"" framework, which incorporates human knowledge and label taxonomy into the intermediate graph representation learning beyond local convolutions. Graphonomy learns global and structured semantic coherency across multiple domains via semantic-aware graph reasoning and transfer, facilitating the mutual benefits of parsing across domains. The framework comprises Intra-Graph Reasoning and Inter-Graph Transfer modules, which extract the semantic graph in each domain and leverage the dependencies among graphs from different domains for bidirectional knowledge transfer. We demonstrate the effectiveness of Graphonomy in two relevant but different image understanding research topics: human parsing and panoptic segmentation. Moreover, our framework offers additional benefits, such as generating human parsing at various levels of granularity by unifying annotations across different datasets.",1
"Electronic health record (EHR) data is sparse and irregular as it is recorded at irregular time intervals, and different clinical variables are measured at each observation point. In this work, we propose a multi-view features integration learning from irregular multivariate time series data by self-attention mechanism in an imputation-free manner. Specifically, we devise a novel multi-integration attention module (MIAM) to extract complex information inherent in irregular time series data. In particular, we explicitly learn the relationships among the observed values, missing indicators, and time interval between the consecutive observations, simultaneously. The rationale behind our approach is the use of human knowledge such as what to measure and when to measure in different situations, which are indirectly represented in the data. In addition, we build an attention-based decoder as a missing value imputer that helps empower the representation learning of the inter-relations among multi-view observations for the prediction task, which operates at the training phase only. We validated the effectiveness of our method over the public MIMIC-III and PhysioNet challenge 2012 datasets by comparing with and outperforming the state-of-the-art methods for in-hospital mortality prediction.",0
"The data in electronic health records (EHR) is not consistent, as it is recorded at different times and there are varying clinical variables measured at each observation point. To address this issue, we suggest a self-attention mechanism for multi-view features integration learning from irregular multivariate time series data without imputation. Our approach involves a unique multi-integration attention module (MIAM) that extracts complex information from irregular time series data by explicitly learning the relationships among the observed values, missing indicators, and time interval between consecutive observations. We use human knowledge to determine what to measure and when to measure in different situations, which indirectly influences the data. Furthermore, we have an attention-based decoder that serves as a missing value imputer and enhances the representation learning of the inter-relations among multi-view observations for the prediction task. Our method has been validated using the public MIMIC-III and PhysioNet challenge 2012 datasets, and it has been proven to outperform state-of-the-art methods for in-hospital mortality prediction.",1
"The rapid spread of COVID-19 cases in recent months has strained hospital resources, making rapid and accurate triage of patients presenting to emergency departments a necessity. Machine learning techniques using clinical data such as chest X-rays have been used to predict which patients are most at risk of deterioration. We consider the task of predicting two types of patient deterioration based on chest X-rays: adverse event deterioration (i.e., transfer to the intensive care unit, intubation, or mortality) and increased oxygen requirements beyond 6 L per day. Due to the relative scarcity of COVID-19 patient data, existing solutions leverage supervised pretraining on related non-COVID images, but this is limited by the differences between the pretraining data and the target COVID-19 patient data. In this paper, we use self-supervised learning based on the momentum contrast (MoCo) method in the pretraining phase to learn more general image representations to use for downstream tasks. We present three results. The first is deterioration prediction from a single image, where our model achieves an area under receiver operating characteristic curve (AUC) of 0.742 for predicting an adverse event within 96 hours (compared to 0.703 with supervised pretraining) and an AUC of 0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours (compared to 0.749 with supervised pretraining). We then propose a new transformer-based architecture that can process sequences of multiple images for prediction and show that this model can achieve an improved AUC of 0.786 for predicting an adverse event at 96 hours and an AUC of 0.848 for predicting mortalities at 96 hours. A small pilot clinical study suggested that the prediction accuracy of our model is comparable to that of experienced radiologists analyzing the same information.",0
"The recent surge in COVID-19 cases has put a strain on hospital resources, necessitating the need for quick and accurate triage of patients in emergency departments. One method used to predict the likelihood of patient deterioration is through machine learning techniques that analyze clinical data, such as chest X-rays. This study focuses on predicting two types of deterioration based on chest X-rays: adverse event deterioration and increased oxygen requirements. However, due to limited COVID-19 patient data, existing solutions rely on supervised pretraining on non-COVID images, which may not generalize well. To address this, the study uses self-supervised learning based on the momentum contrast method during the pretraining phase to learn more generalized image representations. The study presents three main results, the first being that their model achieves an improved AUC for predicting adverse events and oxygen requirements compared to supervised pretraining. They also propose a new transformer-based architecture that can process sequences of multiple images for prediction, which further improves the AUC for predicting adverse events and mortalities at 96 hours. A pilot clinical study showed that the model's prediction accuracy is comparable to that of experienced radiologists analyzing the same information.",1
"We study the generalization of deep learning models in relation to the convex hull of their training sets. A trained image classifier basically partitions its domain via decision boundaries and assigns a class to each of those partitions. The location of decision boundaries inside the convex hull of training set can be investigated in relation to the training samples. However, our analysis shows that in standard image classification datasets, all testing images are considerably outside that convex hull, in the pixel space, in the wavelet space, and in the internal representations learned by deep networks. Therefore, the performance of a trained model partially depends on how its decision boundaries are extended outside the convex hull of its training data. From this perspective which is not studied before, over-parameterization of deep learning models may be considered a necessity for shaping the extension of decision boundaries. At the same time, over-parameterization should be accompanied by a specific training regime, in order to yield a model that not only fits the training set, but also its decision boundaries extend desirably outside the convex hull. To illustrate this, we investigate the decision boundaries of a neural network, with various degrees of parameters, inside and outside the convex hull of its training set. Moreover, we use a polynomial decision boundary to study the necessity of over-parameterization and the influence of training regime in shaping its extensions outside the convex hull of training set.",0
"The focus of our research is the expansion of deep learning models beyond their training set's convex hull. A trained image classifier partitions its domain using decision boundaries to assign classes. We examine the location of these boundaries in relation to the training samples. Our findings show that in standard image classification datasets, testing images fall outside the convex hull in the pixel space, wavelet space, and internal representations learned by deep networks. Therefore, a model's performance depends on extending its decision boundaries beyond the training data's convex hull. Over-parameterization is necessary to achieve this, but a specific training regime is also crucial. To demonstrate this, we analyze decision boundaries of a neural network with varying degrees of parameters inside and outside the convex hull. We also use a polynomial decision boundary to explore the necessity of over-parameterization and the impact of training regime on shaping its extensions beyond the training set's convex hull.",1
"Model-Agnostic Meta-Learning (MAML) and its variants have achieved success in meta-learning tasks on many datasets and settings. On the other hand, we have just started to understand and analyze how they are able to adapt fast to new tasks. For example, one popular hypothesis is that the algorithms learn good representations for transfer, as in multi-task learning. In this work, we contribute by providing a series of empirical and theoretical studies, and discover several interesting yet previously unknown properties of the algorithm. We find MAML adapts better with a deep architecture even if the tasks need only a shallow one (and thus, no representation learning is needed). While echoing previous findings by others that the bottom layers in deep architectures enable representation learning, we also find that upper layers enable fast adaptation by being meta-learned to perform adaptive gradient update when generalizing to new tasks. Motivated by these findings, we study several meta-optimization approaches and propose a new one for learning to optimize adaptively. Those approaches attain stronger performance in meta-learning both shallower and deeper architectures than MAML.",0
"MAML and its variations have been successful in meta-learning tasks across various datasets and settings. However, the mechanism behind their ability to quickly adapt to new tasks is not yet thoroughly understood. Some researchers suggest that these algorithms learn effective representations for transfer, similar to multi-task learning. In this study, we present a series of empirical and theoretical analyses and uncover several previously unknown properties of MAML. We observe that MAML performs better with a deep architecture, even when the tasks require only a shallow one. Our findings indicate that while lower layers in deep architectures enable representation learning, upper layers facilitate rapid adaptation by meta-learning adaptive gradient updates. Building on these insights, we explore different meta-optimization approaches and introduce a new one for adaptive optimization. Our proposed approaches demonstrate superior performance in meta-learning across both shallow and deep architectures compared to MAML.",1
"Recent work on few-shot learning \cite{tian2020rethinking} showed that quality of learned representations plays an important role in few-shot classification performance. On the other hand, the goal of self-supervised learning is to recover useful semantic information of the data without the use of class labels. In this work, we exploit the complementarity of both paradigms via a multi-task framework where we leverage recent self-supervised methods as auxiliary tasks. We found that combining multiple tasks is often beneficial, and that solving them simultaneously can be done efficiently. Our results suggest that self-supervised auxiliary tasks are effective data-dependent regularizers for representation learning. Our code is available at: \url{https://github.com/nathanielsimard/improving-fs-ssl}.",0
"The importance of learned representations in few-shot classification performance was highlighted in recent research on few-shot learning \cite{tian2020rethinking}. Self-supervised learning, on the other hand, aims to retrieve useful semantic data information without the assistance of class labels. This study takes advantage of the complementarity of both approaches by employing a multi-task framework that utilizes modern self-supervised methods as auxiliary tasks. The study discovered that incorporating multiple tasks was frequently advantageous and that addressing them simultaneously was efficient. The findings suggest that self-supervised auxiliary tasks serve as effective data-dependent regularizers for representation learning. The code for this study can be accessed at: \url{https://github.com/nathanielsimard/improving-fs-ssl}.",1
"One-class recognition is traditionally approached either as a representation learning problem or a feature modeling problem. In this work, we argue that both of these approaches have their own limitations; and a more effective solution can be obtained by combining the two. The proposed approach is based on the combination of a generative framework and a one-class classification method. First, we learn generative features using the one-class data with a generative framework. We augment the learned features with the corresponding reconstruction errors to obtain augmented features. Then, we qualitatively identify a suitable feature distribution that reduces the redundancy in the chosen classifier space. Finally, we force the augmented features to take the form of this distribution using an adversarial framework. We test the effectiveness of the proposed method on three one-class classification tasks and obtain state-of-the-art results.",0
"The conventional methods for one-class recognition include representation learning and feature modeling, but both have limitations. This study proposes a more effective solution that combines the two approaches. The proposed method involves a generative framework and a one-class classification method. The generative features are learned using one-class data, and the corresponding reconstruction errors are added to obtain augmented features. A suitable feature distribution is identified to reduce redundancy in the classifier space, and an adversarial framework is used to force the augmented features to match the distribution. The effectiveness of this approach is tested on three one-class classification tasks, and the results show that it outperforms existing methods.",1
"Deep generative models, since their inception, have become increasingly more capable of generating novel and perceptually realistic signals (e.g., images and sound waves). With the emergence of deep models for graph structured data, natural interests seek extensions of these generative models for graphs. Successful extensions were seen recently in the case of learning from a collection of graphs (e.g., protein data banks), but the learning from a single graph has been largely under explored. The latter case, however, is important in practice. For example, graphs in financial and healthcare systems contain so much confidential information that their public accessibility is nearly impossible, but open science in these fields can only advance when similar data are available for benchmarking.   In this work, we propose an approach to generating a doppelganger graph that resembles a given one in many graph properties but nonetheless can hardly be used to reverse engineer the original one, in the sense of a near zero edge overlap. The approach is an orchestration of graph representation learning, generative adversarial networks, and graph realization algorithms. Through comparison with several graph generative models (either parameterized by neural networks or not), we demonstrate that our result barely reproduces the given graph but closely matches its properties. We further show that downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.",0
"Since their inception, deep generative models have become increasingly proficient at generating new and realistic signals such as images and sound waves. With the advent of deep models for graph structured data, there is a natural interest in extending these generative models to graphs. Although successful extensions have been seen in learning from a collection of graphs, the learning from a single graph has been largely unexplored despite its practical importance. Graphs in financial and healthcare systems contain sensitive information, making their public accessibility nearly impossible. However, open science in these fields can only advance when similar data are available for benchmarking. This work proposes an approach to generating a doppelganger graph that closely matches the properties of a given graph but cannot be used to reverse engineer the original one. The approach combines graph representation learning, generative adversarial networks, and graph realization algorithms. Comparison with several graph generative models demonstrates that the resulting graph only vaguely resembles the given graph but closely matches its properties. Downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.",1
"With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.",0
"The emergence of deep representation learning has significantly enhanced the efficiency of reinforcement learning (RL) and made it a highly effective learning framework for acquiring intricate policies in high-dimensional environments. This article offers an overview of deep reinforcement learning (DRL) algorithms and presents a classification of automated driving tasks that have employed (D)RL approaches, highlighting the primary computational challenges in deploying autonomous driving agents in the real world. It also identifies related but non-traditional RL methods such as behavior cloning, imitation learning, and inverse reinforcement learning. The paper further examines the use of simulators for training agents and explores methods for validating, testing, and bolstering existing RL solutions.",1
"Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.",0
"For a considerable amount of time, computer vision experts have been actively researching face recognition, with prior studies primarily focusing on the loss functions applied in facial feature extraction networks. Notably, the enhancements of softmax-based loss functions have significantly improved face recognition performance. However, the increasing number of face identities and the limited GPU memory are becoming a challenging contradiction to overcome. This study examines the optimization goal of softmax-based loss functions and the difficulty of training massive identities. The findings reveal that the significance of negative classes in softmax function for face representation learning is not as crucial as previously believed. The experiment shows that training with only 10% randomly chosen classes for the softmax-based loss functions results in no accuracy loss compared to training with full classes using state-of-the-art models on popular benchmarks. Additionally, the authors introduce an efficient distributed sampling algorithm that considers model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code for this research is available at https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.",1
"Many irregular domains such as social networks, financial transactions, neuron connections, and natural language constructs are represented using graph structures. In recent years, a variety of graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. In many of the real-world applications, the underlying graph changes over time, however, most of the existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs using a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results presented establish a connection between the proposed tensor approach and spectral convolution of tensors. The proposed method TM-GCN is consistent with the Message Passing Neural Network (MPNN) framework, accounting for both spatial and temporal message passing. Numerical experiments on real-world datasets demonstrate the performance of the proposed method for edge classification and link prediction tasks on dynamic graphs. We also consider an application related to the COVID-19 pandemic, and show how our method can be used for early detection of infected individuals from contact tracing data.",0
"Graph structures are commonly used to represent various irregular domains, such as social networks, financial transactions, neuron connections, and natural language constructs. Graph neural networks (GNNs) have been successfully utilized for representation learning and prediction on these graphs. However, most existing GNNs are not suitable for handling dynamic graphs, which are prevalent in real-world applications. To address this challenge, we present a novel technique for learning embeddings of dynamic graphs using a tensor algebra framework. Our approach extends the popular graph convolutional network (GCN) with the tensor M-product technique, enabling the learning of representations of dynamic graphs. The proposed method, TM-GCN, is consistent with the Message Passing Neural Network (MPNN) framework, incorporating both spatial and temporal message passing. Theoretical results demonstrate the connection between the proposed tensor approach and spectral convolution of tensors. We evaluate the performance of TM-GCN with numerical experiments on real-world datasets, specifically edge classification and link prediction tasks on dynamic graphs. Our method also shows potential for early detection of infected individuals from contact tracing data related to the COVID-19 pandemic.",1
"Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.",0
"Adversarial examples can cause a significant shift in the latent network representation of deep neural networks, which makes them vulnerable. To address this issue, we propose a defense strategy called Self-supervised Online Adversarial Purification (SOAP) that combines supervised and self-supervised learning. SOAP uses a self-supervised loss to purify adversarial examples during testing, leveraging the label-independent nature of self-supervised signals. Our approach achieves competitive robust accuracy and requires less training complexity compared to state-of-the-art methods. Furthermore, we demonstrate the robustness of SOAP even when adversaries have knowledge of the purification defense strategy. Our paper is the first to generalize the concept of using self-supervised signals for online test-time purification.",1
"Human pose estimation - the process of recognizing human keypoints in a given image - is one of the most important tasks in computer vision and has a wide range of applications including movement diagnostics, surveillance, or self-driving vehicle. The accuracy of human keypoint prediction is increasingly improved thanks to the burgeoning development of deep learning. Most existing methods solved human pose estimation by generating heatmaps in which the ith heatmap indicates the location confidence of the ith keypoint. In this paper, we introduce novel network structures referred to as multi-resolution representation learning for human keypoint prediction. At different resolutions in the learning process, our networks branch off and use extra layers to learn heatmap generation. We firstly consider the architectures for generating the multi-resolution heatmaps after obtaining the lowest-resolution feature maps. Our second approach allows learning during the process of feature extraction in which the heatmaps are generated at each resolution of the feature extractor. The first and second approaches are referred to as multi-resolution heatmap learning and multi-resolution feature map learning respectively. Our architectures are simple yet effective, achieving good performance. We conducted experiments on two common benchmarks for human pose estimation: MSCOCO and MPII dataset. The code is made publicly available at https://github.com/tqtrunghnvn/SimMRPose.",0
"Recognizing human keypoints in an image, also known as human pose estimation, is a crucial task in computer vision with various applications such as movement diagnostics, surveillance, and self-driving vehicles. The improvement in accuracy of human keypoint prediction is attributed to the advancements in deep learning. Previously, heatmap generation was used to solve the human pose estimation problem, where each heatmap represented the confidence of a specific keypoint. In this study, we propose multi-resolution representation learning network structures for human keypoint prediction. Our networks branch off at different resolutions and use additional layers to generate heatmaps. Two approaches were considered: multi-resolution heatmap learning and multi-resolution feature map learning. Our architectures are uncomplicated yet effective, achieving good performance on two common benchmarks for human pose estimation: MSCOCO and MPII dataset. The code is available at https://github.com/tqtrunghnvn/SimMRPose.",1
"This thesis is a proof of concept for the potential of Variational Auto-Encoder (VAE) on representation learning of real-world Knowledge Graphs (KG). Inspired by successful approaches to the generation of molecular graphs, we evaluate the capabilities of our model, the Relational Graph Variational Auto-Encoder (RGVAE). The impact of the modular hyperparameter choices, encoding through graph convolutions, graph matching and latent space prior, is compared. The RGVAE is first evaluated on link prediction. The mean reciprocal rank (MRR) scores on the two datasets FB15K-237 and WN18RR are compared to the embedding-based model DistMult. A variational DistMult and a RGVAE without latent space prior constraint are implemented as control models. The results show that between different settings, the RGVAE with relaxed latent space, scores highest on both datasets, yet does not outperform the DistMult. Further, we investigate the latent space in a twofold experiment: first, linear interpolation between the latent representation of two triples, then the exploration of each latent dimension in a $95\%$ confidence interval. Both interpolations show that the RGVAE learns to reconstruct the adjacency matrix but fails to disentangle. For the last experiment we introduce a new validation method for the FB15K-237 data set. The relation type-constrains of generated triples are filtered and matched with entity types. The observed rate of valid generated triples is insignificantly higher than the random threshold. All generated and valid triples are unseen. A comparison between different latent space priors, using the $\delta$-VAE method, reveals a decoder collapse. Finally we analyze the limiting factors of our approach compared to molecule generation and propose solutions for the decoder collapse and successful representation learning of multi-relational KGs.",0
"In this thesis, we explore the potential of Variational Auto-Encoder (VAE) for representation learning of real-world Knowledge Graphs (KG), presenting a proof of concept. Our model, the Relational Graph Variational Auto-Encoder (RGVAE), is inspired by successful approaches to molecular graph generation and evaluates the impact of modular hyperparameter choices, encoding through graph convolutions, graph matching, and latent space prior. We compare the performance of the RGVAE with that of the embedding-based model DistMult on link prediction. Our results show that the RGVAE with relaxed latent space performs the best on both datasets, but still does not outperform DistMult. Additionally, we investigate the latent space through two experiments: linear interpolation between the latent representation of two triples and exploration of each latent dimension in a 95% confidence interval. Our findings show that while the RGVAE learns to reconstruct the adjacency matrix, it fails to disentangle. We also introduce a new validation method for the FB15K-237 dataset, where the observed rate of valid generated triples is insignificantly higher than the random threshold. Furthermore, a comparison between different latent space priors using the -VAE method reveals a decoder collapse. Finally, we analyze the limiting factors of our approach compared to molecule generation and propose solutions for successful representation learning of multi-relational KGs.",1
"We propose self-adaptive training -- a unified training algorithm that dynamically calibrates and enhances training process by model predictions without incurring extra computational cost -- to advance both supervised and self-supervised learning of deep neural networks. We analyze the training dynamics of deep networks on training data that are corrupted by, e.g., random noise and adversarial examples. Our analysis shows that model predictions are able to magnify useful underlying information in data and this phenomenon occurs broadly even in the absence of \emph{any} label information, highlighting that model predictions could substantially benefit the training process: self-adaptive training improves the generalization of deep networks under noise and enhances the self-supervised representation learning. The analysis also sheds light on understanding deep learning, e.g., a potential explanation of the recently-discovered double-descent phenomenon in empirical risk minimization and the collapsing issue of the state-of-the-art self-supervised learning algorithms. Experiments on the CIFAR, STL and ImageNet datasets verify the effectiveness of our approach in three applications: classification with label noise, selective classification and linear evaluation. To facilitate future research, the code has been made public available at https://github.com/LayneH/self-adaptive-training.",0
"In this study, we introduce a new training algorithm called self-adaptive training, which can improve the training process of deep neural networks for both supervised and self-supervised learning without increasing computational costs. Our research examines the training dynamics of deep networks and finds that model predictions can uncover useful information in data, even without any label information. This suggests that model predictions can greatly benefit the training process. Our self-adaptive training approach enhances the generalization of deep networks in noisy environments and improves self-supervised representation learning. Additionally, our analysis contributes to a better understanding of deep learning, including the double-descent phenomenon and the collapsing issue of current self-supervised learning algorithms. We conduct experiments on CIFAR, STL, and ImageNet datasets to verify the effectiveness of our approach in various applications, including classification with label noise, selective classification, and linear evaluation. To support future research, we have made the code publicly available at https://github.com/LayneH/self-adaptive-training.",1
"The objective of unsupervised graph representation learning (GRL) is to learn a low-dimensional space of node embeddings that reflect the structure of a given unlabeled graph. Existing algorithms for this task rely on negative sampling objectives that maximize the similarity in node embeddings at nearby nodes (referred to as ""cohesion"") by maintaining positive and negative corpus of node pairs. While positive samples are drawn from node pairs that co-occur in short random walks, conventional approaches construct negative corpus by uniformly sampling random pairs, thus ignoring valuable information about structural dissimilarity among distant node pairs (referred to as ""separation""). In this paper, we present a novel Distance-aware Negative Sampling (DNS) which maximizes the separation of distant node-pairs while maximizing cohesion at nearby node-pairs by setting the negative sampling probability proportional to the pair-wise shortest distances. Our approach can be used in conjunction with any GRL algorithm and we demonstrate the efficacy of our approach over baseline negative sampling methods over downstream node classification tasks on a number of benchmark datasets and GRL algorithms. All our codes and datasets are available at https://github.com/Distance-awareNS/DNS/.",0
"Unsupervised graph representation learning (GRL) aims to acquire a low-dimensional set of node embeddings that mirror the structure of an unlabeled graph. To achieve this, current algorithms rely on negative sampling objectives that maximize similarity between node embeddings of neighboring nodes (known as ""cohesion"") by maintaining a positive and negative corpus of node pairs. Positive samples are derived from node pairs that appear together in short random walks, while negative corpus is established by randomly sampling node pairs uniformly, thus overlooking significant information about structural dissimilarity among distant node pairs (known as ""separation""). This paper introduces a new method called Distance-aware Negative Sampling (DNS) that maximizes separation between distant node pairs while optimizing cohesion between nearby ones by setting the negative sampling probability according to the shortest pairwise distances. This method can be employed with any GRL algorithm, and we demonstrate its effectiveness over baseline negative sampling techniques for downstream node classification tasks across various benchmark datasets and GRL algorithms. All our codes and datasets are accessible at https://github.com/Distance-awareNS/DNS/.",1
"This paper proposes a Disentangled gEnerative cAusal Representation (DEAR) learning method. Unlike existing disentanglement methods that enforce independence of the latent variables, we consider the general case where the underlying factors of interests can be causally correlated. We show that previous methods with independent priors fail to disentangle causally correlated factors. Motivated by this finding, we propose a new disentangled learning method called DEAR that enables causal controllable generation and causal representation learning. The key ingredient of this new formulation is to use a structural causal model (SCM) as the prior for a bidirectional generative model. The prior is then trained jointly with a generator and an encoder using a suitable GAN loss incorporated with supervision. We provide theoretical justification on the identifiability and asymptotic consistency of the proposed method, which guarantees disentangled causal representation learning under appropriate conditions. We conduct extensive experiments on both synthesized and real data sets to demonstrate the effectiveness of DEAR in causal controllable generation, and the benefits of the learned representations for downstream tasks in terms of sample efficiency and distributional robustness.",0
"A new approach to disentangled learning is introduced in this paper, called Disentangled gEnerative cAusal Representation (DEAR) learning. Different from previous methods that require latent variables to be independent, DEAR considers the possibility of causal correlation among the underlying factors of interest. It is demonstrated that disentanglement of causally correlated factors cannot be achieved with methods that use independent priors. In response to this limitation, DEAR is proposed as a new learning method that enables causal controllable generation and causal representation learning. The approach involves using a structural causal model (SCM) as the prior for a bidirectional generative model, with joint training of the prior, generator, and encoder using a GAN loss with supervision. Theoretical justification is provided for the identifiability and asymptotic consistency of DEAR, under certain conditions. Experimental results using synthesized and real datasets demonstrate the effectiveness of DEAR for causal controllable generation and downstream tasks, with improved sample efficiency and distributional robustness.",1
"Short text clustering has far-reaching effects on semantic analysis, showing its importance for multiple applications such as corpus summarization and information retrieval. However, it inevitably encounters the severe sparsity of short text representations, making the previous clustering approaches still far from satisfactory. In this paper, we present a novel attentive representation learning model for shot text clustering, wherein cluster-level attention is proposed to capture the correlations between text representations and cluster representations. Relying on this, the representation learning and clustering for short texts are seamlessly integrated into a unified model. To further ensure robust model training for short texts, we apply adversarial training to the unsupervised clustering setting, by injecting perturbations into the cluster representations. The model parameters and perturbations are optimized alternately through a minimax game. Extensive experiments on four real-world short text datasets demonstrate the superiority of the proposed model over several strong competitors, verifying that robust adversarial training yields substantial performance gains.",0
"The significance of short text clustering in semantic analysis cannot be overstated, as it has numerous applications such as information retrieval and corpus summarization. However, the issue of sparse short text representations poses a challenge to existing clustering approaches, rendering them unsatisfactory. To address this challenge, we present a new attentive representation learning model for short text clustering that employs cluster-level attention to capture the relationship between text and cluster representations. Our model integrates representation learning and clustering into a unified framework, and we ensure robust model training for short texts by applying adversarial training to the unsupervised clustering setting. This involves injecting perturbations into the cluster representations, and optimizing the model parameters and perturbations alternately through a minimax game. Our experiments on four real-world short text datasets demonstrate that our proposed model outperforms several strong competitors, highlighting the substantial performance gains that robust adversarial training can yield.",1
"Graph representation learning has made major strides over the past decade. However, in many relational domains, the input data are not suited for simple graph representations as the relationships between entities go beyond pairwise interactions. In such cases, the relationships in the data are better represented as hyperedges (set of entities) of a non-uniform hypergraph. While there have been works on principled methods for learning representations of nodes of a hypergraph, these approaches are limited in their applicability to tasks on non-uniform hypergraphs (hyperedges with different cardinalities). In this work, we exploit the incidence structure to develop a hypergraph neural network to learn provably expressive representations of variable sized hyperedges which preserve local-isomorphism in the line graph of the hypergraph, while also being invariant to permutations of its constituent vertices. Specifically, for a given vertex set, we propose frameworks for (1) hyperedge classification and (2) variable sized expansion of partially observed hyperedges which captures the higher order interactions among vertices and hyperedges. We evaluate performance on multiple real-world hypergraph datasets and demonstrate consistent, significant improvement in accuracy, over state-of-the-art models.",0
"Over the past ten years, graph representation learning has made significant progress. However, in relational domains, the input data may not be suitable for simple graph representations as the relationships between entities extend beyond pairwise interactions. In such cases, hyperedges (sets of entities) of a non-uniform hypergraph are a better representation of the data's relationships. Although some principled methods exist for learning representations of hypergraph nodes, these approaches have limited applicability to non-uniform hypergraphs with hyperedges of different sizes. This research exploits the incidence structure to introduce a hypergraph neural network that learns expressive representations of variable-sized hyperedges while preserving local-isomorphism in the hypergraph's line graph and being invariant to permutations of its constituent vertices. The research proposes frameworks for hyperedge classification and variable-sized expansion of partially observed hyperedges, capturing the higher-order interactions among vertices and hyperedges. The performance evaluation on several real-world hypergraph datasets demonstrates consistent and significant accuracy improvement over state-of-the-art models.",1
"Recent progress in contrastive learning has revolutionized unsupervised representation learning. Concretely, multiple views (augmentations) from the same image are encouraged to map to the similar embeddings, while views from different images are pulled apart. In this paper, through visualizing and diagnosing classification errors, we observe that current contrastive models are ineffective at localizing the foreground object, limiting their ability to extract discriminative high-level features. This is due to the fact that view generation process considers pixels in an image uniformly. To address this problem, we propose a data-driven approach for learning invariance to backgrounds. It first estimates foreground saliency in images and then creates augmentations by copy-and-pasting the foreground onto a variety of backgrounds. The learning still follows the instance discrimination pretext task, so that the representation is trained to disregard background content and focus on the foreground. We study a variety of saliency estimation methods, and find that most methods lead to improvements for contrastive learning. With this approach (DiLo), significant performance is achieved for self-supervised learning on ImageNet classification, and also for object detection on PASCAL VOC and MSCOCO.",0
"Unsupervised representation learning has been revolutionized by the recent advancements in contrastive learning. Specifically, augmentations from the same image are encouraged to map to similar embeddings, while those from different images are pulled apart, resulting in multiple views. However, we have observed that current contrastive models fail to effectively localize the foreground object, which limits their ability to extract discriminative high-level features. This is because the view generation process treats all pixels in an image equally. To overcome this issue, we propose a data-driven approach to learn invariance to backgrounds. This involves estimating foreground saliency in images and then creating augmentations by pasting the foreground onto various backgrounds. The instance discrimination pretext task is still followed to train the representation to ignore background content and focus on the foreground. We evaluate various saliency estimation methods and find that most of them improve contrastive learning. Our approach, DiLo, achieves significant performance in self-supervised learning for ImageNet classification, as well as in object detection for PASCAL VOC and MSCOCO.",1
"A recent line of work showed that various forms of convolutional kernel methods can be competitive with standard supervised deep convolutional networks on datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while being more amenable to theoretical analysis. In this work, we highlight the importance of a data-dependent feature extraction step that is key to the obtain good performance in convolutional kernel methods. This step typically corresponds to a whitened dictionary of patches, and gives rise to a data-driven convolutional kernel methods. We extensively study its effect, demonstrating it is the key ingredient for high performance of these methods. Specifically, we show that one of the simplest instances of such kernel methods, based on a single layer of image patches followed by a linear classifier is already obtaining classification accuracies on CIFAR-10 in the same range as previous more sophisticated convolutional kernel methods. We scale this method to the challenging ImageNet dataset, showing such a simple approach can exceed all existing non-learned representation methods. This is a new baseline for object recognition without representation learning methods, that initiates the investigation of convolutional kernel models on ImageNet. We conduct experiments to analyze the dictionary that we used, our ablations showing they exhibit low-dimensional properties.",0
"Recent research has demonstrated that convolutional kernel methods, which can be more easily analyzed theoretically, can be just as effective as traditional supervised deep convolutional networks on datasets like CIFAR-10, achieving accuracy rates between 87-90%. To achieve this level of performance, a data-driven feature extraction step is crucial, typically in the form of a whitened dictionary of patches. In this study, we extensively examine the impact of this step and show that it is the key factor in achieving high accuracy rates. Even a simple kernel method based on a single layer of image patches followed by a linear classifier can achieve accuracy rates on CIFAR-10 comparable to more complex methods. We also apply this approach to the ImageNet dataset and demonstrate that it surpasses existing non-learned representation methods, serving as a new baseline for object recognition without representation learning methods. Finally, we examine the dictionary used in our approach and show that it exhibits low-dimensional properties.",1
"Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.",0
"The acquisition of disentangled representations is crucial for developing interpretable models and enabling data generation through style transfer. While this has been thoroughly studied in the context of static data, such as images, using unsupervised learning methods, few works have explored unsupervised disentangled sequential representation learning due to the challenges posed by generating sequential data. This paper introduces the recurrent Wasserstein Autoencoder (R-WAE), a novel framework for generative modeling of sequential data. The R-WAE disentangles input sequences into static and dynamic factors, corresponding to time-invariant and time-varying components, respectively. Our theoretical analysis demonstrates that R-WAE minimizes an upper bound for the penalized form of the Wasserstein distance between the model distribution and the sequential data distribution while simultaneously maximizing the mutual information between the input data and the different disentangled latent factors. This approach outperforms the (recurrent) VAE, which does not explicitly enforce mutual information maximization between the input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, we extend R-WAE to learn a categorical latent representation of actions, enhancing its disentanglement. Our experiments on a variety of datasets demonstrate that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation, both quantitatively and qualitatively.",1
"In the literature of vehicle re-identification (ReID), intensive manual labels such as landmarks, critical parts or semantic segmentation masks are often required to improve the performance. Such extra information helps to detect locally geometric features as a part of representation learning for vehicles. In contrast, in this paper, we aim to address the challenge of {\em automatically} learning to detect geometric features as landmarks {\em with no extra labels}. To the best of our knowledge, we are the {\em first} to successfully learn discriminative geometric features for vehicle ReID based on self-supervised attention. Specifically, we implement an end-to-end trainable deep network architecture consisting of three branches: (1) a global branch as backbone for image feature extraction, (2) an attentional branch for producing attention masks, and (3) a self-supervised branch for regularizing the attention learning with rotated images to locate geometric features. %Our network design naturally leads to an end-to-end multi-task joint optimization. We conduct comprehensive experiments on three benchmark datasets for vehicle ReID, \ie VeRi-776, CityFlow-ReID, and VehicleID, and demonstrate our state-of-the-art performance. %of our approach with the capability of capturing informative vehicle parts with no corresponding manual labels. We also show the good generalization of our approach in other ReID tasks such as person ReID and multi-target multi-camera (MTMC) vehicle tracking. {\em Our demo code is attached in the supplementary file.}",0
"In vehicle re-identification (ReID) literature, extensive manual labels like landmarks, critical parts, or semantic segmentation masks are often essential to enhance performance. Such additional information aids in identifying geometric features locally and is a part of representation learning for vehicles. However, this paper aims to overcome the challenge of learning to detect geometric features as landmarks automatically, without any extra labels. This is achieved through self-supervised attention, making us the first to successfully learn discriminative geometric features for vehicle ReID. Our end-to-end trainable deep network architecture consists of three branches: a global branch for image feature extraction, an attentional branch for producing attention masks, and a self-supervised branch to locate geometric features by regularizing the attention learning with rotated images. We demonstrate our approach's state-of-the-art performance on three benchmark datasets for vehicle ReID and highlight the good generalization for other ReID tasks such as person ReID and multi-target multi-camera (MTMC) vehicle tracking. Our demo code is provided in the supplementary file.",1
"Representation learning (RL) methods learn objects' latent embeddings where information is preserved by distances. Since distances are invariant to certain linear transformations, one may obtain different embeddings while preserving the same information. In dynamic systems, a temporal difference in embeddings may be explained by the stability of the system or by the misalignment of embeddings due to arbitrary transformations. In the literature, embedding alignment has not been defined formally, explored theoretically, or analyzed empirically. Here, we explore the embedding alignment and its parts, provide the first formal definitions, propose novel metrics to measure alignment and stability, and show their suitability through synthetic experiments. Real-world experiments show that both static and dynamic RL methods are prone to produce misaligned embeddings and such misalignment worsens the performance of dynamic network inference tasks. By ensuring alignment, the prediction accuracy raises by up to 90% in static and by 40% in dynamic RL methods.",0
"Representation learning (RL) techniques are utilized to learn the concealed embeddings of objects where information is preserved through distances. As distances are unaffected by specific linear transformations, various embeddings can be obtained while retaining the same information. In dynamic systems, inconsistencies in embeddings may occur due to the stability of the system or arbitrary transformations. Although embedding alignment has not been formally defined, theoretically explored, or empirically analyzed in previous studies, we investigate embedding alignment, its components, and offer initial formal definitions. We propose novel metrics to quantify alignment and stability and demonstrate their effectiveness through synthetic experiments. Our real-world experiments reveal that static and dynamic RL methods are prone to generating misaligned embeddings, and such misalignment deteriorates the performance of dynamic network inference tasks. By ensuring alignment, prediction accuracy improves by up to 90% in static and 40% in dynamic RL methods.",1
"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",0
"Recently, there has been a surge of interest in learning representations for graph-structured data. Graph representation learning methods can be grouped into three categories based on the availability of labeled data. The first category, network embedding, focuses on unsupervised learning of relational structure. The second category, graph regularized neural networks, uses graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third category, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. Despite their popularity, there has been little work on unifying these paradigms. This article proposes a comprehensive taxonomy of representation learning methods for graph-structured data, aimed at unifying disparate bodies of work. The proposed Graph Encoder Decoder Model (GRAPHEDM) generalizes existing algorithms for semi-supervised and unsupervised learning into a single consistent approach. Over thirty existing methods are fitted into this framework to illustrate its generality. This unifying view provides a solid foundation for understanding these methods and enables future research in the area.",1
"Self-supervised representation learning is able to learn semantically meaningful features; however, much of its recent success relies on multiple crops of an image with very few objects. Instead of learning view-invariant representation from simple images, humans learn representations in a complex world with changing scenes by observing object movement, deformation, pose variation, and ego motion. Motivated by this ability, we present a new self-supervised learning representation framework that can be directly deployed on a video stream of complex scenes with many moving objects. Our framework features a simple flow equivariance objective that encourages the network to predict the features of another frame by applying a flow transformation to the features of the current frame. Our representations, learned from high-resolution raw video, can be readily used for downstream tasks on static images. Readout experiments on challenging semantic segmentation, instance segmentation, and object detection benchmarks show that we are able to outperform representations obtained from previous state-of-the-art methods including SimCLR and BYOL.",0
"Although self-supervised representation learning can acquire meaningful features, its recent accomplishments rely on a few objects in multiple crops of an image. In contrast, humans develop representations in a complex world by observing variations such as object movement, ego motion, pose variation, and deformation. We are motivated by this and present a novel self-supervised learning representation framework that can be directly applied to a video stream of complicated scenes with numerous moving objects. Our framework employs a straightforward flow equivariance objective that encourages the network to forecast another frame's features by employing a flow transformation to the current frame's features. Our high-resolution raw video-based representations can be readily utilized for downstream tasks on static images. Readout experiments on challenging benchmarks for semantic segmentation, instance segmentation, and object detection reveal that our representations surpass those acquired from previous state-of-the-art methods like SimCLR and BYOL.",1
"The dominant paradigm for learning video-text representations -- noise contrastive learning -- increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related -- for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of other support samples' visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX and ActivityNet, and MSVD for video-to-text and text-to-video retrieval.",0
"The current approach to learning video-text representations, which is noise contrastive learning, aims to make the representations of related pairs of samples, like those from the same video and text, more similar and those of other pairs more dissimilar. However, we believe that this method is too strict and creates dissimilar representations even for samples that are semantically related, such as videos that are visually similar or depict the same action. To address this issue, we propose a new method that utilizes a generative model to naturally group related samples together. Our method requires each samples caption to be reconstructed using a weighted combination of other support samples visual representations. This approach encourages representations that are not overly specific to individual samples, are reusable across the dataset, and encode shared semantics between samples. Our proposed method outperforms other approaches by a significant margin on a range of datasets, including MSR-VTT, VATEX, ActivityNet, and MSVD, for both video-to-text and text-to-video retrieval.",1
"Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel physics-informed deep learning framework to discover governing partial differential equations (PDEs) from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to (1) approximate the solution of system variables, (2) compute essential derivatives, as well as (3) identify the key derivative terms and parameters that form the structure and explicit expression of the PDEs. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of PDE systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.",0
"Utilizing data to uncover the fundamental governing laws or equations that explain the actions of intricate physical systems can considerably progress our comprehension, simulation, and modeling of such systems across diverse scientific and engineering disciplines. A new approach is presented in this study, which utilizes physics-informed deep learning to identify governing partial differential equations (PDEs) from limited and imprecise data for nonlinear spatiotemporal systems. This method expertly incorporates the advantages of deep neural networks for multifaceted representation learning, physics embedding, automatic differentiation, and sparse regression to (1) approximate the solution of system variables, (2) calculate vital derivatives, and (3) identify the critical derivative terms and parameters that make up the structure and explicit expression of the PDEs. The effectiveness and resilience of this approach are illustrated through numerical and experimental demonstrations of various PDE systems with varying levels of data scarcity and noise, considering different initial/boundary conditions. The outcome of this computational framework highlights the potential for discovering closed-form models in practical applications where gathering vast and precise datasets is unachievable.",1
"We introduce the novel concept of anti-transfer learning for speech processing with convolutional neural networks. While transfer learning assumes that the learning process for a target task will benefit from re-using representations learned for another task, anti-transfer avoids the learning of representations that have been learned for an orthogonal task, i.e., one that is not relevant and potentially misleading for the target task, such as speaker identity for speech recognition or speech content for emotion recognition. In anti-transfer learning, we penalize similarity between activations of a network being trained and another one previously trained on an orthogonal task, which yields more suitable representations. This leads to better generalization and provides a degree of control over correlations that are spurious or undesirable, e.g. to avoid social bias. We have implemented anti-transfer for convolutional neural networks in different configurations with several similarity metrics and aggregation functions, which we evaluate and analyze with several speech and audio tasks and settings, using six datasets. We show that anti-transfer actually leads to the intended invariance to the orthogonal task and to more appropriate features for the target task at hand. Anti-transfer learning consistently improves classification accuracy in all test cases. While anti-transfer creates computation and memory cost at training time, there is relatively little computation cost when using pre-trained models for orthogonal tasks. Anti-transfer is widely applicable and particularly useful where a specific invariance is desirable or where trained models are available and labeled data for orthogonal tasks are difficult to obtain.",0
"Our study introduces the innovative concept of anti-transfer learning for speech processing using convolutional neural networks. Unlike transfer learning, which assumes that re-using representations learned for another task will improve the learning process for a target task, anti-transfer learning avoids learning representations that have been acquired for an unrelated task. These representations may be irrelevant or misleading for the target task, e.g., speaker identity for speech recognition or speech content for emotion recognition. In anti-transfer learning, we penalize the similarity between activations of a network being trained and another network that was trained for an orthogonal task, resulting in more suitable representations. This approach leads to better generalization and provides a degree of control over unwanted correlations, such as social bias. We implemented anti-transfer learning for convolutional neural networks in different configurations with various similarity metrics and aggregation functions, which we evaluated and analyzed using six datasets for different speech and audio tasks. Our results demonstrate that anti-transfer learning leads to the intended invariance to the orthogonal task and more appropriate features for the target task, improving classification accuracy in all test cases. Although anti-transfer learning creates additional computation and memory costs during training, it is widely applicable and particularly useful in scenarios where specific invariance is required or labeled data for orthogonal tasks is difficult to obtain. Pre-trained models for orthogonal tasks can be used to reduce computation costs.",1
"The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.",0
"This paper aims to achieve visual-only self-supervised video representation learning. The paper presents several contributions. Firstly, the authors explore the advantages of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training. Secondly, the authors propose a novel self-supervised co-training scheme to enhance the InfoNCE loss. This method utilizes the RGB streams and optical flow of the same data source by using one view to obtain positive class samples for the other. Thirdly, the authors evaluate the quality of the representation learnt on two different downstream tasks: action recognition and video retrieval. The proposed approach exhibits state-of-the-art or comparable performance with other self-supervised methods, while being considerably more efficient to train, requiring less training data to achieve similar performance.",1
"Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can \emph{also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.",0
"The use of meta-reinforcement learning algorithms can enable robots and other autonomous agents to rapidly acquire new behaviors by making use of past experiences in a related set of training tasks. However, the difficulty of learning from sensory inputs such as images and the extensive data requirements of meta-training have made it challenging to apply meta-RL to real-world robotic systems. In order to accelerate representation learning from visual inputs, latent state models can be used to learn compact state representations from a sequence of observations. This paper argues that latent state models can also perform meta-learning when an appropriate observation space is defined. Using this perspective, the authors present meta-RL with latent dynamics (MELD), an algorithm that leverages inference in a latent state model to quickly acquire new skills given observations and rewards. MELD performs better than previous meta-RL methods in several simulated image-based robotic control problems and allows a real WidowX robotic arm to insert an Ethernet cable into new locations after only 8 hours of real-world meta-training. This is the first meta-RL algorithm trained in a real-world robotic control setting from images.",1
"Recent work on predicting patient outcomes in the Intensive Care Unit (ICU) has focused heavily on the physiological time series data, largely ignoring sparse data such as diagnoses and medications. When they are included, they are usually concatenated in the late stages of a model, which may struggle to learn from rarer disease patterns. Instead, we propose a strategy to exploit diagnoses as relational information by connecting similar patients in a graph. To this end, we propose LSTM-GNN for patient outcome prediction tasks: a hybrid model combining Long Short-Term Memory networks (LSTMs) for extracting temporal features and Graph Neural Networks (GNNs) for extracting the patient neighbourhood information. We demonstrate that LSTM-GNNs outperform the LSTM-only baseline on length of stay prediction tasks on the eICU database. More generally, our results indicate that exploiting information from neighbouring patient cases using graph neural networks is a promising research direction, yielding tangible returns in supervised learning performance on Electronic Health Records.",0
"The recent research on predicting patient outcomes in the Intensive Care Unit (ICU) has primarily focused on physiological time series data, while sparse data such as diagnoses and medications have been overlooked. Even when included, they are often added to the model at a later stage, which could make it difficult to learn from rarer disease patterns. We propose a new approach that utilizes diagnoses as relational information by connecting similar patients in a graph. Our solution is the LSTM-GNN model, which is a hybrid of Long Short-Term Memory networks (LSTMs) and Graph Neural Networks (GNNs). The LSTM-GNN model combines temporal features extraction from LSTMs with patient neighbourhood information extraction from GNNs. We have demonstrated that LSTM-GNNs outperform the LSTM-only baseline in predicting length of stay on the eICU database. Our results suggest that leveraging neighbouring patient cases through graph neural networks is a promising research direction that can lead to significant improvements in supervised learning performance on Electronic Health Records.",1
"In this paper, we study how to simultaneously learn two highly correlated tasks of graph analysis, i.e., community detection and node representation learning. We propose an efficient generative model called VECoDeR for jointly learning Variational Embeddings for Community Detection and node Representation. VECoDeR assumes that every node can be a member of one or more communities. The node embeddings are learned in such a way that connected nodes are not only ""closer"" to each other but also share similar community assignments. A joint learning framework leverages community-aware node embeddings for better community detection. We demonstrate on several graph datasets that VECoDeR effectively out-performs many competitive baselines on all three tasks i.e. node classification, overlapping community detection and non-overlapping community detection. We also show that VECoDeR is computationally efficient and has quite robust performance with varying hyperparameters.",0
"Our research focuses on learning two interconnected tasks in graph analysis, namely, community detection and node representation learning. We present VECoDeR, a productive generative model that enables us to learn Variational Embeddings for Community Detection and node Representation simultaneously. VECoDeR operates on the assumption that each node can belong to one or more communities. The embeddings of nodes are acquired in such a way that connected nodes are not only in close proximity but also share similar community memberships. A joint learning framework utilizes community-aware node embeddings to improve community detection. Our experiments on various graph datasets demonstrate that VECoDeR surpasses many competitive baselines in all three tasks: node classification, overlapping and non-overlapping community detection. We also demonstrate that VECoDeR is efficient in computation and performs consistently well with different hyperparameters.",1
"In machine learning, data is usually represented in a (flat) Euclidean space where distances between points are along straight lines. Researchers have recently considered more exotic (non-Euclidean) Riemannian manifolds such as hyperbolic space which is well suited for tree-like data. In this paper, we propose a representation living on a pseudo-Riemannian manifold of constant nonzero curvature. It is a generalization of hyperbolic and spherical geometries where the nondegenerate metric tensor need not be positive definite. We provide the necessary learning tools in this geometry and extend gradient-based optimization techniques. More specifically, we provide closed-form expressions for distances via geodesics and define a descent direction to minimize some objective function. Our novel framework is applied to graph representations.",0
"Data in machine learning is typically presented in a Euclidean space, where distances between points are linear. However, researchers have recently explored non-Euclidean Riemannian manifolds, such as hyperbolic space, which is ideal for tree-like data. This study introduces a representation that exists on a pseudo-Riemannian manifold of constant, non-zero curvature, which is an extension of hyperbolic and spherical geometries where the metric tensor need not be positive definite. The paper presents learning tools for this geometry and expands gradient-based optimization techniques. Specifically, it provides expressions for distances via geodesics and defines a descent direction to minimize an objective function. This framework is then applied to graph representations.",1
"Network representation learning (NRL) technique has been successfully adopted in various data mining and machine learning applications. Random walk based NRL is one popular paradigm, which uses a set of random walks to capture the network structural information, and then employs word2vec models to learn the low-dimensional representations. However, until now there is lack of a framework, which unifies existing random walk based NRL models and supports to efficiently learn from large networks. The main obstacle comes from the diverse random walk models and the inefficient sampling method for the random walk generation. In this paper, we first introduce a new and efficient edge sampler based on Metropolis-Hastings sampling technique, and theoretically show the convergence property of the edge sampler to arbitrary discrete probability distributions. Then we propose a random walk model abstraction, in which users can easily define different transition probability by specifying dynamic edge weights and random walk states. The abstraction is efficiently supported by our edge sampler, since our sampler can draw samples from unnormalized probability distribution in constant time complexity. Finally, with the new edge sampler and random walk model abstraction, we carefully implement a scalable NRL framework called UniNet. We conduct comprehensive experiments with five random walk based NRL models over eleven real-world datasets, and the results clearly demonstrate the efficiency of UniNet over billion-edge networks.",0
"The NRL technique has found success in various data mining and machine learning applications. One popular NRL paradigm is random walk based, which uses random walks to capture network structural information and word2vec models to learn low-dimensional representations. However, there is currently no unified framework to efficiently learn from large networks due to diverse random walk models and inefficient sampling methods. This paper proposes a new and efficient edge sampler based on Metropolis-Hastings sampling technique, and a random walk model abstraction that allows users to define different transition probabilities. The abstraction is supported by the edge sampler, which can draw samples from unnormalized probability distributions in constant time complexity. This leads to the creation of a scalable NRL framework called UniNet, which is tested on eleven real-world datasets using five random walk based NRL models. Results demonstrate the efficiency of UniNet over billion-edge networks.",1
"Reinforcement Learning has been able to solve many complicated robotics tasks without any need for feature engineering in an end-to-end fashion. However, learning the optimal policy directly from the sensory inputs, i.e the observations, often requires processing and storage of a huge amount of data. In the context of robotics, the cost of data from real robotics hardware is usually very high, thus solutions that achieve high sample-efficiency are needed. We propose a method that aims at learning a mapping from the observations into a lower-dimensional state space. This mapping is learned with unsupervised learning using loss functions shaped to incorporate prior knowledge of the environment and the task. Using the samples from the state space, the optimal policy is quickly and efficiently learned. We test the method on several mobile robot navigation tasks in a simulation environment and also on a real robot.",0
"Reinforcement Learning has proven effective in solving complex robotics tasks without requiring feature engineering, by employing end-to-end approaches. However, the direct learning of the optimal policy from sensory inputs, i.e., observations, often necessitates handling and storing vast amounts of data. In the realm of robotics, obtaining data from actual hardware can be exorbitantly expensive, necessitating the development of solutions that demonstrate high sample-efficiency. Our proposition involves a technique that endeavors to learn a representation of the observations into a state space of lower dimensionality. This representation is acquired using unsupervised learning methodologies that integrate prior knowledge of the task and environment within the loss functions. By utilizing samples obtained from the state space, the optimal policy is learned quickly and efficiently. We evaluate the efficacy of this technique on a variety of mobile robot navigation tasks, both within a simulation environment and with an actual robot.",1
"Generating large-scale synthetic data in simulation is a feasible alternative to collecting/labelling real data for training vision-based deep learning models, albeit the modelling inaccuracies do not generalize to the physical world. In this paper, we present a domain-invariant representation learning (DIRL) algorithm to adapt deep models to the physical environment with a small amount of real data. Existing approaches that only mitigate the covariate shift by aligning the marginal distributions across the domains and assume the conditional distributions to be domain-invariant can lead to ambiguous transfer in real scenarios. We propose to jointly align the marginal (input domains) and the conditional (output labels) distributions to mitigate the covariate and the conditional shift across the domains with adversarial learning, and combine it with a triplet distribution loss to make the conditional distributions disjoint in the shared feature space. Experiments on digit domains yield state-of-the-art performance on challenging benchmarks, while sim-to-real transfer of object recognition for vision-based decluttering with a mobile robot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of a wide variety of objects. Code and supplementary details are available at https://sites.google.com/view/dirl",0
"The possibility of generating synthetic data on a large scale through simulation is a viable option for training vision-based deep learning models instead of collecting and labeling real data. However, the inaccuracies in modeling may not apply to the physical world. This paper presents the domain-invariant representation learning (DIRL) algorithm which can adapt deep models to the physical environment with minimal real data. Existing approaches that only address the covariate shift by aligning marginal distributions across domains and assuming the conditional distributions to be domain-invariant can result in uncertain transfer in real-life situations. To mitigate the covariate and conditional shift across domains, the proposed method aligns both the marginal (input domains) and the conditional (output labels) distributions using adversarial learning, and combines it with a triplet distribution loss to separate the conditional distributions in the shared feature space. The experiments conducted on digit domains demonstrate state-of-the-art performance on challenging benchmarks, while the simulation-to-real transfer of object recognition for vision-based decluttering with a mobile robot increases from 26.8% to 91.0%, resulting in an 86.5% grasping accuracy of various objects. The code and additional details can be found at https://sites.google.com/view/dirl.",1
"We present deep significance clustering (DICE), a framework for jointly performing representation learning and clustering for ""outcome-aware"" stratification. DICE is intended to generate cluster membership that may be used to categorize a population by individual risk level for a targeted outcome. Following the representation learning and clustering steps, we embed the objective function in DICE with a constraint which requires a statistically significant association between the outcome and cluster membership of learned representations. DICE further includes a neural architecture search step to maximize both the likelihood of representation learning and outcome classification accuracy with cluster membership as the predictor. To demonstrate its utility in medicine for patient risk-stratification, the performance of DICE was evaluated using two datasets with different outcome ratios extracted from real-world electronic health records. Outcomes are defined as acute kidney injury (30.4\%) among a cohort of COVID-19 patients, and discharge disposition (36.8\%) among a cohort of heart failure patients, respectively. Extensive results demonstrate that DICE has superior performance as measured by the difference in outcome distribution across clusters, Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index for clustering, and Area under the ROC Curve (AUC) for outcome classification compared to several baseline approaches.",0
"Presented here is the Deep Significance Clustering (DICE) framework, which combines representation learning and clustering to achieve ""outcome-aware"" stratification. DICE generates cluster membership that can be used to classify a given population by individual risk level for a specific outcome. To ensure the statistical significance of the outcome and cluster membership of the learned representations, the objective function in DICE is constrained. Additionally, DICE includes a neural architecture search step to optimize the likelihood of representation learning and outcome classification accuracy using cluster membership as the predictor. To demonstrate the effectiveness of DICE in medical risk stratification, it was evaluated using two datasets extracted from real-world electronic health records with different outcome ratios: acute kidney injury (30.4%) among COVID-19 patients and discharge disposition (36.8%) among heart failure patients. The results indicate that DICE outperforms several baseline approaches in terms of outcome distribution across clusters, Silhouette score, Calinski-Harabasz index, Davies-Bouldin index for clustering, and Area under the ROC Curve (AUC) for outcome classification.",1
"Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved superior performance in tasks such as node classification. However, analyzing heterogeneous graph of different types of nodes and links still brings great challenges for injecting the heterogeneity into a graph neural network. A general remedy is to manually or automatically design meta-paths to transform a heterogeneous graph into a homogeneous graph, but this is suboptimal since the features from the first-order neighbors are not fully leveraged for training and inference. In this paper, we propose simple and effective graph neural networks for heterogeneous graph, excluding the use of meta-paths. Specifically, our models focus on relaxing the heterogeneity stress for model parameters by expanding model capacity of general GNNs in an effective way. Extensive experimental results on six real-world graphs not only show the superior performance of our proposed models over the state-of-the-arts, but also demonstrate the potentially good balance between reducing the heterogeneity stress and increasing the parameter size. Our code is freely available for reproducing our results.",0
"Graph neural networks (GNNs) have been extensively utilized in graph representation learning and have demonstrated exceptional performance in node classification tasks. However, analyzing a diverse range of nodes and links in a heterogeneous graph still poses significant difficulties for integrating heterogeneity into a GNN. One possible solution is to manually or automatically design meta-paths to convert a heterogeneous graph into a homogeneous one. However, this approach is not optimal since it does not fully utilize the features from first-order neighbors for training and inference. Therefore, in this paper, we propose simple and effective GNN models for handling heterogeneous graphs without relying on meta-paths. Our models aim to alleviate the stress on model parameters caused by heterogeneity by expanding the model capacity of general GNNs in a practical manner. We conducted extensive experiments on six real-world graphs, which demonstrate that our proposed models not only outperform existing state-of-the-art methods but also strike a good balance between reducing heterogeneity stress and increasing parameter size. We have made our code available for reproducing our results.",1
"The generalization power of deep-learning models is dependent on rich-labelled data. This supervision using large-scaled annotated information is restrictive in most real-world scenarios where data collection and their annotation involve huge cost. Various domain adaptation techniques exist in literature that bridge this distribution discrepancy. However, a majority of these models require the label sets of both the domains to be identical. To tackle a more practical and challenging scenario, we formulate the problem statement from a partial domain adaptation perspective, where the source label set is a super set of the target label set. Driven by the motivation that image styles are private to each domain, in this work, we develop a method that identifies outlier classes exclusively from image content information and train a label classifier exclusively on class-content from source images. Additionally, elimination of negative transfer of samples from classes private to the source domain is achieved by transforming the soft class-level weights into two clusters, 0 (outlier source classes) and 1 (shared classes) by maximizing the between-cluster variance between them.",0
"Deep-learning models' ability to generalize is reliant on well-labelled data. However, collecting and annotating data can be expensive, making it challenging to obtain large-scaled annotated information for real-world scenarios. Various domain adaptation techniques exist to address this distribution discrepancy; however, most of these models require identical label sets in both domains. To address this practical and challenging scenario, we propose a partial domain adaptation approach where the source label set is a super set of the target label set. Motivated by the fact that image styles are unique to each domain, we develop a method that exclusively identifies outlier classes using image content information and trains a label classifier based on class-content from source images. Furthermore, we eliminate negative transfer of samples from classes exclusive to the source domain by transforming the soft class-level weights into two clusters: 0 (outlier source classes) and 1 (shared classes) by maximizing the between-cluster variance between them.",1
"Unsupervised representation learning is one of the most important problems in machine learning. Recent promising methods are based on contrastive learning. However, contrastive learning often relies on heuristic ideas, and therefore it is not easy to understand what contrastive learning is doing. This paper emphasizes that density ratio estimation is a promising goal for unsupervised representation learning, and promotes understanding to contrastive learning. Our primal contribution is to theoretically show that density ratio estimation unifies three frameworks for unsupervised representation learning: Maximization of mutual information (MI), nonlinear independent component analysis (ICA) and a novel framework for estimation of a lower-dimensional nonlinear subspace proposed in this paper. This unified view clarifies under what conditions contrastive learning can be regarded as maximizing MI, performing nonlinear ICA or estimating the lower-dimensional nonlinear subspace in the proposed framework. Furthermore, we also make theoretical contributions in each of the three frameworks: We show that MI can be maximized through density ratio estimation under certain conditions, while our analysis for nonlinear ICA reveals a novel insight for recovery of the latent source components, which is clearly supported by numerical experiments. In addition, some theoretical conditions are also established to estimate a nonlinear subspace in the proposed framework. Based on the unified view, we propose two practical methods for unsupervised representation learning through density ratio estimation: The first method is an outlier-robust method for representation learning, while the second one is a sample-efficient nonlinear ICA method. Finally, we numerically demonstrate usefulness of the proposed methods in nonlinear ICA and through application to a downstream task for classification.",0
"Unsupervised representation learning is a crucial challenge in machine learning. Recent methods based on contrastive learning have shown promise, but their reliance on heuristic ideas makes it difficult to comprehend their operation. This paper advocates for density ratio estimation as a viable objective for unsupervised representation learning and aims to enhance understanding of contrastive learning. The authors' primary contribution is demonstrating that density ratio estimation unifies three unsupervised representation learning frameworks: mutual information maximization, nonlinear independent component analysis, and a newly proposed framework for estimating a lower-dimensional nonlinear subspace. This unified perspective clarifies the conditions under which contrastive learning maximizes mutual information, performs nonlinear ICA, or estimates the lower-dimensional nonlinear subspace in the proposed framework. The authors also make theoretical contributions to each of these frameworks and propose two practical methods for unsupervised representation learning through density ratio estimation: an outlier-robust method and a sample-efficient nonlinear ICA method. They illustrate the usefulness of their proposed methods through numerical experiments in nonlinear ICA and classification tasks.",1
"We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and loss functions. Compatible means that, if such features are used to compare images, then ""new"" features can be compared directly to ""old"" features, so they can be used interchangeably. This enables visual search systems to bypass computing new features for all previously seen images when updating the embedding models, a process known as backfilling. Backward compatibility is critical to quickly deploy new embedding models that leverage ever-growing large-scale training datasets and improvements in deep learning architectures and training methods. We propose a framework to train embedding models, called backward-compatible training (BCT), as a first step towards backward compatible representation learning. In experiments on learning embeddings for face recognition, models trained with BCT successfully achieve backward compatibility without sacrificing accuracy, thus enabling backfill-free model updates of visual embeddings.",0
"Our proposal offers a method to acquire visual features that are consistent with pre-existing ones, even if they are obtained through different neural network architectures and loss functions and have varying dimensions. This consistency ensures that new and old features can be compared directly, facilitating the interchangeability of these features when comparing images. This approach eliminates the need to compute new features for all previously seen images during the embedding model update process, known as backfilling, allowing for efficient deployment of new embedding models that utilize larger training datasets and improved deep learning architectures and training methods. Our proposed framework, backward-compatible training (BCT), allows for the training of embedding models that are backward-compatible, enabling backfill-free model updates of visual embeddings without sacrificing accuracy. Experiments performed on face recognition embedding models demonstrate the success of our approach in achieving backward compatibility.",1
"Recent advancements in transfer learning have made it a promising approach for domain adaptation via transfer of learned representations. This is especially when relevant when alternate tasks have limited samples of well-defined and labeled data, which is common in the molecule data domain. This makes transfer learning an ideal approach to solve molecular learning tasks. While Adversarial reprogramming has proven to be a successful method to repurpose neural networks for alternate tasks, most works consider source and alternate tasks within the same domain. In this work, we propose a new algorithm, Representation Reprogramming via Dictionary Learning (R2DL), for adversarially reprogramming pretrained language models for molecular learning tasks, motivated by leveraging learned representations in massive state of the art language models. The adversarial program learns a linear transformation between a dense source model input space (language data) and a sparse target model input space (e.g., chemical and biological molecule data) using a k-SVD solver to approximate a sparse representation of the encoded data, via dictionary learning. R2DL achieves the baseline established by state of the art toxicity prediction models trained on domain-specific data and outperforms the baseline in a limited training-data setting, thereby establishing avenues for domain-agnostic transfer learning for tasks with molecule data.",0
"Transfer learning has emerged as a promising strategy for domain adaptation through the transfer of learned representations. This approach is particularly relevant in the domain of molecule data, where alternate tasks often lack labeled data. Therefore, transfer learning is an ideal method to solve molecular learning tasks. However, while Adversarial reprogramming has shown success in repurposing neural networks for alternate tasks, it is mostly limited to tasks within the same domain. This study proposes a new algorithm, Representation Reprogramming via Dictionary Learning (R2DL), which utilizes learned representations in massive language models to adversarially reprogram them for molecular learning tasks. R2DL uses a k-SVD solver to learn a linear transformation between a dense source model input space (language data) and a sparse target model input space (molecule data) through dictionary learning. R2DL achieves the baseline established by state-of-the-art toxicity prediction models trained on domain-specific data and surpasses it in a limited training-data setting, which opens up possibilities for domain-agnostic transfer learning in tasks involving molecule data.",1
"Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.",0
"The objective of person re-identification (Re-ID) is to locate a specific person across various cameras that do not overlap. As deep neural networks have advanced and intelligent video surveillance has become more in demand, the computer vision community has shown a significant increase in interest in Re-ID. The process of creating a person Re-ID system can be divided into two categories: closed-world and open-world settings. The closed-world setting has been widely studied and has achieved excellent results using deep learning techniques on various datasets. We provide a detailed analysis of closed-world person Re-ID from three perspectives: deep feature representation learning, deep metric learning, and ranking optimization. While closed-world Re-ID has reached a performance saturation point, the focus has shifted to the more challenging open-world setting, which is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects and design a powerful AGW baseline that achieves state-of-the-art or at least comparable performance on twelve datasets for four different Re-ID tasks. We introduce a new evaluation metric (mINP) for person Re-ID, which provides an additional criterion to assess the Re-ID system for real applications by indicating the cost of finding all the correct matches. Finally, we discuss some important yet under-investigated open issues.",1
"We present a novel discriminator for GANs that improves realness and diversity of generated samples by learning a structured hypersphere embedding space using spherical circles. The proposed discriminator learns to populate realistic samples around the longest spherical circle, i.e., a great circle, while pushing unrealistic samples toward the poles perpendicular to the great circle. Since longer circles occupy larger area on the hypersphere, they encourage more diversity in representation learning, and vice versa. Discriminating samples based on their corresponding spherical circles can thus naturally induce diversity to generated samples. We also extend the proposed method for conditional settings with class labels by creating a hypersphere for each category and performing class-wise discrimination and update. In experiments, we validate the effectiveness for both unconditional and conditional generation on standard benchmarks, achieving the state of the art.",0
"Our study introduces an innovative discriminator for GANs that enhances the authenticity and variety of the generated samples. We accomplish this by training a structured hypersphere embedding space using spherical circles. Our discriminator is designed to situate realistic samples around the longest spherical circle, which is also known as the great circle, while pushing unrealistic samples towards the poles perpendicular to the great circle. Longer circles occupy larger areas on the hypersphere, which encourages more diversity in representation learning and vice versa. Discriminating samples based on their corresponding spherical circles naturally induces diversity to generated samples. Additionally, we extend our approach to conditional settings with class labels by creating a hypersphere for each category and performing class-wise discrimination and update. We demonstrate the effectiveness of our method for both unconditional and conditional generation on standard benchmarks, achieving the state of the art.",1
"In value-based reinforcement learning (RL), unlike in supervised learning, the agent faces not a single, stationary, approximation problem, but a sequence of value prediction problems. Each time the policy improves, the nature of the problem changes, shifting both the distribution of states and their values. In this paper we take a novel perspective, arguing that the value prediction problems faced by an RL agent should not be addressed in isolation, but rather as a single, holistic, prediction problem. An RL algorithm generates a sequence of policies that, at least approximately, improve towards the optimal policy. We explicitly characterize the associated sequence of value functions and call it the value-improvement path. Our main idea is to approximate the value-improvement path holistically, rather than to solely track the value function of the current policy. Specifically, we discuss the impact that this holistic view of RL has on representation learning. We demonstrate that a representation that spans the past value-improvement path will also provide an accurate value approximation for future policy improvements. We use this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test our hypothesis empirically, we augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path. In a study of Atari 2600 games, the augmented agent achieved approximately double the mean and median performance of the baseline agent.",0
"In supervised learning, the agent faces a single, stationary approximation problem, whereas in value-based reinforcement learning (RL), the agent faces a sequence of value prediction problems. With each improvement in policy, the problem changes, altering both the distribution of states and their values. This paper introduces a novel approach, suggesting that the value prediction problems faced by RL agents should be addressed as a single, comprehensive prediction problem. An RL algorithm generates a sequence of policies that gradually improve towards an optimal policy. The sequence of value functions associated with policy improvements is referred to as the value-improvement path. The paper proposes approximating the value-improvement path holistically, rather than solely tracking the value function of the current policy. This approach has significant implications for representation learning, as a representation that spans the past value-improvement path can provide an accurate value approximation for future policy improvements. The paper uses this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test the hypothesis, the paper augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path and found that the augmented agent achieved approximately double the mean and median performance of the baseline agent in a study of Atari 2600 games.",1
"In recent years, ride-hailing services have been increasingly prevalent as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (e.g., origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.",0
"Ride-hailing services have become increasingly popular due to their convenience for passengers. However, predicting passenger demands in different areas is crucial for effective traffic management and route planning. Past research has evolved from pure time series to graph-structured data, connecting region nodes via various relational edges to model historical passenger demand data. This method allows for the consideration of both spatial and temporal patterns. The constructed graphs encode important information about the directions and volume of passenger demands between two connected regions. However, current graph-based solutions fail to simultaneously consider the dynamic, directed, and weighted (DDW) aspects of the graphs, limiting their expressiveness for passenger demand prediction. Therefore, we propose a new spatiotemporal graph attention network, called Gallat, which comprehensively incorporates the intrinsic properties of DDW graphs. Gallat uses three attention layers to capture spatiotemporal dependencies among different regions across all historical time slots and includes a subtask for pretraining to obtain accurate results quickly. Our experiments on real-world datasets show that Gallat outperforms existing approaches.",1
"Federated Learning (FL) has been proved to be an effective learning framework when data cannot be centralized due to privacy, communication costs, and regulatory restrictions. When training deep learning models under an FL setting, people employ the predefined model architecture discovered in the centralized environment. However, this predefined architecture may not be the optimal choice because it may not fit data with non-identical and independent distribution (non-IID). Thus, we advocate automating federated learning (AutoFL) to improve model accuracy and reduce the manual design effort. We specifically study AutoFL via Neural Architecture Search (NAS), which can automate the design process. We propose a Federated NAS (FedNAS) algorithm to help scattered workers collaboratively searching for a better architecture with higher accuracy. We also build a system based on FedNAS. Our experiments on non-IID dataset show that the architecture searched by FedNAS can outperform the manually predefined architecture.",0
"Federated Learning (FL) has been established as a successful learning framework when data privacy, communication costs, and regulatory constraints prohibit centralization. When training deep learning models in an FL environment, practitioners typically use a pre-established model architecture from the centralized setting. However, this architecture may not be the best choice for non-identical and independent distribution (non-IID) data, leading to suboptimal results. As a solution, we propose automating federated learning (AutoFL) using Neural Architecture Search (NAS) to enhance model accuracy and reduce manual design effort. Our Federated NAS (FedNAS) algorithm enables distributed workers to collaboratively search for a superior architecture with elevated accuracy. Additionally, we have constructed a system based on FedNAS and tested it on a non-IID dataset, demonstrating that the architecture discovered by FedNAS can surpass the manually defined architecture in performance.",1
"Self-supervised pre-training (SSP) employs random image transformations to generate training data for visual representation learning. In this paper, we first present a modeling framework that unifies existing SSP methods as learning to predict pseudo-labels. Then, we propose new data augmentation methods of generating training examples whose pseudo-labels are harder to predict than those generated via random image transformations. Specifically, we use adversarial training and CutMix to create hard examples (HEXA) to be used as augmented views for MoCo-v2 and DeepCluster-v2, leading to two variants HEXA_{MoCo} and HEXA_{DCluster}, respectively. In our experiments, we pre-train models on ImageNet and evaluate them on multiple public benchmarks. Our evaluation shows that the two new algorithm variants outperform their original counterparts, and achieve new state-of-the-art on a wide range of tasks where limited task supervision is available for fine-tuning. These results verify that hard examples are instrumental in improving the generalization of the pre-trained models.",0
"The technique of self-supervised pre-training (SSP) involves using random image transformations to create training data for visual representation learning. This paper introduces a modeling framework that unifies current SSP methods by framing them as learning to predict pseudo-labels. The authors also propose new methods of data augmentation that generate training examples with pseudo-labels that are more challenging to predict than those generated by random image transformations. Adversarial training and CutMix are used to create these challenging examples, which are referred to as hard examples (HEXA) and are used as augmented views for MoCo-v2 and DeepCluster-v2, resulting in two new variants, HEXA_{MoCo} and HEXA_{DCluster}. The models are pre-trained on ImageNet and evaluated on multiple public benchmarks, with the results showing that the new algorithm variants outperform their original counterparts and set new state-of-the-art results in a variety of tasks where limited task supervision is available for fine-tuning. These results demonstrate that hard examples are crucial in enhancing the generalization of pre-trained models.",1
"In order to perform network analysis tasks, representations that capture the most relevant information in the graph structure are needed. However, existing methods do not learn representations that can be interpreted in a straightforward way and that are robust to perturbations to the graph structure. In this work, we address these two limitations by proposing node2coords, a representation learning algorithm for graphs, which learns simultaneously a low-dimensional space and coordinates for the nodes in that space. The patterns that span the low dimensional space reveal the graph's most important structural information. The coordinates of the nodes reveal the proximity of their local structure to the graph structural patterns. In order to measure this proximity by taking into account the underlying graph, we propose to use Wasserstein distances. We introduce an autoencoder that employs a linear layer in the encoder and a novel Wasserstein barycentric layer at the decoder. Node connectivity descriptors, that capture the local structure of the nodes, are passed through the encoder to learn the small set of graph structural patterns. In the decoder, the node connectivity descriptors are reconstructed as Wasserstein barycenters of the graph structural patterns. The optimal weights for the barycenter representation of a node's connectivity descriptor correspond to the coordinates of that node in the low-dimensional space. Experimental results demonstrate that the representations learned with node2coords are interpretable, lead to node embeddings that are stable to perturbations of the graph structure and achieve competitive or superior results compared to state-of-the-art methods in node classification.",0
"To perform network analysis tasks effectively, it is important to have representations that capture the relevant information in a graph structure. However, current methods do not provide easily interpretable representations that are resistant to changes in the graph structure. In this study, we propose a representation learning algorithm called node2coords that overcomes these limitations by simultaneously learning a low-dimensional space and coordinates for nodes within it. The low-dimensional space reveals the most significant structural information in the graph, while the node coordinates reflect the proximity of local structure to the graph's structural patterns. To measure proximity, we use Wasserstein distances. Our autoencoder uses a linear layer in the encoder and a novel Wasserstein barycentric layer in the decoder to reconstruct node connectivity descriptors as Wasserstein barycenters of the graph's structural patterns. The optimal weights for the barycenter representation of a node's connectivity descriptor correspond to its coordinates in the low-dimensional space. Our experimental results show that node2coords provides interpretable representations and achieves competitive or superior results in node classification while remaining stable to perturbations in the graph structure.",1
"In this work, we propose a method to simultaneously perform (i) biometric recognition (i.e., identify the individual), and (ii) device recognition, (i.e., identify the device) from a single biometric image, say, a face image, using a one-shot schema. Such a joint recognition scheme can be useful in devices such as smartphones for enhancing security as well as privacy. We propose to automatically learn a joint representation that encapsulates both biometric-specific and sensor-specific features. We evaluate the proposed approach using iris, face and periocular images acquired using near-infrared iris sensors and smartphone cameras. Experiments conducted using 14,451 images from 15 sensors resulted in a rank-1 identification accuracy of upto 99.81% and a verification accuracy of upto 100% at a false match rate of 1%.",0
"Our work introduces a one-shot method for accomplishing two tasks simultaneously: (i) recognizing a person's biometric data and (ii) identifying the device used to capture the biometric image (such as a face). This joint recognition system can enhance security and privacy on devices like smartphones. Our proposal involves training a model to learn a combined representation that includes both biometric-specific and sensor-specific features. We evaluate our approach using near-infrared iris sensors and smartphone cameras to capture images of faces, irises, and periocular areas. Our experiments, which involved 15 sensors and 14,451 images, resulted in a rank-1 identification accuracy of up to 99.81% and a verification accuracy of up to 100% at a false match rate of 1%.",1
"Representation learning with small labeled data have emerged in many problems, since the success of deep neural networks often relies on the availability of a huge amount of labeled data that is expensive to collect. To address it, many efforts have been made on training sophisticated models with few labeled data in an unsupervised and semi-supervised fashion. In this paper, we will review the recent progresses on these two major categories of methods. A wide spectrum of models will be categorized in a big picture, where we will show how they interplay with each other to motivate explorations of new ideas. We will review the principles of learning the transformation equivariant, disentangled, self-supervised and semi-supervised representations, all of which underpin the foundation of recent progresses. Many implementations of unsupervised and semi-supervised generative models have been developed on the basis of these criteria, greatly expanding the territory of existing autoencoders, generative adversarial nets (GANs) and other deep networks by exploring the distribution of unlabeled data for more powerful representations. We will discuss emerging topics by revealing the intrinsic connections between unsupervised and semi-supervised learning, and propose in future directions to bridge the algorithmic and theoretical gap between transformation equivariance for unsupervised learning and supervised invariance for supervised learning, and unify unsupervised pretraining and supervised finetuning. We will also provide a broader outlook of future directions to unify transformation and instance equivariances for representation learning, connect unsupervised and semi-supervised augmentations, and explore the role of the self-supervised regularization for many learning problems.",0
"Many problems rely on deep neural networks, but collecting a large amount of labeled data can be expensive. To address this issue, sophisticated models have been developed for training with few labeled data in an unsupervised or semi-supervised manner. This paper reviews recent progress in both categories and categorizes a wide spectrum of models to explore new ideas. The principles of learning transformation equivariant, disentangled, self-supervised, and semi-supervised representations are discussed, providing a foundation for recent progress. Unsupervised and semi-supervised generative models have greatly expanded the territory of existing autoencoders, generative adversarial nets, and other deep networks by exploring the distribution of unlabeled data for more powerful representations. The paper discusses emerging topics and proposes future directions to bridge the gap between transformation equivariance for unsupervised learning and supervised invariance for supervised learning, unify unsupervised pretraining and supervised finetuning, and explore the role of self-supervised regularization for many learning problems. Additionally, the paper provides a broader outlook of future directions to unify transformation and instance equivariances for representation learning, connect unsupervised and semi-supervised augmentations, and explore the role of self-supervised regularization.",1
"Graphs are often used to organize data because of their simple topological structure, and therefore play a key role in machine learning. And it turns out that the low-dimensional embedded representation obtained by graph representation learning are extremely useful in various typical tasks, such as node classification, content recommendation and link prediction. However, the existing methods mostly start from the microstructure (i.e., the edges) in the graph, ignoring the mesoscopic structure (high-order local structure). Here, we propose wGCN -- a novel framework that utilizes random walk to obtain the node-specific mesoscopic structures of the graph, and utilizes these mesoscopic structures to reconstruct the graph And organize the characteristic information of the nodes. Our method can effectively generate node embeddings for previously unseen data, which has been proven in a series of experiments conducted on citation networks and social networks (our method has advantages over baseline methods). We believe that combining high-order local structural information can more efficiently explore the potential of the network, which will greatly improve the learning efficiency of graph neural network and promote the establishment of new learning models.",0
"Graphs are frequently utilized to arrange data due to their uncomplicated topological design, making them an essential aspect of machine learning. Graph representation learning produces beneficial low-dimensional embedded representations for various tasks like node classification, content recommendation, and link prediction. However, current methods tend to disregard the mesoscopic structure (high-order local structure) while focusing on the microstructure (i.e., edges) of the graph. To address this, we propose wGCN, a novel framework that leverages random walk to obtain node-specific mesoscopic structures, which are then utilized to reconstruct the graph and organize the nodes' characteristic information. Our approach generates node embeddings for previously unseen data and has been proven effective in experiments conducted on citation networks and social networks, outperforming baseline methods. Incorporating high-order local structural information can enhance network potential and significantly improve graph neural network learning efficiency, paving the way for new learning models.",1
"Deep generative models have achieved great success in areas such as image, speech, and natural language processing in the past few years. Thanks to the advances in graph-based deep learning, and in particular graph representation learning, deep graph generation methods have recently emerged with new applications ranging from discovering novel molecular structures to modeling social networks. This paper conducts a comprehensive survey on deep learning-based graph generation approaches and classifies them into five broad categories, namely, autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators, providing the readers a detailed description of the methods in each class. We also present publicly available source codes, commonly used datasets, and the most widely utilized evaluation metrics. Finally, we highlight the existing challenges and discuss future research directions.",0
"In recent years, deep generative models have been highly successful in various fields such as image, speech, and natural language processing. With advancements in graph-based deep learning, specifically in graph representation learning, deep graph generation techniques have emerged and found new applications in areas such as modeling social networks and discovering novel molecular structures. This paper aims to conduct a comprehensive survey of deep learning-based graph generation approaches and categorize them into five main groups including autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators. Each category is described in detail along with publicly available source codes, commonly used datasets, and widely utilized evaluation metrics. Furthermore, this paper also addresses the current challenges and potential future research directions.",1
"Representation learning has overcome the often arduous and manual featurization of networks through (unsupervised) feature learning as it results in embeddings that can apply to a variety of downstream learning tasks. The focus of representation learning on graphs has focused mainly on shallow (node-centric) or deep (graph-based) learning approaches. While there have been approaches that work on homogeneous and heterogeneous networks with multi-typed nodes and edges, there is a gap in learning edge representations. This paper proposes a novel unsupervised inductive method called AttrE2Vec, which learns a low-dimensional vector representation for edges in attributed networks. It systematically captures the topological proximity, attributes affinity, and feature similarity of edges. Contrary to current advances in edge embedding research, our proposal extends the body of methods providing representations for edges, capturing graph attributes in an inductive and unsupervised manner. Experimental results show that, compared to contemporary approaches, our method builds more powerful edge vector representations, reflected by higher quality measures (AUC, accuracy) in downstream tasks as edge classification and edge clustering. It is also confirmed by analyzing low-dimensional embedding projections.",0
"Representation learning has made the process of network featurization simpler and less manual by utilizing unsupervised feature learning to generate embeddings that can be applied to various downstream learning tasks. The focus of representation learning on graphs has mainly been on shallow or deep learning approaches, although there is a gap in edge representation learning for homogeneous and heterogeneous networks with multi-typed nodes and edges. This paper introduces a new unsupervised inductive method called AttrE2Vec that learns a low-dimensional vector representation for edges in attributed networks by systematically capturing the topological proximity, attributes affinity, and feature similarity of edges. Our method extends the current body of edge embedding research by providing edge representations that capture graph attributes in an inductive and unsupervised manner. Experimental results show that our method outperforms contemporary approaches in edge classification and edge clustering, as reflected by higher quality measures such as AUC and accuracy, and confirmed through low-dimensional embedding projections.",1
"Heterogeneous graphs are pervasive in practical scenarios, where each graph consists of multiple types of nodes and edges. Representation learning on heterogeneous graphs aims to obtain low-dimensional node representations that could preserve both node attributes and relation information. However, most of the existing graph convolution approaches were designed for homogeneous graphs, and therefore cannot handle heterogeneous graphs. Some recent methods designed for heterogeneous graphs are also faced with several issues, including the insufficient utilization of heterogeneous properties, structural information loss, and lack of interpretability. In this paper, we propose HGConv, a novel Heterogeneous Graph Convolution approach, to learn comprehensive node representations on heterogeneous graphs with a hybrid micro/macro level convolutional operation. Different from existing methods, HGConv could perform convolutions on the intrinsic structure of heterogeneous graphs directly at both micro and macro levels: A micro-level convolution to learn the importance of nodes within the same relation, and a macro-level convolution to distinguish the subtle difference across different relations. The hybrid strategy enables HGConv to fully leverage heterogeneous information with proper interpretability. Moreover, a weighted residual connection is designed to aggregate both inherent attributes and neighbor information of the focal node adaptively. Extensive experiments on various tasks demonstrate not only the superiority of HGConv over existing methods, but also the intuitive interpretability of our approach for graph analysis.",0
"In practical situations, heterogeneous graphs are commonly used, which consist of multiple types of nodes and edges. The goal of representation learning on heterogeneous graphs is to obtain low-dimensional node representations that preserve both node attributes and relation information. However, existing graph convolution approaches are designed for homogeneous graphs and cannot handle heterogeneous graphs. Recently proposed methods for heterogeneous graphs face several issues, such as insufficient utilization of heterogeneous properties, structural information loss, and lack of interpretability. In this paper, we propose HGConv, a novel approach for Heterogeneous Graph Convolution, which uses a hybrid micro/macro level convolutional operation to learn comprehensive node representations on heterogeneous graphs. HGConv can perform convolutions on the intrinsic structure of heterogeneous graphs directly at both micro and macro levels, allowing it to fully leverage heterogeneous information with proper interpretability. A weighted residual connection is designed to adaptively aggregate both inherent attributes and neighbor information of the focal node. Extensive experiments on various tasks demonstrate the superiority of HGConv over existing methods, as well as the intuitive interpretability of our approach for graph analysis.",1
"Given a signed social graph, how can we learn appropriate node representations to infer the signs of missing edges? Signed social graphs have received considerable attention to model trust relationships. Learning node representations is crucial to effectively analyze graph data, and various techniques such as network embedding and graph convolutional network (GCN) have been proposed for learning signed graphs. However, traditional network embedding methods are not end-to-end for a specific task such as link sign prediction, and GCN-based methods suffer from a performance degradation problem when their depth increases. In this paper, we propose Signed Graph Diffusion Network (SGDNet), a novel graph neural network that achieves end-to-end node representation learning for link sign prediction in signed social graphs. We propose a random walk technique specially designed for signed graphs so that SGDNet effectively diffuses hidden node features. Through extensive experiments, we demonstrate that SGDNet outperforms state-of-the-art models in terms of link sign prediction accuracy.",0
"How can we obtain appropriate node representations to predict the signs of missing edges in a signed social graph? Signed graphs are widely used to model trust relationships, and learning node representations is crucial for effective graph analysis. Previous techniques, such as network embedding and graph convolutional network (GCN), have been proposed to learn representations for signed graphs. However, traditional network embedding methods are not task-specific for link sign prediction, and GCN-based methods may experience performance degradation with increasing depth. To address this issue, we introduce Signed Graph Diffusion Network (SGDNet), a novel graph neural network that enables end-to-end node representation learning for link sign prediction in signed social graphs. We develop a random walk approach tailored for signed graphs, allowing SGDNet to effectively diffuse hidden node features. Experimental results show that SGDNet outperforms existing models in terms of link sign prediction accuracy.",1
"The ability to segment teeth precisely from digitized 3D dental models is an essential task in computer-aided orthodontic surgical planning. To date, deep learning based methods have been popularly used to handle this task. State-of-the-art methods directly concatenate the raw attributes of 3D inputs, namely coordinates and normal vectors of mesh cells, to train a single-stream network for fully-automated tooth segmentation. This, however, has the drawback of ignoring the different geometric meanings provided by those raw attributes. This issue might possibly confuse the network in learning discriminative geometric features and result in many isolated false predictions on the dental model. Against this issue, we propose a two-stream graph convolutional network (TSGCNet) to learn multi-view geometric information from different geometric attributes. Our TSGCNet adopts two graph-learning streams, designed in an input-aware fashion, to extract more discriminative high-level geometric representations from coordinates and normal vectors, respectively. These feature representations learned from the designed two different streams are further fused to integrate the multi-view complementary information for the cell-wise dense prediction task. We evaluate our proposed TSGCNet on a real-patient dataset of dental models acquired by 3D intraoral scanners, and experimental results demonstrate that our method significantly outperforms state-of-the-art methods for 3D shape segmentation.",0
"Precisely segmenting teeth from digitized 3D dental models is a crucial task in computer-aided orthodontic surgical planning. Deep learning methods have been commonly utilized for this purpose, with state-of-the-art techniques training a single-stream network by concatenating the raw attributes of 3D inputs (coordinates and normal vectors of mesh cells). However, this approach ignores the distinct geometric meanings provided by these attributes, potentially leading to confusion during network learning and resulting in isolated false predictions. To address this issue, we propose a two-stream graph convolutional network (TSGCNet) that extracts multi-view geometric information from different geometric attributes. Our TSGCNet features two input-aware graph-learning streams that extract high-level geometric representations from coordinates and normal vectors, respectively. These representations are fused to integrate multi-view complementary information for cell-wise dense prediction. Evaluation on a real-patient dataset of dental models shows that our TSGCNet significantly outperforms state-of-the-art methods for 3D shape segmentation.",1
"Introspection of deep supervised predictive models trained on functional and structural brain imaging may uncover novel markers of Alzheimer's disease (AD). However, supervised training is prone to learning from spurious features (shortcut learning) impairing its value in the discovery process. Deep unsupervised and, recently, contrastive self-supervised approaches, not biased to classification, are better candidates for the task. Their multimodal options specifically offer additional regularization via modality interactions. In this paper, we introduce a way to exhaustively consider multimodal architectures for contrastive self-supervised fusion of fMRI and MRI of AD patients and controls. We show that this multimodal fusion results in representations that improve the results of the downstream classification for both modalities. We investigate the fused self-supervised features projected into the brain space and introduce a numerically stable way to do so.",0
"The examination of profound supervised predictive models that were trained on functional and structural brain imaging may expose innovative indicators of Alzheimer's disease. However, supervised training is exposed to learning from irrelevant features, which can lessen its value in the process of discovery. Deep unsupervised and recently introduced contrastive self-supervised methods are superior options for this task since they are not inclined towards classification. Their multimodal capabilities provide additional regularization through interactions between modalities. This study presents a comprehensive approach to considering multimodal architectures for the contrastive self-supervised merging of fMRI and MRI for AD patients and controls. The results show that the fusion of these multimodal features enhances downstream classification results for both modalities. The study also examines the fused self-supervised features projected into the brain space and introduces a stable numerical method to do so.",1
"Deep learning and convolutional neural networks (CNNs) have made progress in polarimetric synthetic aperture radar (PolSAR) image classification over the past few years. However, a crucial issue has not been addressed, i.e., the requirement of CNNs for abundant labeled samples versus the insufficient human annotations of PolSAR images. It is well-known that following the supervised learning paradigm may lead to the overfitting of training data, and the lack of supervision information of PolSAR images undoubtedly aggravates this problem, which greatly affects the generalization performance of CNN-based classifiers in large-scale applications. To handle this problem, in this paper, learning transferrable representations from unlabeled PolSAR data through convolutional architectures is explored for the first time. Specifically, a PolSAR-tailored contrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR representation learning and few-shot classification. Different from the utilization of optical processing methods, a diversity stimulation mechanism is constructed to narrow the application gap between optics and PolSAR. Beyond the conventional supervised methods, PCLNet develops an unsupervised pre-training phase based on the proxy objective of instance discrimination to learn useful representations from unlabeled PolSAR data. The acquired representations are transferred to the downstream task, i.e., few-shot PolSAR classification. Experiments on two widely-used PolSAR benchmark datasets confirm the validity of PCLNet. Besides, this work may enlighten how to efficiently utilize the massive unlabeled PolSAR data to alleviate the greedy demands of CNN-based methods for human annotations.",0
"Over the past few years, there has been progress in polarimetric synthetic aperture radar (PolSAR) image classification through the use of deep learning and convolutional neural networks (CNNs). However, an important issue remains unaddressed: CNNs require a large number of labeled samples, while there are insufficient human annotations for PolSAR images. This can lead to overfitting of training data and poor generalization performance of CNN-based classifiers in large-scale applications. To tackle this problem, this paper explores the use of convolutional architectures to learn transferrable representations from unlabeled PolSAR data. A PolSAR-tailored contrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR representation learning and few-shot classification. PCLNet uses a diversity stimulation mechanism to narrow the application gap between optics and PolSAR and develops an unsupervised pre-training phase to learn useful representations from unlabeled PolSAR data. The acquired representations are then transferred to the downstream task of few-shot PolSAR classification. Experiments on two widely-used PolSAR benchmark datasets confirm the effectiveness of PCLNet. This work may also provide insights on how to efficiently utilize the vast amounts of unlabeled PolSAR data to reduce the demands of CNN-based methods for human annotations.",1
"While many approaches exist in the literature to learn low-dimensional representations for data collections in multiple modalities, the generalizability of multi-modal nonlinear embeddings to previously unseen data is a rather overlooked subject. In this work, we first present a theoretical analysis of learning multi-modal nonlinear embeddings in a supervised setting. Our performance bounds indicate that for successful generalization in multi-modal classification and retrieval problems, the regularity of the interpolation functions extending the embedding to the whole data space is as important as the between-class separation and cross-modal alignment criteria. We then propose a multi-modal nonlinear representation learning algorithm that is motivated by these theoretical findings, where the embeddings of the training samples are optimized jointly with the Lipschitz regularity of the interpolators. Experimental comparison to recent multi-modal and single-modal learning algorithms suggests that the proposed method yields promising performance in multi-modal image classification and cross-modal image-text retrieval applications.",0
"Numerous techniques have been proposed in literature to obtain low-dimensional representations for datasets with multiple modalities. However, the topic of generalizing multi-modal nonlinear embeddings to novel data has not received much attention. Our research addresses this gap by presenting a theoretical analysis of learning multi-modal nonlinear embeddings in a supervised setting. Our study reveals that successful generalization in multi-modal classification and retrieval tasks depends not only on between-class separation and cross-modal alignment criteria but also on the regularity of the interpolation functions extending the embedding to the entire data space. To apply these findings, we introduce a multi-modal nonlinear representation learning algorithm that optimizes the embeddings of training samples and the Lipschitz regularity of the interpolators jointly. Our experimental results demonstrate that our proposed method outperforms recent multi-modal and single-modal learning techniques in multi-modal image classification and cross-modal image-text retrieval tasks.",1
"Deep learning is changing many areas in molecular physics, and it has shown great potential to deliver new solutions to challenging molecular modeling problems. Along with this trend arises the increasing demand of expressive and versatile neural network architectures which are compatible with molecular systems. A new deep neural network architecture, Molecular Configuration Transformer (Molecular CT), is introduced for this purpose. Molecular CT is composed of a relation-aware encoder module and a computationally universal geometry learning unit, thus able to account for the relational constraints between particles meanwhile scalable to different particle numbers and invariant w.r.t. the trans-rotational transforms. The computational efficiency and universality make Molecular CT versatile for a variety of molecular learning scenarios and especially appealing for transferable representation learning across different molecular systems. As examples, we show that Molecular CT enables representational learning for molecular systems at different scales, and achieves comparable or improved results on common benchmarks using a more light-weighted structure compared to baseline models.",0
"Many fields in molecular physics are being transformed by deep learning, which has the potential to provide innovative solutions for complex molecular modeling problems. The demand for neural network architectures that are adaptable to molecular systems is growing in tandem with this trend. To meet this demand, a new deep neural network architecture called the Molecular Configuration Transformer (Molecular CT) has been developed. Molecular CT comprises a relation-aware encoder module and a geometry learning unit that is computationally universal, allowing it to consider the relational constraints between particles while remaining scalable to different particle numbers and invariant with respect to trans-rotational transforms. Its computational efficiency and versatility make it suitable for various molecular learning scenarios, particularly transferable representation learning across different molecular systems. Molecular CT has been shown to enable representational learning for molecular systems at various scales and achieve comparable or superior results on common benchmarks using a more lightweight structure compared to baseline models.",1
"Forecasting influenza in a timely manner aids health organizations and policymakers in adequate preparation and decision making. However, effective influenza forecasting still remains a challenge despite increasing research interest. It is even more challenging amidst the COVID pandemic, when the influenza-like illness (ILI) counts are affected by various factors such as symptomatic similarities with COVID-19 and shift in healthcare seeking patterns of the general population. Under the current pandemic, historical influenza models carry valuable expertise about the disease dynamics but face difficulties adapting. Therefore, we propose CALI-Net, a neural transfer learning architecture which allows us to 'steer' a historical disease forecasting model to new scenarios where flu and COVID co-exist. Our framework enables this adaptation by automatically learning when it should emphasize learning from COVID-related signals and when it should learn from the historical model. Thus, we exploit representations learned from historical ILI data as well as the limited COVID-related signals. Our experiments demonstrate that our approach is successful in adapting a historical forecasting model to the current pandemic. In addition, we show that success in our primary goal, adaptation, does not sacrifice overall performance as compared with state-of-the-art influenza forecasting approaches.",0
"Being able to forecast influenza in a timely manner is crucial for health organizations and policymakers to adequately prepare and make informed decisions. However, despite increasing research interest, effective influenza forecasting remains a challenge. This challenge is even greater during the COVID pandemic due to various factors such as symptomatic similarities between the two illnesses and shifts in healthcare seeking patterns among the general population. While historical influenza models provide valuable insights into disease dynamics, they face difficulties in adapting to the current pandemic. To address this issue, we propose CALI-Net, a neural transfer learning architecture that allows us to adapt a historical disease forecasting model to scenarios where flu and COVID coexist. Our approach enables us to automatically learn when to prioritize COVID-related signals and when to rely on the historical model. By leveraging representations learned from historical ILI data and limited COVID-related signals, we successfully adapted our model to the current pandemic without sacrificing overall performance compared to state-of-the-art influenza forecasting approaches.",1
"Self-supervised representation learning is a critical problem in computer vision, as it provides a way to pretrain feature extractors on large unlabeled datasets that can be used as an initialization for more efficient and effective training on downstream tasks. A promising approach is to use contrastive learning to learn a latent space where features are close for similar data samples and far apart for dissimilar ones. This approach has demonstrated tremendous success for pretraining both image and point cloud feature extractors, but it has been barely investigated for multi-modal RGB-D scans, especially with the goal of facilitating high-level scene understanding. To solve this problem, we propose contrasting ""pairs of point-pixel pairs"", where positives include pairs of RGB-D points in correspondence, and negatives include pairs where one of the two modalities has been disturbed and/or the two RGB-D points are not in correspondence. This provides extra flexibility in making hard negatives and helps networks to learn features from both modalities, not just the more discriminating one of the two. Experiments show that this proposed approach yields better performance on three large-scale RGB-D scene understanding benchmarks (ScanNet, SUN RGB-D, and 3RScan) than previous pretraining approaches.",0
"In computer vision, self-supervised representation learning is a crucial matter as it allows for the pretraining of feature extractors on vast, unlabeled datasets that can serve as a starting point for more efficient and effective training on subsequent tasks. One promising method is to use contrastive learning to acquire a latent space where features are close for alike data samples and distant for dissimilar ones. While this strategy has been very successful for pretraining image and point cloud feature extractors, its application to multi-modal RGB-D scans for high-level scene comprehension has been scarcely explored. To tackle this issue, we propose contrasting ""pairs of point-pixel pairs"" that contain positive pairs of RGB-D points in correspondence and negative pairs where one of the modalities has been altered, and/or the two RGB-D points are not in correspondence. This approach offers added flexibility in generating challenging negative examples and assists networks in learning features from both modalities, not just the one that is more distinguishing. Our experiments indicate that this method produces better results than previous pretraining methods on three extensive RGB-D scene comprehension benchmarks (ScanNet, SUN RGB-D, and 3RScan).",1
"Multi-modal generative models represent an important family of deep models, whose goal is to facilitate representation learning on data with multiple views or modalities. However, current deep multi-modal models focus on the inference of shared representations, while neglecting the important private aspects of data within individual modalities. In this paper, we introduce a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. We specifically consider the instance where the latent factor may be of both continuous and discrete nature, leading to the family of general hybrid DMVAE models. We demonstrate the utility of DMVAE on a semi-supervised learning task, where one of the modalities contains partial data labels, both relevant and irrelevant to the other modality. Our experiments on several benchmarks indicate the importance of the private-shared disentanglement as well as the hybrid latent representation.",0
"Deep models known as multi-modal generative models are crucial in representation learning for data with various views or modalities. However, current models mainly focus on shared representations and overlook the private aspects of data within individual modalities. This paper presents a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. The model considers both continuous and discrete latent factors, resulting in hybrid DMVAE models. The DMVAE is evaluated on a semi-supervised learning task with partial data labels in one modality, both relevant and irrelevant to the other modality, and the experiments show the importance of private-shared disentanglement and hybrid latent representation.",1
"As the role played by statistical and computational sciences in climate and environmental modelling and prediction becomes more important, Machine Learning researchers are becoming more aware of the relevance of their work to help tackle the climate crisis. Indeed, being universal nonlinear function approximation tools, Machine Learning algorithms are efficient in analysing and modelling spatially and temporally variable environmental data. While Deep Learning models have proved to be able to capture spatial, temporal, and spatio-temporal dependencies through their automatic feature representation learning, the problem of the interpolation of continuous spatio-temporal fields measured on a set of irregular points in space is still under-investigated. To fill this gap, we introduce here a framework for spatio-temporal prediction of climate and environmental data using deep learning. Specifically, we show how spatio-temporal processes can be decomposed in terms of a sum of products of temporally referenced basis functions, and of stochastic spatial coefficients which can be spatially modelled and mapped on a regular grid, allowing the reconstruction of the complete spatio-temporal signal. Applications on two case studies based on simulated and real-world data will show the effectiveness of the proposed framework in modelling coherent spatio-temporal fields.",0
"The importance of statistical and computational sciences in climate and environmental modelling and prediction is increasing, leading Machine Learning researchers to recognize the value of their work in addressing the climate crisis. Machine Learning algorithms are effective in analyzing and modeling environmental data as universal nonlinear function approximation tools. While Deep Learning models have demonstrated their ability to capture spatial, temporal, and spatio-temporal dependencies, the problem of interpolating continuous spatio-temporal fields measured on irregular points in space remains underexplored. To address this gap, we present a framework for spatio-temporal prediction of climate and environmental data using deep learning. Our approach involves decomposing spatio-temporal processes into a sum of products of temporally referenced basis functions and stochastic spatial coefficients, which can be mapped onto a regular grid to reconstruct the complete spatio-temporal signal. Our proposed framework is effective in modeling coherent spatio-temporal fields, as demonstrated by applications to simulated and real-world data.",1
"Graphs as a type of data structure have recently attracted significant attention. Representation learning of geometric graphs has achieved great success in many fields including molecular, social, and financial networks. It is natural to present proteins as graphs in which nodes represent the residues and edges represent the pairwise interactions between residues. However, 3D protein structures have rarely been studied as graphs directly. The challenges include: 1) Proteins are complex macromolecules composed of thousands of atoms making them much harder to model than micro-molecules. 2) Capturing the long-range pairwise relations for protein structure modeling remains under-explored. 3) Few studies have focused on learning the different attributes of proteins together. To address the above challenges, we propose a new graph neural network architecture to represent the proteins as 3D graphs and predict both distance geometric graph representation and dihedral geometric graph representation together. This gives a significant advantage because this network opens a new path from the sequence to structure. We conducted extensive experiments on four different datasets and demonstrated the effectiveness of the proposed method.",0
"Recently, there has been significant interest in using graphs as a data structure. Geometric graph representation learning has been successful in various fields, such as molecular, social, and financial networks. It is common to represent proteins as graphs, with nodes representing residues and edges representing pairwise interactions between residues. However, studying 3D protein structures as graphs directly has been rare due to several challenges. These include the complexity of proteins, the difficulty in capturing long-range pairwise relations for modeling protein structures, and the lack of studies focusing on learning different protein attributes together. To overcome these challenges, a new graph neural network architecture has been proposed to represent proteins as 3D graphs and predict distance and dihedral geometric graph representations simultaneously. This approach offers a significant advantage because it creates a new path from sequence to structure. Extensive experiments on four datasets demonstrated the effectiveness of the proposed method.",1
"Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited.   With this in mind, we propose a teacher-student scheme to learn representations by training a convnet to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. We provide the implementation code at https://github.com/valeoai/obow.",0
"The field of learning image representations without human supervision is currently active and important. Recently, several approaches have been successful in creating representations that are invariant to various perturbations through contrastive-based instance discrimination training. However, effective visual representations require additional characteristics, such as contextual reasoning skills, which may be better suited to alternative reconstruction-based approaches. To address this, we propose a teacher-student scheme that trains a convnet to reconstruct a bag-of-visual-words representation of an image using a perturbed version of that same image as input. Our approach allows for online training of both the teacher network and student network, as well as an update of the visual-words vocabulary used for the BoW targets. This enables fully online BoW-guided unsupervised learning. Through extensive experiments, we demonstrate that our BoW-based strategy surpasses previous state-of-the-art methods in several applications, including Pascal object detection, Pascal classification, and Places205 classification. Our method even outperforms supervised pre-training, establishing new state-of-the-art results. The implementation code is available at https://github.com/valeoai/obow.",1
"Time series classification problems exist in many fields and have been explored for a couple of decades. However, they still remain challenging, and their solutions need to be further improved for real-world applications in terms of both accuracy and efficiency. In this paper, we propose a hybrid neural architecture, called Self-Attentive Recurrent Convolutional Networks (SARCoN), to learn multi-faceted representations for univariate time series. SARCoN is the synthesis of long short-term memory networks with self-attentive mechanisms and Fully Convolutional Networks, which work in parallel to learn the representations of univariate time series from different perspectives. The component modules of the proposed architecture are trained jointly in an end-to-end manner and they classify the input time series in a cooperative way. Due to its domain-agnostic nature, SARCoN is able to generalize a diversity of domain tasks. Our experimental results show that, compared to the state-of-the-art approaches for time series classification, the proposed architecture can achieve remarkable improvements for a set of univariate time series benchmarks from the UCR repository. Moreover, the self-attention and the global average pooling in the proposed architecture enable visible interpretability by facilitating the identification of the contribution regions of the original time series. An overall analysis confirms that multi-faceted representations of time series aid in capturing deep temporal corrections within complex time series, which is essential for the improvement of time series classification performance. Our work provides a novel angle that deepens the understanding of time series classification, qualifying our proposed model as an ideal choice for real-world applications.",0
"Time series classification is a difficult problem that has been studied for many years across various fields. Despite this, accuracy and efficiency still need to be improved for practical applications. In this study, we introduce a hybrid neural architecture called Self-Attentive Recurrent Convolutional Networks (SARCoN) that can learn diverse representations for univariate time series. SARCoN combines long short-term memory networks with self-attentive mechanisms and Fully Convolutional Networks to learn univariate time series representations from different angles. The component modules of SARCoN are trained together to classify input time series in a cooperative manner. SARCoN can be applied to various domains due to its domain-agnostic nature. Our experiments demonstrate that SARCoN outperforms existing approaches for time series classification and provides interpretability through self-attention and global average pooling. Multi-faceted representations aid in capturing deep temporal patterns in complex time series, which is crucial for improving time series classification performance. Our work provides a new perspective on time series classification and positions SARCoN as a suitable choice for practical applications.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"The use of mesh as a data structure for 3D shapes is a powerful tool that is important in various computer vision and graphics applications. The success of convolutional neural networks in structured data such as images has led to the need to adapt this insight for 3D shapes. However, the irregularity of 3D shape data poses a challenge since the neighbors of each node are unordered. To address this challenge, various graph neural networks have been developed with isotropic filters or predefined local coordinate systems. Nonetheless, these approaches limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that employs adaptive weighting matrices for each node based on the local neighboring structure and shared anisotropic filters. The learnable weighting matrix is similar to the attention matrix in the random synthesizer, a new Transformer model for natural language processing. Our comprehensive experiments demonstrate that our model significantly improves 3D shape reconstruction compared to state-of-the-art methods.",1
"Context modeling is one of the most fertile subfields of visual recognition which aims at designing discriminant image representations while incorporating their intrinsic and extrinsic relationships. However, the potential of context modeling is currently underexplored and most of the existing solutions are either context-free or restricted to simple handcrafted geometric relationships. We introduce in this paper DHCN: a novel Deep Hierarchical Context Network that leverages different sources of contexts including geometric and semantic relationships. The proposed method is based on the minimization of an objective function mixing a fidelity term, a context criterion and a regularizer. The solution of this objective function defines the architecture of a bi-level hierarchical context network; the first level of this network captures scene geometry while the second one corresponds to semantic relationships. We solve this representation learning problem by training its underlying deep network whose parameters correspond to the most influencing bi-level contextual relationships and we evaluate its performances on image annotation using the challenging ImageCLEF benchmark.",0
"Visual recognition subfield, context modeling, is focused on creating image representations that are discriminative while incorporating intrinsic and extrinsic relationships. However, current context modeling solutions are either limited to simple handcrafted geometric relationships or context-free, and the potential of context modeling remains underexplored. In this paper, we introduce a novel approach called DHCN, or Deep Hierarchical Context Network, which leverages various sources of contexts, including both geometric and semantic relationships. Our proposed method involves minimizing an objective function that combines a fidelity term, context criterion, and regularizer. This objective function solution defines a two-level hierarchical context network, where the first level captures scene geometry and the second level captures semantic relationships. We solve the representation learning problem by training the underlying deep network, whose parameters correspond to the most influential bi-level contextual relationships. We evaluate our approach's performance on image annotation using the challenging ImageCLEF benchmark.",1
"Graph Neural Networks (GNNs) are widely used in graph representation learning. However, most GNN methods are designed for either homogeneous or heterogeneous graphs. In this paper, we propose a new model, Hop-Hop Relation-aware Graph Neural Network (HHR-GNN), to unify representation learning for these two types of graphs. HHR-GNN learns a personalized receptive field for each node by leveraging knowledge graph embedding to learn relation scores between the central node's representations at different hops. In neighborhood aggregation, our model simultaneously allows for hop-aware projection and aggregation. This mechanism enables the central node to learn a hop-wise neighborhood mixing that can be applied to both homogeneous and heterogeneous graphs. Experimental results on five benchmarks show the competitive performance of our model compared to state-of-the-art GNNs, e.g., up to 13K faster in terms of time cost per training epoch on large heterogeneous graphs.",0
"Graph Neural Networks (GNNs) have become a popular tool for graph representation learning; however, most existing GNN methods are designed to work with only homogeneous or heterogeneous graphs. This paper proposes a novel approach called the Hop-Hop Relation-aware Graph Neural Network (HHR-GNN), which aims to unify representation learning for both types of graphs. HHR-GNN leverages knowledge graph embedding to learn relation scores between central nodes' representations at different hops, enabling the learning of a personalized receptive field for each node. Our model also simultaneously allows for hop-aware projection and aggregation in neighborhood aggregation, facilitating hop-wise neighborhood mixing for both homogeneous and heterogeneous graphs. Our experimental results demonstrate that HHR-GNN outperforms state-of-the-art GNNs, achieving up to 13K faster training time per epoch on large, heterogeneous graphs.",1
"The graph Laplacian regularization term is usually used in semi-supervised representation learning to provide graph structure information for a model $f(X)$. However, with the recent popularity of graph neural networks (GNNs), directly encoding graph structure $A$ into a model, i.e., $f(A, X)$, has become the more common approach. While we show that graph Laplacian regularization brings little-to-no benefit to existing GNNs, and propose a simple but non-trivial variant of graph Laplacian regularization, called Propagation-regularization (P-reg), to boost the performance of existing GNN models. We provide formal analyses to show that P-reg not only infuses extra information (that is not captured by the traditional graph Laplacian regularization) into GNNs, but also has the capacity equivalent to an infinite-depth graph convolutional network. We demonstrate that P-reg can effectively boost the performance of existing GNN models on both node-level and graph-level tasks across many different datasets.",0
"In semi-supervised representation learning, the graph Laplacian regularization term is typically utilized to provide graph structure information to model $f(X)$. However, the more prevalent approach now with the rise of graph neural networks (GNNs) is to encode graph structure $A$ directly into the model, i.e., $f(A, X)$. Our research reveals that graph Laplacian regularization has little to no impact on existing GNNs. As a result, we propose a novel variant of graph Laplacian regularization called Propagation-regularization (P-reg), which enhances the performance of existing GNN models. Our formal analyses show that P-reg not only incorporates additional information that traditional graph Laplacian regularization fails to capture but also possesses the equivalent capacity of an infinite-depth graph convolutional network. Our experiments demonstrate the efficacy of P-reg in improving the performance of existing GNN models across various datasets and tasks at both node-level and graph-level.",1
"Knowledge Graphs have been one of the fundamental methods for integrating heterogeneous data sources. Integrating heterogeneous data sources is crucial, especially in the biomedical domain, where central data-driven tasks such as drug discovery rely on incorporating information from different biomedical databases. These databases contain various biological entities and relations such as proteins (PDB), genes (Gene Ontology), drugs (DrugBank), diseases (DDB), and protein-protein interactions (BioGRID). The process of semantically integrating heterogeneous biomedical databases is often riddled with imperfections. The quality of data-driven drug discovery relies on the accuracy of the mining methods used and the data's quality as well. Thus, having complete and refined biomedical knowledge graphs is central to achieving more accurate drug discovery outcomes. Here we propose using the latest graph representation learning and embedding models to refine and complete biomedical knowledge graphs. This preliminary work demonstrates learning discrete representations of the integrated biomedical knowledge graph Chem2Bio2RD [3]. We perform a knowledge graph completion and refinement task using a simple top-K cosine similarity measure between the learned embedding vectors to predict missing links between drugs and targets present in the data. We show that this simple procedure can be used alternatively to binary classifiers in link prediction.",0
"Integrating various sources of data is crucial in the biomedical domain, particularly in tasks like drug discovery, where information from different databases must be incorporated. Knowledge Graphs have been a fundamental method for achieving this integration, as biomedical databases contain a range of entities and relationships. However, the process of semantically integrating these databases is often imperfect, and the accuracy of data-driven drug discovery relies on the quality of the data and mining methods used. To improve the accuracy of drug discovery outcomes, complete and refined biomedical knowledge graphs are necessary. In this study, we propose using the latest graph representation learning and embedding models to refine and complete the Chem2Bio2RD knowledge graph. We demonstrate that learning discrete representations of the knowledge graph enables us to perform a knowledge graph completion and refinement task, predicting missing links between drugs and targets through a simple top-K cosine similarity measure. This procedure can be used as an alternative to binary classifiers in link prediction.",1
"Both high-level and high-resolution feature representations are of great importance in various visual understanding tasks. To acquire high-resolution feature maps with high-level semantic information, one common strategy is to adopt dilated convolutions in the backbone networks to extract high-resolution feature maps, such as the dilatedFCN-based methods for semantic segmentation. However, due to many convolution operations are conducted on the high-resolution feature maps, such methods have large computational complexity and memory consumption. In this paper, we propose one novel holistically-guided decoder which is introduced to obtain the high-resolution semantic-rich feature maps via the multi-scale features from the encoder. The decoding is achieved via novel holistic codeword generation and codeword assembly operations, which take advantages of both the high-level and low-level features from the encoder features. With the proposed holistically-guided decoder, we implement the EfficientFCN architecture for semantic segmentation and HGD-FPN for object detection and instance segmentation. The EfficientFCN achieves comparable or even better performance than state-of-the-art methods with only 1/3 of their computational costs for semantic segmentation on PASCAL Context, PASCAL VOC, ADE20K datasets. Meanwhile, the proposed HGD-FPN achieves $>2\%$ higher mean Average Precision (mAP) when integrated into several object detection frameworks with ResNet-50 encoding backbones.",0
"Various visual understanding tasks require both high-level and high-resolution feature representations. To obtain high-resolution feature maps with high-level semantic information, dilated convolutions are commonly used in backbone networks. However, these methods have high computational complexity and memory consumption due to many convolution operations on high-resolution feature maps. In this study, we introduce a novel holistically-guided decoder that utilizes multi-scale features from the encoder to generate high-resolution semantic-rich feature maps. The decoding is achieved through holistic codeword generation and assembly operations that make use of both high-level and low-level features from the encoder. We implement the EfficientFCN architecture for semantic segmentation and HGD-FPN for object detection and instance segmentation using the proposed holistically-guided decoder. The EfficientFCN achieves comparable or better performance than state-of-the-art methods with only 1/3 of their computational costs for semantic segmentation on PASCAL Context, PASCAL VOC, ADE20K datasets. Moreover, the proposed HGD-FPN achieves higher mean Average Precision (mAP) when integrated into several object detection frameworks with ResNet-50 encoding backbones.",1
"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",0
"In the field of self-supervised representation learning, contrastive learning has recently achieved outstanding performance by comparing multiple views of the data. While this method has been successful, the effect of different view choices has not been extensively researched. This study employs theoretical and empirical analysis to explore the significance of view selection. The authors suggest that reducing the mutual information (MI) between views while preserving task-relevant information is crucial. To test this hypothesis, unsupervised and semi-supervised frameworks were developed to learn effective views by reducing MI. Data augmentation was also considered as a means of reducing MI, and the results showed that increasing data augmentation led to decreased MI and improved downstream classification accuracy. Additionally, the study achieved a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification, with a ResNet-50 reaching 73% top-1 linear readout. Furthermore, transferring the models to PASCAL VOC object detection and COCO instance segmentation consistently outperformed supervised pre-training. Code for this study can be found at http://github.com/HobbitLong/PyContrast.",1
"Visual tempo, which describes how fast an action goes, has shown its potential in supervised action recognition. In this work, we demonstrate that visual tempo can also serve as a self-supervision signal for video representation learning. We propose to maximize the mutual information between representations of slow and fast videos via hierarchical contrastive learning (VTHCL). Specifically, by sampling the same instance at slow and fast frame rates respectively, we can obtain slow and fast video frames which share the same semantics but contain different visual tempos. Video representations learned from VTHCL achieve the competitive performances under the self-supervision evaluation protocol for action recognition on UCF-101 (82.1\%) and HMDB-51 (49.2\%). Moreover, comprehensive experiments suggest that the learned representations are generalized well to other downstream tasks including action detection on AVA and action anticipation on Epic-Kitchen. Finally, we propose Instance Correspondence Map (ICM) to visualize the shared semantics captured by contrastive learning.",0
"The potential of visual tempo in supervised action recognition has been demonstrated. This study shows that visual tempo can also act as a self-supervision signal for video representation learning. A method called hierarchical contrastive learning (VTHCL) is proposed to maximize the mutual information between representations of slow and fast videos. By obtaining slow and fast video frames that share the same semantics but have different visual tempos, video representations learned from VTHCL achieve competitive performances under the self-supervision evaluation protocol for action recognition on UCF-101 (82.1%) and HMDB-51 (49.2%). The learned representations are also shown to be well generalized to other downstream tasks, including action detection on AVA and action anticipation on Epic-Kitchen. Finally, the Instance Correspondence Map (ICM) is proposed to visualize the shared semantics captured by contrastive learning.",1
"Message passing neural networks have recently evolved into a state-of-the-art approach to representation learning on graphs. Existing methods perform synchronous message passing along all edges in multiple subsequent rounds and consequently suffer from various shortcomings: Propagation schemes are inflexible since they are restricted to $k$-hop neighborhoods and insensitive to actual demands of information propagation. Further, long-range dependencies cannot be modeled adequately and learned representations are based on correlations of fixed locality. These issues prevent existing methods from reaching their full potential in terms of prediction performance. Instead, we consider a novel asynchronous message passing approach where information is pushed only along the most relevant edges until convergence. Our proposed algorithm can equivalently be formulated as a single synchronous message passing iteration using a suitable neighborhood function, thus sharing the advantages of existing methods while addressing their central issues. The resulting neural network utilizes a node-adaptive receptive field derived from meaningful sparse node neighborhoods. In addition, by learning and combining node representations over differently sized neighborhoods, our model is able to capture correlations on multiple scales. We further propose variants of our base model with different inductive bias. Empirical results are provided for semi-supervised node classification on five real-world datasets following a rigorous evaluation protocol. We find that our models outperform competitors on all datasets in terms of accuracy with statistical significance. In some cases, our models additionally provide faster runtime.",0
"Recently, message passing neural networks have become the go-to method for representation learning on graphs. However, current approaches have limitations due to their inflexible propagation schemes, which are confined to $k$-hop neighborhoods and fail to account for long-range dependencies. Additionally, learned representations rely on fixed locality correlations, hindering performance. To address these issues, we introduce a new asynchronous message passing approach that only pushes relevant information until convergence. Our algorithm can also be formulated as a single synchronous iteration, combining the benefits of existing methods while addressing their shortcomings. Our neural network features a node-adaptive receptive field based on meaningful sparse node neighborhoods, allowing us to capture correlations on multiple scales by learning and combining node representations over differently sized neighborhoods. We present different inductive bias models and evaluate them through a rigorous protocol on five real-world datasets for semi-supervised node classification. Our models outperform competitors in accuracy with statistical significance and, in some cases, provide faster runtime.",1
"Many unsupervised domain adaptive (UDA) person re-identification (ReID) approaches combine clustering-based pseudo-label prediction with feature fine-tuning. However, because of domain gap, the pseudo-labels are not always reliable and there are noisy/incorrect labels. This would mislead the feature representation learning and deteriorate the performance. In this paper, we propose to estimate and exploit the credibility of the assigned pseudo-label of each sample to alleviate the influence of noisy labels, by suppressing the contribution of noisy samples. We build our baseline framework using the mean teacher method together with an additional contrastive loss. We have observed that a sample with a wrong pseudo-label through clustering in general has a weaker consistency between the output of the mean teacher model and the student model. Based on this finding, we propose to exploit the uncertainty (measured by consistency levels) to evaluate the reliability of the pseudo-label of a sample and incorporate the uncertainty to re-weight its contribution within various ReID losses, including the identity (ID) classification loss per sample, the triplet loss, and the contrastive loss. Our uncertainty-guided optimization brings significant improvement and achieves the state-of-the-art performance on benchmark datasets.",0
"Numerous UDA methods for person re-identification (ReID) utilize a combination of clustering-based pseudo-label prediction and feature fine-tuning. However, due to a domain gap, these pseudo-labels are not always trustworthy and may contain noisy or incorrect labels. This can lead to misguided feature representation learning and a decrease in performance. Our study proposes a solution by estimating and utilizing the credibility of each assigned pseudo-label to alleviate the impact of noisy labels. By suppressing the contribution of noisy samples, we build a baseline framework using the mean teacher method and an additional contrastive loss. Our findings indicate that a sample with an incorrect pseudo-label typically has less consistency between the mean teacher model and the student model. Based on this observation, we suggest using uncertainty (measured by consistency levels) to assess the reliability of a sample's pseudo-label and adjust its contribution within various ReID losses, such as the identity classification loss per sample, triplet loss, and contrastive loss. Our uncertainty-guided optimization yields significant improvement and achieves state-of-the-art performance on benchmark datasets.",1
"One significant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the field, neglecting that the subject is dancing as a cheerleader on the field. This is against our original intention towards the video representation learning and may bring scene bias on different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Specifically, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and find that our method surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.",0
"We anticipate that video representation learning will capture object motion, which is a significant factor that sets it apart from image representation learning. However, we have observed that certain action categories in mainstream video datasets are closely linked to the scene where the action occurs, leading the model to encode only scene information. For example, a model may identify a video as featuring football because it sees a field, disregarding the fact that a cheerleader is dancing on the field. This is not our intended outcome for video representation learning and can introduce scene bias in other datasets. To remedy this issue, we propose decoupling scene and motion through two simple operations called DSM. We create a positive and negative clip for each video that are motion-untouched/broken but scene-broken/untouched using Spatial Local Disturbance and Temporal Local Disturbance. Our goal is to bring the positive clip closer and push the negative clip farther from the original clip in the latent space. By doing so, we can weaken scene impact and enhance the network's temporal sensitivity. Our experiments on two tasks with different pre-training datasets and various backbones reveal that our method outperforms SOTA methods by 8.1% and 8.8% respectively in action recognition on UCF101 and HMDB51 datasets using the same backbone.",1
"Generative adversarial networks are the state of the art approach towards learned synthetic image generation. Although early successes were mostly unsupervised, bit by bit, this trend has been superseded by approaches based on labelled data. These supervised methods allow a much finer-grained control of the output image, offering more flexibility and stability. Nevertheless, the main drawback of such models is the necessity of annotated data. In this work, we introduce an novel framework that benefits from two popular learning techniques, adversarial training and representation learning, and takes a step towards unsupervised conditional GANs. In particular, our approach exploits the structure of a latent space (learned by the representation learning) and employs it to condition the generative model. In this way, we break the traditional dependency between condition and label, substituting the latter by unsupervised features coming from the latent space. Finally, we show that this new technique is able to produce samples on demand keeping the quality of its supervised counterpart.",0
"The cutting-edge method for creating synthetic images is through Generative Adversarial Networks (GANs). Initially, these networks relied on unsupervised learning to generate images, but now they have shifted towards supervised techniques that provide greater control over the output image. However, the downside to supervised models is that they require annotated data. To address this issue, we present a novel framework that combines adversarial training and representation learning to create unsupervised conditional GANs. Our approach utilizes the latent space structure learned through representation learning to condition the generative model, breaking the traditional dependency on labeled data. We demonstrate that our method can generate high-quality images on demand, comparable to those produced by supervised models.",1
"Identification of 3D cephalometric landmarks that serve as proxy to the shape of human skull is the fundamental step in cephalometric analysis. Since manual landmarking from 3D computed tomography (CT) images is a cumbersome task even for the trained experts, automatic 3D landmark detection system is in a great need. Recently, automatic landmarking of 2D cephalograms using deep learning (DL) has achieved great success, but 3D landmarking for more than 80 landmarks has not yet reached a satisfactory level, because of the factors hindering machine learning such as the high dimensionality of the input data and limited amount of training data due to ethical restrictions on the use of medical data. This paper presents a semi-supervised DL method for 3D landmarking that takes advantage of anonymized landmark dataset with paired CT data being removed. The proposed method first detects a small number of easy-to-find reference landmarks, then uses them to provide a rough estimation of the entire landmarks by utilizing the low dimensional representation learned by variational autoencoder (VAE). Anonymized landmark dataset is used for training the VAE. Finally, coarse-to-fine detection is applied to the small bounding box provided by rough estimation, using separate strategies suitable for mandible and cranium. For mandibular landmarks, patch-based 3D CNN is applied to the segmented image of the mandible (separated from the maxilla), in order to capture 3D morphological features of mandible associated with the landmarks. We detect 6 landmarks around the condyle all at once, instead of one by one, because they are closely related to each other. For cranial landmarks, we again use VAE-based latent representation for more accurate annotation. In our experiment, the proposed method achieved an averaged 3D point-to-point error of 2.91 mm for 90 landmarks only with 15 paired training data.",0
"The initial step in cephalometric analysis involves identifying 3D cephalometric landmarks that act as a representation of the human skull's shape. Although manual landmarking from 3D computed tomography (CT) images is a challenging task even for trained experts, there is a significant need for an automatic 3D landmark detection system. While automatic landmarking of 2D cephalograms using deep learning (DL) has seen great success recently, 3D landmarking for over 80 landmarks has not been satisfactory due to the high dimensionality of input data and the limited amount of training data. This paper presents a semi-supervised DL approach that uses an anonymized landmark dataset with paired CT data removed. The proposed method first identifies a small number of easy-to-find reference landmarks and then uses them to obtain a rough estimation of the entire landmark set by leveraging the low dimensional representation learned by variational autoencoder (VAE). The anonymized landmark dataset is used to train the VAE. Finally, the method applies coarse-to-fine detection to the small bounding box provided by rough estimation using distinct strategies suitable for the mandible and cranium. For mandibular landmarks, patch-based 3D CNN is employed to capture 3D morphological features of the mandible linked to the landmarks. We detect six landmarks around the condyle simultaneously since they are closely related to one another. For cranial landmarks, we utilize VAE-based latent representation for more precise annotation. In our experiment, with only 15 paired training data, the proposed method achieved an average 3D point-to-point error of 2.91 mm for 90 landmarks.",1
"A molecular and cellular understanding of how SARS-CoV-2 variably infects and causes severe COVID-19 remains a bottleneck in developing interventions to end the pandemic. We sought to use deep learning to study the biology of SARS-CoV-2 infection and COVID-19 severity by identifying transcriptomic patterns and cell types associated with SARS-CoV-2 infection and COVID-19 severity. To do this, we developed a new approach to generating self-supervised edge features. We propose a model that builds on Graph Attention Networks (GAT), creates edge features using self-supervised learning, and ingests these edge features via a Set Transformer. This model achieves significant improvements in predicting the disease state of individual cells, given their transcriptome. We apply our model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung organoids and bronchoalveolar lavage fluid samples of patients with COVID-19, achieving state-of-the-art performance on both datasets with our model. We then borrow from the field of explainable AI (XAI) to identify the features (genes) and cell types that discriminate bystander vs. infected cells across time and moderate vs. severe COVID-19 disease. To the best of our knowledge, this represents the first application of deep learning to identifying the molecular and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using single-cell omics data.",0
"Understanding how SARS-CoV-2 infects and causes severe COVID-19 is critical in developing interventions to end the pandemic. We used deep learning to study the biology of SARS-CoV-2 infection and COVID-19 severity by identifying transcriptomic patterns and cell types associated with them. Our approach involved developing a new method for generating self-supervised edge features, which we incorporated into a model that utilizes Graph Attention Networks and Set Transformers. By applying this model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung organoids and bronchoalveolar lavage fluid samples of patients with COVID-19, we achieved state-of-the-art performance in predicting disease states of individual cells. We also utilized explainable AI (XAI) to identify the genes and cell types that differentiate between bystander and infected cells across time, and moderate and severe COVID-19 disease. This marks the first application of deep learning to identifying the molecular and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using single-cell omics data.",1
"Disentangled representation learning has seen a surge in interest over recent times, generally focusing on new models which optimise one of many disparate disentanglement metrics. Symmetry Based Disentangled Representation learning introduced a robust mathematical framework that defined precisely what is meant by a ""linear disentangled representation"". This framework determined that such representations would depend on a particular decomposition of the symmetry group acting on the data, showing that actions would manifest through irreducible group representations acting on independent representational subspaces. Caselles-Dupre et al [2019] subsequently proposed the first model to induce and demonstrate a linear disentangled representation in a VAE model. In this work we empirically show that linear disentangled representations are not generally present in standard VAE models and that they instead require altering the loss landscape to induce them. We proceed to show that such representations are a desirable property with regard to classical disentanglement metrics. Finally we propose a method to induce irreducible representations which forgoes the need for labelled action sequences, as was required by prior work. We explore a number of properties of this method, including the ability to learn from action sequences without knowledge of intermediate states and robustness under visual noise. We also demonstrate that it can successfully learn 4 independent symmetries directly from pixels.",0
"Recently, there has been a growing interest in disentangled representation learning, which involves creating new models that optimize various disentanglement metrics. The Symmetry Based Disentangled Representation learning approach has introduced a precise mathematical framework that defines a ""linear disentangled representation."" This framework shows that such representations depend on a specific decomposition of the symmetry group that acts on the data, with actions being represented through irreducible group representations acting on independent representational subspaces. In a subsequent study, Caselles-Dupre et al [2019] proposed a model that induces and demonstrates a linear disentangled representation in a VAE model. However, our empirical findings show that standard VAE models do not typically have linear disentangled representations, and that altering the loss landscape is necessary to induce them. We also demonstrate that these representations are desirable in terms of classical disentanglement metrics. Finally, we propose a method for inducing irreducible representations that does not require labelled action sequences, which was necessary in previous work. We explore various properties of this method, including its ability to learn from action sequences without knowledge of intermediate states, its robustness under visual noise, and its success in learning four independent symmetries directly from pixels.",1
"Deep clustering is a fundamental yet challenging task for data analysis. Recently we witness a strong tendency of combining autoencoder and graph neural networks to exploit structure information for clustering performance enhancement. However, we observe that existing literature 1) lacks a dynamic fusion mechanism to selectively integrate and refine the information of graph structure and node attributes for consensus representation learning; 2) fails to extract information from both sides for robust target distribution (i.e., ""groundtruth"" soft labels) generation. To tackle the above issues, we propose a Deep Fusion Clustering Network (DFCN). Specifically, in our network, an interdependency learning-based Structure and Attribute Information Fusion (SAIF) module is proposed to explicitly merge the representations learned by an autoencoder and a graph autoencoder for consensus representation learning. Also, a reliable target distribution generation measure and a triplet self-supervision strategy, which facilitate cross-modality information exploitation, are designed for network training. Extensive experiments on six benchmark datasets have demonstrated that the proposed DFCN consistently outperforms the state-of-the-art deep clustering methods.",0
"Data analysis involves a difficult yet crucial task known as deep clustering. Recently, a trend has emerged that combines autoencoder and graph neural networks to improve clustering performance by leveraging structural information. However, current literature fails to provide a dynamic fusion mechanism that selectively integrates and refines information from graph structure and node attributes for consensus representation learning. Additionally, it does not extract information from both sides to generate robust target distribution. To address these issues, we propose a Deep Fusion Clustering Network (DFCN) that includes an interdependency learning-based Structure and Attribute Information Fusion (SAIF) module. This module explicitly merges the representations learned by an autoencoder and a graph autoencoder to achieve consensus representation learning. We also introduce a reliable target distribution generation measure and a triplet self-supervision strategy for cross-modality information exploitation during network training. Our extensive experiments on six benchmark datasets demonstrate that the proposed DFCN consistently outperforms the state-of-the-art deep clustering methods.",1
"Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale ""Holistic Video Understanding Dataset""~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.   We demonstrate the generalization capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications.",0
"Although benchmarks with rich annotations have advanced video recognition in recent years, research has been mainly limited to human action or sports recognition, leaving a significant gap in describing the overall content of a video. To address this issue, we present a large-scale dataset called ""Holistic Video Understanding Dataset""~(HVU), which is hierarchically organized in a semantic taxonomy that focuses on multi-label and multi-task video understanding. HVU contains approximately 572k videos with 9 million annotations and spans over 3142 labels. The semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts naturally capture real-world scenarios. We demonstrate HVU's generalization capability on three challenging tasks: Video classification, Video captioning, and Video clustering. For video classification, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet), which fuses 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet is trained in an end-to-end manner and focuses on the multi-label and multi-task learning problem. Our experiments validate that holistic representation learning is complementary and can play a key role in enabling many real-world applications.",1
"The effective application of representation learning to real-world problems requires both techniques for learning useful representations, and also robust ways to evaluate properties of representations. Recent work in disentangled representation learning has shown that unsupervised representation learning approaches rely on fully supervised disentanglement metrics, which assume access to labels for ground-truth factors of variation. In many real-world cases ground-truth factors are expensive to collect, or difficult to model, such as for perception. Here we empirically show that a weakly-supervised downstream task based on odd-one-out observations is suitable for model selection by observing high correlation on a difficult downstream abstract visual reasoning task. We also show that a bespoke metric-learning VAE model which performs highly on this task also out-performs other standard unsupervised and a weakly-supervised disentanglement model across several metrics.",0
"To effectively apply representation learning to practical problems, it is essential to have effective techniques for learning valuable representations and reliable methods for evaluating their properties. Recent research in disentangled representation learning has demonstrated that unsupervised approaches for representation learning rely on fully supervised disentanglement metrics, which assume access to labels for ground-truth factors of variation. However, obtaining ground-truth labels for many real-world cases can be expensive or challenging, particularly for perception. In this study, we demonstrate through empirical methods that a weakly-supervised downstream task based on odd-one-out observations is appropriate for model selection, as it shows high correlation with a challenging downstream abstract visual reasoning task. Additionally, our findings show that a custom metric-learning VAE model that performs well on this task outperforms other standard unsupervised and weakly-supervised disentanglement models across multiple metrics.",1
"The success of deep learning based models for computer vision applications requires large scale human annotated data which are often expensive to generate. Self-supervised learning, a subset of unsupervised learning, handles this problem by learning meaningful features from unlabeled image or video data. In this paper, we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data. Experiments on the pretext task show that this proposed approach enables the model to learn spatial context invariant features which help for reliable and explainable performance in downstream tasks like classification of Anterior Cruciate Ligament tear injury from knee MRI. The efficiency of the novel Convolutional Neural Network proposed in this paper is reflected in the experimental results obtained in the downstream task.",0
"Generating large scale human annotated data for deep learning based models in computer vision applications can be costly. Self-supervised learning, a type of unsupervised learning, addresses this challenge by learning meaningful features from unlabeled image or video data. In this study, we present a self-supervised learning strategy to acquire transferable features from MR video clips by requiring the model to learn anatomical features. The pretext task models are created to predict the correct sequence of jumbled image patches that the MR video frames are divided into. To the best of our knowledge, our work is the first to provide an explanation for the decisions made by the models on MR video data, as none of the supervised learning models for injury classification task from MR video have done so. Our proposed approach's experiments on the pretext task reveal that the model can obtain spatial context invariant features, which help achieve reliable and explainable performance in downstream tasks, such as ACL tear injury classification from knee MRI. The experimental results obtained in the downstream task indicate the effectiveness of the novel Convolutional Neural Network proposed in this paper.",1
"Graph representation learning has many real-world applications, from super-resolution imaging, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. In this paper, we propose a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored at a subgraph. Based on this, we then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is verified with numerical examples for random graphs. The effectiveness is demonstrated by real-world applications, including multiresolution analysis for traffic network, and graph neural networks for graph classification tasks.",0
"Graph representation learning is widely used in various fields such as super-resolution imaging, 3D computer vision, drug repurposing, protein classification, and social network analysis. It is crucial to have an appropriate representation of graph data to achieve optimal learning performance for statistical or machine learning models that handle graph-structured data. This paper introduces a novel multiscale representation system for graph data called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system enables the storage of graph data representation on a coarse-grained chain while processing the data at different scales where it is stored on a subgraph. The paper then establishes decimated G-framelet transforms for the decomposition and reconstruction of graph data at multiple resolutions using a data-driven filter bank. Graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms, and a fast algorithm for decimated G-framelet transforms (FGT) is presented, which has a linear computational complexity of O(N) for a graph of size N. The theory of decimated framelets and FGT is verified using numerical examples for random graphs, and its effectiveness is demonstrated in real-world applications, including multiresolution analysis for traffic networks and graph neural networks for graph classification tasks.",1
"Nowadays Knowledge Graphs constitute a mainstream approach for the representation of relational information on big heterogeneous data, however, they may contain a big amount of imputed noise when constructed automatically. To address this problem, different error detection methodologies have been proposed, mainly focusing on path ranking and representation learning. This work presents various mainstream approaches and proposes a hybrid and modular methodology for the task. We compare different methods on two benchmarks and one real-world biomedical publications dataset, showcasing the potential of our approach and providing insights on graph embeddings when dealing with noisy Knowledge Graphs.",0
"In today's world, Knowledge Graphs are widely used to represent relational data on large and diverse datasets. However, when created automatically, these graphs may contain significant amounts of inaccurate information. To solve this problem, various error detection techniques have been suggested, which mostly concentrate on path ranking and representation learning. This study examines several commonly used approaches and introduces a hybrid and modular methodology for handling this issue. We evaluate different methods on two benchmarks and a real-life biomedical publications dataset, demonstrating the effectiveness of our method and offering insights into graph embeddings when handling noisy Knowledge Graphs.",1
"Graph Neural Networks (GNNs) are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. However, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel meta-learning strategy capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly. We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than classically trained models. Our method is model-agnostic and task-agnostic, thus applicable to a wide variety of multi-task domains.",0
"The framework of Graph Neural Networks (GNNs) is utilized for learning graph representations by creating node embeddings that capture structural and feature-related information in a low-dimensional space. Typically, GNNs are trained end-to-end, which results in specialized node embeddings. However, generating node embeddings that can perform multiple tasks with the same or better performance than single-task models is a challenge. To address this, we propose a new meta-learning approach that can produce multi-task node embeddings. Our method overcomes the challenges of learning multiple tasks simultaneously by teaching the model to adapt quickly to individual tasks with just a few steps of gradient descent. We demonstrate that our approach generates embeddings that can perform multiple tasks with similar or superior performance to traditional models. The method is model-agnostic and task-agnostic, making it suitable for a wide range of multi-task applications.",1
"Events in the real world are correlated across nearby points in time, and we must learn from this temporally smooth data. However, when neural networks are trained to categorize or reconstruct single items, the common practice is to randomize the order of training items. What are the effects of temporally smooth training data on the efficiency of learning? We first tested the effects of smoothness in training data on incremental learning in feedforward nets and found that smoother data slowed learning. Moreover, sampling so as to minimize temporal smoothness produced more efficient learning than sampling randomly. If smoothness generally impairs incremental learning, then how can networks be modified to benefit from smoothness in the training data? We hypothesized that two simple brain-inspired mechanisms, leaky memory in activation units and memory-gating, could enable networks to rapidly extract useful representations from smooth data. Across all levels of data smoothness, these brain-inspired architectures achieved more efficient category learning than feedforward networks. This advantage persisted, even when leaky memory networks with gating were trained on smooth data and tested on randomly-ordered data. Finally, we investigated how these brain-inspired mechanisms altered the internal representations learned by the networks. We found that networks with multi-scale leaky memory and memory-gating could learn internal representations that un-mixed data sources which vary on fast and slow timescales across training samples. Altogether, we identified simple mechanisms enabling neural networks to learn more quickly from temporally smooth data, and to generate internal representations that separate timescales in the training signal.",0
"Learning from temporally smooth data in the real world is essential. However, neural networks are commonly trained to classify or reconstruct individual items by randomizing the order of training data. This raises the question of how smooth training data affects learning efficiency. Our research discovered that smoother data negatively impacted incremental learning in feedforward nets, and random sampling was less efficient than sampling to minimize temporal smoothness. We explored the idea that brain-inspired mechanisms, specifically leaky memory and memory-gating, could help networks extract useful representations from smooth data. These architectures outperformed feedforward networks in category learning, and even when trained on smooth data and tested on random data, the advantage persisted. We also found that these mechanisms allowed networks to learn internal representations that separated timescales in the training signal. Overall, our study identified simple mechanisms that can help neural networks learn more quickly from temporally smooth data.",1
"A novel unsupervised deep learning method is developed to identify individual-specific large scale brain functional networks (FNs) from resting-state fMRI (rsfMRI) in an end-to-end learning fashion. Our method leverages deep Encoder-Decoder networks and conventional brain decomposition models to identify individual-specific FNs in an unsupervised learning framework and facilitate fast inference for new individuals with one forward pass of the deep network. Particularly, convolutional neural networks (CNNs) with an Encoder-Decoder architecture are adopted to identify individual-specific FNs from rsfMRI data by optimizing their data fitting and sparsity regularization terms that are commonly used in brain decomposition models. Moreover, a time-invariant representation learning module is designed to learn features invariant to temporal orders of time points of rsfMRI data. The proposed method has been validated based on a large rsfMRI dataset and experimental results have demonstrated that our method could obtain individual-specific FNs which are consistent with well-established FNs and are informative for predicting brain age, indicating that the individual-specific FNs identified truly captured the underlying variability of individualized functional neuroanatomy.",0
"We have developed a new method for identifying individual-specific large scale brain functional networks (FNs) from resting-state fMRI (rsfMRI) using unsupervised deep learning. Our approach utilizes deep Encoder-Decoder networks and conventional brain decomposition models to identify individual-specific FNs in an unsupervised learning framework, making it easy to use with new individuals. To achieve this, we have employed convolutional neural networks (CNNs) with Encoder-Decoder architecture to optimize data fitting and sparsity regularization terms commonly used in brain decomposition models. Additionally, we have designed a time-invariant representation learning module to learn features that are invariant to temporal orders of time points of rsfMRI data. We have evaluated our method using a large rsfMRI dataset and found that it produces individual-specific FNs that are consistent with well-established FNs and can be used to predict brain age. These results suggest that our method captures the underlying variability of individualized functional neuroanatomy.",1
"Low-dimension graph embeddings have proved extremely useful in various downstream tasks in large graphs, e.g., link-related content recommendation and node classification tasks, etc. Most existing embedding approaches take nodes as the basic unit for information aggregation, e.g., node perception fields in GNN or con-textual nodes in random walks. The main drawback raised by such node-view is its lack of support for expressing the compound relationships between nodes, which results in the loss of a certain degree of graph information during embedding. To this end, this paper pro-poses PairE(Pair Embedding), a solution to use ""pair"", a higher level unit than a ""node"" as the core for graph embeddings. Accordingly, a multi-self-supervised auto-encoder is designed to fulfill two pretext tasks, to reconstruct the feature distribution for respective pairs and their surrounding context. PairE has three major advantages: 1) Informative, embedding beyond node-view are capable to preserve richer information of the graph; 2) Simple, the solutions provided by PairE are time-saving, storage-efficient, and require the fewer hyper-parameters; 3) High adaptability, with the introduced translator operator to map pair embeddings to the node embeddings, PairE can be effectively used in both the link-based and the node-based graph analysis. Experiment results show that PairE consistently outperforms the state of baselines in all four downstream tasks, especially with significant edges in the link-prediction and multi-label node classification tasks.",0
"The use of low-dimensional graph embeddings has proven to be beneficial in various downstream tasks in large graphs, such as link-related content recommendation and node classification. However, most current embedding methods focus on nodes as the primary unit for information aggregation, such as node perception fields or contextual nodes. This node-centric approach lacks the ability to express complex relationships between nodes, resulting in the loss of important graph information during embedding. To address this issue, the proposed PairE (Pair Embedding) solution utilizes pairs as a higher-level unit for graph embeddings. A multi-self-supervised auto-encoder is employed to complete two pretext tasks: reconstructing the feature distribution for each pair and its surrounding context. PairE offers three significant benefits: 1) it preserves richer graph information beyond the node-centric approach, 2) it provides time-saving and storage-efficient solutions with fewer hyper-parameters, and 3) it is highly adaptable for both link-based and node-based graph analysis with the use of a translator operator to map pair embeddings to node embeddings. Experimental results demonstrate that PairE outperforms current baselines in all downstream tasks, particularly in link-prediction and multi-label node classification tasks.",1
"Recent years have witnessed the emergence and flourishing of hierarchical graph pooling neural networks (HGPNNs) which are effective graph representation learning approaches for graph level tasks such as graph classification. However, current HGPNNs do not take full advantage of the graph's intrinsic structures (e.g., community structure). Moreover, the pooling operations in existing HGPNNs are difficult to be interpreted. In this paper, we propose a new interpretable graph pooling framework - CommPOOL, that can capture and preserve the hierarchical community structure of graphs in the graph representation learning process. Specifically, the proposed community pooling mechanism in CommPOOL utilizes an unsupervised approach for capturing the inherent community structure of graphs in an interpretable manner. CommPOOL is a general and flexible framework for hierarchical graph representation learning that can further facilitate various graph-level tasks. Evaluations on five public benchmark datasets and one synthetic dataset demonstrate the superior performance of CommPOOL in graph representation learning for graph classification compared to the state-of-the-art baseline methods, and its effectiveness in capturing and preserving the community structure of graphs.",0
"Over the years, hierarchical graph pooling neural networks (HGPNNs) have gained popularity for their effectiveness in graph representation learning, especially for graph classification. However, the current HGPNNs do not fully utilize the intrinsic structures of a graph, such as its community structure, and the existing pooling operations are not easily interpretable. This study introduces CommPOOL, a new framework for interpretable graph pooling that captures and preserves the hierarchical community structure of graphs during the representation learning process. The proposed mechanism utilizes an unsupervised approach to interpret the inherent community structure of graphs. CommPOOL is a flexible and general framework that can enhance different graph-level tasks. The study evaluated CommPOOL on several benchmark datasets and a synthetic dataset, and the results showed that CommPOOL outperformed the state-of-the-art baseline methods in graph representation learning for graph classification. Additionally, CommPOOL was effective in capturing and preserving the community structure of graphs.",1
"One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https://github.com/elad-amrani/ssml.",0
"Leveraging multimodal data is crucial for enabling machine learning models to understand and tackle real-world problems. However, annotating such data is difficult and expensive. To address this issue, self-supervised multimodal methods have been proposed that combine vision and language to learn multimodal representations without annotation. However, these methods often overlook high levels of noise, leading to suboptimal outcomes. In this study, we demonstrate that multimodal density estimation can help reduce the problem of noise estimation for multimodal data. Our proposed noise estimation building block for multimodal representation learning relies solely on the inherent correlation between different modalities. We show that our noise estimation can be widely integrated and achieve results comparable to state-of-the-art performance on five benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Additionally, we present a theoretical probabilistic error bound supporting our empirical results and examine failure cases. The code for our approach is available at https://github.com/elad-amrani/ssml.",1
"Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders using adversarial training. Furthermore, we exert additional constraints on embedding space by introducing reconstruction loss and classification loss. Then we fuse the encoded representations using hierarchical graph neural network which explicitly explores unimodal, bimodal and trimodal interactions in multi-stage. Our method achieves state-of-the-art performance on multiple datasets. Visualization of the learned embeddings suggests that the joint embedding space learned by our method is discriminative. code is available at: \url{https://github.com/TmacMai/ARGF_multimodal_fusion}",0
"The acquisition of a joint embedding space for different modalities is crucial for multimodal fusion. Conventional approaches for modality fusion are unsuccessful in accomplishing this objective, which leads to a modality gap that severely affects cross-modal fusion. The current study puts forth a new adversarial encoder-decoder-classifier framework to learn a modality-independent embedding space. Since the modalities have different distributions, we translate the source modalities' distributions to the target modality's distribution using adversarial training to decrease the modality gap. Additionally, we implement additional constraints on the embedding space by introducing reconstruction and classification losses. Then, we integrate the encoded representations using a hierarchical graph neural network that investigates unimodal, bimodal, and trimodal interactions in multiple stages. Our method achieves top-notch performance on various datasets. The learned embeddings' visualization indicates that our approach's joint embedding space is discriminative. The code is available at: \url{https://github.com/TmacMai/ARGF_multimodal_fusion}",1
"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",0
"In this article, we introduce a new type of deep learning architecture for tabular data called TabNet. This architecture is both high-performing and easy to interpret, thanks to its use of sequential attention to select the most important features for decision-making. By focusing on these salient features, TabNet is able to learn more efficiently than other neural network and decision tree models. Additionally, TabNet provides interpretable feature attributions and insights into the model's overall behavior. We also demonstrate the effectiveness of self-supervised learning for tabular data, which greatly improves performance when unlabeled data is available.",1
"In this work we propose for the first time a transformer-based framework for unsupervised representation learning of multivariate time series. Pre-trained models can be potentially used for downstream tasks such as regression and classification, forecasting and missing value imputation. By evaluating our models on several benchmark datasets for multivariate time series regression and classification, we show that not only does our modeling approach represent the most successful method employing unsupervised learning of multivariate time series presented to date, but also that it exceeds the current state-of-the-art performance of supervised methods; it does so even when the number of training samples is very limited, while offering computational efficiency. Finally, we demonstrate that unsupervised pre-training of our transformer models offers a substantial performance benefit over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples through the unsupervised objective.",0
"The primary objective of this study is to introduce a transformer-based framework, which is the first of its kind, for unsupervised representation learning of multivariate time series. The pre-trained models developed using this framework can potentially be employed for various downstream tasks, such as regression and classification, forecasting, and missing value imputation. Through a comprehensive evaluation of our models on several benchmark datasets, we establish that our modeling approach outperforms all previous methods that employ unsupervised learning of multivariate time series. Moreover, our approach also surpasses the current state-of-the-art performance of supervised methods, even with a limited number of training samples, while ensuring computational efficiency. Furthermore, we demonstrate that unsupervised pre-training of our transformer models offers a significant performance advantage over fully supervised learning, without leveraging additional unlabeled data, by reusing the same data samples through the unsupervised objective.",1
"Floorplans are commonly used to represent the layout of buildings. In computer aided-design (CAD) floorplans are usually represented in the form of hierarchical graph structures. Research works towards computational techniques that facilitate the design process, such as automated analysis and optimization, often use simple floorplan representations that ignore the semantics of the space and do not take into account usage related analytics. We present a floorplan embedding technique that uses an attributed graph to represent the geometric information as well as design semantics and behavioral features of the inhabitants as node and edge attributes. A Long Short-Term Memory (LSTM) Variational Autoencoder (VAE) architecture is proposed and trained to embed attributed graphs as vectors in a continuous space. A user study is conducted to evaluate the coupling of similar floorplans retrieved from the embedding space with respect to a given input (e.g., design layout). The qualitative, quantitative and user-study evaluations show that our embedding framework produces meaningful and accurate vector representations for floorplans. In addition, our proposed model is a generative model. We studied and showcased its effectiveness for generating new floorplans. We also release the dataset that we have constructed and which, for each floorplan, includes the design semantics attributes as well as simulation generated human behavioral features for further study in the community.",0
"Buildings' layouts are commonly represented by floorplans. In computer aided-design, hierarchical graph structures are usually utilized to depict floorplans. However, research aimed at improving the design process through automated analysis and optimization often employs simplistic floorplan representations that disregard the space's meaning and usage-related analytics. To address this issue, we introduce a floorplan embedding technique that uses an attributed graph to incorporate geometric data, design semantics, and behavioral characteristics of the inhabitants as node and edge attributes. The technique employs a Long Short-Term Memory (LSTM) Variational Autoencoder (VAE) architecture to embed attributed graphs as vectors in a continuous space. A user study was conducted to assess the effectiveness of the embedding framework in coupling similar floorplans retrieved from the embedding space with respect to a given input. The study found that our embedding framework produces meaningful and accurate vector representations for floorplans and is a generative model that can generate new floorplans. We also provide the dataset we constructed, which includes design semantics attributes and simulation-generated human behavioral features for further community study.",1
"In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These two tasks aim at reading and understanding scene text in images for question answering and image caption generation, respectively. In contrast to the conventional vision-language pre-training that fails to capture scene text and its relationship with the visual and text modalities, TAP explicitly incorporates scene text (generated from OCR engines) in pre-training. With three pre-training tasks, including masked language modeling (MLM), image-text (contrastive) matching (ITM), and relative (spatial) position prediction (RPP), TAP effectively helps the model learn a better aligned representation among the three modalities: text word, visual object, and scene text. Due to this aligned representation learning, even pre-trained on the same downstream task dataset, TAP already boosts the absolute accuracy on the TextVQA dataset by +5.4%, compared with a non-TAP baseline. To further improve the performance, we build a large-scale dataset based on the Conceptual Caption dataset, named OCR-CC, which contains 1.4 million scene text-related image-text pairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA, +8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.",0
"The paper proposes a new approach called Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These tasks require understanding and interpreting scene text in images to generate image captions and answer questions. The conventional vision-language pre-training fails to capture the relationship between scene text and the other modalities. However, TAP incorporates scene text generated from OCR engines in pre-training through three tasks: masked language modeling (MLM), image-text (contrastive) matching (ITM), and relative (spatial) position prediction (RPP). This aligned representation learning improves the accuracy of TAP on the TextVQA dataset by +5.4% compared to a non-TAP baseline. To further enhance performance, a new dataset called OCR-CC is introduced, which contains 1.4 million scene text-related image-text pairs. Pre-trained on this OCR-CC dataset, TAP outperforms the state of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA, +8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.",1
"Style analysis of artwork in computer vision predominantly focuses on achieving results in target image generation through optimizing understanding of low level style characteristics such as brush strokes. However, fundamentally different techniques are required to computationally understand and control qualities of art which incorporate higher level style characteristics. We study style representations learned by neural network architectures incorporating these higher level characteristics. We find variation in learned style features from incorporating triplets annotated by art historians as supervision for style similarity. Networks leveraging statistical priors or pretrained on photo collections such as ImageNet can also derive useful visual representations of artwork. We align the impact of these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and use these representations to perform zero-shot classification of artists. To facilitate this work, we also present the first large-scale dataset of portraits prepared for computational analysis.",0
"The primary focus of style analysis in computer vision is on generating target images by optimizing low level style characteristics like brush strokes. However, understanding and controlling higher level style characteristics in art requires different computational techniques. Our research involves studying style representations learned by neural network architectures that incorporate these higher level characteristics. By using triplets annotated by art historians as a supervision for style similarity, we have observed variation in learned style features. Additionally, networks using statistical priors or pre-trained on photo collections like ImageNet can also derive useful visual representations of artwork. We align the impact of these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and use these representations for zero-shot classification of artists. To support this work, we have introduced the first large-scale dataset of portraits for computational analysis.",1
"Contrastive self-supervised learning has emerged as a promising approach to unsupervised visual representation learning. In general, these methods learn global (image-level) representations that are invariant to different views (i.e., compositions of data augmentation) of the same image. However, many visual understanding tasks require dense (pixel-level) representations. In this paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised learning of dense representations. VADeR learns pixelwise representations by forcing local features to remain constant over different viewing conditions. Specifically, this is achieved through pixel-level contrastive learning: matching features (that is, features that describes the same location of the scene on different views) should be close in an embedding space, while non-matching features should be apart. VADeR provides a natural representation for dense prediction tasks and transfers well to downstream tasks. Our method outperforms ImageNet supervised pretraining (and strong unsupervised baselines) in multiple dense prediction tasks.",0
"Unsupervised visual representation learning has shown promise with the emergence of contrastive self-supervised learning. Although these methods typically learn image-level representations that are invariant to different views, many visual tasks require pixel-level representations. This paper proposes View-Agnostic Dense Representation (VADeR), which learns dense representations through pixel-level contrastive learning. VADeR enforces constant local features across different viewing conditions to achieve this. Matching features are encouraged to be close in an embedding space, while non-matching features should be separate. VADeR provides a natural representation for dense prediction tasks and performs better than ImageNet supervised pretraining and strong unsupervised baselines in multiple dense prediction tasks.",1
"The artistic style of a painting is a rich descriptor that reveals both visual and deep intrinsic knowledge about how an artist uniquely portrays and expresses their creative vision. Accurate categorization of paintings across different artistic movements and styles is critical for large-scale indexing of art databases. However, the automatic extraction and recognition of these highly dense artistic features has received little to no attention in the field of computer vision research. In this paper, we investigate the use of deep self-supervised learning methods to solve the problem of recognizing complex artistic styles with high intra-class and low inter-class variation. Further, we outperform existing approaches by almost 20% on a highly class imbalanced WikiArt dataset with 27 art categories. To achieve this, we train the EnAET semi-supervised learning model (Wang et al., 2019) with limited annotated data samples and supplement it with self-supervised representations learned from an ensemble of spatial and non-spatial transformations.",0
"The style of a painting provides a detailed description of how an artist represents their creative vision through both visual and intrinsic elements. It is crucial to accurately categorize paintings into different artistic movements and styles for comprehensive indexing of art databases. Nevertheless, the computer vision research community has not given much attention to automatically extracting and recognizing these complex artistic features. This study explores the use of deep self-supervised learning methods to solve the problem of recognizing intricate artistic styles with high intra-class and low inter-class variability. We surpass existing methods by almost 20% on a WikiArt dataset with 27 art categories that are heavily imbalanced. To achieve this, we train the EnAET semi-supervised learning model (Wang et al., 2019) with limited annotated data samples and enhance it with self-supervised representations learned from an assortment of spatial and non-spatial transformations.",1
"Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.",0
"Value Iteration Networks (VINs) have become popular for integrating planning algorithms into deep reinforcement learning to enhance performance on tasks that require long-range reasoning and comprehension of environment dynamics. However, there are limitations such as the model lacking incentives to execute meaningful planning computations, the assumption of a discrete underlying state space, and a fixed and known Markov decision process (MDP). To address these limitations, we propose eXecuted Latent Value Iteration Networks (XLVINs) that leverage recent developments in contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. XLVINs can successfully deploy VIN-style models on generic environments and outperform model-free baselines across three general MDP setups. Moreover, XLVINs can match the performance of VIN-like models when the underlying MDP is discrete, fixed, and known.",1
"We create a framework for bootstrapping visual representation learning from a primitive visual grouping capability. We operationalize grouping via a contour detector that partitions an image into regions, followed by merging of those regions into a tree hierarchy. A small supervised dataset suffices for training this grouping primitive. Across a large unlabeled dataset, we apply this learned primitive to automatically predict hierarchical region structure. These predictions serve as guidance for self-supervised contrastive feature learning: we task a deep network with producing per-pixel embeddings whose pairwise distances respect the region hierarchy. Experiments demonstrate that our approach can serve as state-of-the-art generic pre-training, benefiting downstream tasks. We additionally explore applications to semantic region search and video-based object instance tracking.",0
"Our approach involves establishing a foundation for initiating visual representation learning through a rudimentary visual grouping capability. We achieve this by utilizing a contour detector to divide an image into regions, which are then combined into a tree hierarchy. Training this grouping primitive necessitates only a small supervised dataset. We utilize this learned primitive to predict automatic hierarchical region structure across a vast unlabeled dataset. These predictions guide self-supervised contrastive feature learning by instructing a deep network to generate per-pixel embeddings that adhere to the region hierarchy's pairwise distances. Experiments illustrate that our method is a cutting-edge generic pre-training technique that improves downstream tasks. We also explore potential applications for semantic region search and video-based object instance tracking.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"There is currently a lot of interest in self-supervised learning, which enables neural networks to develop strong representations from large amounts of unlabeled data. Furthermore, multi-task learning can enhance representation learning by training networks on related tasks at the same time, resulting in notable advances in performance. Our paper introduces three original self-supervised auxiliary tasks that leverage graph-based neural network models to achieve competitive results on standard semi-supervised graph classification tasks. We use Graph Convolutional Networks as a foundation, as they are one of the most promising methods for capturing relationships among structured data points.",1
"Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.",0
"Self-supervised learning approaches for computer vision now rely heavily on contrastive learning as a key component. This involves training visual representations by embedding two augmented versions of the same image close to each other while pushing apart the embeddings of different images. Recent studies have shown that effective contrastive learning requires heavy data augmentation and large sets of negatives. Data mixing strategies, either at the image or feature level, have also been found to improve supervised and semi-supervised learning. However, the importance of hard negatives in contrastive learning has been overlooked. To address this, we propose a hard negative mixing strategy at the feature level. This approach provides more meaningful negative samples and facilitates better and faster learning without requiring an increase in memory size. Our exhaustive ablation study shows that our approach improves the quality of visual representations learned by a state-of-the-art self-supervised learning method in linear classification, object detection, and instance segmentation tasks.",1
"Deep model-based Reinforcement Learning (RL) has the potential to substantially improve the sample-efficiency of deep RL. While various challenges have long held it back, a number of papers have recently come out reporting success with deep model-based methods. This is a great development, but the lack of a consistent metric to evaluate such methods makes it difficult to compare various approaches. For example, the common single-task sample-efficiency metric conflates improvements due to model-based learning with various other aspects, such as representation learning, making it difficult to assess true progress on model-based RL. To address this, we introduce an experimental setup to evaluate model-based behavior of RL methods, inspired by work from neuroscience on detecting model-based behavior in humans and animals. Our metric based on this setup, the Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior, even if the method uses a poor representation and provides insight in how close a method's behavior is from optimal model-based behavior. We use our setup to evaluate the model-based behavior of MuZero on a variation of the classic Mountain Car task.",0
"The use of deep model-based Reinforcement Learning (RL) holds great potential for significantly enhancing the sample-efficiency of deep RL. Despite facing several challenges in the past, recent papers have reported success with deep model-based approaches. However, the lack of a consistent evaluation metric makes it challenging to compare different methods. The current single-task sample-efficiency metric does not solely reflect improvements due to model-based learning, but also other factors such as representation learning. To overcome this, we present an experimental arrangement inspired by neuroscience to evaluate model-based behavior of RL methods. We introduce a metric based on this setup, called Local Change Adaptation (LoCA) regret, which measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior and provide insights into the proximity of a method's behavior to optimal model-based behavior, even if it uses a poor representation. We apply our setup to evaluate MuZero's model-based behavior on a variation of the classic Mountain Car task.",1
"We study a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\tilde{O}(\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network.",0
"Our research focuses on contextual bandits, a general class where each context-action pair has a raw feature vector, but we do not know the reward generating function. We have developed a new learning algorithm that utilizes deep representation learning by transforming the raw feature vector using the last hidden layer of a deep ReLU neural network. To explore, we use an upper confidence bound (UCB) approach in the last linear layer (shallow exploration). Our algorithm achieves $\tilde{O}(\sqrt{T})$ finite-time regret under standard assumptions, where $T$ is the learning time horizon. Our method is more computationally efficient than existing neural contextual bandit algorithms because it only explores in the last layer of the deep neural network.",1
"Variational mutual information (MI) estimators are widely used in unsupervised representation learning methods such as contrastive predictive coding (CPC). A lower bound on MI can be obtained from a multi-class classification problem, where a critic attempts to distinguish a positive sample drawn from the underlying joint distribution from $(m-1)$ negative samples drawn from a suitable proposal distribution. Using this approach, MI estimates are bounded above by $\log m$, and could thus severely underestimate unless $m$ is very large. To overcome this limitation, we introduce a novel estimator based on a multi-label classification problem, where the critic needs to jointly identify multiple positive samples at the same time. We show that using the same amount of negative samples, multi-label CPC is able to exceed the $\log m$ bound, while still being a valid lower bound of mutual information. We demonstrate that the proposed approach is able to lead to better mutual information estimation, gain empirical improvements in unsupervised representation learning, and beat a current state-of-the-art knowledge distillation method over 10 out of 13 tasks.",0
"Unsupervised representation learning methods such as contrastive predictive coding (CPC) widely use variational mutual information (MI) estimators. To obtain a lower bound on MI, a critic distinguishes a positive sample from $(m-1)$ negative samples using a multi-class classification problem. However, this approach can severely underestimate MI unless $m$ is very large. To address this issue, we introduce a novel estimator based on a multi-label classification problem where the critic identifies multiple positive samples simultaneously. With the same number of negative samples, multi-label CPC exceeds the $\log m$ bound while still being a valid lower bound of mutual information. Our proposed approach leads to better mutual information estimation, shows empirical improvements in unsupervised representation learning, and outperforms a current state-of-the-art knowledge distillation method in 10 out of 13 tasks.",1
"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose Dy- ERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.",0
"In recent times, there has been a growing interest in acquiring knowledge about temporal knowledge graphs (KGs) that capture the changing relationships between entities over time. These KGs often exhibit multiple non-Euclidean structures, such as cyclic and hierarchical structures. However, the current embedding techniques for temporal KGs typically learn entity representations and their dynamic evolution in Euclidean space, which may not effectively capture these intrinsic structures. To overcome this, we present Dy-ERNIE, which is a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds that reflect a wide variety of geometric structures on temporal KGs. Furthermore, to capture the dynamics of temporal KGs, we enable the entity representations to evolve based on a velocity vector defined in the tangent space at each timestamp. We analyze the impact of geometric spaces on representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Our experiments on three real-world datasets show significantly improved performance, indicating that the evolution of embeddings on Riemannian manifolds can more appropriately model the dynamics of multi-relational graph data.",1
"We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the wholes cene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects' appearance, such as shadow and lighting, and provides control over each object's 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity).",0
"Introducing BlockGAN, an image generation model that directly learns object-aware 3D scene representations from unlabelled 2D images. Current scene representation learning either disregards the background or considers the whole scene as a single object. Conversely, approaches that account for scene compositionality deem scene objects as mere image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we devised BlockGAN to initially generate 3D features of the background and foreground objects, then combine them into 3D features for the entire scene, and finally render them into lifelike images. This allows BlockGAN to reason over occlusion and interaction between the objects' appearance, such as lighting and shadow, and delivers control over each object's 3D pose and identity, while preserving image realism. BlockGAN is trained end-to-end using only unlabelled single images, without the necessity for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments reveal that using explicit 3D features to depict objects enables BlockGAN to learn disentangled representations, both in terms of objects (foreground and background) and their properties (pose and identity).",1
"Contrastive representation learning has been recently proved to be very efficient for self-supervised training. These methods have been successfully used to train encoders which perform comparably to supervised training on downstream classification tasks. A few works have started to build a theoretical framework around contrastive learning in which guarantees for its performance can be proven. We provide extensions of these results to training with multiple negative samples and for multiway classification. Furthermore, we provide convergence guarantees for the minimization of the contrastive training error with gradient descent of an overparametrized deep neural encoder, and provide some numerical experiments that complement our theoretical findings",0
"Recently, self-supervised training has shown great efficiency through contrastive representation learning. This method has successfully trained encoders that perform comparably to supervised training on classification tasks. Several studies have developed a theoretical framework around contrastive learning to prove its performance guarantees. We extend these results to training with multiple negative samples and multiway classification, and provide convergence guarantees for minimizing contrastive training error using gradient descent of an overparametrized deep neural encoder. Additionally, numerical experiments are provided to complement our theoretical findings.",1
"Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.",0
"In the past few years, significant advancements have been made in self-supervised representation learning. Amongst these, contrastive instance learning has proven to be more effective than supervised learning. Despite the growing interest in contrastive instance learning, the reasons for its success remain largely unknown. This paper aims to uncover some of the factors contributing to its effectiveness by identifying best practices. Our extensive empirical analysis not only provides insights but also establishes a set of guidelines for achieving success in self-supervised representation learning.",1
"Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning (ML) method for achieving an automated support for qualitative analyses of students' writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK) prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning (ML) for natural language processing (NLP) holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",0
"Verbal data qualitative analysis is crucial in the learning sciences, but it is a time-consuming and labor-intensive process that limits the amount of data that researchers can include in their studies. This study aims to develop a statistical machine learning method that automates the process of analyzing students' writing, particularly in introductory biology laboratory reports, to determine the sophistication of their argumentation and reasoning. The research team used a set of lab reports from an undergraduate biology course to train a natural language processing pipeline, which included word embeddings and a Long Short Term Memory model. The algorithm was able to accurately capture the scoring of the lab reports, approaching the inter-rater reliability of human analysis. This study demonstrates the potential of machine learning for natural language processing in assisting learning sciences researchers in conducting qualitative studies on a larger scale.",1
"High dimensional data analysis for exploration and discovery includes three fundamental tasks: dimensionality reduction, clustering, and visualization. When the three associated tasks are done separately, as is often the case thus far, inconsistencies can occur among the tasks in terms of data geometry and others. This can lead to confusing or misleading data interpretation. In this paper, we propose a novel neural network-based method, called Consistent Representation Learning (CRL), to accomplish the three associated tasks end-to-end and improve the consistencies. The CRL network consists of two nonlinear dimensionality reduction (NLDR) transformations: (1) one from the input data space to the latent feature space for clustering, and (2) the other from the clustering space to the final 2D or 3D space for visualization. Importantly, the two NLDR transformations are performed to best satisfy local geometry preserving (LGP) constraints across the spaces or network layers, to improve data consistencies along with the processing flow. Also, we propose a novel metric, clustering-visualization inconsistency (CVI), for evaluating the inconsistencies. Extensive comparative results show that the proposed CRL neural network method outperforms the popular t-SNE and UMAP-based and other contemporary clustering and visualization algorithms in terms of evaluation metrics and visualization.",0
"The exploration and discovery of high-dimensional data involve three main tasks: dimensionality reduction, clustering, and visualization. However, performing these tasks separately can lead to inconsistencies that may result in misleading data interpretation. Therefore, we present a new approach called Consistent Representation Learning (CRL), which uses a neural network to perform all three tasks in a single end-to-end process, improving consistency. The CRL network comprises of two nonlinear dimensionality reduction (NLDR) transformations: one from the input data space to the latent feature space for clustering, and the other from the clustering space to the final 2D or 3D space for visualization. The NLDR transformations are performed to satisfy local geometry preserving (LGP) constraints across the spaces or network layers, enhancing data consistency. We also introduce a novel metric, clustering-visualization inconsistency (CVI), to evaluate inconsistencies. Our comparative results show that the CRL neural network method outperforms other contemporary clustering and visualization algorithms, including the popular t-SNE and UMAP-based methods, in terms of evaluation metrics and visualization.",1
"The performance of generative zero-shot methods mainly depends on the quality of generated features and how well the model facilitates knowledge transfer between visual and semantic domains. The quality of generated features is a direct consequence of the ability of the model to capture the several modes of the underlying data distribution. To address these issues, we propose a new two-level joint maximization idea to augment the generative network with an inference network during training which helps our model capture the several modes of the data and generate features that better represent the underlying data distribution. This provides strong cross-modal interaction for effective transfer of knowledge between visual and semantic domains. Furthermore, existing methods train the zero-shot classifier either on generate synthetic image features or latent embeddings produced by leveraging representation learning. In this work, we unify these paradigms into a single model which in addition to synthesizing image features, also utilizes the representation learning capabilities of the inference network to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several state-of-the-art methods, and show its performance. We also perform ablation studies to analyze and understand our method more carefully for the Generalized Zero-shot Learning task.",0
"The effectiveness of generative zero-shot methods relies heavily on the quality of the generated features and the model's ability to facilitate the transfer of knowledge between visual and semantic domains. The quality of these features is directly influenced by the model's capacity to capture various modes of the underlying data distribution. Our proposed solution involves a two-level joint maximization approach during training, whereby an inference network is added to the generative network to capture various modes of data and produce features that better represent the data distribution. This approach enables effective cross-modal interaction and knowledge transfer. Unlike existing methods that train the zero-shot classifier on either synthetic image features or latent embeddings, our unified model synthesizes image features and utilizes the inference network's representation learning capabilities to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on the CUB, FLO, AWA1, and AWA2 benchmark datasets against state-of-the-art methods, demonstrating its superior performance. Additionally, we perform ablation studies to gain a deeper understanding of our approach for the Generalized Zero-shot Learning task.",1
"Estimating mutual information between continuous random variables is often intractable and extremely challenging for high-dimensional data. Recent progress has leveraged neural networks to optimize variational lower bounds on mutual information. Although showing promise for this difficult problem, the variational methods have been theoretically and empirically proven to have serious statistical limitations: 1) many methods struggle to produce accurate estimates when the underlying mutual information is either low or high; 2) the resulting estimators may suffer from high variance. Our approach is based on training a classifier that provides the probability that a data sample pair is drawn from the joint distribution rather than from the product of its marginal distributions. Moreover, we establish a direct connection between mutual information and the average log odds estimate produced by the classifier on a test set, leading to a simple and accurate estimator of mutual information. We show theoretically that our method and other variational approaches are equivalent when they achieve their optimum, while our method sidesteps the variational bound. Empirical results demonstrate high accuracy of our approach and the advantages of our estimator in the context of representation learning. Our demo is available at https://github.com/RayRuizhiLiao/demi_mi_estimator.",0
"Calculating mutual information for continuous random variables is a difficult task, especially for high-dimensional data. Recently, researchers have utilized neural networks to optimize variational lower bounds on mutual information, although these methods have limitations. They struggle to produce accurate results when the underlying mutual information is low or high and may suffer from high variance. Our approach involves training a classifier to determine the probability that a data sample pair is drawn from the joint distribution rather than from the product of its marginal distributions. We establish a direct link between mutual information and the average log odds estimate produced by the classifier on a test set, resulting in a simple and reliable estimator of mutual information. Our method is equivalent to other variational approaches when they reach their optimum, but our approach avoids the variational bound. Our method has demonstrated high accuracy and is advantageous for representation learning. To see our demo, visit https://github.com/RayRuizhiLiao/demi_mi_estimator.",1
"Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that creates synchronization constraints in the weight update step across layers and is not biologically plausible. Recent advances in unsupervised contrastive representation learning point to the question of whether a learning algorithm can also be made local, that is, the updates of lower layers do not directly depend on the computation of upper layers. While Greedy InfoMax separately learns each block with a local objective, we found that it consistently hurts readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly send feedbacks to lower blocks. This simple design closes the performance gap between local learning and end-to-end contrastive learning algorithms for the first time. Aside from standard ImageNet experiments, we also show results on complex downstream tasks such as object detection and instance segmentation directly using readout features.",0
"The traditional method for deep neural nets involves end-to-end backpropagation to learn the weights, but this creates synchronization constraints that go against biological plausibility. Unsupervised contrastive representation learning has led to the idea of making learning algorithms local, where weight updates in lower layers do not depend on upper layers. However, the Greedy InfoMax approach that learns each block separately with a local objective has been found to negatively impact readout accuracy in state-of-the-art unsupervised contrastive learning algorithms. By overlapping local blocks, we can increase decoder depth and allow upper blocks to send feedback to lower blocks, which closes the performance gap between local learning and end-to-end contrastive learning algorithms. In addition to ImageNet experiments, we also demonstrate success in complex tasks like object detection and instance segmentation using readout features.",1
"Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at $\ell_\infty$-distance at most $\epsilon$, thus allowing data consumers to certify individual fairness by proving $\epsilon$-robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",0
"Enforcing fairness constraints without sacrificing utility for downstream users can be achieved through fair representation learning. A set of desired fairness constraints, known as individual fairness, seeks to treat similar individuals similarly. In this study, we introduce a novel method that allows data consumers to obtain certificates of individual fairness for both existing and new data points. The method involves mapping similar individuals to close latent representations and using this proximity to certify individual fairness. By learning and certifying a representation where similar individuals are within an $\ell_\infty$-distance of $\epsilon$ or less, data consumers can prove $\epsilon$-robustness of their classifier and certify individual fairness. Our approach is scalable and expressive, as demonstrated by experimental evaluation on five real-world datasets and various fairness constraints.",1
"Canine mammary carcinoma (CMC) has been used as a model to investigate the pathogenesis of human breast cancer and the same grading scheme is commonly used to assess tumor malignancy in both. One key component of this grading scheme is the density of mitotic figures (MF). Current publicly available datasets on human breast cancer only provide annotations for small subsets of whole slide images (WSIs). We present a novel dataset of 21 WSIs of CMC completely annotated for MF. For this, a pathologist screened all WSIs for potential MF and structures with a similar appearance. A second expert blindly assigned labels, and for non-matching labels, a third expert assigned the final labels. Additionally, we used machine learning to identify previously undetected MF. Finally, we performed representation learning and two-dimensional projection to further increase the consistency of the annotations. Our dataset consists of 13,907 MF and 36,379 hard negatives. We achieved a mean F1-score of 0.791 on the test set and of up to 0.696 on a human breast cancer dataset.",0
"The grading scheme for assessing tumor malignancy in both canine mammary carcinoma (CMC) and human breast cancer involves evaluating the density of mitotic figures (MF). However, publicly available datasets for human breast cancer only offer annotations for a small subset of whole slide images (WSIs). To address this limitation, we introduced a new dataset featuring 21 fully annotated WSIs of CMC, with a pathologist identifying potential MF and a second expert assigning labels while a third expert resolved discrepancies. Our approach also involved deploying machine learning to identify previously undetected MF, while representation learning and two-dimensional projection were used to increase annotation consistency. The resulting dataset comprises 13,907 MF and 36,379 hard negatives, with a mean F1-score of 0.791 on the test set and up to 0.696 on a human breast cancer dataset.",1
"Feature-based transfer is one of the most effective methodologies for transfer learning. Existing studies usually assume that the learned new feature representation is truly \emph{domain-invariant}, and thus directly train a transfer model $\mathcal{M}$ on source domain. In this paper, we consider a more realistic scenario where the new feature representation is suboptimal and small divergence still exists across domains. We propose a new learning strategy with a transfer model called Randomized Transferable Machine (RTM). More specifically, we work on source data with the new feature representation learned from existing feature-based transfer methods. The key idea is to enlarge source training data populations by randomly corrupting source data using some noises, and then train a transfer model $\widetilde{\mathcal{M}}$ that performs well on all the corrupted source data populations. In principle, the more corruptions are made, the higher the probability of the target data can be covered by the constructed source populations, and thus better transfer performance can be achieved by $\widetilde{\mathcal{M}}$. An ideal case is with infinite corruptions, which however is infeasible in reality. We develop a marginalized solution with linear regression model and dropout noise. With a marginalization trick, we can train an RTM that is equivalently to training using infinite source noisy populations without truly conducting any corruption. More importantly, such an RTM has a closed-form solution, which enables very fast and efficient training. Extensive experiments on various real-world transfer tasks show that RTM is a promising transfer model.",0
"Transfer learning can be achieved through feature-based transfer, which is a highly effective methodology. Previous studies have assumed that the new feature representation is completely domain-invariant, allowing for direct training of a transfer model on the source domain. However, in reality, suboptimal new feature representations and small divergences across domains exist. In this paper, we propose a new learning strategy using a transfer model called Randomized Transferable Machine (RTM). This involves enlarging the source training data populations by randomly corrupting source data using noises, and training a transfer model $\widetilde{\mathcal{M}}$ that performs well on all the corrupted source data populations. The more corruptions made, the higher the probability of covering the target data, leading to better transfer performance by $\widetilde{\mathcal{M}}$. Although infinite corruptions are ideal, they are not feasible. To solve this, we develop a marginalized solution with a linear regression model and dropout noise. This allows for training an RTM that is equivalent to training with infinite source noisy populations without any actual corruption, and has a closed-form solution for fast and efficient training. Extensive experiments on various real-world transfer tasks demonstrate the effectiveness of RTM as a promising transfer model.",1
"Recent advances in unsupervised representation learning have experienced remarkable progress, especially with the achievements of contrastive learning, which regards each image as well its augmentations as a separate class, while does not consider the semantic similarity among images. This paper proposes a new kind of data augmentation, named Center-wise Local Image Mixture, to expand the neighborhood space of an image. CLIM encourages both local similarity and global aggregation while pulling similar images. This is achieved by searching local similar samples of an image, and only selecting images that are closer to the corresponding cluster center, which we denote as center-wise local selection. As a result, similar representations are progressively approaching the clusters, while do not break the local similarity. Furthermore, image mixture is used as a smoothing regularization to avoid overconfidence on the selected samples. Besides, we introduce multi-resolution augmentation, which enables the representation to be scale invariant. Integrating the two augmentations produces better feature representation on several unsupervised benchmarks. Notably, we reach 75.5% top-1 accuracy with linear evaluation over ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels, as well as consistently outperforming supervised pretraining on several downstream transfer tasks.",0
"Remarkable progress has been made in unsupervised representation learning, particularly through contrastive learning, which treats each image and its augmentations as separate classes without considering semantic similarity between images. This paper proposes Center-wise Local Image Mixture (CLIM) as a novel data augmentation method to increase an image's neighborhood space. CLIM promotes local similarity and global aggregation while pulling similar images closer to corresponding cluster centers. This is achieved by selecting images that are closer to the center in center-wise local selection and using image mixture as a smoothing regularization to avoid overconfidence on selected samples. Additionally, multi-resolution augmentation ensures scale invariance. Integrating these augmentations improves feature representation on unsupervised benchmarks, achieving 75.5% top-1 accuracy with linear evaluation over ResNet-50 and 59.3% top-1 accuracy with only 1% labels, outperforming supervised pretraining on several downstream transfer tasks.",1
"Self-supervised learning achieves superior performance in many domains by extracting useful representations from the unlabeled data. However, most of traditional self-supervised methods mainly focus on exploring the inter-sample structure while less efforts have been concentrated on the underlying intra-temporal structure, which is important for time series data. In this paper, we present SelfTime: a general self-supervised time series representation learning framework, by exploring the inter-sample relation and intra-temporal relation of time series to learn the underlying structure feature on the unlabeled time series. Specifically, we first generate the inter-sample relation by sampling positive and negative samples of a given anchor sample, and intra-temporal relation by sampling time pieces from this anchor. Then, based on the sampled relation, a shared feature extraction backbone combined with two separate relation reasoning heads are employed to quantify the relationships of the sample pairs for inter-sample relation reasoning, and the relationships of the time piece pairs for intra-temporal relation reasoning, respectively. Finally, the useful representations of time series are extracted from the backbone under the supervision of relation reasoning heads. Experimental results on multiple real-world time series datasets for time series classification task demonstrate the effectiveness of the proposed method. Code and data are publicly available at https://haoyfan.github.io/.",0
"Many domains benefit from self-supervised learning as it extracts valuable representations from unlabeled data. However, traditional methods tend to focus more on exploring inter-sample structure while neglecting the crucial underlying intra-temporal structure, particularly for time series data. To address this issue, we introduce SelfTime, a comprehensive self-supervised time series representation learning framework that explores both inter-sample and intra-temporal relations to uncover the latent structural features of unlabeled time series. Firstly, we generate inter-sample relations by sampling positive and negative samples from a given anchor sample, and intra-temporal relations by sampling time pieces from the same anchor. Then, we employ a shared feature extraction backbone combined with two separate relation reasoning heads to quantify the relationships between sample pairs for inter-sample relation reasoning and time piece pairs for intra-temporal relation reasoning. Finally, we extract useful representations of time series from the backbone supervised by relation reasoning heads. The effectiveness of the proposed method is demonstrated through experiments on multiple real-world time series datasets for time series classification tasks. Code and data are available at https://haoyfan.github.io/.",1
"Recent unsupervised contrastive representation learning follows a Single Instance Multi-view (SIM) paradigm where positive pairs are usually constructed with intra-image data augmentation. In this paper, we propose an effective approach called Beyond Single Instance Multi-view (BSIM). Specifically, we impose more accurate instance discrimination capability by measuring the joint similarity between two randomly sampled instances and their mixture, namely spurious-positive pairs. We believe that learning joint similarity helps to improve the performance when encoded features are distributed more evenly in the latent space. We apply it as an orthogonal improvement for unsupervised contrastive representation learning, including current outstanding methods SimCLR, MoCo, and BYOL. We evaluate our learned representations on many downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC 2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial gains with a large margin almost on all these tasks compared with prior arts.",0
"The prevailing approach in recent unsupervised contrastive representation learning is the Single Instance Multi-view (SIM) paradigm, which constructs positive pairs using intra-image data augmentation. This paper introduces a more effective method called Beyond Single Instance Multi-view (BSIM), which enhances instance discrimination by measuring joint similarity between two randomly sampled instances and their mixture, or spurious-positive pairs. By improving joint similarity, we can achieve better performance when features are evenly distributed in the latent space. We apply this method as an orthogonal improvement to outstanding unsupervised contrastive representation learning methods, such as SimCLR, MoCo, and BYOL. We evaluate our approach on various downstream benchmarks, including ImageNet-1k and PASCAL VOC 2007 for linear classification, and MS COCO 2017 and VOC for object detection. Our results show significant improvement over previous methods on all tasks.",1
"We apply a Transformer architecture, specifically BERT, to learn flexible and high quality molecular representations for drug discovery problems. We study the impact of using different combinations of self-supervised tasks for pre-training, and present our results for the established Virtual Screening and QSAR benchmarks. We show that: i) The selection of appropriate self-supervised task(s) for pre-training has a significant impact on performance in subsequent downstream tasks such as Virtual Screening. ii) Using auxiliary tasks with more domain relevance for Chemistry, such as learning to predict calculated molecular properties, increases the fidelity of our learnt representations. iii) Finally, we show that molecular representations learnt by our model `MolBert' improve upon the current state of the art on the benchmark datasets.",0
"To improve drug discovery, we utilized a Transformer architecture called BERT to develop flexible and high-quality molecular representations. Our research examined the effects of various self-supervised tasks for pre-training, and we present our findings for the Virtual Screening and QSAR benchmarks. Our study revealed the following: a) Proper selection of self-supervised tasks for pre-training is crucial for downstream task performance, such as Virtual Screening. b) Incorporating auxiliary tasks that are more relevant to Chemistry, like learning to predict molecular properties, improves the accuracy of our representations. c) Our model, MolBert, provides better molecular representations than the current state of the art, as demonstrated by the benchmark datasets.",1
"Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily focusing on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images.",0
"Learning disentanglement is crucial for achieving controllable generation and obtaining disentangled representations. However, current disentanglement methods have limitations such as difficulty with high-resolution images, focusing primarily on learning disentangled representations, and non-identifiability in the unsupervised setting. To address these limitations, we have designed new architectures and loss functions based on StyleGAN (Karras et al., 2019) for semi-supervised high-resolution disentanglement learning. We have created two complex high-resolution synthetic datasets for systematic testing and investigated the impact of limited supervision. We have found that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We have also proposed new metrics to quantify generator controllability and have observed a crucial trade-off between disentangled representation learning and controllable generation. Finally, we have considered semantic fine-grained image editing to achieve better generalization to unseen images.",1
"Leveraging temporal information has been regarded as essential for developing video understanding models. However, how to properly incorporate temporal information into the recent successful instance discrimination based contrastive self-supervised learning (CSL) framework remains unclear. As an intuitive solution, we find that directly applying temporal augmentations does not help, or even impair video CSL in general. This counter-intuitive observation motivates us to re-design existing video CSL frameworks, for better integration of temporal knowledge.   To this end, we present Temporal-aware Contrastive self-supervised learningTaCo, as a general paradigm to enhance video CSL. Specifically, TaCo selects a set of temporal transformations not only as strong data augmentation but also to constitute extra self-supervision for video understanding. By jointly contrasting instances with enriched temporal transformations and learning these transformations as self-supervised signals, TaCo can significantly enhance unsupervised video representation learning. For instance, TaCo demonstrates consistent improvement in downstream classification tasks over a list of backbones and CSL approaches. Our best model achieves 85.1% (UCF-101) and 51.6% (HMDB-51) top-1 accuracy, which is a 3% and 2.4% relative improvement over the previous state-of-the-art.",0
"The integration of temporal information is crucial for the development of video comprehension models. However, it is not clear how to effectively incorporate temporal information into the successful instance discrimination-based contrastive self-supervised learning (CSL) framework. Despite the common belief that temporal augmentations would improve video CSL, we have found that this approach can be ineffective or even detrimental. This unexpected finding has spurred us to redesign the existing video CSL frameworks to better integrate temporal knowledge. Our proposed solution, Temporal-aware Contrastive self-supervised learning (TaCo), is a comprehensive approach that enhances video CSL by selecting a set of temporal transformations to serve not only as data augmentation but also as self-supervision for video comprehension. By jointly contrasting instances with enriched temporal transformations and learning these transformations as self-supervised signals, TaCo can significantly enhance unsupervised video representation learning. Our approach has consistently demonstrated improved performance in downstream classification tasks across a range of backbones and CSL approaches. Our best model achieves a top-1 accuracy of 85.1% (UCF-101) and 51.6% (HMDB-51), representing a 3% and 2.4% relative improvement over the previous state-of-the-art.",1
"Multi-view action recognition (MVAR) leverages complementary temporal information from different views to improve the learning performance. Obtaining informative view-specific representation plays an essential role in MVAR. Attention has been widely adopted as an effective strategy for discovering discriminative cues underlying temporal data. However, most existing MVAR methods only utilize attention to extract representation for each view individually, ignoring the potential to dig latent patterns based on mutual-support information in attention space. To this end, we propose a collaborative attention mechanism (CAM) for solving the MVAR problem in this paper. The proposed CAM detects the attention differences among multi-view, and adaptively integrates frame-level information to benefit each other. Specifically, we extend the long short-term memory (LSTM) to a Mutual-Aid RNN (MAR) to achieve the multi-view collaboration process. CAM takes advantages of view-specific attention pattern to guide another view and discover potential information which is hard to be explored by itself. It paves a novel way to leverage attention information and enhances the multi-view representation learning. Extensive experiments on four action datasets illustrate the proposed CAM achieves better results for each view and also boosts multi-view performance.",0
"In order to improve learning performance, Multi-view action recognition (MVAR) utilizes complementary temporal information from different perspectives. Obtaining informative, view-specific representations is crucial for MVAR, and attention has proven to be an effective tool for discovering discriminative cues in temporal data. Unfortunately, most existing MVAR methods only use attention to extract representation for each view separately, ignoring the potential for latent pattern discovery based on mutual-support information in attention space. This paper proposes a collaborative attention mechanism (CAM) to address this issue. The CAM detects attention differences among multi-view and integrates frame-level information to benefit each other, using a Mutual-Aid RNN (MAR) to achieve multi-view collaboration. By using view-specific attention patterns to guide each other and explore potential information, CAM enhances multi-view representation learning. Extensive experiments on four action datasets show that the proposed CAM improves results for each view and boosts multi-view performance.",1
"Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.",0
"While deep learning has been successful in areas like image, voice, and graphs, there has been limited progress in deep representation learning for domains lacking structure between features. This includes tabular datasets with different demographic and clinical factors where feature interactions are not given beforehand. In this study, we present Group-Connected Multilayer Perceptron (GMLP) networks to facilitate deep representation learning in these domains. GMLP learns expressive feature combinations and reduces network complexity by defining local group-wise operations. During training, the network learns a sparse feature grouping matrix using temperature annealing softmax and an entropy loss term to promote sparsity. The suggested architecture resembles binary trees with group-wise operations followed by pooling operations to combine information and reduce group number as the network deepens. We evaluated GMLP on various real-world datasets and provided visualizations on MNIST and synthesized data. Results demonstrate GMLP's ability to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance.",1
"Adversarial training (AT) is one of the most effective defenses against adversarial attacks for deep learning models. In this work, we advocate incorporating the hypersphere embedding (HE) mechanism into the AT procedure by regularizing the features onto compact manifolds, which constitutes a lightweight yet effective module to blend in the strength of representation learning. Our extensive analyses reveal that AT and HE are well coupled to benefit the robustness of the adversarially trained models from several aspects. We validate the effectiveness and adaptability of HE by embedding it into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as the FreeAT and FastAT strategies. In the experiments, we evaluate our methods under a wide range of adversarial attacks on the CIFAR-10 and ImageNet datasets, which verifies that integrating HE can consistently enhance the model robustness for each AT framework with little extra computation.",0
"One of the most effective defenses against adversarial attacks for deep learning models is adversarial training (AT). The authors of this work suggest adding the hypersphere embedding (HE) mechanism to the AT procedure. This involves regularizing the features onto compact manifolds, which is a lightweight yet effective module for representation learning. The authors found that AT and HE work well together to improve the robustness of adversarially trained models. They embedded HE into popular AT frameworks such as PGD-AT, ALP, and TRADES, as well as FreeAT and FastAT strategies to validate its effectiveness and adaptability. The experiments involved testing the methods against a wide range of adversarial attacks on the CIFAR-10 and ImageNet datasets. The results consistently showed that integrating HE improved model robustness for each AT framework with minimal additional computation.",1
"The three-dimensional shape and conformation of small-molecule ligands are critical for biomolecular recognition, yet encoding 3D geometry has not improved ligand-based virtual screening approaches. We describe an end-to-end deep learning approach that operates directly on small-molecule conformational ensembles and identifies key conformational poses of small-molecules. Our networks leverage two levels of representation learning: 1) individual conformers are first encoded as spatial graphs using a graph neural network, and 2) sampled conformational ensembles are represented as sets using an attention mechanism to aggregate over individual instances. We demonstrate the feasibility of this approach on a simple task based on bidentate coordination of biaryl ligands, and show how attention-based pooling can elucidate key conformational poses in tasks based on molecular geometry. This work illustrates how set-based learning approaches may be further developed for small molecule-based virtual screening.",0
"Biomolecular recognition heavily relies on the 3D shape and conformation of small-molecule ligands. However, improving ligand-based virtual screening approaches by encoding 3D geometry has not been fruitful. In this study, we present an end-to-end deep learning technique that directly operates on small-molecule conformational ensembles and identifies crucial conformational poses of small-molecules. Our networks employ two levels of representation learning: firstly, individual conformers are encoded as spatial graphs using a graph neural network, and secondly, sampled conformational ensembles are represented as sets using an attention mechanism to aggregate over individual instances. To demonstrate the feasibility of this approach, we perform a simple task based on bidentate coordination of biaryl ligands and show how attention-based pooling can clarify key conformational poses in tasks based on molecular geometry. Our work showcases the potential of set-based learning approaches to further develop small molecule-based virtual screening.",1
"We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After illustrating an end-to-end training procedure based on a quadratic penalty optimization algorithm with warm start, we quantitatively evaluate the proposed method's effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to $\beta$-VAE on a number of disentanglement metrics when few training points are available, while being less sensitive to randomness and hyperparameter selection than $\beta$-VAE. We also present a deterministic initialization of Constr-DRKM's training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.",0
"Constr-DRKM is a new approach for learning disentangled data representations through unsupervised deep kernel methods. We enhance the original deep restricted kernel machine formulation for kernel PCA by imposing orthogonality constraints on the latent variables to facilitate disentanglement and optimize without a stabilized objective. We demonstrate the effectiveness of our method through an end-to-end training procedure based on quadratic penalty optimization with warm start. Our approach performs similarly to $\beta$-VAE on disentanglement metrics for datasets with few training points, but is less sensitive to randomness and hyperparameter selection. We also present a deterministic initialization of our training algorithm that significantly improves reproducibility. Finally, we examine the role of the number of layers in our methodology, showing that lower layers act as local feature detectors while deeper layers use learned representations to reproduce higher-level features.",1
"We introduce CellSegmenter, a structured deep generative model and an amortized inference framework for unsupervised representation learning and instance segmentation tasks. The proposed inference algorithm is convolutional and parallelized, without any recurrent mechanisms, and is able to resolve object-object occlusion while simultaneously treating distant non-occluding objects independently. This leads to extremely fast training times while allowing extrapolation to arbitrary number of instances. We further introduce a transparent posterior regularization strategy that encourages scene reconstructions with fewest localized objects and a low-complexity background. We evaluate our method on a challenging synthetic multi-MNIST dataset with a structured background and achieve nearly perfect accuracy with only a few hundred training epochs. Finally, we show segmentation results obtained for a cell nuclei imaging dataset, demonstrating the ability of our method to provide high-quality segmentations while also handling realistic use cases involving large number of instances.",0
"CellSegmenter is a novel deep generative model that utilizes an amortized inference framework for unsupervised representation learning and instance segmentation tasks. The convolutional and parallelized inference algorithm proposed does not require any recurrent mechanisms and can effectively handle object-object occlusion and distant non-occluding objects. The model's fast training times allow for extrapolation to any number of instances. Additionally, the model incorporates a posterior regularization strategy that promotes scene reconstructions with fewer localized objects and a low-complexity background. We evaluate our approach on a synthetic multi-MNIST dataset and achieve excellent accuracy with minimal training epochs. Further, we demonstrate the effectiveness of our model on a cell nuclei imaging dataset, showcasing its ability to handle real-world scenarios with a large number of instances while producing high-quality segmentations.",1
"We introduce Exemplar VAEs, a family of generative models that bridge the gap between parametric and non-parametric, exemplar based generative models. Exemplar VAE is a variant of VAE with a non-parametric prior in the latent space based on a Parzen window estimator. To sample from it, one first draws a random exemplar from a training set, then stochastically transforms that exemplar into a latent code and a new observation. We propose retrieval augmented training (RAT) as a way to speed up Exemplar VAE training by using approximate nearest neighbor search in the latent space to define a lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-one-out and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Importantly, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17% to 0.69% and from 8.56% to 8.16%.",0
"Exemplar VAEs are a group of generative models that bridge the gap between parametric and non-parametric, exemplar-based generative models. These models are a variation of VAEs that have a non-parametric prior in the latent space, which is based on a Parzen window estimator. To draw a sample, the model selects a random exemplar from a training set and then transforms it stochastically into a latent code and a new observation. To expedite Exemplar VAE training, we propose retrieval augmented training (RAT), which employs an approximate nearest neighbor search in the latent space to establish a lower bound on log marginal likelihood. Exemplar leave-one-out and subsampling are used to learn the model parameters to boost generalization. Experiments show that Exemplar VAEs are effective in density estimation and representation learning. Notably, using Exemplar VAEs for generative data augmentation on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17% to 0.69% and from 8.56% to 8.16%.",1
"Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.",0
"Ad-hoc Video Search (AVS) is a crucial aspect of managing and retrieving multimedia data, involving the retrieval of unlabeled videos through textual queries. The success of AVS relies on cross-modal representation learning, which involves encoding both video and query sentences into a shared semantic space for similarity computation. With previous works demonstrating success in combining multiple sentence encoders, this paper presents a new and general method called Sentence Encoder Assembly (SEA) that effectively leverages diverse sentence encoders. The novelty of SEA lies in its ability to support text-video matching in multiple encoder-specific common spaces, thereby preventing the matching from being dominated by a specific encoder. Additionally, SEA proposes multi-space multi-loss learning to explore complementarities among individual common spaces. Extensive experiments on four benchmarks demonstrate that SEA outperforms existing approaches and is easy to implement, making it a promising solution for AVS and advancing the task through the incorporation of new sentence encoders.",1
"Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling.",0
"The use of classification for density-ratio estimation is a fundamental aspect of unsupervised learning. It has laid the groundwork for cutting-edge techniques in generative modelling and representation learning, with its applications continuing to expand. Nevertheless, it has a significant drawback: it is unable to effectively determine ratios p/q when the two densities vary significantly. Our empirical observations reveal that this occurs when the KL divergence between p and q surpasses ten nats. To address this issue, we propose a new framework, telescoping density-ratio estimation (TRE), that allows for the estimation of ratios between vastly different densities in high-dimensional spaces. Our experiments demonstrate that TRE can offer significant improvements compared to current single-ratio approaches for mutual information estimation, representation learning, and energy-based modelling.",1
"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",0
"The effectiveness of graph embedding (GE) methods in modeling multi-relational data has been demonstrated as they embed nodes (and/or edges) in a low-dimensional semantic space. However, these existing GE models are not practical for real-world applications since they do not account for the streaming nature of incoming data. To resolve this issue, we have examined the problem of continual graph representation learning. The goal is to continually train a GE model on new data in order to learn the increasingly complex multi-relational data while avoiding catastrophic forgetting of previously learned knowledge. Furthermore, we have proposed a disentangle-based continual graph representation learning (DiCGRL) framework that is inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL can effectively address the catastrophic forgetting issue and outperform state-of-the-art continual learning models.",1
"Self-supervised representation learning has witnessed significant leaps fueled by recent progress in Contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we study this problem in detail and propose novel approaches to mitigate the effects of false negatives. The proposed methods exhibit consistent and significant improvements over existing contrastive learning-based models. They achieve new state-of-the-art performance on ImageNet evaluations, achieving 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels, as well as transferring to downstream tasks.",0
"Recent advancements in Contrastive learning have led to significant progress in Self-supervised representation learning. This approach aims to learn transformations that bring similar input pairs closer together while pushing dissimilar pairs further apart. Generating positive pairs is relatively easy, such as using different views of an image. However, defining negative pairs, which are samples from different images with no regard for their semantic content or visual features, is challenging. The problem with contrastive learning is that false negatives can negatively impact representation learning, leading to the discarding of semantic information and slow convergence. This paper delves into this issue and proposes novel methods to mitigate the effects of false negatives. These methods have shown consistent and significant improvements over existing contrastive learning models, achieving state-of-the-art performance on ImageNet evaluations. For instance, the proposed methods achieved a 5.8% absolute improvement in top-1 accuracy when finetuning with 1% labels and demonstrated transferability to downstream tasks.",1
"Today's most popular approaches to keypoint detection involve very complex network architectures that aim to learn holistic representations of all keypoints. In this work, we take a step back and ask: Can we simply learn a local keypoint representation from the output of a standard backbone architecture? This will help make the network simpler and more robust, particularly if large parts of the object are occluded. We demonstrate that this is possible by looking at the problem from the perspective of representation learning. Specifically, the keypoint kernels need to be chosen to optimize three types of distances in the feature space: Features of the same keypoint should be similar to each other, while differing from those of other keypoints, and also being distinct from features from the background clutter. We formulate this optimization process within a framework, which we call CoKe, which includes supervised contrastive learning. CoKe needs to make several approximations to enable representation learning process on large datasets. In particular, we introduce a clutter bank to approximate non-keypoint features, and a momentum update to compute the keypoint representation while training the feature extractor. Our experiments show that CoKe achieves state-of-the-art results compared to approaches that jointly represent all keypoints holistically (Stacked Hourglass Networks, MSS-Net) as well as to approaches that are supervised by detailed 3D object geometry (StarMap). Moreover, CoKe is robust and performs exceptionally well when objects are partially occluded and significantly outperforms related work on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D).",0
"The current trend in keypoint detection involves the use of complex network architectures that try to learn holistic representations of all keypoints. However, we propose a simpler approach where we learn a local keypoint representation from a standard backbone architecture. This approach can make the network more robust, especially when large parts of the object are occluded. Our focus is on representation learning, where we optimize three types of distances in the feature space: features of the same keypoint should be similar while differing from those of other keypoints and the background clutter. We call our framework CoKe, which includes supervised contrastive learning. To enable representation learning on large datasets, we make several approximations, such as introducing a clutter bank to approximate non-keypoint features and a momentum update to compute the keypoint representation while training the feature extractor. Our experiments show that CoKe outperforms state-of-the-art approaches, including those that represent all keypoints holistically and those that are supervised by detailed 3D object geometry. CoKe also performs exceptionally well when objects are partially occluded and outperforms related work on various datasets, such as PASCAL3D+, MPII, and ObjectNet3D.",1
"Reinforcement Learning (RL) has recently been applied to sequential estimation and prediction problems identifying and developing hypothetical treatment strategies for septic patients, with a particular focus on offline learning with observational data. In practice, successful RL relies on informative latent states derived from sequential observations to develop optimal treatment strategies. To date, how best to construct such states in a healthcare setting is an open question. In this paper, we perform an empirical study of several information encoding architectures using data from septic patients in the MIMIC-III dataset to form representations of a patient state. We evaluate the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. We find that sequentially formed state representations facilitate effective policy learning in batch settings, validating a more thoughtful approach to representation learning that remains faithful to the sequential and partial nature of healthcare data.",0
"Recently, Reinforcement Learning (RL) has been utilized for sequential estimation and prediction issues, particularly for identifying and developing hypothetical treatment strategies for septic patients through offline learning with observational data. The success of RL relies on informative latent states derived from sequential observations to develop optimal treatment strategies. However, constructing such states in a healthcare setting is still an open question. In this study, we empirically examine several information encoding architectures using septic patient data in the MIMIC-III dataset to form representations of a patient state. We analyze the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. Our findings demonstrate that sequentially formed state representations enable effective policy learning in batch settings, highlighting the importance of a thoughtful approach to representation learning that accurately reflects the sequential and partial nature of healthcare data.",1
"Estimating individual and average treatment effects from observational data is an important problem in many domains such as healthcare and e-commerce. In this paper, we advocate balance regularization of multi-head neural network architectures. Our work is motivated by representation learning techniques to reduce differences between treated and untreated distributions that potentially arise due to confounding factors. We further regularize the model by encouraging it to predict control outcomes for individuals in the treatment group that are similar to control outcomes in the control group. We empirically study the bias-variance trade-off between different weightings of the regularizers, as well as between inductive and transductive inference.",0
"The problem of estimating individual and average treatment effects from observational data is significant in various fields, including healthcare and e-commerce. Our paper proposes balance regularization of multi-head neural network architectures. Our approach is inspired by representation learning techniques that aim to minimize dissimilarities between treated and untreated distributions, which may result from confounding factors. In addition, we incorporate regularization by encouraging the model to predict control outcomes for treated individuals that are comparable to control outcomes in the control group. We conduct an empirical analysis of the bias-variance trade-off of different weightings of the regularizers and between inductive and transductive inference.",1
"A fundamental task in data exploration is to extract simplified low dimensional representations that capture intrinsic geometry in data, especially for faithfully visualizing data in two or three dimensions. Common approaches to this task use kernel methods for manifold learning. However, these methods typically only provide an embedding of fixed input data and cannot extend to new data points. Autoencoders have also recently become popular for representation learning. But while they naturally compute feature extractors that are both extendable to new data and invertible (i.e., reconstructing original features from latent representation), they have limited capabilities to follow global intrinsic geometry compared to kernel-based manifold learning. We present a new method for integrating both approaches by incorporating a geometric regularization term in the bottleneck of the autoencoder. Our regularization, based on the diffusion potential distances from the recently-proposed PHATE visualization method, encourages the learned latent representation to follow intrinsic data geometry, similar to manifold learning algorithms, while still enabling faithful extension to new data and reconstruction of data in the original feature space from latent coordinates. We compare our approach with leading kernel methods and autoencoder models for manifold learning to provide qualitative and quantitative evidence of our advantages in preserving intrinsic structure, out of sample extension, and reconstruction. Our method is easily implemented for big-data applications, whereas other methods are limited in this regard.",0
"One of the key tasks in data exploration is to create simplified, low dimensional representations that capture the intrinsic geometry of the data. This is especially important for visualizing data in two or three dimensions. While kernel methods for manifold learning are commonly used for this task, they only provide an embedding for fixed input data and cannot be extended to new data points. Autoencoders have recently gained popularity as a representation learning tool, as they compute feature extractors that are both invertible and extendable to new data. However, compared to kernel-based manifold learning, autoencoders have limited abilities to follow global intrinsic geometry. To address this limitation, we propose a new method that incorporates a geometric regularization term in the bottleneck of the autoencoder. This regularization encourages the learned latent representation to follow intrinsic data geometry, similar to manifold learning algorithms, while still enabling faithful extension to new data and reconstruction of data in the original feature space from latent coordinates. Our method outperforms leading kernel methods and autoencoder models for manifold learning in preserving intrinsic structure, out of sample extension, and reconstruction. Moreover, our method is easily implemented for big-data applications, which is a significant advantage over other methods.",1
"Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the ""distillation"" of features at various layers of an expert network, pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given the input data. We show that considering multiple intermediate hints in distillation leads to better exploiting the expert's knowledge and more distinctive discrepancy compared to solely utilizing the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework for the localization of anomalous regions. Despite the striking contrast between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to the SOTA methods on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly detection and localization.",0
"The use of unsupervised representation learning has proven crucial in detecting and localizing anomalies in images. However, there are two main challenges in learning such a representation. Firstly, the sample size is often too small to learn a generalizable representation using conventional techniques. Secondly, the learned features must be able to discriminate between normal and anomalous samples, even though only normal samples are available during training. To address these challenges, we propose using a simpler cloner network to distill features from various layers of an expert network pre-trained on ImageNet. We then use the discrepancy between the expert and cloner networks' intermediate activation values to detect and localize anomalies. By considering multiple intermediate hints during distillation, we can better exploit the expert's knowledge and achieve more distinctive discrepancies compared to using only the last layer activation values. Our framework incorporates interpretability algorithms to localize anomalous regions without the need for special or intensive training procedures. Despite differences between test datasets and ImageNet, our method achieves competitive or superior results compared to state-of-the-art methods on various datasets for both anomaly detection and localization.",1
"Graph representation learning has attracted lots of attention recently. Existing graph neural networks fed with the complete graph data are not scalable due to limited computation and memory costs. Thus, it remains a great challenge to capture rich information in large-scale graph data. Besides, these methods mainly focus on supervised learning and highly depend on node label information, which is expensive to obtain in the real world. As to unsupervised network embedding approaches, they overemphasize node proximity instead, whose learned representations can hardly be used in downstream application tasks directly. In recent years, emerging self-supervised learning provides a potential solution to address the aforementioned problems. However, existing self-supervised works also operate on the complete graph data and are biased to fit either global or very local (1-hop neighborhood) graph structures in defining the mutual information based loss terms.   In this paper, a novel self-supervised representation learning method via Subgraph Contrast, namely \textsc{Subg-Con}, is proposed by utilizing the strong correlation between central nodes and their sampled subgraphs to capture regional structure information. Instead of learning on the complete input graph data, with a novel data augmentation strategy, \textsc{Subg-Con} learns node representations through a contrastive loss defined based on subgraphs sampled from the original graph instead. Compared with existing graph representation learning approaches, \textsc{Subg-Con} has prominent performance advantages in weaker supervision requirements, model learning scalability, and parallelization. Extensive experiments verify both the effectiveness and the efficiency of our work compared with both classic and state-of-the-art graph representation learning approaches on multiple real-world large-scale benchmark datasets from different domains.",0
"Recently, there has been a lot of interest in graph representation learning. However, current graph neural networks that rely on complete graph data are not scalable due to limited computation and memory costs, making it difficult to capture rich information in large-scale graph data. Additionally, these methods mainly focus on supervised learning and depend heavily on node label information, which is costly to obtain in real-world scenarios. On the other hand, unsupervised network embedding approaches overemphasize node proximity, resulting in representations that are not directly applicable in downstream tasks. Although self-supervised learning provides a potential solution, existing methods also operate on complete graph data and are biased towards fitting either global or very local structures. In this paper, we propose a novel self-supervised representation learning method called \textsc{Subg-Con} that utilizes the strong correlation between central nodes and their sampled subgraphs to capture regional structure information. Our approach learns node representations through a contrastive loss defined based on subgraphs sampled from the original graph instead of the complete input graph data. Compared to existing graph representation learning approaches, \textsc{Subg-Con} has notable performance advantages in terms of weaker supervision requirements, model learning scalability, and parallelization. We conduct extensive experiments on multiple real-world large-scale benchmark datasets from different domains to verify the effectiveness and efficiency of our approach compared to classic and state-of-the-art graph representation learning methods.",1
"Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.",0
"Transfer learning has been a major success in deep learning, particularly in language and vision applications. By pre-training a network on a large source set, such as ImageNet, and fine-tuning it on a smaller target set, performance can be improved. However, little is known about its effectiveness in 3D point cloud understanding, which presents an opportunity due to the difficulty of annotating 3D data. This study aims to facilitate research in 3D representation learning, focusing on high-level scene understanding tasks. Diverse datasets and tasks were selected to evaluate the effect of unsupervised pre-training on a large source set of 3D scenes. Encouragingly, the findings demonstrate improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets. The learned representation can generalize across domains, and the improvement was similar to that of supervised pre-training. This suggests that future efforts should focus on scaling data collection rather than detailed annotation. The study highlights the need for more research on unsupervised pretext task design for 3D deep learning.",1
"Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our ""SimSiam"" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.",0
"Siamese networks are commonly used in recent models for unsupervised visual representation learning, aiming to maximize similarity between two image augmentations while avoiding collapsing solutions. Surprisingly, our study demonstrates that simple Siamese networks can still learn meaningful representations even without negative sample pairs, large batches, or momentum encoders. Although collapsing solutions are possible, a stop-gradient operation is vital in preventing them. We propose a hypothesis about stop-gradient implications and conduct proof-of-concept experiments to verify it. Our SimSiam method achieves competitive results on ImageNet and downstream tasks, and we hope our simple baseline will encourage further exploration of Siamese architectures for unsupervised representation learning. The code will be made available.",1
"We propose a model-agnostic pipeline to recover graph signals from an expert system by exploiting the content addressable memory property of restricted Boltzmann machine and the representational ability of a neural network. The proposed pipeline requires the deep neural network that is trained on a downward machine learning task with clean data, data which is free from any form of corruption or incompletion. We show that denoising the representations learned by the deep neural networks is usually more effective than denoising the data itself. Although this pipeline can deal with noise in any dataset, it is particularly effective for graph-structured datasets.",0
"Our proposal is a pipeline that can retrieve graph signals from an expert system. It is independent of any specific model and makes use of the restricted Boltzmann machine's content addressable memory property and the neural network's representational ability. To implement this pipeline, a deep neural network trained with clean data, that is free from corruption or incompleteness, is necessary. Our research indicates that denoising the learned representations of the deep neural network is more effective than denoising the data itself. Although this pipeline can handle noise in any dataset, it is especially advantageous for graph-structured datasets.",1
"Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years. Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems. Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible. The alternative is to use the available data and build models incorporating fair representation learning. In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s). Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features. We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased. The code is available at https://github.com/QingyuZhao/BR-Net/.",0
"Undoubtedly, bias in datasets or tasks is a crucial obstacle in the field of machine learning that has sparked significant discussions in recent years. The issues caused by bias can vary from misleading correlations between variables in medical research to the problem of race bias in gender or facial recognition systems. It can be extremely challenging, if not impossible, to address all types of bias during the dataset curation stage. Therefore, an alternative approach is to use existing data and develop models that incorporate fair representation learning. This study proposes a model that uses adversarial training with two competing objectives. The aim is to learn features that have maximum discriminative power with respect to the task, while also having minimal statistical mean dependence with the protected (bias) variable(s). The model includes a new adversarial loss function that encourages the elimination of any correlation between the bias and the learned features. The proposed method is applied to synthetic data, medical images (with task bias), and a dataset for gender classification (with dataset bias). The results show that our method generates features that not only improve prediction performance but also eliminate bias. The code for this study is available at https://github.com/QingyuZhao/BR-Net/.",1
"We present a new framework for self-supervised representation learning by formulating it as a ranking problem in an image retrieval context on a large number of random views (augmentations) obtained from images. Our work is based on two intuitions: first, a good representation of images must yield a high-quality image ranking in a retrieval task; second, we would expect random views of an image to be ranked closer to a reference view of that image than random views of other images. Hence, we model representation learning as a learning to rank problem for image retrieval. We train a representation encoder by maximizing average precision (AP) for ranking, where random views of an image are considered positively related, and that of the other images considered negatives. The new framework, dubbed S2R2, enables computing a global objective on multiple views, compared to the local objective in the popular contrastive learning framework, which is calculated on pairs of views. In principle, by using a ranking criterion, we eliminate reliance on object-centric curated datasets. When trained on STL10 and MS-COCO, S2R2 outperforms SimCLR and the clustering-based contrastive learning model, SwAV, while being much simpler both conceptually and at implementation. On MS-COCO, S2R2 outperforms both SwAV and SimCLR with a larger margin than on STl10. This indicates that S2R2 is more effective on diverse scenes and could eliminate the need for an object-centric large training dataset for self-supervised representation learning.",0
"A new framework for self-supervised representation learning is introduced in this paper. The framework is based on the idea of formulating representation learning as a ranking problem in an image retrieval context using a large number of random views (augmentations) obtained from images. The authors suggest that a good representation of images should result in a high-quality image ranking in a retrieval task, and that random views of an image should be ranked closer to a reference view of that image than random views of other images. The framework, called S2R2, trains a representation encoder by maximizing average precision (AP) for ranking. This approach eliminates the reliance on object-centric curated datasets and enables computing a global objective on multiple views, which is a significant improvement over the local objective in the popular contrastive learning framework. S2R2 outperforms SimCLR and the clustering-based contrastive learning model, SwAV, on STL10 and MS-COCO, indicating its effectiveness on diverse scenes and its potential to eliminate the need for an object-centric large training dataset for self-supervised representation learning.",1
"Causal modeling has been recognized as a potential solution to many challenging problems in machine learning (ML). Here, we describe how a recently proposed counterfactual approach developed to deconfound linear structural causal models can still be used to deconfound the feature representations learned by deep neural network (DNN) models. The key insight is that by training an accurate DNN using softmax activation at the classification layer, and then adopting the representation learned by the last layer prior to the output layer as our features, we have that, by construction, the learned features will fit well a (multi-class) logistic regression model, and will be linearly associated with the labels. As a consequence, deconfounding approaches based on simple linear models can be used to deconfound the feature representations learned by DNNs. We validate the proposed methodology using colored versions of the MNIST dataset. Our results illustrate how the approach can effectively combat confounding and improve model stability in the context of dataset shifts generated by selection biases.",0
"Many complex problems in machine learning can be solved with causal modeling. This article explains how a counterfactual method, originally designed for linear structural causal models, can also be used to deconfound the feature representations learned by deep neural network models. By training a precise DNN using softmax activation, we can use the features learned by the last layer before the output layer to fit a logistic regression model, and these features will be associated with the labels in a linear manner. As a result, we can use linear models to deconfound the feature representations in DNNs. The effectiveness of this method is demonstrated using colored MNIST data, which shows how it can improve model stability in cases of dataset shifts caused by selection biases.",1
"Representation learning on graphs has emerged as a powerful mechanism to automate feature vector generation for downstream machine learning tasks. The advances in representation on graphs have centered on both homogeneous and heterogeneous graphs, where the latter presenting the challenges associated with multi-typed nodes and/or edges. In this paper, we consider the additional challenge of evolving graphs. We ask the question of whether the advances in representation learning for static graphs can be leveraged for dynamic graphs and how? It is important to be able to incorporate those advances to maximize the utility and generalization of methods. To that end, we propose the Framework for Incremental Learning of Dynamic Networks Embedding (FILDNE), which can utilize any existing static representation learning method for learning node embeddings, while keeping the computational costs low. FILDNE integrates the feature vectors computed using the standard methods over different timesteps into a single representation by developing a convex combination function and alignment mechanism. Experimental results on several downstream tasks, over seven real-world data sets, show that FILDNE is able to reduce memory and computational time costs while providing competitive quality measure gains with respect to the contemporary methods for representation learning on dynamic graphs.",0
"The use of representation learning on graphs has become a useful tool for generating feature vectors for machine learning tasks. This technique has been applied to both homogeneous and heterogeneous graphs, with the latter presenting additional challenges due to the presence of multi-typed nodes and/or edges. In this study, we tackle the challenge of evolving graphs and investigate whether the progress made in representation learning for static graphs can be applied to dynamic graphs. We propose the Framework for Incremental Learning of Dynamic Networks Embedding (FILDNE), which can employ existing static representation learning methods for learning node embeddings, while minimizing computational costs. FILDNE combines the feature vectors computed using standard methods over different timesteps into a single representation using a convex combination function and alignment mechanism. Our experiments on several real-world datasets demonstrate that FILDNE reduces memory and computational time costs while producing competitive quality measure gains compared to contemporary methods for representation learning on dynamic graphs. It is crucial to incorporate these advances to enhance the utility and generalization of methods.",1
"Few-Shot Learning (FSL) aims to improve a model's generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage interpolation consistency, including interpolating hidden features, that imposes linear behavior locally and data augmentation consistency that learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.",0
"The goal of Few-Shot Learning (FSL) is to enhance a model's ability to make accurate predictions in low data situations. Recent advancements in FSL have been achieved through metric learning, meta learning, and representation learning, among others. Despite these improvements, FSL remains difficult due to two persistent challenges. Firstly, during training and testing, the seen and unseen classes are distinct, leading to a distribution shift. Secondly, during testing, there is limited labeled data for previously unseen classes, making it challenging to extrapolate from labeled support examples to unlabeled query examples. To address these issues, we propose Hybrid Consistency Training, which combines interpolation consistency and data augmentation consistency to produce stable embeddings. To overcome the second challenge, we utilize unlabeled examples to iteratively normalize features and adapt prototypes for more reliable prototype-based transductive inference. We demonstrate that our approach outperforms state-of-the-art methods on five FSL datasets by 2% to 5%, and more significantly, by 7% to 8% on more complex cross-domain FSL tasks.",1
"We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradistinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance-level fidelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level fidelity for there construction/synthesis), both being contradistinctive. Extensive experimental results by DC-VAE across different resolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a significant qualitative and quantitative performance enhancement over the baseline VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning.",0
"We introduce a novel generative autoencoder model, called the dual contradistinctive generative autoencoder (DC-VAE), that incorporates two distinct losses to enhance the performance of existing generative autoencoders. Our model enables simultaneous reconstruction and sampling with the integration of an instance-level discriminative loss and a set-level adversarial loss, both of which are contradistinctive. We report extensive experimental results across various resolutions, demonstrating how the DC-VAE outperforms baseline VAEs without any architectural changes. The model achieves state-of-the-art or competitive results in image reconstruction, synthesis, interpolation, and representation learning, and is generally applicable to a wide range of computer vision and machine learning tasks.",1
"Contrastive learning has achieved great success in self-supervised visual representation learning, but existing approaches mostly ignored spatial information which is often crucial for visual representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination and (ii) it surpasses existing pre-training methods in a series of downstream tasks while shrinking the pre-training costs by half. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.",0
"Although contrastive learning has been successful in self-supervised visual representation learning, it has generally overlooked the importance of spatial information in visual representation. This paper introduces heterogeneous contrastive learning (HCL), a method that integrates spatial information into the encoding stage to address the inconsistency between the contrastive objective and strong data augmentation operations. HCL is demonstrated to be effective in achieving higher accuracy in instance discrimination and outperforming previous pre-training methods in a variety of downstream tasks while halving pre-training costs. Additionally, our approach enhances efficiency in visual representations, providing a valuable insight for future research in self-supervised visual representation learning.",1
"We present a novel nonparametric algorithm for symmetry-based disentangling of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER). GEOMANCER provides a partial answer to the question posed by Higgins et al. (2018): is it possible to learn how to factorize a Lie group solely from observations of the orbit of an object it acts on? We show that fully unsupervised factorization of a data manifold is possible if the true metric of the manifold is known and each factor manifold has nontrivial holonomy -- for example, rotation in 3D. Our algorithm works by estimating the subspaces that are invariant under random walk diffusion, giving an approximation to the de Rham decomposition from differential geometry. We demonstrate the efficacy of GEOMANCER on several complex synthetic manifolds. Our work reduces the question of whether unsupervised disentangling is possible to the question of whether unsupervised metric learning is possible, providing a unifying insight into the geometric nature of representation learning.",0
"The Geometric Manifold Component Estimator (GEOMANCER) is a new nonparametric algorithm that can disentangle data manifolds based on symmetry. GEOMANCER addresses the question of whether it is possible to factorize a Lie group from observations of the orbit of an object it acts on, which was posed by Higgins et al. (2018). Our algorithm demonstrates that fully unsupervised factorization of a data manifold is possible if the true metric of the manifold is known and each factor manifold has nontrivial holonomy, such as rotation in 3D. By estimating the subspaces invariant under random walk diffusion, GEOMANCER approximates the de Rham decomposition from differential geometry. We have tested GEOMANCER on several complex synthetic manifolds and found it to be effective. Our work unifies the geometric nature of representation learning and reduces the question of unsupervised disentangling to the possibility of unsupervised metric learning.",1
"Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using $\beta$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.",0
"Although diagnostic interviews for mental health disorders have undergone extensive standardization, they still involve a significant amount of subjective judgment. Previous studies have shown that EEG-based neural measures can serve as reliable objective indicators of depression and even predict its course. However, their clinical usefulness has been limited by two factors: the lack of automated methods to handle the inherent noise in EEG data at scale and the absence of knowledge about which aspects of the EEG signal may indicate a clinical disorder. To address these issues, this study employs an unsupervised pipeline from recent deep representation learning literature, which uses $\beta$-VAE to learn a disentangled representation that can denoise the signal and a Symbol-Concept Association Network (SCAN) to extract interpretable features associated with a sparse set of clinical labels. The results demonstrate that this method outperforms the canonical hand-engineered baseline classification method, particularly in terms of participant age and depression diagnosis. Moreover, the method recovers a representation that can automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories and supports fast supervised re-mapping to various clinical labels. This enables clinicians to reuse a single EEG representation regardless of updates to the standardized diagnostic system. Finally, the factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, which allows for human interpretability and post-hoc expert analysis of the model's recommendations.",1
"In this paper we show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach termed Cycle Encoding Prediction (CEP) that is able to effectively represent high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backward as well as backward-forward temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of the video stream and applies loss functions that encourage both temporal cycle closure as well as contrastive feature separation. Architecturally, the underpinning network structure utilises a single feature encoder for all video snippets, adding two predictive modules that learn temporal forward and backward transitions. We apply our framework for pretext training of networks for action recognition tasks. We report significantly improved results for the standard datasets UCF101 and HMDB51. Detailed ablation studies support the effectiveness of the proposed components. We publish source code for the CEP components in full with this paper.",0
"The aim of this paper is to demonstrate how action classification can be improved by learning video feature spaces that maximize predictability of temporal cycles. A new learning approach called Cycle Encoding Prediction (CEP) is introduced, which effectively captures the high-level spatio-temporal structure of unlabelled video content. CEP creates a latent space that preserves the concept of closed forward-backward and backward-forward temporal loops, using bi-directional temporal coherence of the video stream as a self-supervision signal. To encourage both temporal cycle closure and contrastive feature separation, CEP applies loss functions. The network architecture of CEP uses a single feature encoder for all video snippets, with two predictive modules added to learn temporal forward and backward transitions. The proposed framework is applied for pretext training of networks for action recognition tasks, resulting in significantly improved results for standard datasets UCF101 and HMDB51. Detailed ablation studies support the effectiveness of the proposed components, and the source code for the CEP components is made available with this paper.",1
"Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community.",0
"Unsupervised visual representation learning has seen significant advancements through the use of contrastive learning. Some self-supervised algorithms, pre-trained on ImageNet, have shown better transfer learning performance than fully-supervised methods, suggesting that human labels may not be necessary for learning transferable visual features. However, this paper argues that semantic labels can still be useful and that fully-supervised and self-supervised methods are seeking different types of features. To address this issue, the authors introduce a new algorithm called Supervised Contrastive Adjustment in Neighborhood (SCAN), which prevents semantic guidance from interfering with appearance feature embedding. In downstream tasks, SCAN outperforms previous fully-supervised and self-supervised methods, highlighting the usefulness of semantic labels in assisting self-supervised methods and pointing to a new direction for the research community.",1
"This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE",0
"In this article, TCE: Temporally Coherent Embeddings for self-supervised video representation learning is introduced. The proposed approach utilizes the inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than relying on ranking or predictive proxy tasks. The authors believe that nearby frames in learned representations will benefit from demonstrating similar properties, as high-level visual information in the world changes smoothly. To achieve this, the TCE model is trained to encode videos such that adjacent frames exist close to each other and videos are separated from one another. The resulting robust representations are evaluated on a downstream task of video action recognition using multiple challenging benchmarks, including Kinetics400, UCF101, and HMDB51. The TCE pre-trained representations, using a simple but effective 2D-CNN backbone and only RGB stream inputs, outperform all previous self-supervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper are available for download at https://github.com/csiro-robotics/TCE.",1
"Patient representation learning refers to learning a dense mathematical representation of a patient that encodes meaningful information from Electronic Health Records (EHRs). This is generally performed using advanced deep learning methods. This study presents a systematic review of this field and provides both qualitative and quantitative analyses from a methodological perspective. We identified studies developing patient representations from EHRs with deep learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. After screening 363 articles, 49 papers were included for a comprehensive data collection. We noticed a typical workflow starting with feeding raw data, applying deep learning models, and ending with clinical outcome predictions as evaluations of the learned representations. Specifically, learning representations from structured EHR data was dominant (37 out of 49 studies). Recurrent Neural Networks were widely applied as the deep learning architecture (LSTM: 13 studies, GRU: 11 studies). Disease prediction was the most common application and evaluation (31 studies). Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns of EHR data, and code availability was assured in 20 studies. We show the importance and feasibility of learning comprehensive representations of patient EHR data through a systematic review. Advances in patient representation learning techniques will be essential for powering patient-level EHR analyses. Future work will still be devoted to leveraging the richness and potential of available EHR data. Knowledge distillation and advanced learning techniques will be exploited to assist the capability of learning patient representation further.",0
"The process of patient representation learning involves using complex deep learning methods to develop a condensed mathematical representation of a patient that contains significant information from their Electronic Health Records (EHRs). This study offers a methodical review of the field, including both qualitative and quantitative analyses. The research team conducted a thorough search of MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library, ultimately selecting 49 papers for comprehensive data collection out of 363 screened articles. The majority of studies focused on developing patient representations from structured EHR data using Recurrent Neural Networks, particularly LSTM and GRU. Disease prediction was the most common application and evaluation, and the lack of available benchmark datasets was attributed to concerns over privacy. The study emphasizes the importance of patient representation learning for advancing EHR analysis and suggests that future work will explore knowledge distillation and advanced learning techniques to enhance the capability of learning patient representation.",1
"Recently, convolutional neural networks (CNNs)-based facial landmark detection methods have achieved great success. However, most of existing CNN-based facial landmark detection methods have not attempted to activate multiple correlated facial parts and learn different semantic features from them that they can not accurately model the relationships among the local details and can not fully explore more discriminative and fine semantic features, thus they suffer from partial occlusions and large pose variations. To address these problems, we propose a cross-order cross-semantic deep network (CCDN) to boost the semantic features learning for robust facial landmark detection. Specifically, a cross-order two-squeeze multi-excitation (CTM) module is proposed to introduce the cross-order channel correlations for more discriminative representations learning and multiple attention-specific part activation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is designed to drive the network to learn cross-order cross-semantic features from different activation for facial landmark detection. It is interesting to show that by integrating the CTM module and COCS regularizer, the proposed CCDN can effectively activate and learn more fine and complementary cross-order cross-semantic features to improve the accuracy of facial landmark detection under extremely challenging scenarios. Experimental results on challenging benchmark datasets demonstrate the superiority of our CCDN over state-of-the-art facial landmark detection methods.",0
"Facial landmark detection using convolutional neural networks (CNNs) has been highly successful, although most current methods do not activate multiple correlated facial parts or learn different semantic features from them. As a result, they struggle with partial occlusions and large pose variations. To overcome these issues, we introduce the cross-order cross-semantic deep network (CCDN) which employs a cross-order two-squeeze multi-excitation (CTM) module and a cross-order cross-semantic (COCS) regularizer. The CTM module enables the network to learn discriminative representations and activate attention-specific parts, while the COCS regularizer drives the network to learn cross-order cross-semantic features for facial landmark detection. Our proposed method significantly improves the accuracy of facial landmark detection under challenging conditions, as demonstrated by experiments on benchmark datasets.",1
"With the rapid emergence of graph representation learning, the construction of new large-scale datasets are necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all of these desired properties. We introduce MalNet, the largest public graph database ever constructed, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 44x larger graphs on average, and 63x the classes. We provide a detailed analysis of MalNet, discussing its properties and provenance. The unprecedented scale and diversity of MalNet offers exciting opportunities to advance the frontiers of graph representation learning---enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publically available at www.mal-net.org.",0
"In order to accurately assess the capabilities and limitations of different graph representation learning techniques, it is crucial to create new large-scale datasets. By analyzing existing graph databases, we have identified three key components that are necessary for advancing this field: large graphs, a large number of graphs, and class diversity. However, no single graph database currently possesses all of these desired properties. To address this issue, we have developed MalNet, which is the largest public graph database ever constructed. MalNet consists of over 1.2 million graphs, with an average of 17k nodes and 39k edges per graph, across 47 different types and 696 families. In comparison to the popular REDDIT-12K database, MalNet offers 105 times more graphs, graphs that are 44 times larger on average, and 63 times more classes. We provide a detailed analysis of MalNet's properties and provenance, and believe that its unprecedented scale and diversity will enable new discoveries and research into imbalanced classification, explainability, and the impact of class hardness. The database is publicly available at www.mal-net.org.",1
"In this paper, we focus on unsupervised representation learning for skeleton-based action recognition. Existing approaches usually learn action representations by sequential prediction but they suffer from the inability to fully learn semantic information. To address this limitation, we propose a novel framework named Prototypical Contrast and Reverse Prediction (PCRP), which not only creates reverse sequential prediction to learn low-level information (e.g., body posture at every frame) and high-level pattern (e.g., motion order), but also devises action prototypes to implicitly encode semantic similarity shared among sequences. In general, we regard action prototypes as latent variables and formulate PCRP as an expectation-maximization task. Specifically, PCRP iteratively runs (1) E-step as determining the distribution of prototypes by clustering action encoding from the encoder, and (2) M-step as optimizing the encoder by minimizing the proposed ProtoMAE loss, which helps simultaneously pull the action encoding closer to its assigned prototype and perform reverse prediction task. Extensive experiments on N-UCLA, NTU 60, and NTU 120 dataset present that PCRP outperforms state-of-the-art unsupervised methods and even achieves superior performance over some of supervised methods. Codes are available at https://github.com/Mikexu007/PCRP.",0
"The focus of our paper is on unsupervised representation learning for skeleton-based action recognition. However, existing approaches that learn action representations through sequential prediction are limited in their ability to fully learn semantic information. To overcome this limitation, we propose a new framework called Prototypical Contrast and Reverse Prediction (PCRP). Our framework creates reverse sequential prediction to learn both low-level information (such as body posture at every frame) and high-level patterns (such as motion order) while also devising action prototypes to implicitly encode semantic similarity shared among sequences. We regard action prototypes as latent variables and formulate PCRP as an expectation-maximization task, with the E-step determining the distribution of prototypes by clustering action encoding from the encoder and the M-step optimizing the encoder by minimizing the proposed ProtoMAE loss. This loss helps pull the action encoding closer to its assigned prototype and perform reverse prediction task. Our extensive experiments on N-UCLA, NTU 60, and NTU 120 datasets show that PCRP outperforms state-of-the-art unsupervised methods and even achieves superior performance than some supervised methods. The codes for PCRP are available at https://github.com/Mikexu007/PCRP.",1
"In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze the mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint videotext representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperforms the state-of-the-arts, demonstrating its superiority in video-text representation learning.",0
"This article presents ActBERT, which uses unsupervised learning to create joint representations of video and text from unlabelled data. To achieve this, ActBERT uses global action information to encourage interactions between linguistic text and local objects. By pairing video sequences and text descriptions, ActBERT can identify visual and text clues to model visual and text relationships in detail. ActBERT also uses an ENtangled Transformer block to encode and combine three sources of information: global actions, local objects, and linguistic descriptions. This approach extracts contextual information to identify global-local connections, allowing the joint representation to be aware of fine-grained objects and human intent. Finally, ActBERT is tested on various video and language tasks, showing superior performance compared to existing methods.",1
"Intelligently reasoning about the world often requires integrating data from multiple modalities, as any individual modality may contain unreliable or incomplete information. Prior work in multimodal learning fuses input modalities only after significant independent processing. On the other hand, the brain performs multimodal processing almost immediately. This divide between conventional multimodal learning and neuroscience suggests that a detailed study of early multimodal fusion could improve artificial multimodal representations. To facilitate the study of early multimodal fusion, we create a convolutional LSTM network architecture that simultaneously processes both audio and visual inputs, and allows us to select the layer at which audio and visual information combines. Our results demonstrate that immediate fusion of audio and visual inputs in the initial C-LSTM layer results in higher performing networks that are more robust to the addition of white noise in both audio and visual inputs.",0
"The ability to logically reason about the world often requires combining data from multiple sources, as any one source may be incomplete or unreliable. Previous research on multimodal learning has typically fused different input sources after they have been processed independently. In contrast, the brain processes multiple sources simultaneously. This disparity between conventional multimodal learning and neuroscience suggests that investigating early multimodal fusion could enhance artificial multimodal representations. To enable this study, we designed a convolutional LSTM network architecture that can process both audio and visual inputs and determine the layer at which the two sources should be combined. Our findings indicate that immediately fusing audio and visual inputs at the initial C-LSTM layer results in more robust networks that perform better, even when subjected to white noise.",1
"Identification of disease genes, which are a set of genes associated with a disease, plays an important role in understanding and curing diseases. In this paper, we present a biomedical knowledge graph designed specifically for this problem, propose a novel machine learning method that identifies disease genes on such graphs by leveraging recent advances in network biology and graph representation learning, study the effects of various relation types on prediction performance, and empirically demonstrate that our algorithms outperform its closest state-of-the-art competitor in disease gene identification by 24.1%. We also show that we achieve higher precision than Open Targets, the leading initiative for target identification, with respect to predicting drug targets in clinical trials for Parkinson's disease.",0
"The detection of disease genes, a group of genes linked to a disease, is crucial in comprehending and remedying illnesses. In this article, we introduce a biomedical knowledge graph specially designed to tackle this issue. We propose a unique machine learning approach that identifies disease genes on said graphs by utilizing recent advancements in network biology and graph representation learning. We analyze the impact of various types of relationships on prediction accuracy and prove through empirical evidence that our algorithms outperform the closest state-of-the-art competitor in detecting disease genes by 24.1%. We also demonstrate that we achieve higher accuracy than Open Targets, the leading initiative for identifying targets, in predicting drug targets in clinical trials for Parkinson's disease.",1
"Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information theoretic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.",0
"The aim of graph representation learning is to acquire node representations that maintain both structural data and node features. These node representations can then be utilized in various downstream tasks like node clustering and classification. In cases where the graph is heterogeneous, this task becomes even more challenging than homogeneous graph node learning. This paper proposes an unsupervised graph neural network named Heterogeneous Deep Graph Infomax (HDGI) that is inspired by information theoretic-based learning algorithms. The HDGI uses meta-path structure to examine the connections that involve semantics in heterogeneous graphs and employs a graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI teaches higher-level node representations that can be used in graph-related tasks. The experiment results show that HDGI outperforms other unsupervised graph representation learning methods in both classification and clustering tasks. By employing the learned representations in a parametric model like logistic regression, we even achieve comparable performance in node classification tasks when compared to state-of-the-art supervised end-to-end GNN models.",1
"Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world.",0
"The progress made in visual navigation techniques has resulted in the development of intelligent navigation agents that can learn significant representations from raw RGB images, enabling them to undertake a broad range of tasks involving structural and semantic reasoning. However, the majority of navigation policies based on learning are trained and tested in virtual environments. To be practically useful, these policies must be transferred to the physical world. This article suggests an unsupervised domain adaptation approach for visual navigation. Our technique involves translating the images in the target domain to the source domain while maintaining consistency with the navigation policy's learned representations. Our method outperforms various baselines across two distinct navigation tasks in simulation. Furthermore, we demonstrate that our method can transfer navigation policies learned in simulation to the physical world.",1
"As a hot research topic, many multi-view clustering approaches are proposed over the past few years. Nevertheless, most existing algorithms merely take the consensus information among different views into consideration for clustering. Actually, it may hinder the multi-view clustering performance in real-life applications, since different views usually contain diverse statistic properties. To address this problem, we propose a novel Tensor-based Intrinsic Subspace Representation Learning (TISRL) for multi-view clustering in this paper. Concretely, the rank preserving decomposition is proposed firstly to effectively deal with the diverse statistic information contained in different views. Then, to achieve the intrinsic subspace representation, the tensor-singular value decomposition based low-rank tensor constraint is also utilized in our method. It can be seen that specific information contained in different views is fully investigated by the rank preserving decomposition, and the high-order correlations of multi-view data are also mined by the low-rank tensor constraint. The objective function can be optimized by an augmented Lagrangian multiplier based alternating direction minimization algorithm. Experimental results on nine common used real-world multi-view datasets illustrate the superiority of TISRL.",0
"Over the past few years, numerous multi-view clustering approaches have been proposed as a hot research topic. However, the majority of existing algorithms only consider the consensus information among different views when clustering, which can impede performance in real-life applications as different views often possess diverse statistical properties. To tackle this issue, we present a novel Tensor-based Intrinsic Subspace Representation Learning (TISRL) approach for multi-view clustering in this paper. Our method utilizes rank preserving decomposition to effectively handle the statistical information present in different views, and the tensor-singular value decomposition based low-rank tensor constraint to achieve intrinsic subspace representation. By incorporating both techniques, we fully explore the specific information contained in different views and mine the high-order correlations of multi-view data. To optimize the objective function, we employ an augmented Lagrangian multiplier based alternating direction minimization algorithm. Results from experiments on nine commonly used real-world multi-view datasets demonstrate the superior performance of TISRL.",1
"Although multi-view learning has made signifificant progress over the past few decades, it is still challenging due to the diffificulty in modeling complex correlations among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets), which aims to fully and flflexibly take advantage of multiple partial views. We fifirst provide a formal defifinition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the learned latent representations. For completeness, the task of learning latent multi-view representation is specififically translated to a degradation process by mimicking data transmission, such that the optimal tradeoff between consistency and complementarity across different views can be implicitly achieved. Equipped with adversarial strategy, our model stably imputes missing views, encoding information from all views for each sample to be encoded into latent representation to further enhance the completeness. Furthermore, a nonparametric classifification loss is introduced to produce structured representations and prevent overfifitting, which endows the algorithm with promising generalization under view-missing cases. Extensive experimental results validate the effectiveness of our algorithm over existing state of the arts for classifification, representation learning and data imputation.",0
"Despite significant progress in multi-view learning over the past few decades, it remains challenging to model complex correlations between different views, especially when views are missing. To tackle this challenge, we propose a novel framework called Cross Partial Multi-View Networks (CPM-Nets) that can fully and flexibly leverage multiple partial views. Firstly, we provide a formal definition of completeness and versatility for multi-view representation and then demonstrate the versatility of learned latent representations. To achieve completeness, we translate the task of learning latent multi-view representation into a degradation process that mimics data transmission, thereby achieving an optimal tradeoff between consistency and complementarity across different views. Our model can impute missing views using an adversarial strategy, encoding information from all views into the latent representation to enhance completeness. Additionally, we introduce a nonparametric classification loss to produce structured representations and prevent overfitting, enabling promising generalization under view-missing cases. Extensive experimental results confirm the effectiveness of our algorithm for classification, representation learning, and data imputation compared to existing state-of-the-art methods.",1
"Graph neural networks~(GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on enough labels or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping~(DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.",0
"The application of deep learning techniques to graph-structured data has yielded promising results in graph representation learning through Graph Neural Networks (GNNs). However, the current reliance on labels or well-designed negative samples has posed significant challenges. To overcome these issues, we introduce Deep Graph Bootstrapping (DGB), a novel self-supervised graph representation method. DGB comprises two neural networks, online and target networks, with different augmented views of the initial graph as inputs. The online network is trained to predict the target network, while the target network is updated with a slow-moving average of the online network, enabling mutual learning. Consequently, DGB can learn graph representation without negative examples in an unsupervised manner. We also summarize three augmentation methods for graph-structured data and apply them to DGB, demonstrating how they affect performance. Our experiments on benchmark datasets reveal that DGB outperforms current state-of-the-art methods.",1
"This paper introduces a novel method for self-supervised video representation learning via feature prediction. In contrast to the previous methods that focus on future feature prediction, we argue that a supervisory signal arising from unobserved past frames is complementary to one that originates from the future frames. The rationale behind our method is to encourage the network to explore the temporal structure of videos by distinguishing between future and past given present observations. We train our model in a contrastive learning framework, where joint encoding of future and past provides us with a comprehensive set of temporal hard negatives via swapping. We empirically show that utilizing both signals enriches the learned representations for the downstream task of action recognition. It outperforms independent prediction of future and past.",0
"In this paper, a new approach to self-supervised video representation learning through feature prediction is presented. Unlike previous methods that concentrate on predicting future features, the authors argue that a supervisory signal based on unobserved past frames complements one based on future frames. The aim of the method is to encourage the network to explore the temporal structure of videos by distinguishing between past and future frames in relation to the present observations. The model is trained using a contrastive learning framework, which enables the joint encoding of past and future, thereby providing a comprehensive set of temporal hard negatives through swapping. The authors demonstrate through experiments that using both signals enhances the learned representations for the downstream task of action recognition, surpassing the results of independent prediction of future and past.",1
"In this study, we present a dynamic graph representation learning model on weighted graphs to accurately predict the network capacity of connections between viewers in a live video streaming event. We propose EGAD, a neural network architecture to capture the graph evolution by introducing a self-attention mechanism on the weights between consecutive graph convolutional networks. In addition, we account for the fact that neural architectures require a huge amount of parameters to train, thus increasing the online inference latency and negatively influencing the user experience in a live video streaming event. To address the problem of the high online inference of a vast number of parameters, we propose a knowledge distillation strategy. In particular, we design a distillation loss function, aiming to first pretrain a teacher model on offline data, and then transfer the knowledge from the teacher to a smaller student model with less parameters. We evaluate our proposed model on the link prediction task on three real-world datasets, generated by live video streaming events. The events lasted 80 minutes and each viewer exploited the distribution solution provided by the company Hive Streaming AB. The experiments demonstrate the effectiveness of the proposed model in terms of link prediction accuracy and number of required parameters, when evaluated against state-of-the-art approaches. In addition, we study the distillation performance of the proposed model in terms of compression ratio for different distillation strategies, where we show that the proposed model can achieve a compression ratio up to 15:100, preserving high link prediction accuracy. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://stefanosantaris.github.io/EGAD.",0
"This research introduces a model that utilizes dynamic graph representation learning on weighted graphs to predict network capacity for live video streaming events. The proposed neural network architecture, EGAD, captures graph evolution through self-attention on the weights between consecutive graph convolutional networks. However, neural architectures require a large number of parameters, which negatively impact the user experience. To tackle this issue, the study proposes a knowledge distillation strategy that pretrains a teacher model on offline data and transfers its knowledge to a smaller student model. The proposed model is evaluated on three real-world datasets from 80-minute events, showing high accuracy in link prediction and requiring fewer parameters than state-of-the-art approaches. The study also analyzes the compression ratio achieved through different distillation strategies, demonstrating that the proposed model can achieve a compression ratio of up to 15:100 while preserving high link prediction accuracy. The evaluation datasets and implementation are publicly available for reproduction purposes at https://stefanosantaris.github.io/EGAD.",1
"Dynamic graph representation learning strategies are based on different neural architectures to capture the graph evolution over time. However, the underlying neural architectures require a large amount of parameters to train and suffer from high online inference latency, that is several model parameters have to be updated when new data arrive online. In this study we propose Distill2Vec, a knowledge distillation strategy to train a compact model with a low number of trainable parameters, so as to reduce the latency of online inference and maintain the model accuracy high. We design a distillation loss function based on Kullback-Leibler divergence to transfer the acquired knowledge from a teacher model trained on offline data, to a small-size student model for online data. Our experiments with publicly available datasets show the superiority of our proposed model over several state-of-the-art approaches with relative gains up to 5% in the link prediction task. In addition, we demonstrate the effectiveness of our knowledge distillation strategy, in terms of number of required parameters, where Distill2Vec achieves a compression ratio up to 7:100 when compared with baseline approaches. For reproduction purposes, our implementation is publicly available at https://stefanosantaris.github.io/Distill2Vec.",0
"The learning strategies for dynamic graph representation involve various neural architectures that aim to capture the evolution of graphs over time. However, these architectures require a large number of parameters for training and suffer from high latency during online inference. This means that several model parameters need to be updated whenever new data is received. To address this issue, we propose a knowledge distillation strategy called Distill2Vec, which trains a compact model with fewer trainable parameters to reduce online inference latency while maintaining high model accuracy. Our approach involves a distillation loss function based on Kullback-Leibler divergence to transfer knowledge from a teacher model trained on offline data to a smaller student model for online data. Our experiments with publicly available datasets demonstrate that our proposed model outperforms several state-of-the-art approaches, achieving relative gains of up to 5% in the link prediction task. Moreover, our knowledge distillation strategy achieves a compression ratio of up to 7:100 compared to baseline approaches in terms of the number of required parameters. To facilitate reproducibility, we have made our implementation publicly available at https://stefanosantaris.github.io/Distill2Vec.",1
"In self-supervised learning, a system is tasked with achieving a surrogate objective by defining alternative targets on a set of unlabeled data. The aim is to build useful representations that can be used in downstream tasks, without costly manual annotation. In this work, we propose a novel self-supervised formulation of relational reasoning that allows a learner to bootstrap a signal from information implicit in unlabeled data. Training a relation head to discriminate how entities relate to themselves (intra-reasoning) and other entities (inter-reasoning), results in rich and descriptive representations in the underlying neural network backbone, which can be used in downstream tasks such as classification and image retrieval. We evaluate the proposed method following a rigorous experimental procedure, using standard datasets, protocols, and backbones. Self-supervised relational reasoning outperforms the best competitor in all conditions by an average 14% in accuracy, and the most recent state-of-the-art model by 3%. We link the effectiveness of the method to the maximization of a Bernoulli log-likelihood, which can be considered as a proxy for maximizing the mutual information, resulting in a more efficient objective with respect to the commonly used contrastive losses.",0
"Self-supervised learning involves a system achieving a surrogate objective by defining alternative targets on a set of unlabeled data. The goal is to create useful representations for downstream tasks without the need for costly manual annotation. Our work proposes a new self-supervised approach to relational reasoning that allows a learner to extract implicit information from unlabeled data. By training a relation head to distinguish how entities relate to themselves (intra-reasoning) and other entities (inter-reasoning), the neural network backbone can generate rich and detailed representations for classification and image retrieval. We conducted a rigorous experiment using standard datasets, protocols, and backbones and found that self-supervised relational reasoning outperforms all competitors by an average of 14% in accuracy, with a 3% improvement over the most recent state-of-the-art model. We attribute the effectiveness of our method to maximizing the Bernoulli log-likelihood, which acts as a proxy for maximizing mutual information and is more efficient than commonly used contrastive losses.",1
"De novo genome assembly focuses on finding connections between a vast amount of short sequences in order to reconstruct the original genome. The central problem of genome assembly could be described as finding a Hamiltonian path through a large directed graph with a constraint that an unknown number of nodes and edges should be avoided. However, due to local structures in the graph and biological features, the problem can be reduced to graph simplification, which includes removal of redundant information. Motivated by recent advancements in graph representation learning and neural execution of algorithms, in this work we train the MPNN model with max-aggregator to execute several algorithms for graph simplification. We show that the algorithms were learned successfully and can be scaled to graphs of sizes up to 20 times larger than the ones used in training. We also test on graphs obtained from real-world genomic data---that of a lambda phage and E. coli.",0
"The main goal of de novo genome assembly is to reconstruct the original genome by identifying links between numerous short sequences. The challenge in genome assembly involves determining a Hamiltonian path through a large directed graph while avoiding a variable number of nodes and edges. However, the problem can be simplified by removing redundant information due to local structures in the graph and biological features. Using recent advancements in graph representation learning and neural algorithms, we trained the MPNN model with max-aggregator to perform various graph simplification tasks. Our study demonstrates that the algorithms were effectively learned and can be applied to graphs up to 20 times larger than those used for training. We tested our approach on real-world genomic data from the lambda phage and E. coli.",1
"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.   Project Page: https://ssnl.github.io/hypersphere   Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform",0
"Effective implementation of contrastive representation learning has proven to be highly successful. This study highlights two key factors associated with the contrastive loss: (1) the closeness of features from positive pairs and (2) the uniformity of the resulting distribution of normalized features on the hypersphere. We demonstrate that the contrastive loss optimizes these factors asymptotically, and examine their favorable impact on downstream tasks. We introduce a quantifiable metric for each property through empirical analysis. Comprehensive experiments on standard language and vision datasets support the strong correlation between these metrics and the performance of downstream tasks. Interestingly, directly optimizing for these two metrics leads to representations that are comparable or even superior to those obtained through contrastive learning. The project page and code for this study may be found at https://ssnl.github.io/hypersphere, https://github.com/SsnL/align_uniform, and https://github.com/SsnL/moco_align_uniform.",1
"To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship.   We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer & wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring.",0
"Up until now, research on mobile devices equipped with sensors has been primarily focused on the supervised task of recognizing human activity, such as walking and running. However, this has shown limited success in inferring high-level health outcomes from low-level signals, such as acceleration. To address this issue, we introduce a new method of self-supervised representation learning that uses both activity and heart rate signals without semantic labels. This is achieved through a deep neural network that sets heart rate responses as a supervisory signal for the activity data, taking advantage of their underlying physiological relationship. Our model is evaluated using the largest free-living combined-sensing dataset, which includes over 280,000 hours of wrist accelerometer and wearable ECG data. The results show that the resulting embeddings can be used in various downstream tasks through transfer learning with linear classifiers, capturing personalized and physiologically meaningful information. This information can be used to predict health, fitness, and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers with a higher than 70 AUC. Overall, our proposed method is the first multimodal self-supervised approach for behavioral and physiological data, with significant implications for large-scale health and lifestyle monitoring.",1
"Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at https://github.com/Randl/kmeans_selfsuper",0
"The idea of unsupervised learning has always been attractive to machine learning experts as it eliminates the need for expensive and complicated data labeling processes. However, dealing with unsupervised learning of intricate data is challenging, and even the most effective techniques demonstrate weaker performance compared to their supervised counterparts. In computer vision, self-supervised deep learning has become a valuable tool for representation learning. Nevertheless, these approaches have not yet been assessed in a fully unsupervised environment. In this paper, we propose a straightforward method for unsupervised classification that relies on self-supervised representations. We tested our approach using various recent self-supervised techniques and discovered that it delivers competitive outcomes for ImageNet classification, achieving a 39% accuracy with 1000 clusters and 46% with overclustering. We propose that unsupervised evaluation be included in the standard benchmarks for self-supervised learning. Our code is accessible at https://github.com/Randl/kmeans_selfsuper.",1
"Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).",0
"There is a massive amount of data generated by human drivers that could potentially enhance autonomous driving systems. However, creating end-to-end driving models that directly map sensor data into driving actions presents interpretability issues and struggles to handle false correlations. Instead, our suggestion is to use action-based driving data to learn representations. By pre-training an affordance-based driving model using this method, we discovered that it can surpass pure end-to-end driving models while being more comprehensible, even with a small amount of weakly annotated imagery. Additionally, we demonstrate how this approach outperforms past techniques that rely on learning inverse dynamics models or heavy human supervision such as ImageNet.",1
"Graph neural network (GNN) has recently been established as an effective representation learning framework on graph data. However, the popular message passing models rely on local permutation invariant aggregate functions, which gives rise to the concerns about their representational power. Here, we introduce the concept of automorphic equivalence to theoretically analyze GNN's expressiveness in differentiating node's structural role. We show that the existing message passing GNNs have limitations in learning expressive representations. Moreover, we design a novel GNN class that leverages learnable automorphic equivalence filters to explicitly differentiate the structural roles of each node's neighbors, and uses a squeeze-and-excitation module to fuse various structural information. We theoretically prove that the proposed model is expressive in terms of generating distinct representations for nodes with different structural feature. Besides, we empirically validate our model on eight real-world graph data, including social network, e-commerce co-purchase network and citation network, and show that it consistently outperforms strong baselines.",0
"Recently, Graph neural network (GNN) has emerged as a potent tool for learning representations on graph data. However, the effectiveness of the widely used message passing models that rely on local permutation invariant aggregate functions has raised concerns about their ability to represent information adequately. To address this issue, we introduce the concept of automorphic equivalence to assess GNN's capability to differentiate nodes based on their structural roles. Our analysis reveals that the current message passing GNNs have limited capacity to learn expressive representations. To overcome this limitation, we propose a novel GNN class that employs learnable automorphic equivalence filters to differentiate each node's neighbors' structural roles explicitly. Additionally, we use a squeeze-and-excitation module to fuse various structural information. The proposed model's theoretical analysis proves its expressiveness in generating distinct representations for nodes with different structural features. Moreover, we evaluate our model on eight real-world graph datasets, including social networks, e-commerce co-purchase networks, and citation networks, and demonstrate its consistent superiority over strong baselines.",1
"To fully exploit the performance potential of modern multi-core processors, machine learning and data mining algorithms for big data must be parallelized in multiple ways. Today's CPUs consist of multiple cores, each following an independent thread of control, and each equipped with multiple arithmetic units which can perform the same operation on a vector of multiple data objects. Graph embedding, i.e. converting the vertices of a graph into numerical vectors is a data mining task of high importance and is useful for graph drawing (low-dimensional vectors) and graph representation learning (high-dimensional vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by Minimizing the Predictive Entropy), an information-theoretic method which can generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD (Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single Instructions Multiple Data, using AVX-512) parallelism. We propose general ideas applicable in other graph-based algorithms like \emph{vectorized hashing} and \emph{vectorized reduction}. Our experimental evaluation demonstrates the superiority of our approach.",0
"In order to fully utilize the capabilities of modern multi-core processors, it is necessary to parallelize machine learning and data mining algorithms for big data in multiple ways. Contemporary CPUs are composed of several cores, each executing an independent thread of control, and each equipped with multiple arithmetic units that can perform the same operation on a vector of multiple data objects. Graph embedding is an important data mining task that involves converting the vertices of a graph into numerical vectors, which is useful for both graph drawing and graph representation learning. This paper proposes MulticoreGEMPE, a method that utilizes information theory to generate low and high-dimensional vectors. MulticoreGEMPE employs both MIMD (Multiple Instructions Multiple Data) and SIMD (Single Instructions Multiple Data) parallelism through OpenMP and AVX-512, respectively. Additionally, we present general concepts that can be applied to other graph-based algorithms, such as vectorized hashing and vectorized reduction. Our experimental results demonstrate the superiority of our approach.",1
"Graph representation learning is a fundamental task in various applications that strives to learn low-dimensional embeddings for nodes that can preserve graph topology information. However, many existing methods focus on static graphs while ignoring evolving graph patterns. Inspired by the success of graph convolutional networks(GCNs) in static graph embedding, we propose a novel k-core based temporal graph convolutional network, the CTGCN, to learn node representations for dynamic graphs. In contrast to previous dynamic graph embedding methods, CTGCN can preserve both local connective proximity and global structural similarity while simultaneously capturing graph dynamics. In the proposed framework, the traditional graph convolution is generalized into two phases, feature transformation and feature aggregation, which gives the CTGCN more flexibility and enables the CTGCN to learn connective and structural information under the same framework. Experimental results on 7 real-world graphs demonstrate that the CTGCN outperforms existing state-of-the-art graph embedding methods in several tasks, including link prediction and structural role classification. The source code of this work can be obtained from \url{https://github.com/jhljx/CTGCN}.",0
"Learning low-dimensional embeddings for nodes that preserve graph topology information is a crucial aspect of various applications, known as graph representation learning. However, existing methods primarily focus on static graphs, disregarding the patterns of evolving graphs. To address this, we propose a novel k-core based temporal graph convolutional network, CTGCN, inspired by the success of graph convolutional networks (GCNs) for static graph embedding. CTGCN can preserve both local connective proximity and global structural similarity while simultaneously capturing graph dynamics. In this framework, the traditional graph convolution is transformed into two phases, feature transformation, and feature aggregation, giving CTGCN more flexibility to learn connective and structural information under the same framework. Our experimental results on seven real-world graphs show that CTGCN outperforms existing state-of-the-art graph embedding methods in various tasks, including link prediction and structural role classification. The source code for this work can be obtained from \url{https://github.com/jhljx/CTGCN}.",1
"State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at https://github.com/mila-iqia/atari-representation-learning",0
"The ability to capture latent generative factors of an environment, also known as state representation learning, is vital for creating intelligent agents that can perform various tasks. However, learning such representations without supervision from rewards is a difficult problem. To address this challenge, we present a method that uses a neural encoder to learn state representations by maximizing mutual information across spatially and temporally distinct features of the observations. Additionally, we introduce a new benchmark based on Atari 2600 games to evaluate how well these representations capture the ground truth state variables. This framework for evaluating representation learning models will be essential for future research. We also compare our approach with other state-of-the-art generative and contrastive representation learning techniques. The code for this work can be found at https://github.com/mila-iqia/atari-representation-learning.",1
"Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the ""neighborhood aggregation"" scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.",0
"The importance of graph representation learning cannot be overstated as it is crucial for a wide range of graph analytical tasks such as community detection and node classification. Graph convolutional networks (GCNs) have been successfully used for graph representation learning by aggregating features from the neighborhoods, following the ""neighborhood aggregation"" scheme. Despite their promising performance, existing GCN-based models have difficulty capturing the complex non-linearity of graph data. This paper provides a theoretical explanation of why GCNs barely outperform linear models and proposes a novel GraphAIR framework that models both neighborhood interaction and aggregation to better capture the non-linearity of graph data. Comprehensive experiments on benchmark tasks such as link prediction and node classification using public datasets demonstrate the effectiveness of the proposed method.",1
"Progress in the field of machine learning has been fueled by the introduction of benchmark datasets pushing the limits of existing algorithms. Enabling the design of datasets to test specific properties and failure modes of learning algorithms is thus a problem of high interest, as it has a direct impact on innovation in the field. In this sense, we introduce Synbols -- Synthetic Symbols -- a tool for rapidly generating new datasets with a rich composition of latent features rendered in low resolution images. Synbols leverages the large amount of symbols available in the Unicode standard and the wide range of artistic font provided by the open font community. Our tool's high-level interface provides a language for rapidly generating new distributions on the latent features, including various types of textures and occlusions. To showcase the versatility of Synbols, we use it to dissect the limitations and flaws in standard learning algorithms in various learning setups including supervised learning, active learning, out of distribution generalization, unsupervised representation learning, and object counting.",0
"The advancement of machine learning has been propelled by benchmark datasets that push the boundaries of current algorithms. The development of datasets to test distinct qualities and weaknesses of learning algorithms is an area of great interest as it directly impacts innovation in the field. Therefore, we have created Synbols, or Synthetic Symbols, which is a tool that can rapidly create new datasets with a diverse range of latent features depicted in low resolution images. By utilizing the vast array of symbols available in the Unicode standard and the broad selection of artistic fonts provided by the open font community, our tool allows for the swift generation of new distributions on latent features, including several types of textures and occlusions. We demonstrate the versatility of Synbols by using it to identify limitations and defects in standard learning algorithms in various learning scenarios, such as supervised learning, active learning, out-of-distribution generalization, unsupervised representation learning, and object counting.",1
"Data-driven graph learning models a network by determining the strength of connections between its nodes. The data refers to a graph signal which associates a value with each graph node. Existing graph learning methods either use simplified models for the graph signal, or they are prohibitively expensive in terms of computational and memory requirements. This is particularly true when the number of nodes is high or there are temporal changes in the network. In order to consider richer models with a reasonable computational tractability, we introduce a graph learning method based on representation learning on graphs. Representation learning generates an embedding for each graph node, taking the information from neighbouring nodes into account. Our graph learning method further modifies the embeddings to compute the graph similarity matrix. In this work, graph learning is used to examine brain networks for brain state identification. We infer time-varying brain graphs from an extensive dataset of intracranial electroencephalographic (iEEG) signals from ten patients. We then apply the graphs as input to a classifier to distinguish seizure vs. non-seizure brain states. Using the binary classification metric of area under the receiver operating characteristic curve (AUC), this approach yields an average of 9.13 percent improvement when compared to two widely used brain network modeling methods.",0
"The process of data-driven graph learning involves determining the strength of connections between nodes in a network. A graph signal is used to assign a value to each node, but current graph learning methods either use simplified models for the graph signal or are too computationally intensive, especially when the network has a large number of nodes or undergoes temporal changes. To address this issue, we propose a graph learning method that uses representation learning on graphs to generate embeddings for each node while considering information from neighboring nodes. These embeddings are then modified to compute the graph similarity matrix. Our approach is applied to analyzing brain networks for brain state identification, using intracranial electroencephalographic (iEEG) signals from ten patients to infer time-varying brain graphs. The resulting graphs are used as input to a classifier to distinguish seizure vs. non-seizure brain states, and our approach yields an average improvement of 9.13% in AUC compared to two commonly used brain network modeling methods.",1
"Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself.",0
"Coupling flow-based invertible neural networks (CF-INNs) have various applications in machine learning, including image synthesis and representation learning. However, their desirable feature of analytic invertibility is offset by the restriction of functional forms, raising the question of their representation power. Are CF-INNs capable of approximating all invertible functions, or is there a risk of rendering the model class unreliable? By demonstrating a criterion, we can answer this question: a CF-INN is universal if its layers include affine coupling and invertible linear functions as special cases. As a result, we can positively resolve a previously unsolved issue: whether normalizing flow models based on affine coupling can be universal distributional approximators. The demonstration of universality also leads to the proof of a general theorem that establishes the equivalence of universality for certain diffeomorphism classes, a theoretical insight that is noteworthy in its own right.",1
"Finding well-defined clusters in data represents a fundamental challenge for many data-driven applications, and largely depends on good data representation. Drawing on literature regarding representation learning, studies suggest that one key characteristic of good latent representations is the ability to produce semantically mixed outputs when decoding linear interpolations of two latent representations. We propose the Mixing Consistent Deep Clustering method which encourages interpolations to appear realistic while adding the constraint that interpolations of two data points must look like one of the two inputs. By applying this training method to various clustering (non-)specific autoencoder models we found that using the proposed training method systematically changed the structure of learned representations of a model and it improved clustering performance for the tested ACAI, IDEC, and VAE models on the MNIST, SVHN, and CIFAR-10 datasets. These outcomes have practical implications for numerous real-world clustering tasks, as it shows that the proposed method can be added to existing autoencoders to further improve clustering performance.",0
"The task of identifying distinct clusters in data is a significant challenge for many data-driven applications, which heavily relies on appropriate data representation. Based on research on representation learning, it has been suggested that an essential feature of effective latent representations is their ability to generate semantically diverse outputs when decoding linear interpolations of two latent representations. Our proposed Mixing Consistent Deep Clustering approach aims to enhance the realism of interpolations while enforcing the constraint that interpolations of two data points should resemble one of the two inputs. By implementing this training method across various autoencoder models, both clustering and non-clustering specific, we discovered that it systematically alters the structure of learned representations and enhances clustering performance across the MNIST, SVHN, and CIFAR-10 datasets for the tested ACAI, IDEC, and VAE models. This research has significant practical implications for a variety of real-world clustering tasks, as it confirms that the suggested method can enhance clustering performance by incorporating it into existing autoencoders.",1
"Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.",0
"Recently, graph representation learning has been applied to a wide range of problems, including computer graphics, chemistry, high energy physics, and social media. The popularity of graph neural networks has led to interest in developing scalable methods for very large graphs, such as those found in Facebook or Twitter social networks. To reduce computational costs in most of these approaches, a sampling strategy is employed, retaining only a subset of node neighbors or subgraphs during training. However, in this paper, we introduce a new, efficient, and scalable graph deep learning architecture that avoids the need for graph sampling by using graph convolutional filters of varying sizes that can be precomputed for efficient training and inference. Our architecture also allows for the use of different local graph operators to suit the task at hand. Through extensive experimental evaluation on various open benchmarks, we demonstrate that our approach is competitive with other state-of-the-art architectures while requiring significantly less training and inference time. Additionally, we achieve state-of-the-art results on ogbn-papers100M, the largest public graph dataset, containing over 110 million nodes and 1.5 billion edges.",1
"Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with significantly fewer dimensions than standard Euclidean spaces. However, the viability and benefits of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the first theoretical guarantees for learning a classifier in hyperbolic rather than Euclidean space. Specifically, we consider the problem of learning a large-margin classifier for data possessing a hierarchical structure. Our first contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efficiently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classifier directly in hyperbolic space.",0
"Representation learning in hyperbolic spaces has gained significant interest due to its ability to represent hierarchical data with fewer dimensions compared to Euclidean spaces. However, the advantages of hyperbolic spaces for downstream machine learning tasks have not been thoroughly explored. This paper presents the first theoretical guarantees for learning a classifier in hyperbolic space, focusing on the problem of learning a large-margin classifier for hierarchical data. The paper introduces a hyperbolic perceptron algorithm that converges to a separating hyperplane and an efficient algorithm to learn a large-margin hyperplane using adversarial examples. Additionally, the paper proves that hyperbolic space is superior for learning the classifier directly for hierarchical data that embeds well in hyperbolic space due to its low embedding dimension.",1
"We introduce a novel self-supervised pretext task for learning representations from audio-visual content. Prior work on audio-visual representation learning leverages correspondences at the video level. Approaches based on audio-visual correspondence (AVC) predict whether audio and video clips originate from the same or different video instances. Audio-visual temporal synchronization (AVTS) further discriminates negative pairs originated from the same video instance but at different moments in time. While these approaches learn high-quality representations for downstream tasks such as action recognition, their training objectives disregard spatial cues naturally occurring in audio and visual signals. To learn from these spatial cues, we tasked a network to perform contrastive audio-visual spatial alignment of 360{\deg} video and spatial audio. The ability to perform spatial alignment is enhanced by reasoning over the full spatial content of the 360{\deg} video using a transformer architecture to combine representations from multiple viewpoints. The advantages of the proposed pretext task are demonstrated on a variety of audio and visual downstream tasks, including audio-visual correspondence, spatial alignment, action recognition, and video semantic segmentation.",0
"Our research presents a new method for learning representations from audio-visual content through a self-supervised pretext task. Prior research focused on video-level correspondences, using audio-visual correspondence (AVC) to predict if clips originated from the same video instance and audio-visual temporal synchronization (AVTS) to discriminate negative pairs from the same video instance but at different times. Although these approaches produce high-quality representations for action recognition, they ignore spatial cues in audio and visual signals. To capture these cues, we designed a network to perform contrastive audio-visual spatial alignment of 360{\deg} video and spatial audio. We used a transformer architecture to combine representations from multiple viewpoints to enhance the ability to perform spatial alignment. Our proposed pretext task has several benefits, as demonstrated in various downstream tasks such as audio-visual correspondence, spatial alignment, action recognition, and video semantic segmentation.",1
"Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets.",0
"Recently, unsupervised learning approaches that employ contrastive learning have garnered attention and yielded promising outcomes. These techniques typically strive to acquire representations that are unaffected by variations at the instance level, which arise from diverse viewpoints of the same instance. Our research introduces Invariance Propagation, which focuses on developing representations that are invariant to category-level variations, which are derived from different instances belonging to the same category. Our method employs a recursive approach to identify semantically consistent samples residing in high-density regions of the representation space. We utilize a hard sampling strategy to optimize the agreement between the anchor sample and its hard positive counterparts, which offer more intra-class variations to capture more abstract invariance. As a result, using a ResNet-50 as the backbone, our approach obtains an accuracy of 71.3% for top-1 classification on ImageNet and 78.2% for top-5 classification fine-tuning on only 1% of the labels, surpassing previous results. Furthermore, our method achieves state-of-the-art performance on several downstream tasks, such as linear classification on Places205 and Pascal VOC, as well as transfer learning on small-scale datasets.",1
"Due to the widespread applications in real-world scenarios, metro ridership prediction is a crucial but challenging task in intelligent transportation systems. However, conventional methods either ignore the topological information of metro systems or directly learn on physical topology, and cannot fully explore the patterns of ridership evolution. To address this problem, we model a metro system as graphs with various topologies and propose a unified Physical-Virtual Collaboration Graph Network (PVCGN), which can effectively learn the complex ridership patterns from the tailor-designed graphs. Specifically, a physical graph is directly built based on the realistic topology of the studied metro system, while a similarity graph and a correlation graph are built with virtual topologies under the guidance of the inter-station passenger flow similarity and correlation. These complementary graphs are incorporated into a Graph Convolution Gated Recurrent Unit (GC-GRU) for spatial-temporal representation learning. Further, a Fully-Connected Gated Recurrent Unit (FC-GRU) is also applied to capture the global evolution tendency. Finally, we develop a Seq2Seq model with GC-GRU and FC-GRU to forecast the future metro ridership sequentially. Extensive experiments on two large-scale benchmarks (e.g., Shanghai Metro and Hangzhou Metro) well demonstrate the superiority of our PVCGN for station-level metro ridership prediction. Moreover, we apply the proposed PVCGN to address the online origin-destination (OD) ridership prediction and the experiment results show the universality of our method. Our code and benchmarks are available at https://github.com/HCPLab-SYSU/PVCGN.",0
"Forecasting metro ridership is a difficult but essential task in the field of intelligent transportation systems due to its extensive use in real-world situations. However, conventional methods overlook the topological information of metro systems or merely rely on physical topology, which restricts their ability to analyze ridership patterns comprehensively. To overcome this challenge, we propose a Physical-Virtual Collaboration Graph Network (PVCGN) that models metro systems as graphs with diverse topologies and efficiently learns intricate ridership patterns. Our approach involves constructing a physical graph based on the actual topology of the metro system and building a similarity graph and a correlation graph using virtual topologies, which are guided by the inter-station passenger flow similarity and correlation. These graphs complement each other and are incorporated into a Graph Convolution Gated Recurrent Unit (GC-GRU) for spatial-temporal representation learning. We also employ a Fully-Connected Gated Recurrent Unit (FC-GRU) to capture the global evolution tendency. Finally, we develop a Seq2Seq model with GC-GRU and FC-GRU to predict future metro ridership sequentially. Our experiments, conducted on two significant benchmarks (Shanghai Metro and Hangzhou Metro), demonstrate the superiority of PVCGN in station-level metro ridership prediction. Additionally, we successfully apply PVCGN to online origin-destination (OD) ridership prediction, proving its universality. Access to our code and benchmarks is available at https://github.com/HCPLab-SYSU/PVCGN.",1
"Recent works found that fine-tuning and joint training---two popular approaches for transfer learning---do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.",0
"Recent studies have shown that fine-tuning and joint training, two popular techniques for transfer learning, may not always lead to improved accuracy on downstream tasks. Our objective is to gain a deeper understanding of the circumstances under which fine-tuning and joint training may be suboptimal or detrimental for transfer learning. To achieve this, we have created semi-synthetic datasets that enable us to compare source-specific features and transferable features. Our observations indicate that pre-training may not be incentivized to learn transferable features, and joint training may result in the simultaneous learning of source-specific features and overfitting to the target. To surpass the limitations of fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to generate transferable features. MeRLin accomplishes this by meta-learning representations that perform well on target validation data when combined with a head fit on top of the representations. We have proven that MeRLin recovers the target ground-truth model using a quadratic neural net parameterization and a source distribution containing both transferable and source-specific features. Pre-training and joint training are shown to fail at learning transferable features on the same distribution, whereas MeRLin outperforms the previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.",1
"We conduct the first study of its kind to generate and evaluate vector representations for chess pieces. In particular, we uncover the latent structure of chess pieces and moves, as well as predict chess moves from chess positions. We share preliminary results which anticipate our ongoing work on a neural network architecture that learns these embeddings directly from supervised feedback.",0
"Our study is the pioneering research that produces and assesses vector representations for chess pieces. Specifically, we reveal the underlying framework of chess pieces and their moves, and can forecast chess moves based on chess positions. We present initial findings that prefigure our continuous efforts on a neural network design that acquires these embeddings via supervised feedback.",1
"Representation learning from 3D point clouds is challenging due to their inherent nature of permutation invariance and irregular distribution in space. Existing deep learning methods follow a hierarchical feature extraction paradigm in which high-level abstract features are derived from low-level features. However, they fail to exploit different granularity of information due to the limited interaction between these features. To this end, we propose Multi-Abstraction Refinement Network (MARNet) that ensures an effective exchange of information between multi-level features to gain local and global contextual cues while effectively preserving them till the final layer. We empirically show the effectiveness of MARNet in terms of state-of-the-art results on two challenging tasks: Shape classification and Coarse-to-fine grained semantic segmentation. MARNet significantly improves the classification performance by 2% over the baseline and outperforms the state-of-the-art methods on semantic segmentation task.",0
"The innate characteristics of 3D point clouds, such as their permutation invariance and irregular spatial distribution, make it difficult to learn representation from them. Although current deep learning methods utilize a hierarchical feature extraction approach to derive high-level abstract features from low-level ones, they do not take advantage of the varying granularity of information that can be extracted due to limited interaction between these features. To address this limitation, we introduce Multi-Abstraction Refinement Network (MARNet), which facilitates effective communication between multi-level features to capture local and global contextual cues while preserving them until the final layer. Our empirical results demonstrate the effectiveness of MARNet, particularly in shape classification and coarse-to-fine grained semantic segmentation, where it outperforms state-of-the-art methods. MARNet enhances the classification performance by 2% over the baseline and significantly improves the semantic segmentation task's results.",1
"Potential-based reward shaping provides an approach for designing good reward functions, with the purpose of speeding up learning. However, automatically finding potential functions for complex environments is a difficult problem (in fact, of the same difficulty as learning a value function from scratch). We propose a new framework for learning potential functions by leveraging ideas from graph representation learning. Our approach relies on Graph Convolutional Networks which we use as a key ingredient in combination with the probabilistic inference view of reinforcement learning. More precisely, we leverage Graph Convolutional Networks to perform message passing from rewarding states. The propagated messages can then be used as potential functions for reward shaping to accelerate learning. We verify empirically that our approach can achieve considerable improvements in both small and high-dimensional control problems.",0
"The use of potential-based reward shaping is a promising method for creating effective reward functions that enhance learning speed. However, the process of discovering potential functions for intricate environments is challenging and akin to learning a value function from scratch. To address this issue, we propose a novel approach that incorporates graph representation learning principles. Specifically, we utilize Graph Convolutional Networks to facilitate message passing from rewarding states, which we then use as potential functions for reward shaping to expedite learning. By applying this framework to small and high-dimensional control problems, we demonstrate significant improvements in performance.",1
"Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext",0
"The completion of video-text tasks in the real world often involves various levels of granularity, such as frames and words, clip and sentences, or videos and paragraphs, each with its own meaning. To address this issue, we introduce the Cooperative hierarchical Transformer (COOT) in this paper, which takes advantage of this hierarchy information and models the interactions between different levels of granularity and modalities. The method has three key components: an attention-aware feature aggregation layer that considers the local temporal context (intra-level, e.g., within a clip), a contextual transformer that learns the connections between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss that links video and text. The resulting approach outperforms the state of the art on various benchmarks while also having fewer parameters. The code for this method is publicly available at https://github.com/gingsi/coot-videotext.",1
"We present a Reverse Reinforcement Learning (Reverse RL) approach for representing retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as ""how much fuel will be consumed in expectation if we drive from A to B?"". GVFs, however, cannot answer questions like ""how much fuel do we expect a car to have given it is at B at time $t$?"". To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the influence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection.",0
"Our paper introduces a new method called Reverse Reinforcement Learning (Reverse RL) to capture retrospective knowledge. While General Value Functions (GVFs) have been successful in representing predictive knowledge about future outcomes, they cannot address questions related to past events. For instance, if we want to know how much fuel a car at point B had at time t, we need to consider its full tank and its journey to B. This type of knowledge is known as retrospective knowledge, as it depends on past events. To address this, we propose Reverse GVFs, which can be trained via Reverse RL to capture retrospective knowledge. Our experiments demonstrate the effectiveness of Reverse GVFs in representation learning and anomaly detection.",1
"This paper reviews the novel concept of controllable variational autoencoder (ControlVAE), discusses its parameter tuning to meet application needs, derives its key analytic properties, and offers useful extensions and applications. ControlVAE is a new variational autoencoder (VAE) framework that combines the automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows us to precisely control the KL-divergence to a desired value (set point), which is effective in avoiding posterior collapse and learning disentangled representations. In order to improve the ELBO over the regular VAE, we provide simplified theoretical analysis to inform setting the set point of KL-divergence for ControlVAE. We observe that compared to other methods that seek to balance the two terms in VAE's objective, ControlVAE leads to better learning dynamics. In particular, it can achieve a good trade-off between reconstruction quality and KL-divergence. We evaluate the proposed method on three tasks: image generation, language modeling and disentangled representation learning. The results show that ControlVAE can achieve much better reconstruction quality than the other methods for comparable disentanglement. On the language modeling task, ControlVAE can avoid posterior collapse (KL vanishing) and improve the diversity of generated text. Moreover, our method can change the optimization trajectory, improving the ELBO and the reconstruction quality for image generation.",0
"This article presents the concept of ControlVAE, a new framework that combines automatic control theory with the basic VAE to stabilize KL-divergence of VAE models to a specified value. By using a non-linear PI controller, ControlVAE can dynamically adjust the weight of the KL-divergence term in the evidence lower bound by using the output KL-divergence as feedback. This allows precise control of the KL-divergence to avoid posterior collapse and achieve better learning dynamics. The authors provide simplified theoretical analysis to inform the setting of the KL-divergence set point for ControlVAE and observe that it leads to better learning dynamics compared to other methods. They evaluate ControlVAE on three tasks and find that it achieves better reconstruction quality and comparable disentanglement, avoids posterior collapse and improves the diversity of generated text. The method can also change the optimization trajectory, improving the ELBO and reconstruction quality for image generation.",1
"Recently proposed neural architecture search (NAS) algorithms adopt neural predictors to accelerate the architecture search. The capability of neural predictors to accurately predict the performance metrics of neural architecture is critical to NAS, and the acquisition of training datasets for neural predictors is time-consuming. How to obtain a neural predictor with high prediction accuracy using a small amount of training data is a central problem to neural predictor-based NAS. Here, we firstly design a new architecture encoding scheme that overcomes the drawbacks of existing vector-based architecture encoding schemes to calculate the graph edit distance of neural architectures. To enhance the predictive performance of neural predictors, we devise two self-supervised learning methods from different perspectives to pre-train the architecture embedding part of neural predictors to generate a meaningful representation of neural architectures. The first one is to train a carefully designed two branch graph neural network model to predict the graph edit distance of two input neural architectures. The second method is inspired by the prevalently contrastive learning, and we present a new contrastive learning algorithm that utilizes a central feature vector as a proxy to contrast positive pairs against negative pairs. Experimental results illustrate that the pre-trained neural predictors can achieve comparable or superior performance compared with their supervised counterparts with several times less training samples. We achieve state-of-the-art performance on the NASBench-101 and NASBench201 benchmarks when integrating the pre-trained neural predictors with an evolutionary NAS algorithm.",0
"Neural architecture search (NAS) algorithms have recently started using neural predictors to speed up the process. The accuracy of these predictors in predicting the performance metrics of neural architectures is crucial for NAS, but acquiring the necessary training datasets takes up a lot of time. To resolve this issue, the main challenge lies in obtaining a neural predictor with high accuracy using a small amount of training data. To address this, we propose a new architecture encoding scheme that overcomes the limitations of existing vector-based schemes, enabling the calculation of graph edit distance of neural architectures. In addition, we present two self-supervised learning methods to pre-train the architecture embedding part of neural predictors for better predictive performance. The first method involves training a two-branch graph neural network model, while the second method is inspired by contrastive learning. Our experimental results demonstrate that the pre-trained neural predictors can achieve comparable or even superior performance compared to their supervised counterparts with significantly fewer training samples. When integrated with an evolutionary NAS algorithm, our pre-trained neural predictors achieve state-of-the-art performance on the NASBench-101 and NASBench201 benchmarks.",1
"Self-supervised, multi-modal learning has been successful in holistic representation of complex scenarios. This can be useful to consolidate information from multiple modalities which have multiple, versatile uses. Its application in surgical robotics can lead to simultaneously developing a generalised machine understanding of the surgical process and reduce the dependency on quality, expert annotations which are generally difficult to obtain. We develop a self-supervised, multi-modal representation learning paradigm that learns representations for surgical gestures from video and kinematics. We use an encoder-decoder network configuration that encodes representations from surgical videos and decodes them to yield kinematics. We quantitatively demonstrate the efficacy of our learnt representations for gesture recognition (with accuracy between 69.6 % and 77.8 %), transfer learning across multiple tasks (with accuracy between 44.6 % and 64.8 %) and surgeon skill classification (with accuracy between 76.8 % and 81.2 %). Further, we qualitatively demonstrate that our self-supervised representations cluster in semantically meaningful properties (surgeon skill and gestures).",0
"Multi-modal, self-supervised learning has shown promise in capturing comprehensive representations of intricate scenarios. This approach can effectively integrate information from various modalities with versatile applications. In the context of surgical robotics, it can facilitate the development of a generalized machine understanding of the surgical process while minimizing the reliance on expert annotations, which can be arduous to obtain. Our study presents a self-supervised, multi-modal representation learning framework that learns surgical gesture representations from video and kinematics. We leverage an encoder-decoder network to encode surgical videos' representations and decode them to generate kinematics. Our results demonstrate the effectiveness of our learned representations in gesture recognition (achieving an accuracy range of 69.6% to 77.8%), transfer learning across multiple tasks (achieving an accuracy range of 44.6% to 64.8%), and surgeon skill classification (achieving an accuracy range of 76.8% to 81.2%). Additionally, we demonstrate that our self-supervised representations cluster in semantically meaningful properties, such as surgeon skill and gestures.",1
"Studies recently accomplished on the Enteric Nervous System have shown that chronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus, the development of recognition methods able to identify whether or not the EGC are affected by these type of diseases may be helpful in its diagnoses. In this work, we propose the use of pattern recognition and machine learning techniques to evaluate if a given animal EGC image was obtained from a healthy individual or one affect by a chronic degenerative disease. In the proposed approach, we have performed the classification task with handcrafted features and deep learning based techniques, also known as non-handcrafted features. The handcrafted features were obtained from the textural content of the ECG images using texture descriptors, such as the Local Binary Pattern (LBP). Moreover, the representation learning techniques employed in the approach are based on different Convolutional Neural Network (CNN) architectures, such as AlexNet and VGG16, with and without transfer learning. The complementarity between the handcrafted and non-handcrafted features was also evaluated with late fusion techniques. The datasets of EGC images used in the experiments, which are also contributions of this paper, are composed of three different chronic degenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The experimental results, supported by statistical analysis, shown that the proposed approach can distinguish healthy cells from the sick ones with a recognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13% (Diabetes Mellitus), being achieved by combining classifiers obtained both feature scenarios.",0
"Recent studies on the Enteric Nervous System have revealed that chronic degenerative diseases impact Enteric Glial Cells (EGC). Identifying whether EGC are affected by such diseases is crucial for accurate diagnoses. This study proposes using pattern recognition and machine learning techniques to determine if an EGC image is from a healthy or chronically diseased individual. Two feature scenarios were used: handcrafted features obtained from textural content using descriptors like Local Binary Pattern (LBP) and non-handcrafted features based on Convolutional Neural Network (CNN) architectures like AlexNet and VGG16. Transfer learning was also employed. Late fusion techniques were used to evaluate complementarity between the two feature scenarios. The study used EGC image datasets from three chronic degenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The results demonstrated that the proposed approach could distinguish healthy cells from sick ones with a recognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13% (Diabetes Mellitus), by combining classifiers from both feature scenarios.",1
"For mental disorders, patients' underlying mental states are non-observed latent constructs which have to be inferred from observed multi-domain measurements such as diagnostic symptoms and patient functioning scores. Additionally, substantial heterogeneity in the disease diagnosis between patients needs to be addressed for optimizing individualized treatment policy in order to achieve precision medicine. To address these challenges, we propose an integrated learning framework that can simultaneously learn patients' underlying mental states and recommend optimal treatments for each individual. This learning framework is based on the measurement theory in psychiatry for modeling multiple disease diagnostic measures as arising from the underlying causes (true mental states). It allows incorporation of the multivariate pre- and post-treatment outcomes as well as biological measures while preserving the invariant structure for representing patients' latent mental states. A multi-layer neural network is used to allow complex treatment effect heterogeneity. Optimal treatment policy can be inferred for future patients by comparing their potential mental states under different treatments given the observed multi-domain pre-treatment measurements. Experiments on simulated data and a real-world clinical trial data show that the learned treatment polices compare favorably to alternative methods on heterogeneous treatment effects, and have broad utilities which lead to better patient outcomes on multiple domains.",0
"Patients suffering from mental disorders have non-observable latent constructs that must be inferred from multi-domain measurements such as diagnostic symptoms and patient functioning scores. The heterogeneity in disease diagnosis among patients poses a challenge to developing individualized treatment policies for precision medicine. To overcome these challenges, we propose an integrated learning framework that can learn patients' underlying mental states and recommend personalized treatments. This framework is based on the measurement theory in psychiatry and enables the incorporation of pre- and post-treatment outcomes, biological measures, and complex treatment effect heterogeneity. It uses a multi-layer neural network to infer optimal treatment policies for future patients by comparing their potential mental states under different treatments. Our experiments on both simulated and real-world clinical trial data demonstrate that this approach leads to better patient outcomes across multiple domains and outperforms alternative methods on heterogeneous treatment effects.",1
"We propose a tree-based algorithm for classification and regression problems in the context of functional data analysis, which allows to leverage representation learning and multiple splitting rules at the node level, reducing generalization error while retaining the interpretability of a tree. This is achieved by learning a weighted functional $L^{2}$ space by means of constrained convex optimization, which is then used to extract multiple weighted integral features from the input functions, in order to determine the binary split for each internal node of the tree. The approach is designed to manage multiple functional inputs and/or outputs, by defining suitable splitting rules and loss functions that can depend on the specific problem and can also be combined with scalar and categorical data, as the tree is grown with the original greedy CART algorithm. We focus on the case of scalar-valued functional inputs defined on unidimensional domains and illustrate the effectiveness of our method in both classification and regression tasks, through a simulation study and four real world applications.",0
"Our proposed algorithm for functional data analysis solves classification and regression problems using a tree-based approach. The algorithm utilizes representation learning and multiple splitting rules at the node level to reduce generalization error while maintaining the interpretability of a tree. We achieve this by learning a weighted functional $L^{2}$ space using constrained convex optimization, which extracts multiple weighted integral features from input functions to determine binary splits for each internal node in the tree. Our approach can handle multiple functional inputs and/or outputs by defining suitable splitting rules and loss functions that can depend on the specific problem and can also be combined with scalar and categorical data, while growing the tree using the original greedy CART algorithm. We demonstrate the effectiveness of our method in both classification and regression tasks using scalar-valued functional inputs defined on unidimensional domains, through a simulation study and four real-world applications.",1
"Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values, and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods.",0
"There are two approaches to machine learning with missing data: feature imputation and label prediction. Feature imputation estimates missing values based on observed values, while label prediction learns downstream labels from incomplete data. However, current imputation models have limited learning capabilities and rely heavily on assumptions, while label prediction models can face scalability issues and rely on heuristics. This paper presents GRAPE, a graph-based framework that addresses missing data using a bipartite graph representation. GRAPE formulates feature imputation and label prediction as edge-level and node-level prediction tasks, respectively, which are solved using Graph Neural Networks. Results from nine benchmark datasets show that GRAPE outperforms existing state-of-the-art methods with 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks.",1
"We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation.",0
"Our proposal introduces a new unsupervised generative model that can extract object identity from other low-level features in data that is imbalanced across classes. We examined the limitations of InfoGAN, which assumes uniformity, and found that it fails to properly distinguish object identity in such data. Our approach involves discovering the discrete latent factor that is invariant to identity-preserving transformations in real images, and using it to identify the appropriate latent distribution that represents object identity. Our method was tested on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets, and proved to be effective in disentangling object identity as a latent factor of variation.",1
"Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration.",0
"The goal of heterogeneous graph representation learning is to acquire low-dimensional vector representations of diverse entities and relations to boost downstream tasks. Current approaches either capture semantic relationships but indirectly utilize node/edge attributes in a complex manner, or directly employ node/edge attributes without considering semantic relationships. Furthermore, they have limited scalability when involving multiple convolution operations. To address these issues, this article proposes a flexible and effective framework called Graph Information Propagation Network (GripNet). The GripNet framework introduces a novel supergraph data structure that comprises supervertices and superedges. A supervertex represents a semantically-coherent subgraph, while a superedge defines a propagation path for information between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We create various large-scale graphs and compare GripNet with other methods to demonstrate its superiority in link prediction, node classification, and data integration.",1
"Frequency spectrum has played a significant role in learning unique and discriminating features for object recognition. Both low and high frequency information present in images have been extracted and learnt by a host of representation learning techniques, including deep learning. Inspired by this observation, we introduce a novel class of adversarial attacks, namely `WaveTransform', that creates adversarial noise corresponding to low-frequency and high-frequency subbands, separately (or in combination). The frequency subbands are analyzed using wavelet decomposition; the subbands are corrupted and then used to construct an adversarial example. Experiments are performed using multiple databases and CNN models to establish the effectiveness of the proposed WaveTransform attack and analyze the importance of a particular frequency component. The robustness of the proposed attack is also evaluated through its transferability and resiliency against a recent adversarial defense algorithm. Experiments show that the proposed attack is effective against the defense algorithm and is also transferable across CNNs.",0
"The utilization of frequency spectrum has been crucial in acquiring distinct and distinguishing characteristics for recognizing objects. Various techniques, such as deep learning, have been utilized to extract and comprehend both low and high frequency data in images. In light of this, we introduce a new type of adversarial attacks, called ""WaveTransform"", which generates adversarial noise separately or simultaneously for low and high frequency subbands. Wavelet decomposition is used to analyze the frequency subbands, which are then corrupted to construct the adversarial example. The effectiveness of the WaveTransform attack and the significance of a particular frequency component are established through experiments using multiple databases and CNN models. Moreover, the proposed attack's transferability and resilience are evaluated against a recent adversarial defense algorithm. The experimental results reveal that the proposed attack is potent against the defense algorithm and can also be transferred across CNNs.",1
"Learning representations of sets of nodes in a graph is crucial for applications ranging from node-role discovery to link prediction and molecule classification. Graph Neural Networks (GNNs) have achieved great success in graph representation learning. However, expressive power of GNNs is limited by the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical representations for graph substructures that may in fact be very different. More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only focus on representing entire graphs and they are computationally inefficient as they cannot utilize sparsity of the underlying graph. Here we propose and mathematically analyze a general class of structure-related features, termed Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while providing strictly more expressive power than the 1-WL test. DE captures the distance between the node set whose representation is to be learned and each node in the graph. To capture the distance DE can apply various graph-distance measures such as shortest path distance or generalized PageRank scores. We propose two ways for GNNs to use DEs (1) as extra node features, and (2) as controllers of message aggregation in GNNs. Both approaches can utilize the sparse structure of the underlying graph, which leads to computational efficiency and scalability. We also prove that DE can distinguish node sets embedded in almost all regular graphs where traditional GNNs always fail. We evaluate DE on three tasks over six real networks: structural role prediction, link prediction, and triangle prediction. Results show that our models outperform GNNs without DE by up-to 15\% in accuracy and AUROC. Furthermore, our models also significantly outperform other state-of-the-art methods especially designed for the above tasks.",0
"Acquiring knowledge about sets of nodes in a graph is critical for various applications, such as identifying node roles, predicting links, and classifying molecules. Graph Neural Networks (GNNs) have been successful in learning graph representations. However, their ability to express themselves is limited by the 1-Weisfeiler-Lehman (WL) test, which results in GNNs producing identical representations for graph substructures that are fundamentally different. Although more powerful GNNs have been proposed, which mimic higher-order WL tests, they tend to focus solely on graph representation and are computationally inefficient as they cannot exploit the sparsity of the underlying graph. In this paper, we introduce a new class of structure-related features known as Distance Encoding (DE), which provides GNNs with additional expressive power compared to the 1-WL test while enabling them to represent any set of nodes. DE captures the distance between the node set that requires representation and each node in the graph, and it uses various graph-distance measures, such as the shortest path distance or the generalized PageRank scores, to achieve this. We propose two ways in which GNNs can utilize DEs: (1) as extra node features and (2) as message aggregation controllers. Both approaches can exploit the sparsity of the underlying graph, making them computationally efficient and scalable. We also prove that DE can differentiate between node sets embedded in almost all regular graphs where traditional GNNs fail. We evaluate DE on three tasks across six real networks: structural role prediction, link prediction, and triangle prediction. The results show that our models outperform GNNs without DE by up to 15% in terms of accuracy and AUROC. Moreover, our models significantly outperform other state-of-the-art methods that are specifically designed for these tasks.",1
"Most deep learning based image inpainting approaches adopt autoencoder or its variants to fill missing regions in images. Encoders are usually utilized to learn powerful representational spaces, which are important for dealing with sophisticated learning tasks. Specifically, in image inpainting tasks, masks with any shapes can appear anywhere in images (i.e., free-form masks) which form complex patterns. It is difficult for encoders to capture such powerful representations under this complex situation. To tackle this problem, we propose a self-supervised Siamese inference network to improve the robustness and generalization. It can encode contextual semantics from full resolution images and obtain more discriminative representations. we further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine both the restored and known regions in a smooth way. This multi-scale architecture is beneficial for decoding discriminative representations learned by encoders into images layer by layer. In this way, unknown regions will be filled naturally from outside to inside. Qualitative and quantitative experiments on multiple datasets, including facial and natural datasets (i.e., Celeb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our proposed method outperforms state-of-the-art methods in generating high-quality inpainting results.",0
"Many deep learning-based methods for image inpainting use autoencoders or similar models to fill in missing areas of an image. Encoders are typically employed to learn powerful representations that are crucial for handling complex learning tasks. However, when it comes to image inpainting, free-form masks can appear anywhere in images, resulting in intricate patterns that are difficult for encoders to capture. To address this issue, we introduce a self-supervised Siamese inference network that improves robustness and generalization by encoding contextual semantics from full-resolution images and obtaining more discriminative representations. Additionally, we propose a multi-scale decoder with a novel dual attention fusion module (DAF) that smoothly combines restored and known regions. This multi-scale approach enables encoders to decode discriminative representations into images layer by layer, filling in unknown regions from the outside in. Our method outperforms state-of-the-art techniques in generating high-quality inpainting results across multiple datasets, including Celeb-HQ, Pairs Street View, Places2, and ImageNet, as evidenced by qualitative and quantitative experiments.",1
"We prove a Chernoff-type bound for sums of matrix-valued random variables sampled via a regular (aperiodic and irreducible) finite Markov chain. Specially, consider a random walk on a regular Markov chain and a Hermitian matrix-valued function on its state space. Our result gives exponentially decreasing bounds on the tail distributions of the extreme eigenvalues of the sample mean matrix. Our proof is based on the matrix expander (regular undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].   Our matrix Chernoff bound for Markov chains can be applied to analyze the behavior of co-occurrence statistics for sequential data, which have been common and important data signals in machine learning. We show that given a regular Markov chain with $n$ states and mixing time $\tau$, we need a trajectory of length $O(\tau (\log{(n)}+\log{(\tau)})/\epsilon^2)$ to achieve an estimator of the co-occurrence matrix with error bound $\epsilon$. We conduct several experiments and the experimental results are consistent with the exponentially fast convergence rate from theoretical analysis. Our result gives the first bound on the convergence rate of the co-occurrence matrix and the first sample complexity analysis in graph representation learning.",0
"Our paper presents a Chernoff-type bound for matrix-valued random variables that are sampled using a regular finite Markov chain, which is aperiodic and irreducible. Specifically, we consider a Hermitian matrix-valued function on the state space of a random walk on a regular Markov chain. Our result provides an exponentially decreasing bound on the tail distributions of the extreme eigenvalues of the mean matrix from the samples. To prove this, we use the matrix expander Chernoff bound from Garg et al. (STOC '18) and scalar Chernoff-Hoeffding bounds for Markov chains from Chung et al. (STACS '12). Our matrix Chernoff bound has applications in analyzing co-occurrence statistics for sequential data, which are important signals in machine learning. We demonstrate that with a trajectory of length $O(\tau (\log{(n)}+\log{(\tau)})/\epsilon^2)$, where $\tau$ is the mixing time of a regular Markov chain with $n$ states, we can estimate the co-occurrence matrix with an error bound of $\epsilon$. Our experiments confirm the fast convergence rate predicted by our theoretical analysis. We believe that our work is the first to provide a bound on the convergence rate of the co-occurrence matrix and a sample complexity analysis in graph representation learning.",1
"Although Generative Adversarial Networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.",0
"GANs have made significant advancements in face synthesis, but there is still a lack of understanding regarding what GANs learn in the latent representation to create photo-realistic images from a random code. This study introduces a framework called InterFaceGAN, which interprets the disentangled face representation learned by state-of-the-art GAN models. By identifying linear subspaces in the latent space where GANs learn various semantics, we can manipulate facial attributes without retraining the model. The study also disentangles facial semantics through subspace projection and enables more precise control of attribute manipulation, including gender, age, expression, eyeglasses, face pose, and artifact correction. Additionally, the approach is evaluated quantitatively through face identity analysis and layer-wise analysis. The study also applies the approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on synthetic data. The results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.",1
"Geometric scattering has recently gained recognition in graph representation learning, and recent work has shown that integrating scattering features in graph convolution networks (GCNs) can alleviate the typical oversmoothing of features in node representation learning. However, scattering methods often rely on handcrafted design, requiring careful selection of frequency bands via a cascade of wavelet transforms, as well as an effective weight sharing scheme to combine together low- and band-pass information. Here, we introduce a new attention-based architecture to produce adaptive task-driven node representations by implicitly learning node-wise weights for combining multiple scattering and GCN channels in the network. We show the resulting geometric scattering attention network (GSAN) outperforms previous networks in semi-supervised node classification, while also enabling a spectral study of extracted information by examining node-wise attention weights.",0
"Recently, geometric scattering has become popular in graph representation learning. Studies have demonstrated that incorporating scattering features into graph convolution networks (GCNs) can alleviate the issue of oversmoothing typical of node representation learning. However, scattering methods usually require handcrafted design, which involves selecting frequency bands carefully through a series of wavelet transforms, as well as an effective weight sharing mechanism to combine low- and band-pass information. In this study, we propose a novel attention-based architecture that generates task-driven node representations by implicitly learning node-wise weights for combining multiple scattering and GCN channels in the network. Our resulting geometric scattering attention network (GSAN) outperforms previous networks in semi-supervised node classification and enables a spectral analysis of the extracted information by examining node-specific attention weights.",1
"We present Cycle-Contrastive Learning (CCL), a novel self-supervised method for learning video representation. Following a nature that there is a belong and inclusion relation of video and its frames, CCL is designed to find correspondences across frames and videos considering the contrastive representation in their domains respectively. It is different from recent approaches that merely learn correspondences across frames or clips. In our method, the frame and video representations are learned from a single network based on an R3D architecture, with a shared non-linear transformation for embedding both frame and video features before the cycle-contrastive loss. We demonstrate that the video representation learned by CCL can be transferred well to downstream tasks of video understanding, outperforming previous methods in nearest neighbour retrieval and action recognition tasks on UCF101, HMDB51 and MMAct.",0
"Introducing Cycle-Contrastive Learning (CCL), a fresh technique for acquiring video representation through self-supervision. CCL acknowledges the relationship between videos and their frames, and endeavors to identify correspondences across both domains by leveraging their contrastive representation. Unlike other recent methods that only learn correspondences within frames or clips, CCL employs a single network based on an R3D architecture to learn frame and video representations. The representations are then embedded using a shared nonlinear transformation before the application of cycle-contrastive loss. Our results demonstrate that the video representation obtained through CCL can be effectively applied to video understanding tasks, surpassing previous methods in nearest neighbor retrieval and action recognition on UCF101, HMDB51, and MMAct.",1
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",0
"Contrastive Learning has become popular recently for its achievements in self-supervised representation learning in the field of computer vision. Nevertheless, Contrastive Learning has its origins dating back to the 1990s and has been utilized in various domains, such as Metric Learning and natural language processing. This study includes an extensive survey of the literature and proposes a general framework for Contrastive Representation Learning that simplifies and consolidates several contrastive learning approaches. Furthermore, this study categorizes each component of contrastive learning to differentiate it from other machine learning methods. The paper also examines the inductive biases found in any contrastive learning system and scrutinizes the framework from different viewpoints in various sub-fields of machine learning. It provides numerous examples of how contrastive learning has been applied in diverse domains, such as computer vision, natural language processing, audio processing, and Reinforcement Learning. Finally, this study discusses upcoming research directions and the challenges faced in the field of Contrastive Learning.",1
"The increasing impact of black box models, and particularly of unsupervised ones, comes with an increasing interest in tools to understand and interpret them. In this paper, we consider in particular how to characterise visual groupings discovered automatically by deep neural networks, starting with state-of-the-art clustering methods. In some cases, clusters readily correspond to an existing labelled dataset. However, often they do not, yet they still maintain an ""intuitive interpretability"". We introduce two concepts, visual learnability and describability, that can be used to quantify the interpretability of arbitrary image groupings, including unsupervised ones. The idea is to measure (1) how well humans can learn to reproduce a grouping by measuring their ability to generalise from a small set of visual examples (learnability) and (2) whether the set of visual examples can be replaced by a succinct, textual description (describability). By assessing human annotators as classifiers, we remove the subjective quality of existing evaluation metrics. For better scalability, we finally propose a class-level captioning system to generate descriptions for visual groupings automatically and compare it to human annotators using the describability metric.",0
"As black box models, particularly unsupervised ones, become increasingly influential, there is a growing need for tools to comprehend and interpret them. This study focuses on characterizing visual clusters discovered by deep neural networks, using cutting-edge clustering techniques. While some clusters correspond to pre-existing datasets, many do not, yet they remain ""intuitively interpretable"". To quantify the interpretability of any image clusters, including unsupervised ones, we introduce two concepts: visual learnability and describability. These concepts evaluate how easily humans can reproduce a grouping and whether the group can be described succinctly. To eliminate subjectivity, we assess human annotators as classifiers. To improve scalability, we propose a class-level captioning system that automatically generates descriptions for visual clusters and compare it to human annotators using the describability metric.",1
"We consider the identifiability theory of probabilistic models and establish sufficient conditions under which the representations learned by a very broad family of conditional energy-based models are unique in function space, up to a simple transformation. In our model family, the energy function is the dot-product between two feature extractors, one for the dependent variable, and one for the conditioning variable. We show that under mild conditions, the features are unique up to scaling and permutation. Our results extend recent developments in nonlinear ICA, and in fact, they lead to an important generalization of ICA models. In particular, we show that our model can be used for the estimation of the components in the framework of Independently Modulated Component Analysis (IMCA), a new generalization of nonlinear ICA that relaxes the independence assumption. A thorough empirical study shows that representations learned by our model from real-world image datasets are identifiable, and improve performance in transfer learning and semi-supervised learning tasks.",0
"The theory of probabilistic models' identifiability is examined, and we establish sufficient conditions for the uniqueness of representations learned by a wide range of conditional energy-based models in function space, with minor transformations. Our energy function is created by the dot-product of two feature extractors, one for the dependent variable and one for the conditioning variable, and we indicate that the features are unique with mild conditions, up to scaling and permutation. Our findings extend recent advancements in nonlinear ICA and offer a significant generalization of ICA models. Our model is suitable for the estimation of components in Independently Modulated Component Analysis (IMCA), a new generalization of nonlinear ICA that relaxes the independence assumption. Our thorough empirical research demonstrates that our model's learned representations from real-world image datasets are identifiable and enhance performance in transfer learning and semi-supervised learning tasks.",1
"Heterogeneous Information Networks (HINs), involving a diversity of node types and relation types, are pervasive in many real-world applications. Recently, increasing attention has been paid to heterogeneous graph representation learning (HGRL) which aims to embed rich structural and semantics information in HIN into low-dimensional node representations. To date, most HGRL models rely on manual customisation of meta paths to capture the semantics underlying the given HIN. However, the dependency on the handcrafted meta-paths requires rich domain knowledge which is extremely difficult to obtain for complex and semantic rich HINs. Moreover, strictly defined meta-paths will limit the HGRL's access to more comprehensive information in HINs. To fully unleash the power of HGRL, we present a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN), to design different meta-paths for the nodes in a HIN. Specifically, RL-HGNN models the meta-path design process as a Markov Decision Process and uses a policy network to adaptively design a meta-path for each node to learn its effective representations. The policy network is trained with deep reinforcement learning by exploiting the performance of the model on a downstream task. We further propose an extension, RL-HGNN++, to ameliorate the meta-path design procedure and accelerate the training process. Experimental results demonstrate the effectiveness of RL-HGNN, and reveals that it can identify meaningful meta-paths that would have been ignored by human knowledge.",0
"Heterogeneous Information Networks (HINs) are widely used in various real-world applications and consist of various node types and relation types. Recently, there has been an increased focus on heterogeneous graph representation learning (HGRL), which aims to create low-dimensional node representations while incorporating structural and semantic information from HINs. However, most HGRL models rely on manually customized meta paths to capture the underlying semantics, which requires extensive domain knowledge and may limit access to comprehensive information. To address these issues, we introduce a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN) that utilizes a policy network to adaptively design meta-paths for each node in the HIN. The meta-path design process is modeled as a Markov Decision Process and trained with deep reinforcement learning, and we propose an extension (RL-HGNN++) to improve the meta-path design procedure and accelerate the training process. Our experimental results demonstrate that RL-HGNN can identify meaningful meta-paths that would have been overlooked by human knowledge.",1
"Graph Neural Networks achieve remarkable results on problems with structured data but come as black-box predictors. Transferring existing explanation techniques, such as occlusion, fails as even removing a single node or edge can lead to drastic changes in the graph. The resulting graphs can differ from all training examples, causing model confusion and wrong explanations. Thus, we argue that explicability must use graphs compliant with the distribution underlying the training data. We coin this property Distribution Compliant Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE) technique following this paradigm. An experimental study supports the efficacy of CoGE.",0
"Although Graph Neural Networks are effective at solving problems involving structured data, they function as opaque predictors. Applying established explanation methods, including occlusion, proves ineffective because the removal of even a single node or edge can significantly alter the graph. This leads to graphs that differ from all training examples, confusing the model and providing incorrect explanations. As a result, we advocate for the use of graphs that conform to the distribution underlying the training data for the sake of explicability. We refer to this property as Distribution Compliant Explanation (DCE) and introduce a new Contrastive GNN Explanation (CoGE) technique that adheres to this principle. Our experimental findings support the effectiveness of CoGE.",1
"Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.",0
"Reinforcement learning (RL) algorithms that use deep networks with high capacity can directly learn from image observations. However, this can pose challenges due to the high-dimensional observation spaces that require the policy to solve two problems: representation learning and task learning. This study addresses these issues separately by explicitly learning latent representations that can enhance RL from images. The study introduces the stochastic latent actor-critic (SLAC) algorithm, which is a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC unifies stochastic sequential models and RL into a single method by learning a compact latent representation and then performing RL in the model's learned latent space. Experimental evaluation shows that SLAC outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. The study provides code and videos of the results, which are available on their website.",1
"Model compression becomes a recent trend due to the requirement of deploying neural networks on embedded and mobile devices. Hence, both accuracy and efficiency are of critical importance. To explore a balance between them, a knowledge distillation strategy is proposed for general visual representation learning. It utilizes our well-designed activation map adaptive module to replace some blocks of the teacher network, exploring the most appropriate supervisory features adaptively during the training process. Using the teacher's hidden layer output to prompt the student network to train so as to transfer effective semantic information.To verify the effectiveness of our strategy, this paper applied our method to cifar-10 dataset. Results demonstrate that the method can boost the accuracy of the student network by 0.6% with 6.5% loss reduction, and significantly improve its training speed.",0
"Recently, there has been a trend in model compression due to the need for deploying neural networks on mobile and embedded devices. The importance of both accuracy and efficiency cannot be overstated. In order to achieve a balance between the two, this study proposes a knowledge distillation strategy for general visual representation learning. This method uses an activation map adaptive module to replace certain blocks of the teacher network, which enables the selection of appropriate supervisory features during the training process. The teacher's hidden layer output prompts the student network to train, resulting in the transfer of effective semantic information. To demonstrate the effectiveness of this strategy, the method was applied to the cifar-10 dataset. The results show that the accuracy of the student network can be improved by 0.6% with a 6.5% loss reduction, and training speed can be significantly improved.",1
"Self-supervised learning approaches leverage unlabeled samples to acquire generic knowledge about different concepts, hence allowing for annotation-efficient downstream task learning. In this paper, we propose a novel self-supervised method that leverages multiple imaging modalities. We introduce the multimodal puzzle task, which facilitates rich representation learning from multiple image modalities. The learned representations allow for subsequent fine-tuning on different downstream tasks. To achieve that, we learn a modality-agnostic feature embedding by confusing image modalities at the data-level. Together with the Sinkhorn operator, with which we formulate the puzzle solving optimization as permutation matrix inference instead of classification, they allow for efficient solving of multimodal puzzles with varying levels of complexity. In addition, we also propose to utilize cross-modal generation techniques for multimodal data augmentation used for training self-supervised tasks. In other words, we exploit synthetic images for self-supervised pretraining, instead of downstream tasks directly, in order to circumvent quality issues associated with synthetic images, while improving data-efficiency and representations quality. Our experimental results, which assess the gains in downstream performance and data-efficiency, show that solving our multimodal puzzles yields better semantic representations, compared to treating each modality independently. Our results also highlight the benefits of exploiting synthetic images for self-supervised pretraining. We showcase our approach on four downstream tasks: Brain tumor segmentation and survival days prediction using four MRI modalities, Prostate segmentation using two MRI modalities, and Liver segmentation using unregistered CT and MRI modalities. We outperform many previous solutions, and achieve results competitive to state-of-the-art.",0
"The utilization of unlabeled samples to obtain general knowledge about various concepts is the basis of self-supervised learning approaches. This leads to efficient annotation of downstream tasks. This paper introduces a new self-supervised method that employs multiple imaging modalities for rich representation learning. The proposed method involves the use of the multimodal puzzle task to obtain a modality-agnostic feature embedding. This is achieved by confusing image modalities at the data-level and using the Sinkhorn operator to solve the puzzle. The method also utilizes cross-modal generation techniques to augment the multimodal data for self-supervised tasks. The experimental results show that our approach yields better semantic representations and is more data-efficient than treating each modality independently. The method is applied to four downstream tasks, namely, brain tumor segmentation, survival days prediction using four MRI modalities, prostate segmentation using two MRI modalities, and liver segmentation using unregistered CT and MRI modalities. The proposed method outperforms many previous solutions and achieves competitive results.",1
"Current autoencoder-based disentangled representation learning methods achieve disentanglement by penalizing the (aggregate) posterior to encourage statistical independence of the latent factors. This approach introduces a trade-off between disentangled representation learning and reconstruction quality since the model does not have enough capacity to learn correlated latent variables that capture detail information present in most image data. To overcome this trade-off, we present a novel multi-stage modelling approach where the disentangled factors are first learned using a preexisting disentangled representation learning method (such as $\beta$-TCVAE); then, the low-quality reconstruction is improved with another deep generative model that is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangled factors. Taken together, our multi-stage modelling approach results in a single, coherent probabilistic model that is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. We demonstrate that our multi-stage model has much higher reconstruction quality than current state-of-the-art methods with equivalent disentanglement performance across multiple standard benchmarks.",0
"Disentangled representation learning methods currently rely on penalizing the posterior to encourage statistical independence of latent factors. However, this approach comes with a trade-off between disentangled representation learning and reconstruction quality. This is due to the model's inability to learn correlated latent variables that capture detailed information present in image data. To address this trade-off, we propose a novel multi-stage modelling approach. The first stage involves learning disentangled factors using a pre-existing disentangled representation learning method. The second stage utilizes a deep generative model to improve the low-quality reconstruction by modeling the missing correlated latent variables. This maintains conditioning on the previously learned disentangled factors while adding detail information. Our multi-stage modelling approach yields a single, coherent probabilistic model that adheres to the principle of D-separation. It can be realized with various model classes, including likelihood-based models, implicit models, and tractable models. We demonstrate that our approach achieves much higher reconstruction quality than current state-of-the-art methods while maintaining equivalent disentanglement performance across multiple standard benchmarks.",1
"Crowdsourced data used in machine learning services might carry sensitive information about attributes that users do not want to share. Various methods have been proposed to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy. However, little is known about the theory behind these methods. In light of this gap, we develop a novel theoretical framework for attribute obfuscation. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its inference guarantees against worst-case adversaries. Meanwhile, it is clear that in general there is a tension between minimizing information leakage and maximizing task accuracy. To understand this, we prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage. We conduct experiments on two real-world datasets to corroborate the inference guarantees and validate this trade-off. Our results indicate that, among several alternatives, the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization.",0
"The use of crowdsourced data in machine learning services may contain sensitive information that users want to keep private. While methods have been suggested to minimize the potential leakage of sensitive attributes and increase task accuracy, little is understood about the theory behind these methods. To address this issue, we establish a new theoretical framework for attribute obfuscation. Our framework proposes a minimax optimization formulation to safeguard the given attribute and analyze its inference guarantees against worst-case adversaries. However, there is a clear trade-off between minimizing information leakage and maximizing task accuracy. To comprehend this, we demonstrate an information-theoretic lower bound to accurately define the fundamental trade-off between accuracy and information leakage. We test our framework on two real-world datasets to confirm our inference guarantees and validate this trade-off. Our findings reveal that the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization, compared to other alternatives.",1
"Existing Neural Architecture Search (NAS) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly understood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels considerably improve the downstream architecture search efficiency. To explain these observations, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps to map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies.",0
"Current Neural Architecture Search (NAS) methods use either unsuitable discrete encodings or supervise learning-based methods to jointly learn architecture representations and optimize architecture search, leading to search bias and limited understanding of the learned representations. In fact, coupling architecture representation learning and search makes it challenging to preserve the structural properties of neural architectures in the latent space, resulting in less effective search performance. To address this issue, our study demonstrates that pre-training architecture representations using only neural architectures, without their accuracies as labels, can significantly enhance the efficiency of downstream architecture search. We explain this observation by showing how unsupervised architecture representation learning encourages neural architectures with similar connections and operators to cluster together, leading to better mapping of neural architectures with similar performance to the same regions in the latent space. Consequently, diverse downstream search strategies benefit greatly from this smooth transition of architectures in the latent space.",1
"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.",0
"Learning representations from graph-structured data is difficult due to the importance of both node features and graph structure. Graph Neural Networks (GNNs) can combine information from both sources, but are vulnerable to adversarial attacks. To address this issue, we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that balances representation expressiveness and robustness. GIB maximizes mutual information between the representation and the target while constraining mutual information between the representation and input data. Unlike general Information Bottleneck (IB), GIB also regularizes structural information. We propose two sampling algorithms for structural regularization and instantiate GIB with two models: GIB-Cat and GIB-Bern. Our experiments show that our models are more resilient to adversarial attacks than state-of-the-art graph defense models, achieving up to 31% improvement with adversarial perturbations of graph structure and node features.",1
"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.",0
"Reasoning in knowledge graphs can tackle various important issues, and representation learning has proven highly effective in transductive reasoning, which involves predicting new outcomes for already observed entities. This applies to both attributed graphs, where each entity has an initial feature vector, and non-attributed graphs, where the initial information comes from known relations with other entities. While attributed graphs have been widely explored for out-of-sample reasoning, i.e., predicting outcomes for entities not seen during training, non-attributed graphs are surprisingly under-researched. This study focuses on the out-of-sample representation learning problem for non-attributed knowledge graphs, including creating benchmark datasets, developing models and baselines, and conducting empirical analyses and comparisons.",1
"Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance.   We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an AUROC value of $95.8 \pm 1.2$ (mean $\pm$ SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often sub-par performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption.   Code available at https://github.com/ORippler/gaussian-ad-mvtec",0
"Detecting anomalies in images is a significant problem in computer vision, involving identifying images and substructures that deviate from the norm. Commonly, anomaly detection (AD) algorithms attempt to learn a normality model from task-specific data. However, these algorithms are limited to semi-supervised approaches that mostly use normal data due to the difficulty of accessing anomalies on a large scale and the ambiguous nature of their appearance. In contrast, we propose an alternative approach in which deep feature representations learned by discriminative models on large natural image datasets are used to detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using only normal data. By using the Mahalanobis distance as the anomaly score, we achieve better results than the current state of the art on the MVTec AD dataset. We also analyze why the learned representations are discriminative for the AD task using Principal Component Analysis and find that the principal components containing little variance in normal data are crucial for discriminating between normal and anomalous instances. By selectively fitting a MVG to these relevant components only, we reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption. Code is available at https://github.com/ORippler/gaussian-ad-mvtec.",1
"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",0
"Representing molecules for property prediction has traditionally been done through GNNs and chemical fingerprints. However, in NLP, transformers have emerged as the standard for representation learning due to their successful transfer to downstream tasks. The software ecosystem around transformers is also advancing rapidly, with libraries such as HuggingFace and BertViz simplifying training and introspection. In this study, we present ChemBERTa, which is one of the first attempts to systematically evaluate transformers on molecular property prediction tasks. ChemBERTa scales well with pretraining dataset size and shows competitive downstream performance on MoleculeNet, as well as offering useful attention-based visualization modalities. Our findings indicate that transformers show promise for future molecular representation learning and property prediction. To support these efforts, we have released a curated dataset of 77M SMILES from PubChem, which is suitable for large-scale self-supervised pretraining.",1
"Zero-shot classification is a generalization task where no instance from the target classes is seen during training. To allow for test-time transfer, each class is annotated with semantic information, commonly in the form of attributes or text descriptions. While classical zero-shot learning does not explicitly forbid using information from other datasets, the approaches that achieve the best absolute performance on image benchmarks rely on features extracted from encoders pretrained on Imagenet. This approach relies on hyper-optimized Imagenet-relevant parameters from the supervised classification setting, entangling important questions about the suitability of those parameters and how they were learned with more fundamental questions about representation learning and generalization. To remove these distractors, we propose a more challenging setting: Zero-Shot Learning from scratch (ZFS), which explicitly forbids the use of encoders fine-tuned on other datasets. Our analysis on this setting highlights the importance of local information, and compositional representations.",0
"The task of zero-shot classification involves generalizing to unseen target classes without any prior exposure to them during training. Semantic information such as text descriptions or attributes is used to annotate each class and facilitate transfer at test-time. Although traditional zero-shot learning does not prohibit the use of information from other datasets, the best-performing approaches on image benchmarks typically rely on features obtained from encoders that were pretrained on Imagenet. This dependence raises concerns about the suitability and acquisition of the Imagenet-relevant parameters used in supervised classification. To address this issue, we propose a more challenging variant called Zero-Shot Learning from Scratch (ZFS), which explicitly forbids the use of encoders fine-tuned on other datasets. Our study of this setting underscores the significance of local information and compositional representations.",1
"We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the ""free"" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.",0
"Our work introduces VILLA, the first known attempt at implementing large-scale adversarial training for vision-and-language (V+L) representation learning. The training process of VILLA involves two stages: (i) task-agnostic adversarial pre-training, followed by (ii) task-specific adversarial finetuning. Rather than applying adversarial perturbations to image pixels and textual tokens, we propose conducting adversarial training in the embedding space of each modality. To facilitate large-scale training, we use the ""free"" adversarial training strategy, and incorporate KL-divergence-based regularization to encourage greater invariance in the embedding space. By applying VILLA to current leading V+L models, we surpass previous benchmarks on a wide array of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.",1
"We provide new statistical guarantees for transfer learning via representation learning--when transfer is achieved by learning a feature representation shared across different tasks. This enables learning on new tasks using far less data than is required to learn them in isolation. Formally, we consider $t+1$ tasks parameterized by functions of the form $f_j \circ h$ in a general function class $\mathcal{F} \circ \mathcal{H}$, where each $f_j$ is a task-specific function in $\mathcal{F}$ and $h$ is the shared representation in $\mathcal{H}$. Letting $C(\cdot)$ denote the complexity measure of the function class, we show that for diverse training tasks (1) the sample complexity needed to learn the shared representation across the first $t$ training tasks scales as $C(\mathcal{H}) + t C(\mathcal{F})$, despite no explicit access to a signal from the feature representation and (2) with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with $C(\mathcal{F})$. Our results depend upon a new general notion of task diversity--applicable to models with general tasks, features, and losses--as well as a novel chain rule for Gaussian complexities. Finally, we exhibit the utility of our general framework in several models of importance in the literature.",0
"We have developed new statistical guarantees for transfer learning using representation learning. This approach involves learning a shared feature representation across various tasks, which allows for learning on new tasks with less data than would be necessary for learning them separately. Our approach involves $t+1$ tasks that are parameterized by functions of the form $f_j \circ h$, where $f_j$ is a task-specific function and $h$ is the shared representation. We demonstrate that for diverse training tasks, the sample complexity required to learn the shared representation across the first $t$ training tasks scales as $C(\mathcal{H}) + t C(\mathcal{F})$, even with no explicit access to a signal from the feature representation. Additionally, with an accurate estimate of the representation, the sample complexity required to learn a new task scales only with $C(\mathcal{F})$. Our findings depend on a new general notion of task diversity and a novel chain rule for Gaussian complexities. We also show the effectiveness of our approach in various models commonly used in literature.",1
"Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due to their ability to scale to different resolutions. However, despite the success of deep learning-based models applied to rasterized images, the problem of vector graphics representation learning and generation remains largely unexplored. In this work, we propose a novel hierarchical generative network, called DeepSVG, for complex SVG icons generation and interpolation. Our architecture effectively disentangles high-level shapes from the low-level commands that encode the shape itself. The network directly predicts a set of shapes in a non-autoregressive fashion. We introduce the task of complex SVG icons generation by releasing a new large-scale dataset along with an open-source library for SVG manipulation. We demonstrate that our network learns to accurately reconstruct diverse vector graphics, and can serve as a powerful animation tool by performing interpolations and other latent space operations. Our code is available at https://github.com/alexandre01/deepsvg.",0
"Modern 2D interfaces commonly employ Scalable Vector Graphics (SVG) as they can adapt to varying resolutions. However, while deep learning-based models have been successful with rasterized images, the task of learning and generating vector graphics remains mostly unexplored. This study introduces a novel hierarchical generative network, called DeepSVG, for creating and interpolating complex SVG icons. The architecture separates high-level shapes from low-level commands, and predicts sets of shapes directly. A large-scale dataset and open-source library for SVG manipulation are also provided. The network accurately reconstructs various vector graphics and enables powerful animation tools through interpolations and other latent space operations. The code is available at https://github.com/alexandre01/deepsvg.",1
"High-throughput molecular profiling technologies have produced high-dimensional multi-omics data, enabling systematic understanding of living systems at the genome scale. Studying molecular interactions across different data types helps reveal signal transduction mechanisms across different classes of molecules. In this paper, we develop a novel Bayesian representation learning method that infers the relational interactions across multi-omics data types. Our method, Bayesian Relational Learning (BayReL) for multi-omics data integration, takes advantage of a priori known relationships among the same class of molecules, modeled as a graph at each corresponding view, to learn view-specific latent variables as well as a multi-partite graph that encodes the interactions across views. Our experiments on several real-world datasets demonstrate enhanced performance of BayReL in inferring meaningful interactions compared to existing baselines.",0
"The use of advanced molecular profiling technologies has resulted in the creation of complex multi-omics data sets, which have enabled the systematic study of living systems at the genome level. By investigating molecular interactions across different types of data, it is possible to uncover signal transduction mechanisms across various categories of molecules. In this article, we propose a new Bayesian representation learning technique that can determine relational interactions within multi-omics data sets. Our approach, called Bayesian Relational Learning (BayReL), utilizes a known graph structure for the same type of molecules in each corresponding view to learn view-specific latent variables and a multi-partite graph that represents interactions across views. Our experiments on real-world data sets demonstrate that BayReL outperforms existing methods in identifying meaningful interactions.",1
"Unsupervised and self-supervised learning approaches have become a crucial tool to learn representations for downstream prediction tasks. While these approaches are widely used in practice and achieve impressive empirical gains, their theoretical understanding largely lags behind. Towards bridging this gap, we present a unifying perspective where several such approaches can be viewed as imposing a regularization on the representation via a learnable function using unlabeled data. We propose a discriminative theoretical framework for analyzing the sample complexity of these approaches, which generalizes the framework of (Balcan and Blum, 2010) to allow learnable regularization functions. Our sample complexity bounds show that, with carefully chosen hypothesis classes to exploit the structure in the data, these learnable regularization functions can prune the hypothesis space, and help reduce the amount of labeled data needed. We then provide two concrete examples of functional regularization, one using auto-encoders and the other using masked self-supervision, and apply our framework to quantify the reduction in the sample complexity bound of labeled data. We also provide complementary empirical results to support our analysis.",0
"The use of unsupervised and self-supervised learning methods has become essential for developing representations that can be used for predicting outcomes. Despite their practical applications and impressive results, there is little theoretical understanding of these methods. In an attempt to bridge this gap, we present a unified perspective that considers these approaches as imposing a regularization on the representation through a learnable function that uses unlabeled data. We propose a discriminative theoretical framework to analyze the sample complexity of these methods, which extends the Balcan and Blum framework to incorporate learnable regularization functions. Our sample complexity bounds show that these functions can reduce the amount of labeled data needed by pruning the hypothesis space, especially when hypothesis classes are chosen to exploit the data structure. We provide two examples of functional regularization using auto-encoders and masked self-supervision and apply our framework to quantify the reduction in the sample complexity bound of labeled data. We also offer empirical results to support our analysis.",1
"Spatiotemporal representations learned using 3D convolutional neural networks (CNN) are currently used in state-of-the-art approaches for action related tasks. However, 3D-CNN are notorious for being memory and compute resource intensive as compared with more simple 2D-CNN architectures. We propose to hallucinate spatiotemporal representations from a 3D-CNN teacher with a 2D-CNN student. By requiring the 2D-CNN to predict the future and intuit upcoming activity, it is encouraged to gain a deeper understanding of actions and how they evolve. The hallucination task is treated as an auxiliary task, which can be used with any other action related task in a multitask learning setting. Thorough experimental evaluation shows that the hallucination task indeed helps improve performance on action recognition, action quality assessment, and dynamic scene recognition tasks. From a practical standpoint, being able to hallucinate spatiotemporal representations without an actual 3D-CNN can enable deployment in resource-constrained scenarios, such as with limited computing power and/or lower bandwidth. Codebase is available here: https://github.com/ParitoshParmar/HalluciNet.",0
"Currently, state-of-the-art approaches for action-related tasks use spatiotemporal representations learned through 3D convolutional neural networks (CNN). However, these networks are known for their high demand for memory and computing resources compared to simpler 2D-CNN architectures. To address this issue, we suggest the use of a 2D-CNN student to hallucinate spatiotemporal representations from a 3D-CNN teacher. By predicting future events and upcoming activities, the 2D-CNN is encouraged to gain a deeper understanding of how actions evolve. This auxiliary task can be combined with other action-related tasks in a multitask learning setting. Through thorough experimentation, we have observed that hallucination enhances performance in action recognition, action quality assessment, and dynamic scene recognition tasks. This approach is particularly beneficial in scenarios with limited computing power and/or lower bandwidth. The codebase for this method can be accessed at https://github.com/ParitoshParmar/HalluciNet.",1
"A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.",0
"Self-supervised representation learning often involves contrasting pairs of samples that are either similar or dissimilar in meaning. When labels are not accessible, negative points are typically chosen randomly, which means that they could actually have the same label. Our study shows that selecting negative examples from genuinely different labels leads to better results, as seen in a synthetic environment where labels are available. Based on this observation, we introduce a debiased contrastive objective that corrects for the sampling of same-label datapoints even without knowledge of the actual labels. Our proposed objective consistently outperforms the current state-of-the-art in vision, language, and reinforcement learning benchmarks. Furthermore, we establish generalization bounds for the downstream classification task.",1
"Learning interpretable and interpolatable latent representations has been an emerging research direction, allowing researchers to understand and utilize the derived latent space for further applications such as visual synthesis or recognition. While most existing approaches derive an interpolatable latent space and induces smooth transition in image appearance, it is still not clear how to observe desirable representations which would contain semantic information of interest. In this paper, we aim to learn meaningful representations and simultaneously perform semantic-oriented and visually-smooth interpolation. To this end, we propose an angular triplet-neighbor loss (ATNL) that enables learning a latent representation whose distribution matches the semantic information of interest. With the latent space guided by ATNL, we further utilize spherical semantic interpolation for generating semantic warping of images, allowing synthesis of desirable visual data. Experiments on MNIST and CMU Multi-PIE datasets qualitatively and quantitatively verify the effectiveness of our method.",0
"The study of interpretable and interpolatable latent representations has become a popular research focus, as it allows researchers to comprehend and apply the derived latent space in various applications, such as visual synthesis and recognition. Although many existing methods have developed interpolatable latent spaces that result in smooth changes in image appearance, it is still uncertain how to generate desired representations that contain important semantic information. In this research, we propose an approach to learning significant representations while also performing semantic-oriented and visually-smooth interpolation. Our method utilizes the angular triplet-neighbor loss (ATNL) to guide the learning of a latent representation that matches the desired semantic information. Additionally, we use spherical semantic interpolation to produce semantic warping of images, allowing for the synthesis of desired visual data. Our technique is verified through experiments on the MNIST and CMU Multi-PIE datasets that demonstrate its effectiveness both quantitatively and qualitatively.",1
"Value estimation is a critical component of the reinforcement learning (RL) paradigm. The question of how to effectively learn value predictors from data is one of the major problems studied by the RL community, and different approaches exploit structure in the problem domain in different ways. Model learning can make use of the rich transition structure present in sequences of observations, but this approach is usually not sensitive to the reward function. In contrast, model-free methods directly leverage the quantity of interest from the future, but receive a potentially weak scalar signal (an estimate of the return). We develop an approach for representation learning in RL that sits in between these two extremes: we propose to learn what to model in a way that can directly help value prediction. To this end, we determine which features of the future trajectory provide useful information to predict the associated return. This provides tractable prediction targets that are directly relevant for a task, and can thus accelerate learning the value function. The idea can be understood as reasoning, in hindsight, about which aspects of the future observations could help past value prediction. We show how this can help dramatically even in simple policy evaluation settings. We then test our approach at scale in challenging domains, including on 57 Atari 2600 games.",0
"The estimation of value is a crucial element in the reinforcement learning (RL) paradigm. The RL community is preoccupied with discovering effective methods to learn value predictors from data, and various approaches utilize the structure of the problem in different ways. Model learning can use the transition structure in sequences of observations, but it is generally insensitive to the reward function. In contrast, model-free methods directly exploit the future's quantity of interest but receive a potentially weak scalar signal. We propose a representation learning approach that strikes a balance between these two extremes. We suggest learning what to model to aid value prediction directly. To accomplish this, we identify which features of future trajectories provide valuable information to forecast the associated return. This provides achievable prediction targets that are directly relevant to the task and can, therefore, expedite value function learning. The concept is akin to retrospective reasoning about which aspects of future observations could aid past value prediction. We demonstrate how this can be of significant benefit, even in simple policy evaluation settings. We then evaluate our strategy on a vast scale in challenging domains, including 57 Atari 2600 games.",1
"Self-supervised representation learning is an emerging research topic for its powerful capacity in learning with unlabeled data. As a mainstream self-supervised learning method, augmentation-based contrastive learning has achieved great success in various computer vision tasks that lack manual annotations. Despite current progress, the existing methods are often limited by extra cost on memory or storage, and their performance still has large room for improvement. Here we present a self-supervised representation learning method, namely AAG, which is featured by an auxiliary augmentation strategy and GNT-Xent loss. The auxiliary augmentation is able to promote the performance of contrastive learning by increasing the diversity of images. The proposed GNT-Xent loss enables a steady and fast training process and yields competitive accuracy. Experiment results demonstrate the superiority of AAG to previous state-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG achieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5% higher than the best result of SimCLR with batch size 1024.",0
"Learning representations through self-supervision is a promising area of research due to its ability to learn from unlabeled data. Augmentation-based contrastive learning, a widely used self-supervised learning method, has shown impressive results in various computer vision tasks that lack manual annotations. However, current approaches have limitations in terms of memory and storage requirements, and there is still room for further improvement in their performance. In this study, we propose a self-supervised representation learning method called AAG, which employs an auxiliary augmentation strategy and GNT-Xent loss. The auxiliary augmentation enhances contrastive learning by increasing image diversity, while the GNT-Xent loss facilitates a steady and rapid training process, resulting in competitive accuracy. Experiments on CIFAR10, CIFAR100, and SVHN demonstrate that AAG outperforms previous state-of-the-art methods, achieving a top-1 accuracy of 94.5% on CIFAR10 with a batch size of 64, which is 0.5% higher than the best result of SimCLR with a batch size of 1024.",1
"In the computational prediction of chemical compound properties, molecular descriptors and fingerprints encoded to low dimensional vectors are used. The selection of proper molecular descriptors and fingerprints is both important and challenging as the performance of such models is highly dependent on descriptors. To overcome this challenge, natural language processing models that utilize simplified molecular input line-entry system as input were studied, and several transformer-variant models achieved superior results when compared with conventional methods. In this study, we explored the structural differences of the transformer-variant model and proposed a new self-attention based model. The representation learning performance of the self-attention module was evaluated in a multi-task learning environment using imbalanced chemical datasets. The experiment results showed that our model achieved competitive outcomes on several benchmark datasets. The source code of our experiment is available at https://github.com/arwhirang/sa-mtl and the dataset is available from the same URL.",0
"To predict chemical compound properties using computation, low dimensional vectors encoded with molecular descriptors and fingerprints are utilized, but selecting appropriate descriptors and fingerprints is challenging. The performance of such models is highly dependent on these descriptors. To tackle this issue, natural language processing models that take simplified molecular input line-entry system as input were explored, and transformer-variant models were found to perform better than conventional methods. In this study, we investigated the structural differences of transformer-variant models and proposed a new self-attention based model. We evaluated the representation learning performance of the self-attention module in a multi-task learning environment using imbalanced chemical datasets. Our experiment results demonstrate that our model achieved competitive outcomes on several benchmark datasets. The source code for our experiment and the dataset are both available at https://github.com/arwhirang/sa-mtl.",1
"Unsupervised anomaly detection aims to identify anomalous samples from highly complex and unstructured data, which is pervasive in both fundamental research and industrial applications. However, most existing methods neglect the complex correlation among data samples, which is important for capturing normal patterns from which the abnormal ones deviate. In this paper, we propose a method of Correlation aware unsupervised Anomaly detection via Deep Gaussian Mixture Model (CADGMM), which captures the complex correlation among data points for high-quality low-dimensional representation learning. Specifically, the relations among data samples are correlated firstly in forms of a graph structure, in which, the node denotes the sample and the edge denotes the correlation between two samples from the feature space. Then, a dual-encoder that consists of a graph encoder and a feature encoder, is employed to encode both the feature and correlation information of samples into the low-dimensional latent space jointly, followed by a decoder for data reconstruction. Finally, a separate estimation network as a Gaussian Mixture Model is utilized to estimate the density of the learned latent vector, and the anomalies can be detected by measuring the energy of the samples. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method.",0
"The goal of unsupervised anomaly detection is to locate unusual instances within intricate and disorganized data, which is a common occurrence in academic research and industry applications. Nonetheless, many current techniques disregard the intricate relationships between data samples, which are crucial for identifying regular patterns and deviations from them. This study introduces a Correlation-aware Unsupervised Anomaly Detection via Deep Gaussian Mixture Model (CADGMM) approach that addresses this issue by incorporating complex correlations into the data representation process. The method utilizes a graph structure to express the relationships between samples, with nodes representing samples and edges representing correlations between two features. A dual-encoder, made up of a graph encoder and a feature encoder, is then employed to encode both the features and correlations of samples. A decoder is used for data reconstruction, and a separate estimation network is used as a Gaussian Mixture Model to estimate the density of the learned latent vector. The energy of the samples is then used to detect anomalies. The effectiveness of the proposed technique is validated through experiments conducted on real-world datasets.",1
"We propose a novel capsule network based variational encoder architecture, called Bayesian capsules (B-Caps), to modulate the mean and standard deviation of the sampling distribution in the latent space. We hypothesized that this approach can learn a better representation of features in the latent space than traditional approaches. Our hypothesis was tested by using the learned latent variables for image reconstruction task, where for MNIST and Fashion-MNIST datasets, different classes were separated successfully in the latent space using our proposed model. Our experimental results have shown improved reconstruction and classification performances for both datasets adding credence to our hypothesis. We also showed that by increasing the latent space dimension, the proposed B-Caps was able to learn a better representation when compared to the traditional variational auto-encoders (VAE). Hence our results indicate the strength of capsule networks in representation learning which has never been examined under the VAE settings before.",0
"Our study introduces a unique capsule network-based variational encoder architecture named Bayesian capsules (B-Caps) that modulates the mean and standard deviation of the sampling distribution in the latent space. Our approach aims to improve the representation of features in the latent space compared to traditional methods. To test this hypothesis, we utilized the learned latent variables for an image reconstruction task on the MNIST and Fashion-MNIST datasets. Our proposed model successfully separated different classes in the latent space, resulting in improved reconstruction and classification performances for both datasets. Additionally, our study revealed that increasing the latent space dimension allowed B-Caps to learn a better representation compared to traditional variational auto-encoders (VAE). This demonstrates the potential of capsule networks in representation learning, which has not been explored under the VAE framework before.",1
"3D Point clouds are a rich source of information that enjoy growing popularity in the vision community. However, due to the sparsity of their representation, learning models based on large point clouds is still a challenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure Extraction pipeline, a simple yet powerful feature transform and keypoint detector. Graphite enables intensive down-sampling of point clouds with keypoint detection accompanied by a descriptor. We construct a generic graph-based learning scheme to describe point cloud regions and extract salient points. To this end, we take advantage of 6D pose information and metric learning to learn robust descriptions and keypoints across different scans. We Reformulate the 3D keypoint pipeline with graph neural networks which allow efficient processing of the point set while boosting its descriptive power which ultimately results in more accurate 3D registrations. We demonstrate our lightweight descriptor on common 3D descriptor matching and point cloud registration benchmarks and achieve comparable results with the state of the art. Describing 100 patches of a point cloud and detecting their keypoints takes only ~0.018 seconds with our proposed network.",0
"The vision community is increasingly interested in the valuable information contained in 3D point clouds. However, learning models based on these point clouds remains a challenge due to their sparse representation. In this study, we present Graphite, a powerful and straightforward feature transform and keypoint detector that uses a GRAPH-Induced feaTure Extraction pipeline. Graphite can intensively down-sample point clouds while detecting keypoints and generating descriptors. We establish a universal graph-based learning approach to describe point cloud regions and extract significant points, utilizing 6D pose information and metric learning to create robust descriptions and keypoints across various scans. We utilize graph neural networks to reformulate the 3D keypoint pipeline, enabling efficient processing of the point set while improving its descriptive power, ultimately leading to more precise 3D registrations. Our lightweight descriptor achieves comparable results with state-of-the-art methods on common 3D descriptor matching and point cloud registration benchmarks. Our proposed network only takes approximately 0.018 seconds to describe 100 patches of a point cloud and detect their keypoints.",1
"To leverage enormous unlabeled data on distributed edge devices, we formulate a new problem in federated learning called Federated Unsupervised Representation Learning (FURL) to learn a common representation model without supervision while preserving data privacy. FURL poses two new challenges: (1) data distribution shift (Non-IID distribution) among clients would make local models focus on different categories, leading to the inconsistency of representation spaces. (2) without the unified information among clients in FURL, the representations across clients would be misaligned. To address these challenges, we propose Federated Constrastive Averaging with dictionary and alignment (FedCA) algorithm. FedCA is composed of two key modules: (1) dictionary module to aggregate the representations of samples from each client and share with all clients for consistency of representation space and (2) alignment module to align the representation of each client on a base model trained on a public data. We adopt the contrastive loss for local model training. Through extensive experiments with three evaluation protocols in IID and Non-IID settings, we demonstrate that FedCA outperforms all baselines with significant margins.",0
"In order to utilize vast amounts of unlabeled data stored on separate edge devices, we introduce a new concept in federated learning called Federated Unsupervised Representation Learning (FURL). This method allows for the creation of a shared representation model without supervision, while still protecting data privacy. However, FURL presents two challenges; firstly, the distribution of data among clients may differ, causing local models to focus on different categories and leading to inconsistencies in representation spaces. Secondly, without unified information among clients, representations may be misaligned. To overcome these challenges, we propose the Federated Contrastive Averaging with dictionary and alignment (FedCA) algorithm, which contains two key modules: a dictionary module to aggregate sample representations from each client and align them for consistency, and an alignment module to align each client's representation with a base model trained on public data. We employ the contrastive loss for local model training. Our extensive experiments demonstrate that FedCA outperforms all baselines with significant margins across three evaluation protocols in both IID and Non-IID settings.",1
"Learning causal effects from observational data greatly benefits a variety of domains such as health care, education and sociology. For instance, one could estimate the impact of a new drug on specific individuals to assist the clinic plan and improve the survival rate. In this paper, we focus on studying the problem of estimating Conditional Average Treatment Effect (CATE) from observational data. The challenges for this problem are two-fold: on the one hand, we have to derive a causal estimator to estimate the causal quantity from observational data, where there exists confounding bias; on the other hand, we have to deal with the identification of CATE when the distribution of covariates in treatment and control groups are imbalanced. To overcome these challenges, we propose a neural network framework called Adversarial Balancing-based representation learning for Causal Effect Inference (ABCEI), based on the recent advances in representation learning. To ensure the identification of CATE, ABCEI uses adversarial learning to balance the distributions of covariates in treatment and control groups in the latent representation space, without any assumption on the form of the treatment selection/assignment function. In addition, during the representation learning and balancing process, highly predictive information from the original covariate space might be lost. ABCEI can tackle this information loss problem by preserving useful information for predicting causal effects under the regularization of a mutual information estimator. The experimental results show that ABCEI is robust against treatment selection bias, and matches/outperforms the state-of-the-art approaches. Our experiments show promising results on several datasets, representing different health care domains among others.",0
"The use of observational data to learn causal effects has significant benefits for various fields including health care, education, and sociology. For instance, it can help clinics to plan and improve survival rates by estimating the impact of a new drug on specific individuals. This paper focuses on the problem of estimating Conditional Average Treatment Effect (CATE) from observational data, which is challenging due to confounding bias and imbalanced covariate distributions in treatment and control groups. To address these challenges, we propose a neural network framework called Adversarial Balancing-based representation learning for Causal Effect Inference (ABCEI) that utilizes recent advances in representation learning. ABCEI uses adversarial learning to balance covariate distributions in the latent representation space, without assuming the form of the treatment selection/assignment function. Additionally, it preserves useful information for predicting causal effects by regularizing mutual information estimator. Experimental results show that ABCEI is robust against treatment selection bias and outperforms state-of-the-art approaches on several health care datasets.",1
"Causal inference, or counterfactual prediction, is central to decision making in healthcare, policy and social sciences. To de-bias causal estimators with high-dimensional data in observational studies, recent advances suggest the importance of combining machine learning models for both the propensity score and the outcome function. We propose a novel scalable method to learn double-robust representations for counterfactual predictions, leading to consistent causal estimation if the model for either the propensity score or the outcome, but not necessarily both, is correctly specified. Specifically, we use the entropy balancing method to learn the weights that minimize the Jensen-Shannon divergence of the representation between the treated and control groups, based on which we make robust and efficient counterfactual predictions for both individual and average treatment effects. We provide theoretical justifications for the proposed method. The algorithm shows competitive performance with the state-of-the-art on real world and synthetic data.",0
"The prediction of causality, also known as counterfactual prediction, plays a crucial role in decision-making processes in healthcare, policy, and social sciences. Recent developments have highlighted the significance of utilizing machine learning models for both the propensity score and the outcome function to eliminate bias in causal estimators with vast observational data. Our innovative approach proposes a scalable method for obtaining double-robust representations to predict counterfactuals that guarantee consistent causal estimation if the model for either the propensity score or the outcome is accurately specified. We achieve this goal by implementing the entropy balancing method, which allows us to learn the weights that minimize the Jensen-Shannon divergence of the representation between the treated and control groups. This, in turn, enables us to make robust and efficient individual and average treatment effect predictions. The proposed method is theoretically substantiated, and we demonstrate its competitive performance with state-of-the-art techniques using both real-world and synthetic data.",1
"We present $\Gamma$-nets, a method for generalizing value function estimation over timescale. By using the timescale as one of the estimator's inputs we can estimate value for arbitrary timescales. As a result, the prediction target for any timescale is available and we are free to train on multiple timescales at each timestep. Here we empirically evaluate $\Gamma$-nets in the policy evaluation setting. We first demonstrate the approach on a square wave and then on a robot arm using linear function approximation. Next, we consider the deep reinforcement learning setting using several Atari video games. Our results show that $\Gamma$-nets can be effective for predicting arbitrary timescales, with only a small cost in accuracy as compared to learning estimators for fixed timescales. $\Gamma$-nets provide a method for compactly making predictions at many timescales without requiring a priori knowledge of the task, making it a valuable contribution to ongoing work on model-based planning, representation learning, and lifelong learning algorithms.",0
"The method of $\Gamma$-nets is introduced as a means of extending value function estimation across timescales. By including the timescale as an input, we can estimate value for any timescale. This allows us to train on multiple timescales at each timestep and obtain the prediction target for every timescale. We evaluate the effectiveness of $\Gamma$-nets in policy evaluation through empirical analysis. We demonstrate the approach on a square wave and a robot arm using linear function approximation, and on several Atari video games using deep reinforcement learning. Our results show that $\Gamma$-nets are a useful tool for predicting arbitrary timescales with only a slight reduction in accuracy compared to learning estimators for fixed timescales. $\Gamma$-nets are a valuable contribution to the current work on model-based planning, representation learning, and lifelong learning algorithms, as they enable the making of predictions at various timescales without prior knowledge of the task.",1
"We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation $r$ of the training data, and then fitting a simple (e.g., linear) classifier $g$ to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if $\mathsf{C}(g) \ll n$, where $\mathsf{C}(g)$ is an appropriately-defined measure of the simple classifier $g$'s complexity, and $n$ is the number of training samples. We stress that our bound is independent of the complexity of the representation $r$. We do not make any structural or conditional-independence assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures. We show that our bound is non-vacuous for many popular representation-learning based classifiers on CIFAR-10 and ImageNet, including SimCLR, AMDIM and MoCo.",0
"Our study presents a novel limit on the generalization gap of classifiers derived from self-supervised learning, where a simple classifier is used to fit labels to a representation learned from training data. Our findings demonstrate that the generalization gap tends to zero if the measure of complexity of the simple classifier, denoted as $\mathsf{C}(g)$, is significantly lower than the number of training samples, $n$. Notably, our limit is independent of the complexity of the learned representation. We do not rely on any structural or conditional-independence assumptions in the representation learning process, which can utilize the same dataset for classification later. Instead, we assume natural noise-robustness and rationality conditions in the training procedure, which are widely applicable to standard architectures and ensure small performance degradation with label noise and the absence of labels. Our results show that our limit remains valid for popular representation-based classifiers such as SimCLR, AMDIM, and MoCo on CIFAR-10 and ImageNet.",1
"Knowledge graph (KG) representation learning methods have achieved competitive performance in many KG-oriented tasks, among which the best ones are usually based on graph neural networks (GNNs), a powerful family of networks that learns the representation of an entity by aggregating the features of its neighbors and itself. However, many KG representation learning scenarios only provide the structure information that describes the relationships among entities, causing that entities have no input features. In this case, existing aggregation mechanisms are incapable of inducing embeddings of unseen entities as these entities have no pre-defined features for aggregation. In this paper, we present a decentralized KG representation learning approach, decentRL, which encodes each entity from and only from the embeddings of its neighbors. For optimization, we design an algorithm to distill knowledge from the model itself such that the output embeddings can continuously gain knowledge from the corresponding original embeddings. Extensive experiments show that the proposed approach performed better than many cutting-edge models on the entity alignment task, and achieved competitive performance on the entity prediction task. Furthermore, under the inductive setting, it significantly outperformed all baselines on both tasks.",0
"Numerous KG representation learning methods have excelled in KG-based tasks, with the most effective ones being rooted in graph neural networks (GNNs). GNNs learn an entity's representation by aggregating its neighbors' features and its own. However, certain KG representation learning scenarios only provide structural information, leaving entities with no input features. Therefore, existing aggregation mechanisms cannot produce embeddings for unseen entities without predefined features for aggregation. In this paper, we propose a decentralized KG representation learning method called decentRL, which encodes entities solely based on their neighbors' embeddings. We also create an algorithm that extracts knowledge from the model to allow the output embeddings to continuously gain knowledge from the original embeddings. Our experiments reveal that our approach outperforms several advanced models on entity alignment and achieves comparable results on entity prediction. Moreover, in the inductive setting, it outperforms all baselines on both tasks.",1
"Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on $51$ out of $57$ games.",0
"To reduce the cost of supervised signal, self-supervised learning has been adopted to pretrain representations using solely unlabeled data. These methods utilize heuristic proxy classification tasks and data augmentations to achieve considerable success, but our comprehension of this success is limited. This study aims to analyze self-supervised representation learning with a causal framework and demonstrates how data augmentations can be utilized more effectively through explicit invariance constraints on the proxy classifiers employed during pretraining. This leads to the introduction of a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), which enforces invariant prediction of proxy targets across augmentations through an invariance regularizer, resulting in improved generalization guarantees. Additionally, the study generalizes contrastive learning, a specific self-supervised method, using causality and provides an alternative theoretical explanation for the success of these methods. Empirically, ReLIC surpasses other methods in terms of robustness and out-of-distribution generalization on ImageNet, and even achieves above human-level performance on 51 out of 57 games on Atari.",1
"Graph convolution network (GCN) attracts intensive research interest with broad applications. While existing work mainly focused on designing novel GCN architectures for better performance, few of them studied a practical yet challenging problem: How to learn GCNs from data with extremely limited annotation? In this paper, we propose a new learning method by sampling strategy and model compression to overcome this challenge. Our approach has multifold advantages: 1) the adaptive sampling strategy largely suppresses the GCN training deviation over uniform sampling; 2) compressed GCN-based methods with a smaller scale of parameters need fewer labeled data to train; 3) the smaller scale of training data is beneficial to reduce the human resource cost to label them. We choose six popular GCN baselines and conduct extensive experiments on three real-world datasets. The results show that by applying our method, all GCN baselines cut down the annotation requirement by as much as 90$\%$ and compress the scale of parameters more than 6$\times$ without sacrificing their strong performance. It verifies that the training method could extend the existing semi-supervised GCN-based methods to the scenarios with the extremely small scale of labeled data.",0
"The Graph Convolution Network (GCN) has gained significant attention in research due to its wide range of applications. While previous studies have mainly focused on developing new GCN architectures to enhance performance, few have addressed the challenge of learning GCNs from data with limited annotation. This paper presents a novel learning approach that utilizes sampling strategy and model compression to overcome this challenge. The proposed method has several benefits, including the suppression of GCN training deviation through adaptive sampling, the reduction of labeled data requirements through compressed GCN-based methods, and a decrease in the cost of labeling training data. The study examines six popular GCN baselines and conducts extensive experiments on three real-world datasets. The results demonstrate that the proposed method can reduce annotation requirements by up to 90% and parameter scale by over 6x without sacrificing performance. This approach extends the existing semi-supervised GCN-based methods to small scale labeled data scenarios.",1
"We consider the problem of unsupervised domain adaptation for image classification. To learn target-domain-aware features from the unlabeled data, we create a self-supervised pretext task by augmenting the unlabeled data with a certain type of transformation (specifically, image rotation) and ask the learner to predict the properties of the transformation. However, the obtained feature representation may contain a large amount of irrelevant information with respect to the main task. To provide further guidance, we force the feature representation of the augmented data to be consistent with that of the original data. Intuitively, the consistency introduces additional constraints to representation learning, therefore, the learned representation is more likely to focus on the right information about the main task. Our experimental results validate the proposed method and demonstrate state-of-the-art performance on classical domain adaptation benchmarks. Code is available at https://github.com/Jiaolong/ss-da-consistency.",0
"This passage discusses the issue of unsupervised domain adaptation for image classification and proposes a solution. To generate target-domain-aware features from unlabeled data, a self-supervised pretext task is created using image rotation. However, the resulting feature representation may contain irrelevant information. To address this, the feature representation of augmented data is aligned with that of the original data, which enhances representation learning and improves the focus on relevant information for the main task. The effectiveness of this method is confirmed through experimental results, and the code is available at https://github.com/Jiaolong/ss-da-consistency.",1
"Since its inception, the neural estimation of mutual information (MI) has demonstrated the empirical success of modeling expected dependency between high-dimensional random variables. However, MI is an aggregate statistic and cannot be used to measure point-wise dependency between different events. In this work, instead of estimating the expected dependency, we focus on estimating point-wise dependency (PD), which quantitatively measures how likely two outcomes co-occur. We show that we can naturally obtain PD when we are optimizing MI neural variational bounds. However, optimizing these bounds is challenging due to its large variance in practice. To address this issue, we develop two methods (free of optimizing MI variational bounds): Probabilistic Classifier and Density-Ratio Fitting. We demonstrate the effectiveness of our approaches in 1) MI estimation, 2) self-supervised representation learning, and 3) cross-modal retrieval task.",0
"The estimation of mutual information (MI) using neural networks has proven to be effective in modeling expected dependency between high-dimensional random variables. However, while MI is useful in measuring aggregate statistics, it cannot measure point-wise dependency between different events. This study focuses on estimating point-wise dependency (PD), which measures the likelihood of two outcomes co-occurring. PD can be obtained naturally while optimizing MI neural variational bounds, but this proves challenging due to high variance. To address this, two methods are developed - Probabilistic Classifier and Density-Ratio Fitting - that do not require optimizing MI variational bounds. These approaches are shown to be effective in MI estimation, self-supervised representation learning, and cross-modal retrieval task.",1
"Improving sample efficiency is a key research problem in reinforcement learning (RL), and CURL, which uses contrastive learning to extract high-level features from raw pixels of individual video frames, is an efficient algorithm~\citep{srinivas2020curl}. We observe that consecutive video frames in a game are highly correlated but CURL deals with them independently. To further improve data efficiency, we propose a new algorithm, masked contrastive representation learning for RL, that takes the correlation among consecutive inputs into consideration. In addition to the CNN encoder and the policy network in CURL, our method introduces an auxiliary Transformer module to leverage the correlations among video frames. During training, we randomly mask the features of several frames, and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning where the reconstructed features should be similar to the ground-truth ones while dissimilar to others. During inference, the CNN encoder and the policy network are used to take actions, and the Transformer module is discarded. Our method achieves consistent improvements over CURL on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code is available at https://github.com/teslacool/m-curl.",0
"Reinforcement learning (RL) poses a challenge in terms of sample efficiency. CURL is an efficient algorithm that uses contrastive learning to extract high-level features from raw pixels of individual video frames, addressing this problem. However, we have observed that consecutive video frames in a game are highly correlated, which CURL does not take into account. To enhance data efficiency, we propose a novel approach called masked contrastive representation learning for RL. This approach considers the correlation among consecutive inputs by introducing an auxiliary Transformer module in addition to the CNN encoder and policy network used in CURL. During training, we randomly mask the features of several frames and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning, ensuring that the reconstructed features are similar to the ground-truth ones while dissimilar to others. During inference, the CNN encoder and policy network are employed to take actions, and the Transformer module is discarded. Our method outperforms CURL consistently, achieving improvements on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code for our method is available at https://github.com/teslacool/m-curl.",1
"An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",0
"Limiting the information capacity of the latent representation is a crucial aspect of autoencoders. In this study, the rank of the codes' covariance matrix is minimized implicitly by exploiting the fact that gradient descent learning in multi-layer linear networks results in solutions with minimum rank. By adding extra linear layers between the encoder and decoder, the system learns low-dimensional representations spontaneously. The IRMAE model, which is uncomplicated, deterministic, and produces condensed latent spaces, is introduced. The effectiveness of this approach is demonstrated through various image generation and representation learning assignments.",1
"In this paper, we introduce InstantEmbedding, an efficient method for generating single-node representations using local PageRank computations. We theoretically prove that our approach produces globally consistent representations in sublinear time. We demonstrate this empirically by conducting extensive experiments on real-world datasets with over a billion edges. Our experiments confirm that InstantEmbedding requires drastically less computation time (over 9,000 times faster) and less memory (by over 8,000 times) to produce a single node's embedding than traditional methods including DeepWalk, node2vec, VERSE, and FastRP. We also show that our method produces high quality representations, demonstrating results that meet or exceed the state of the art for unsupervised representation learning on tasks like node classification and link prediction.",0
"InstantEmbedding is a novel approach we present in this paper, which generates single-node representations through local PageRank computations. Our theoretical analysis confirms that our method produces globally consistent representations in sublinear time. We further validate our approach through extensive empirical experiments on real-world datasets with billions of edges. Our results demonstrate that InstantEmbedding requires significantly less computation time (over 9,000 times faster) and memory (over 8,000 times smaller) compared to traditional methods such as DeepWalk, node2vec, VERSE, and FastRP. Moreover, our method produces high-quality representations that surpass the state of the art for unsupervised representation learning in tasks such as node classification and link prediction.",1
"In this paper, we address self-supervised representation learning from human skeletons for action recognition. Previous methods, which usually learn feature presentations from a single reconstruction task, may come across the overfitting problem, and the features are not generalizable for action recognition. Instead, we propose to integrate multiple tasks to learn more general representations in a self-supervised manner. To realize this goal, we integrate motion prediction, jigsaw puzzle recognition, and contrastive learning to learn skeleton features from different aspects. Skeleton dynamics can be modeled through motion prediction by predicting the future sequence. And temporal patterns, which are critical for action recognition, are learned through solving jigsaw puzzles. We further regularize the feature space by contrastive learning. Besides, we explore different training strategies to utilize the knowledge from self-supervised tasks for action recognition. We evaluate our multi-task self-supervised learning approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Our experiments on the NW-UCLA, NTU RGB+D, and PKUMMD datasets show remarkable performance for action recognition, demonstrating the superiority of our method in learning more discriminative and general features. Our project website is available at https://langlandslin.github.io/projects/MSL/.",0
"The aim of this paper is to discuss the issue of self-supervised representation learning from human skeletons for action recognition. The conventional method of learning feature presentations from a single reconstruction task is not ideal as it may result in overfitting and features that are not generalizable for action recognition. To overcome this, we propose incorporating multiple tasks to learn more generalized representations in a self-supervised manner. This involves combining motion prediction, jigsaw puzzle recognition, and contrastive learning to learn skeleton features from different perspectives. Motion prediction is used to model skeleton dynamics by predicting future sequences, while jigsaw puzzles help to learn temporal patterns crucial for action recognition. Contrastive learning is also used to regularize the feature space. Furthermore, we explore various training strategies to use self-supervised task knowledge for action recognition. We evaluate our multi-task self-supervised learning approach by training action classifiers in unsupervised, semi-supervised, and fully-supervised settings. Our experiments on the NW-UCLA, NTU RGB+D, and PKUMMD datasets demonstrate the superiority of our method in learning more discriminative and general features, resulting in remarkable performance for action recognition. Our project website can be accessed at https://langlandslin.github.io/projects/MSL/.",1
"Representation Learning in a heterogeneous space with mixed variables of numerical and categorical types has interesting challenges due to its complex feature manifold. Moreover, feature learning in an unsupervised setup, without class labels and a suitable learning loss function, adds to the problem complexity. Further, the learned representation and subsequent predictions should not reflect discriminatory behavior towards certain sensitive groups or attributes. The proposed feature map should preserve maximum variations present in the data and needs to be fair with respect to the sensitive variables. We propose, in the first phase of our work, an efficient encoder-decoder framework to capture the mixed-domain information. The second phase of our work focuses on de-biasing the mixed space representations by adding relevant fairness constraints. This ensures minimal information loss between the representations before and after the fairness-preserving projections. Both the information content and the fairness aspect of the final representation learned has been validated through several metrics where it shows excellent performance. Our work (FairMixRep) addresses the problem of Mixed Space Fair Representation learning from an unsupervised perspective and learns a Universal representation that is timely, unique, and a novel research contribution.",0
"Learning representations in a heterogeneous space that contains both numerical and categorical variables poses interesting challenges due to the complexity of the feature manifold. The problem becomes even more complex in an unsupervised setup where there are no class labels or a suitable learning loss function. Additionally, the learned representation and predictions should not exhibit discriminatory behavior towards sensitive groups or attributes. To address these challenges, it is necessary to develop a feature map that preserves maximum variations in the data and is fair with respect to sensitive variables. In the first phase of our work, we propose an encoder-decoder framework to efficiently capture mixed-domain information. The second phase focuses on de-biasing the mixed space representations by incorporating relevant fairness constraints. We validate our approach, FairMixRep, using several metrics, demonstrating excellent performance in both the information content and fairness aspects of the final representation. Our work provides a timely, unique, and novel research contribution to the problem of Mixed Space Fair Representation learning from an unsupervised perspective.",1
"In this paper, we study network representation learning for tripartite heterogeneous networks which learns node representation features for networks with three types of node entities. We argue that tripartite networks are common in real world applications, and the essential challenge of the representation learning is the heterogeneous relations between various node types and links in the network. To tackle the challenge, we develop a tripartite heterogeneous network embedding called TriNE. The method considers unique user-item-tag tripartite relationships, to build an objective function to model explicit relationships between nodes (observed links), and also capture implicit relationships between tripartite nodes (unobserved links across tripartite node sets). The method organizes metapath guided random walks to create heterogeneous neighborhood for all node types in the network. This information is then utilized to train a heterogeneous skip-gram model based on a joint optimization. Experiments on real-world tripartite networks validate the performance of TriNE for the online user response prediction using embedding node features.",0
"The focus of this research is on network representation learning for tripartite heterogeneous networks, whereby node representation features are learned for networks that consist of three types of node entities. The prevalence of tripartite networks in real-world applications is highlighted, and the main challenge of representation learning is identified as the heterogeneous relations between different node types and links in the network. To address this challenge, a tripartite heterogeneous network embedding approach called TriNE is developed. TriNE considers unique user-item-tag tripartite relationships to build an objective function that models both explicit relationships between observed links and implicit relationships between unobserved links across tripartite node sets. The approach uses metapath guided random walks to create heterogeneous neighborhoods for all node types in the network, and this information is utilized to train a heterogeneous skip-gram model through joint optimization. Experiments on real-world tripartite networks confirm the effectiveness of TriNE in predicting online user response using embedding node features.",1
"Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.",0
"The task of Group re-identification (G-ReID) is significant but has received less attention. It presents unique challenges not only in terms of individual appearance changes, which have been extensively studied in general person re-identification (ReID), but also in relation to changes in group layout and membership. Therefore, the primary objective of G-ReID is to develop robust representations that can handle such changes. To tackle this problem, we propose the Transferred Single and Couple Representation Learning Network (TSCN). Our approach has two main advantages: Firstly, since there is a shortage of labeled training samples for G-ReID, current methods primarily rely on inadequate hand-crafted features. To overcome this issue, we consider a group as multiple individuals and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style, enabling us to learn single representations using deep learning models. Secondly, we introduce a novel couple representation learning technique that considers the neighborhood relationship within a group, resulting in more discriminative power for G-ReID tasks. Additionally, we utilize an unsupervised weight learning method to adaptively fuse the results of different views based on result patterns. Our extensive experiments demonstrate that our approach surpasses state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and 39.0\% CMC-1 on the DukeMCMT dataset.",1
"Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank nature of multiway data and has demonstrated great potential in a variety of important applications. In this paper, nonnegative tensor ring (NTR) decomposition and graph regularized NTR (GNTR) decomposition are proposed, where the former equips TR decomposition with local feature extraction by imposing nonnegativity on the core tensors and the latter is additionally able to capture manifold geometry information of tensor data, both significantly extend the applications of TR decomposition for nonnegative multiway representation learning. Accelerated proximal gradient based methods are derived for NTR and GNTR. The experimental result demonstrate that the proposed algorithms can extract parts-based basis with rich colors and rich lines from tensor objects that provide more interpretable and meaningful representation, and hence yield better performance than the state-of-the-art tensor based methods in clustering and classification tasks.",0
"The Tensor Ring (TR) decomposition has proven to be a valuable tool in uncovering the low-rank nature of multiway data, and has been successfully applied in various important fields. This paper introduces two new methods: Nonnegative Tensor Ring (NTR) decomposition, which enhances TR decomposition by incorporating local feature extraction through the imposition of nonnegativity on core tensors, and Graph Regularized NTR (GNTR) decomposition, which additionally captures manifold geometry information of tensor data. These methods significantly broaden the scope of TR decomposition for nonnegative multiway representation learning. Accelerated proximal gradient-based techniques are used for both NTR and GNTR. Results from experiments indicate that these algorithms can extract basis parts with rich colors and lines from tensor objects, thus providing more meaningful and interpretable representations. As a result, they outperform the state-of-the-art tensor-based methods in clustering and classification tasks.",1
"Over the past decade, multivariate time series classification (MTSC) has received great attention with the advance of sensing techniques. Current deep learning methods for MTSC are based on convolutional and recurrent neural network, with the assumption that time series variables have the same effect to each other. Thus they cannot model the pairwise dependencies among variables explicitly. What's more, current spatial-temporal modeling methods based on GNNs are inherently flat and lack the capability of aggregating node information in a hierarchical manner. To address this limitation and attain expressive global representation of MTS, we propose a graph pooling based framework MTPool and view MTSC task as graph classification task. With graph structure learning and temporal convolution, MTS slices are converted to graphs and spatial-temporal features are extracted. Then, we propose a novel graph pooling method, which uses an ``encoder-decoder'' mechanism to generate adaptive centroids for cluster assignments. GNNs and graph pooling layers are used for joint graph representation learning and graph coarsening. With multiple graph pooling layers, the input graphs are hierachically coarsened to one node. Finally, differentiable classifier takes this coarsened one-node graph as input to get the final predicted class. Experiments on 10 benchmark datasets demonstrate MTPool outperforms state-of-the-art methods in MTSC tasks.",0
"In recent years, there has been significant interest in multivariate time series classification (MTSC) due to advancements in sensing techniques. However, current deep learning approaches for MTSC, which rely on convolutional and recurrent neural networks, assume that time series variables have equal impact on each other, and therefore cannot effectively model pairwise dependencies. In addition, current spatial-temporal modeling methods based on graph neural networks lack the ability to aggregate node information hierarchically. To overcome these limitations and achieve more expressive global representations of MTS, we propose MTPool, a graph pooling-based framework that views MTSC as a graph classification task. MTS slices are converted into graphs and spatial-temporal features are extracted using graph structure learning and temporal convolution. Our novel graph pooling method uses an ""encoder-decoder"" mechanism to generate adaptive centroids for cluster assignments, and GNNs and graph pooling layers are used for joint graph representation learning and graph coarsening. Multiple graph pooling layers are employed to hierarchically coarsen the input graphs to a single node, which is then input to a differentiable classifier for final prediction. Experiments on 10 benchmark datasets demonstrate that MTPool outperforms existing state-of-the-art methods in MTSC tasks.",1
"Graph Neural Network (GNN) aggregates the neighborhood of each node into the node embedding and shows its powerful capability for graph representation learning. However, most existing GNN variants aggregate the neighborhood information in a fixed non-injective fashion, which may map different graphs or nodes to the same embedding, reducing the model expressiveness. We present a theoretical framework to design a continuous injective set function for neighborhood aggregation in GNN. Using the framework, we propose expressive GNN that aggregates the neighborhood of each node with a continuous injective set function, so that a GNN layer maps similar nodes with similar neighborhoods to similar embeddings, different nodes to different embeddings and the equivalent nodes or isomorphic graphs to the same embeddings. Moreover, the proposed expressive GNN can naturally learn expressive representations for graphs with continuous node attributes. We validate the proposed expressive GNN (ExpGNN) for graph classification on multiple benchmark datasets including simple graphs and attributed graphs. The experimental results demonstrate that our model achieves state-of-the-art performances on most of the benchmarks.",0
"The Graph Neural Network (GNN) is a powerful tool for learning graph representations by combining the neighborhood of each node into its embedding. However, most current GNN variations use a fixed non-injective method to aggregate neighborhood data, which can cause different graphs or nodes to be mapped to the same embedding, reducing the model's effectiveness. To combat this issue, we have developed a theoretical framework for designing a continuous injective set function for neighborhood aggregation in GNN. Our proposed expressive GNN uses this framework to aggregate neighborhood information with a continuous injective set function, allowing GNN layers to map similar nodes with similar neighborhoods to similar embeddings, different nodes to different embeddings, and equivalent nodes or isomorphic graphs to the same embeddings. The proposed expressive GNN can also naturally learn expressive representations for graphs with continuous node attributes. We have tested the model for graph classification on multiple benchmark datasets, including simple and attributed graphs. Our experimental results demonstrate that our model achieves state-of-the-art performance on most of the benchmarks.",1
"Graph convolutional networks (GCNs) have been widely used for representation learning on graph data, which can capture structural patterns on a graph via specifically designed convolution and readout operations. In many graph classification applications, GCN-based approaches have outperformed traditional methods. However, most of the existing GCNs are inefficient to preserve local information of graphs -- a limitation that is especially problematic for graph classification. In this work, we propose a locality-preserving dense GCN with graph context-aware node representations. Specifically, our proposed model incorporates a local node feature reconstruction module to preserve initial node features into node representations, which is realized via a simple but effective encoder-decoder mechanism. To capture local structural patterns in neighbourhoods representing different ranges of locality, dense connectivity is introduced to connect each convolutional layer and its corresponding readout with all previous convolutional layers. To enhance node representativeness, the output of each convolutional layer is concatenated with the output of the previous layer's readout to form a global context-aware node representation. In addition, a self-attention module is introduced to aggregate layer-wise representations to form the final representation. Experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods in terms of classification accuracy.",0
"Representation learning on graph data using Graph Convolutional Networks (GCNs) is a popular technique that captures structural patterns on a graph through specially designed convolution and readout operations. GCN-based approaches have proven more successful than traditional methods in graph classification applications. However, most of the existing GCNs fail to preserve local information. This limitation poses a challenge for graph classification. In this study, we present a new model, called a Locality-Preserving Dense GCN, with graph context-aware node representations. Our model includes a local node feature reconstruction module that preserves initial node features while using a simple but effective encoder-decoder mechanism. To capture local structural patterns in neighbourhoods representing different ranges of locality, dense connectivity is introduced. It connects each convolutional layer and its corresponding readout with all previous convolutional layers. To enhance node representativeness, the output of each convolutional layer is concatenated with the output of the previous layer's readout to form a global context-aware node representation. Additionally, a self-attention module is introduced to aggregate layer-wise representations to form the final representation. The proposed model outperformed state-of-the-art methods in terms of classification accuracy in experiments conducted on benchmark datasets.",1
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",0
"In various research fields, graph neural networks have recently gained considerable attention and demonstrated remarkable performance. However, most of these algorithms have assumed pairwise relationships between objects of interest, which is not always the case in real-world applications. In situations where relationships extend beyond pairwise formulations, deep embeddings on high-order graph-structured data can be efficiently learned by introducing two end-to-end trainable operators to the family of graph neural networks: hypergraph convolution and hypergraph attention. Hypergraph convolution performs convolution on a hypergraph, while hypergraph attention leverages an attention module to enhance representation learning. With these two operators, a graph neural network becomes a more flexible model that can be applied to diverse applications where non-pairwise relationships are observed. Experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",1
"An unsolved fundamental problem in biology and ecology is to predict observable traits (phenotypes) from a new genetic constitution (genotype) of an organism under environmental perturbations (e.g., drug treatment). The emergence of multiple omics data provides new opportunities but imposes great challenges in the predictive modeling of genotype-phenotype associations. Firstly, the high-dimensionality of genomics data and the lack of labeled data often make the existing supervised learning techniques less successful. Secondly, it is a challenging task to integrate heterogeneous omics data from different resources. Finally, the information transmission from DNA to phenotype involves multiple intermediate levels of RNA, protein, metabolite, etc. The higher-level features (e.g., gene expression) usually have stronger discriminative power than the lower level features (e.g., somatic mutation). To address above issues, we proposed a novel Cross-LEvel Information Transmission network (CLEIT) framework. CLEIT aims to explicitly model the asymmetrical multi-level organization of the biological system. Inspired by domain adaptation, CLEIT first learns the latent representation of high-level domain then uses it as ground-truth embedding to improve the representation learning of the low-level domain in the form of contrastive loss. In addition, we adopt a pre-training-fine-tuning approach to leveraging the unlabeled heterogeneous omics data to improve the generalizability of CLEIT. We demonstrate the effectiveness and performance boost of CLEIT in predicting anti-cancer drug sensitivity from somatic mutations via the assistance of gene expressions when compared with state-of-the-art methods.",0
"Predicting observable traits of an organism from its genetic constitution under environmental changes is a challenge in biology and ecology. The emergence of omics data offers new opportunities but also presents challenges in predictive modeling of genotype-phenotype associations. The high-dimensionality of genomics data and lack of labeled data make existing supervised learning techniques less successful, while integrating heterogeneous omics data from different resources is difficult. Furthermore, information transmission from DNA to phenotype involves multiple intermediate levels, with higher-level features having stronger discriminative power than lower-level features. To address these issues, we propose the Cross-LEvel Information Transmission (CLEIT) network framework, which explicitly models the multi-level organization of the biological system. CLEIT uses a pre-training-fine-tuning approach to leverage unlabeled omics data and improve generalizability. We demonstrate the effectiveness of CLEIT in predicting anti-cancer drug sensitivity from somatic mutations via gene expressions when compared to state-of-the-art methods.",1
"Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives.",0
"Machine learning commonly employs graphs as a means of structured data representation, yet graphs only account for pairwise relations between nodes and do not sufficiently capture the higher-order relations present in many real-world datasets. Hypergraphs, on the other hand, offer a natural representation for such complex relations. However, learning node representations in a hypergraph is more intricate than in a graph as it involves information propagation at both the hyperedge and hypergraph levels. Current approaches often convert the hypergraph structure to a graph for use in existing geometric deep learning algorithms, resulting in information loss and sub-optimal utilization of the hypergraph's expressive power. Our novel HyperSAGE hypergraph learning framework employs a two-level neural message passing strategy to propagate information accurately and efficiently through hypergraphs. Its flexible design enables various ways of aggregating neighborhood information. Unlike transductive approaches, HyperSAGE is inductive, inspired by the GraphSAGE method, enabling deployment in evolving or partially observed hypergraphs. Extensive experimentation shows that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets and is more stable in learning node representations due to its higher expressive power.",1
"Meta-learning approaches have shown great success in vision and language domains. However, few studies discuss the practice of meta-learning for large-scale industrial applications. Although e-commerce companies have spent many efforts on learning representations to provide a better user experience, we argue that such efforts cannot be stopped at this step. In addition to learning a strong profile, the challenging question about how to effectively transfer the learned representation is raised simultaneously. This paper introduces the contributions that we made to address these challenges from three aspects. 1) Meta-learning model: In the context of representation learning with e-commerce user behavior data, we propose a meta-learning framework called the Meta-Profile Network, which extends the ideas of matching network and relation network for knowledge transfer and fast adaptation; 2) Encoding strategy: To keep high fidelity of large-scale long-term sequential behavior data, we propose a time-heatmap encoding strategy that allows the model to encode data effectively; 3) Deep network architecture: A multi-modal model combined with multi-task learning architecture is utilized to address the cross-domain knowledge learning and insufficient label problems. Moreover, we argue that an industrial model should not only have good performance in terms of accuracy, but also have better robustness and uncertainty performance under extreme conditions. We evaluate the performance of our model with extensive control experiments in various extreme scenarios, i.e. out-of-distribution detection, data insufficiency and class imbalance scenarios. The Meta-Profile Network shows significant improvement in the model performance when compared to baseline models.",0
"While meta-learning approaches have demonstrated success in the domains of vision and language, there is a lack of research on their application to large-scale industrial settings. Despite e-commerce companies' efforts to improve user experience by learning representations, it is important to consider how to transfer these representations effectively. This paper addresses these challenges through three contributions: 1) a meta-learning model called the Meta-Profile Network that utilizes matching and relation networks; 2) a time-heatmap encoding strategy to maintain the fidelity of long-term sequential behavior data; and 3) a multi-modal model utilizing multi-task learning architecture to overcome cross-domain knowledge learning and insufficient labeling challenges. The authors emphasize that industrial models must exhibit not only high accuracy but also robustness and uncertainty performance in extreme conditions. The Meta-Profile Network outperforms baseline models in various extreme scenarios, as demonstrated through extensive control experiments.",1
"Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo and project website.",0
"Autonomous driving systems currently consist of a perception system and a decision system, which are both made up of various subsystems that rely heavily on human heuristics. To avoid extensive human engineering and improve performance with increasing data and computation resources, an end-to-end approach may be more effective. The perception system is particularly suited for an end-to-end framework since it does not require online driving exploration. This paper presents a novel end-to-end approach for autonomous driving perception, which utilizes a latent space to capture relevant features for perception through sequential latent representation learning. The proposed method can solve detection, tracking, localization, and mapping problems with minimal human engineering and no online map storage. The approach is evaluated in a realistic urban driving simulator using both camera images and lidar point clouds as sensor inputs. The project website and github repository contain codes and videos of the work.",1
"Disentangled representation learning has undoubtedly benefited from objective function surgery. However, a delicate balancing act of tuning is still required in order to trade off reconstruction fidelity versus disentanglement. Building on previous successes of penalizing the total correlation in the latent variables, we propose TCWAE (Total Correlation Wasserstein Autoencoder). Working in the WAE paradigm naturally enables the separation of the total-correlation term, thus providing disentanglement control over the learned representation, while offering more flexibility in the choice of reconstruction cost. We propose two variants using different KL estimators and perform extensive quantitative comparisons on data sets with known generative factors, showing competitive results relative to state-of-the-art techniques. We further study the trade off between disentanglement and reconstruction on more-difficult data sets with unknown generative factors, where the flexibility of the WAE paradigm in the reconstruction term improves reconstructions.",0
"Objective function surgery has undoubtedly been beneficial for disentangled representation learning. However, achieving a balance between reconstruction accuracy and disentanglement still requires careful tuning. Our proposed Total Correlation Wasserstein Autoencoder (TCWAE) builds on the success of penalizing the total correlation in latent variables. Working within the WAE framework allows for the separation of the total-correlation term, providing better control over disentanglement while allowing for greater flexibility in reconstruction cost selection. We offer two variants that use different KL estimators and conduct extensive quantitative comparisons on datasets with known generative factors, demonstrating competitive results compared to state-of-the-art techniques. We also investigate the trade-off between disentanglement and reconstruction on more challenging datasets with unknown generative factors, where the WAE framework's reconstruction term flexibility improves reconstructions.",1
"Person re-identification (Re-ID) aims to match a target person across camera views at different locations and times. Existing Re-ID studies focus on the short-term cloth-consistent setting, under which a person re-appears in different camera views with the same outfit. A discriminative feature representation learned by existing deep Re-ID models is thus dominated by the visual appearance of clothing. In this work, we focus on a much more difficult yet practical setting where person matching is conducted over long-duration, e.g., over days and months and therefore inevitably under the new challenge of changing clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is much understudied due to the lack of large scale datasets. The first contribution of this work is a new LTCC dataset containing people captured over a long period of time with frequent clothing changes. As a second contribution, we propose a novel Re-ID method specifically designed to address the cloth-changing challenge. Specifically, we consider that under cloth-changes, soft-biometrics such as body shape would be more reliable. We, therefore, introduce a shape embedding module as well as a cloth-elimination shape-distillation module aiming to eliminate the now unreliable clothing appearance features and focus on the body shape information. Extensive experiments show that superior performance is achieved by the proposed model on the new LTCC dataset. The code and dataset will be available at https://naiq.github.io/LTCC_Perosn_ReID.html.",0
"The objective of Person re-identification (Re-ID) is to identify an individual across various camera views, locations, and times. Previous Re-ID studies have primarily focused on the short-term cloth-consistent scenario, where individuals appear in different camera views wearing the same clothing. Consequently, the deep Re-ID models used in these studies rely heavily on visual clothing appearance for discriminative feature representation. In this study, we tackle a more complex and practical problem of person matching over extended periods, such as days or months, where clothing changes are inevitable. This issue, known as Long-Term Cloth-Changing (LTCC) Re-ID, has received limited research attention due to the lack of large-scale datasets. Our study makes two significant contributions. Firstly, we present a new LTCC dataset that captures individuals over an extended period with frequent clothing changes. Secondly, we propose a novel Re-ID method that addresses the cloth-changing challenge. Our approach considers body shape as a more reliable soft-biometric feature under cloth changes. To eliminate the unreliable clothing appearance features and focus on body shape information, we introduce a shape embedding module and a cloth-elimination shape-distillation module. Extensive experiments demonstrate that our proposed model outperforms existing models on the new LTCC dataset. We will make the code and dataset available at https://naiq.github.io/LTCC_Perosn_ReID.html.",1
"Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at https://github.com/arthurdouillard/incremental_learning.pytorch",0
"Despite the attention given to lifelong learning, current research struggles to overcome catastrophic forgetting and retain knowledge during long periods of incremental learning. Our proposed solution, PODNet, draws inspiration from representation learning and effectively balances the retention of old classes with the learning of new ones. This sets PODNet apart from existing models, which have yet to explore this type of incremental learning. Our innovations include an efficient spatial-based distillation-loss and multiple proxy vectors for each class, which we thoroughly validate by comparing PODNet with three state-of-the-art models on CIFAR100, ImageNet100, and ImageNet1000. Our results demonstrate that PODNet outperforms existing models, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. The code for PODNet is available at https://github.com/arthurdouillard/incremental_learning.pytorch.",1
"In graph neural networks (GNNs), pooling operators compute local summaries of input graphs to capture their global properties, and they are fundamental for building deep GNNs that learn hierarchical representations. In this work, we propose the Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarser graphs while preserving the overall graph topology. During training, the GNN learns new node representations and fits them to a pyramid of coarsened graphs, which is computed offline in a pre-processing stage. NDP consists of three steps. First, a node decimation procedure selects the nodes belonging to one side of the partition identified by a spectral algorithm that approximates the \maxcut{} solution. Afterwards, the selected nodes are connected with Kron reduction to form the coarsened graph. Finally, since the resulting graph is very dense, we apply a sparsification procedure that prunes the adjacency matrix of the coarsened graph to reduce the computational cost in the GNN. Notably, we show that it is possible to remove many edges without significantly altering the graph structure. Experimental results show that NDP is more efficient compared to state-of-the-art graph pooling operators while reaching, at the same time, competitive performance on a significant variety of graph classification tasks.",0
"Pooling operators are essential in graph neural networks (GNNs) as they enable the computation of local summaries of input graphs, which are crucial for capturing global properties and building hierarchical representations in deep GNNs. In this study, we present the Node Decimation Pooling (NDP) operator for GNNs, which generates coarser graphs while preserving the overall graph topology. During training, NDP learns new node representations and fits them to a pyramid of coarsened graphs, which is computed offline in a pre-processing stage. The NDP operator comprises of three steps: first, a node decimation process selects nodes from one side of the partition identified by a spectral algorithm that approximates the maximum cut solution. Second, the selected nodes are connected with Kron reduction to form the coarsened graph. Finally, since the resulting graph is very dense, a sparsification procedure is applied to prune the adjacency matrix of the coarsened graph and reduce the computational cost in the GNN. Remarkably, we demonstrate that many edges can be removed without significantly altering the graph structure. Our experimental results demonstrate that NDP outperforms state-of-the-art graph pooling operators in terms of efficiency while achieving competitive performance on a range of graph classification tasks.",1
"We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This two-step encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates.",0
"We have developed a technique called Motion Estimation via Variational Autoencoder (MEVA) that can generate precise 3D human pose and motion estimates from RGB video sequences. Our approach involves breaking down the temporal sequence of human motion into two stages. Firstly, we use an auto-encoder-based motion compression to capture the general human motion estimation, and then we refine the motion through residual representation learning to include person-specific motion details. This method results in both smooth and accurate 3D human pose and motion estimates, as demonstrated through experiments.",1
"In this work we consider partially observable environments with sparse rewards. We present a self-supervised representation learning method for image-based observations, which arranges embeddings respecting temporal distance of observations. This representation is empirically robust to stochasticity and suitable for novelty detection from the error of a predictive forward model. We consider episodic and life-long uncertainties to guide the exploration. We propose to estimate the missing information about the environment with the world model, which operates in the learned latent space. As a motivation of the method, we analyse the exploration problem in a tabular Partially Observable Labyrinth. We demonstrate the method on image-based hard exploration environments from the Atari benchmark and report significant improvement with respect to prior work. The source code of the method and all the experiments is available at https://github.com/htdt/lwm.",0
"This study examines environments that are only partially observable and have sparse rewards. The researchers introduce a method for self-supervised representation learning that uses image-based observations and organizes embeddings based on the temporal distance between observations. This representation is shown to be robust to stochasticity and can detect novelty through predictive forward models. The study also considers the uncertainties of both episodic and life-long exploration, and proposes to use a world model to estimate missing information in the learned latent space. The researchers analyze the exploration problem in a Partially Observable Labyrinth and demonstrate the effectiveness of the method on challenging Atari benchmark environments. The source code and experiments are available at https://github.com/htdt/lwm.",1
"Contrastive learning has been adopted as a core method for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, this task labels crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment strategy is that it can not reflect the heterogeneous similarity between the query crop and each crop from other images, taking them as equally negative, while some of them may even belong to the same semantic class as the query. To address this issue, inspired by consistency regularization in semi-supervised learning on unlabeled data, we propose Consistent Contrast (CO2), which introduces a consistency regularization term into the current contrastive learning framework. Regarding the similarity of the query crop to each crop from other images as ""unlabeled"", the consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for these downstream tasks.",0
"Unsupervised visual representation learning commonly employs contrastive learning as a fundamental method. In the absence of human annotation, instance discrimination is usually conducted, where a query image crop is labeled as positive and crops from other images are labeled as negative. Nonetheless, this labeling method is limited as it does not account for the heterogeneous similarity between the query crop and other crops from different images, assuming them to be equally negative, despite some belonging to the same semantic class as the query crop. To address this issue, we propose Consistent Contrast (CO2) by incorporating a consistency regularization term in the current contrastive learning framework, inspired by consistency regularization in semi-supervised learning. This term treats the similarity of the query crop to crops from other images as unlabeled and uses the similarity of a positive crop as a pseudo label to ensure consistency between the two similarities. Results indicate that CO2 enhances Momentum Contrast (MoCo) by improving top-1 accuracy on ImageNet linear protocol by 2.9%, and top-5 accuracy on 1% and 10% labeled semi-supervised settings by 3.8% and 1.1%, respectively. CO2 also demonstrates improved visual representations for image classification, object detection, and semantic segmentation on PASCAL VOC.",1
"Contrastive approaches to representation learning have recently shown great promise. In contrast to generative approaches, these contrastive models learn a deterministic encoder with no notion of uncertainty or confidence. In this paper, we introduce a simple approach based on ""contrasting distributions"" that learns to assign uncertainty for pretrained contrastive representations. In particular, we train a deep network from a representation to a distribution in representation space, whose variance can be used as a measure of confidence. In our experiments, we show that this deep uncertainty model can be used (1) to visually interpret model behavior, (2) to detect new noise in the input to deployed models, (3) to detect anomalies, where we outperform 10 baseline methods across 11 tasks with improvements of up to 14% absolute, and (4) to classify out-of-distribution examples where our fully unsupervised model is competitive with supervised methods.",0
"Recently, contrastive approaches to representation learning have displayed high potential. These models differ from generative approaches in that they develop a definite encoder without considering uncertainty or confidence. Our paper proposes a straightforward method based on ""contrasting distributions"" that trains a deep network from a representation to a distribution in representation space. This enables us to determine the variance as an indication of confidence for pretrained contrastive representations. Our experiments demonstrate that this deep uncertainty model can be utilized to (1) visually interpret model behavior, (2) identify new noise in input for deployed models, (3) discover anomalies more effectively than 10 baseline methods across 11 tasks with enhancements of up to 14% absolute, and (4) classify out-of-distribution examples where our completely unsupervised model competes with supervised methods.",1
"Multivariate time series (MTS) data are becoming increasingly ubiquitous in diverse domains, e.g., IoT systems, health informatics, and 5G networks. To obtain an effective representation of MTS data, it is not only essential to consider unpredictable dynamics and highly variable lengths of these data but also important to address the irregularities in the sampling rates of MTS. Existing parametric approaches rely on manual hyperparameter tuning and may cost a huge amount of labor effort. Therefore, it is desirable to learn the representation automatically and efficiently. To this end, we propose an autonomous representation learning approach for multivariate time series (TimeAutoML) with irregular sampling rates and variable lengths. As opposed to previous works, we first present a representation learning pipeline in which the configuration and hyperparameter optimization are fully automatic and can be tailored for various tasks, e.g., anomaly detection, clustering, etc. Next, a negative sample generation approach and an auxiliary classification task are developed and integrated within TimeAutoML to enhance its representation capability. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoML outperforms competing approaches on various tasks by a large margin. In fact, it achieves the best anomaly detection performance among all comparison algorithms on 78 out of all 85 UCR datasets, acquiring up to 20% performance improvement in terms of AUC score.",0
"Multivariate time series (MTS) data are becoming increasingly prevalent in various domains such as IoT systems, health informatics, and 5G networks. However, effectively representing MTS data requires accounting for their unpredictable dynamics, highly variable lengths, and irregular sampling rates. Existing parametric approaches require manual hyperparameter tuning and involve significant labor costs. Therefore, we propose an autonomous representation learning approach for MTS, called TimeAutoML. Unlike previous approaches, TimeAutoML offers a fully automatic configuration and hyperparameter optimization tailored for various tasks such as anomaly detection and clustering. We also integrate a negative sample generation approach and an auxiliary classification task to improve TimeAutoML's representation capability. Empirical studies on real-world datasets show that TimeAutoML significantly outperforms competing approaches and achieves the best anomaly detection performance among all comparison algorithms on 78 out of all 85 UCR datasets, with up to a 20% improvement in AUC score.",1
"It has been demonstrated that hidden representation learned by a deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve the privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide a formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.",0
"A deep model's hidden representation can contain private information from the input, which can be used to recover such information accurately. To tackle this issue, we introduce a new approach called Differentially Private Neural Representation (DPNR) to safeguard privacy in extracted text representations. DPNR incorporates Differential Privacy (DP) for a formal privacy guarantee and improves privacy by masking words with dropout. To maintain the usefulness of the learned representation, we integrate DP-noisy representation into a robust training process that creates a fair and robust target model across demographic variables. Our experiments show that DPNR significantly reduces privacy leakage without compromising the main task's performance across different parameter settings on benchmark datasets.",1
"We present MIX'EM, a novel solution for unsupervised image classification. MIX'EM generates representations that by themselves are sufficient to drive a general-purpose clustering algorithm to deliver high-quality classification. This is accomplished by building a mixture of embeddings module into a contrastive visual representation learning framework in order to disentangle representations at the category level. It first generates a set of embedding and mixing coefficients from a given visual representation, and then combines them into a single embedding. We introduce three techniques to successfully train MIX'EM and avoid degenerate solutions; (i) diversify mixture components by maximizing entropy, (ii) minimize instance conditioned component entropy to enforce a clustered embedding space, and (iii) use an associative embedding loss to enforce semantic separability. By applying (i) and (ii), semantic categories emerge through the mixture coefficients, making it possible to apply (iii). Subsequently, we run K-means on the representations to acquire semantic classification. We conduct extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78\%, 82\%, and 44\%, respectively. To achieve robust and high accuracy, it is essential to use the mixture components to initialize K-means. Finally, we report competitive baselines (70\% on STL10) obtained by applying K-means to the ""normalized"" representations learned using the contrastive loss.",0
"Introducing MIX'EM, an innovative method for image classification without supervision. MIX'EM produces representations that can be utilized by a general-purpose clustering algorithm to provide accurate classification. The technique involves creating a module that incorporates a mixture of embeddings into a framework for learning contrastive visual representations, which disentangles representations at the category level. MIX'EM generates a set of embedding and mixing coefficients from a given visual representation and merges them into a single embedding. To train MIX'EM effectively and prevent degenerate solutions, we have implemented three techniques: (i) maximizing entropy to diversify mixture components, (ii) minimizing instance conditioned component entropy to establish a clustered embedding space, and (iii) applying an associative embedding loss to ensure semantic separability. By applying (i) and (ii), semantic categories are derived through the mixture coefficients, which enables (iii) to be employed. Finally, we use K-means on the representations to obtain semantic classification. We conducted extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78\%, 82\%, and 44\%, respectively. To achieve robust and high accuracy, using the mixture components to initiate K-means is crucial. Lastly, we report competitive baselines (70\% on STL10) by applying K-means to the ""normalized"" representations learned using the contrastive loss.",1
"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.",0
"The use of function approximation in reinforcement learning can lead to instability and divergence, particularly when combined with off-policy learning and Bellman updates. Deep reinforcement learning has addressed these issues by adjusting and regularizing the representation, with a focus on auxiliary tasks. This suggests that representation learning could offer a reliable means of ensuring stability. The present study formally demonstrates that certain state representations exist under which the TD algorithm remains stable, even when learning off-policy. We evaluate representation learning methods that rely on a policy's transition matrix, such as proto-value functions, along three criteria: approximation error, stability, and ease of estimation. We find that a Schur basis offers convergence guarantees but is challenging to estimate from samples. However, an orthogonal basis of the corresponding Krylov subspace is a better option for a fixed reward function. We conclude by showing that these stable representations can be learned using stochastic gradient descent, which could lead to improved representation learning techniques for deep networks.",1
"This work introduces a new unsupervised representation learning technique called Deep Convolutional Transform Learning (DCTL). By stacking convolutional transforms, our approach is able to learn a set of independent kernels at different layers. The features extracted in an unsupervised manner can then be used to perform machine learning tasks, such as classification and clustering. The learning technique relies on a well-sounded alternating proximal minimization scheme with established convergence guarantees. Our experimental results show that the proposed DCTL technique outperforms its shallow version CTL, on several benchmark datasets.",0
"A novel method of unsupervised representation learning, known as Deep Convolutional Transform Learning (DCTL), is presented in this study. The approach utilizes convolutional transforms that are stacked to acquire a variety of independent kernels across different layers. These extracted features can be leveraged for machine learning tasks like clustering and classification. The learning technique relies on an alternating proximal minimization scheme that is robust and has known convergence guarantees. Our experimental findings demonstrate that DCTL outperforms its shallow variant, CTL, on various benchmark datasets.",1
"This paper challenges the common assumption that the weight $\beta$, in $\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\beta$-VAE, with $\beta < 1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper removes the inherent trade-off between reconstruction accuracy and disentanglement for $\beta$-VAE. Existing methods, such as $\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to control the trade-off to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\beta$ (for disentanglement) and the need for a small $\beta$. Instead, we propose DynamicVAE that maintains a different $\beta$ at different stages of training, thereby decoupling disentanglement and reconstruction accuracy. In order to evolve the weight, $\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two.",0
"The common belief that a weight $\beta$ larger than $1$ is necessary for effective disentanglement of latent factors in $\beta$-VAE is challenged in this paper. The authors propose DynamicVAE, which maintains a different $\beta$ at different stages of training to improve both disentanglement and reconstruction accuracy. Existing methods assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for better disentanglement. ControlVAE attempts to control the trade-off to a more favorable point, but fails to eliminate the conflict between the need for a large $\beta$ for disentanglement and the need for a small $\beta$. DynamicVAE uses a modified incremental PI controller and a hybrid annealing method to smoothly evolve the value of KL-divergence and theoretically proves its stability. Evaluation results on three benchmark datasets show that DynamicVAE significantly improves reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The proposed method separates disentangled representation learning and reconstruction, removing the inherent tension between the two.",1
"Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.",0
"Natural videos depict events that result from the interactions between actors and objects over time, often involving multiple activities and object categories. To effectively capture the complex visual and semantic context of such videos, our proposal involves the use of two graphs - a spatio-temporal visual graph with attributed nodes and edges that represent different interaction types, and a symbolic graph that models semantic relationships. We introduce a graph neural network that enhances the representations of actors, objects, and their interactions on this hybrid graph. Unlike existing approaches, our method employs specialized attention-based message functions for diverse node and edge types, incorporates visual edge features, integrates visual evidence with label relationships, and performs global reasoning in the semantic space. Our experimental results, including temporal action localization on the Charades dataset, demonstrate that our approach outperforms existing methods and achieves state-of-the-art performance.",1
"Automatically finding good and general remote sensing representations allows to perform transfer learning on a wide range of applications - improving the accuracy and reducing the required number of training samples. This paper investigates development of generic remote sensing representations, and explores which characteristics are important for a dataset to be a good source for representation learning. For this analysis, five diverse remote sensing datasets are selected and used for both, disjoint upstream representation learning and downstream model training and evaluation. A common evaluation protocol is used to establish baselines for these datasets that achieve state-of-the-art performance. As the results indicate, especially with a low number of available training samples a significant performance enhancement can be observed when including additionally in-domain data in comparison to training models from scratch or fine-tuning only on ImageNet (up to 11% and 40%, respectively, at 100 training samples). All datasets and pretrained representation models are published online.",0
"This paper focuses on the development of generic remote sensing representations that can be used for transfer learning across various applications. By identifying the key characteristics of a good dataset for representation learning, the study analyzes five remote sensing datasets through disjoint upstream representation learning and downstream model training and evaluation. A standardized evaluation protocol is used to establish baseline performance levels for these datasets, which achieve state-of-the-art results. The findings suggest that incorporating in-domain data with a low number of training samples can significantly enhance performance, compared to training models from scratch or fine-tuning only on ImageNet. All the datasets and pretrained representation models are accessible online.",1
"Event-based cameras record an asynchronous stream of per-pixel brightness changes. As such, they have numerous advantages over the standard frame-based cameras, including high temporal resolution, high dynamic range, and no motion blur. Due to the asynchronous nature, efficient learning of compact representation for event data is challenging. While it remains not explored the extent to which the spatial and temporal event ""information"" is useful for pattern recognition tasks. In this paper, we focus on single-layer architectures. We analyze the performance of two general problem formulations: the direct and the inverse, for unsupervised feature learning from local event data (local volumes of events described in space-time). We identify and show the main advantages of each approach. Theoretically, we analyze guarantees for an optimal solution, possibility for asynchronous, parallel parameter update, and the computational complexity. We present numerical experiments for object recognition. We evaluate the solution under the direct and the inverse problem and give a comparison with the state-of-the-art methods. Our empirical results highlight the advantages of both approaches for representation learning from event data. We show improvements of up to 9 % in the recognition accuracy compared to the state-of-the-art methods from the same class of methods.",0
"Compared to frame-based cameras, event-based cameras record per-pixel brightness changes asynchronously and offer benefits like high dynamic range, temporal resolution, and no motion blur. However, learning efficient representations from event data is challenging due to its asynchronous nature. The usefulness of spatial and temporal event information for pattern recognition tasks remains unexplored. In this study, we focus on single-layer architectures and analyze two problem formulations: direct and inverse, for unsupervised feature learning from local event data. We identify the advantages of each approach, analyze guarantees for optimal solutions, and assess computational complexity. Our experiments on object recognition show that both approaches offer advantages for representation learning from event data, with up to 9% improvement in recognition accuracy compared to the state-of-the-art methods.",1
"We propose a novel framework, called Markov-Lipschitz deep learning (MLDL), to tackle geometric deterioration caused by collapse, twisting, or crossing in vector-based neural network transformations for manifold-based representation learning and manifold data generation. A prior constraint, called locally isometric smoothness (LIS), is imposed across-layers and encoded into a Markov random field (MRF)-Gibbs distribution. This leads to the best possible solutions for local geometry preservation and robustness as measured by locally geometric distortion and locally bi-Lipschitz continuity. Consequently, the layer-wise vector transformations are enhanced into well-behaved, LIS-constrained metric homeomorphisms. Extensive experiments, comparisons, and ablation study demonstrate significant advantages of MLDL for manifold learning and manifold data generation. MLDL is general enough to enhance any vector transformation-based networks. The code is available at https://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.",0
"Our solution to address geometric deterioration in vector-based neural network transformations is the Markov-Lipschitz deep learning (MLDL) framework. This is particularly useful for manifold-based representation learning and generating manifold data. We incorporate a prior constraint, known as locally isometric smoothness (LIS), throughout the layers using a Markov random field (MRF)-Gibbs distribution. By doing so, we achieve optimal local geometry preservation and robustness, as indicated by locally geometric distortion and locally bi-Lipschitz continuity. This results in well-behaved, LIS-constrained metric homeomorphisms for the layer-wise vector transformations. Our experiments, comparisons, and ablation study show that MLDL offers significant advantages for manifold learning and manifold data generation. It is applicable to any vector transformation-based networks, and the code is available at https://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.",1
"The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.",0
"The current research in Content-Based Video Retrieval aims to obtain a video representation that can describe the long-range semantic dependencies of relevant events and incidents. However, existing methods typically process individual frames or short clips as separate entities, which makes it challenging to model such dependencies. To address this issue, we introduce TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that employs the self-attention mechanism to incorporate long-range temporal information between frame-level features. We propose a supervised contrastive learning method that uses automatic hard negative mining and memory bank mechanism to train TCA on video retrieval datasets. We conduct extensive experiments on several video retrieval tasks, including CC_WEB_VIDEO, FIVR-200K, and EVVE. Our proposed method outperforms state-of-the-art methods with video-level features by approximately 17% mAP on FIVR-200K, while providing competitive results with 22 times faster inference time compared to frame-level features.",1
"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.",0
"Introducing HERO, a unique framework that facilitates comprehensive video+language omni-representation learning on a large scale. HERO captures multimodal inputs through a hierarchical structure that involves a Cross-modal Transformer for capturing local context of a video frame via multimodal fusion and a Temporal Transformer for capturing global video context. Alongside the standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, HERO incorporates two new pre-training tasks: Video-Subtitle Matching (VSM) that involves predicting both global and local temporal alignment, and Frame Order Modeling (FOM) that involves predicting the correct order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain a deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments show that HERO achieves new state-of-the-art results on multiple benchmarks across various domains, including Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference, and Video Captioning tasks. Additionally, we introduce two challenging new benchmarks, How2QA and How2R, for Video QA and Retrieval, collected from diverse video content across multimodalities.",1
"We propose a novel tree-based ensemble method named Selective Cascade of Residual ExtraTrees (SCORE). SCORE draws inspiration from representation learning, incorporates regularized regression with variable selection features, and utilizes boosting to improve prediction and reduce generalization errors. We also develop a variable importance measure to increase the explainability of SCORE. Our computer experiments show that SCORE provides comparable or superior performance in prediction against ExtraTrees, random forest, gradient boosting machine, and neural networks; and the proposed variable importance measure for SCORE is comparable to studied benchmark methods. Finally, the predictive performance of SCORE remains stable across hyper-parameter values, suggesting potential robustness to hyperparameter specification.",0
"The new ensemble method we present is called Selective Cascade of Residual ExtraTrees (SCORE), which utilizes ideas from representation learning and incorporates regularized regression with variable selection features. Boosting is used to enhance prediction and minimize generalization errors. To increase the interpretability of SCORE, we also introduce a measure of variable importance. Our experiments demonstrate that SCORE performs comparably or better than other methods such as ExtraTrees, random forest, gradient boosting machine, and neural networks. Additionally, our proposed variable importance measure for SCORE is similar to established benchmark methods. Finally, the predictive ability of SCORE is consistent across different hyperparameter values, indicating potential robustness to hyperparameter selection.",1
"The recently occurred representation learning make an attractive performance in NLP and complex network, it is becoming a fundamental technology in machine learning and data mining. How to use representation learning to improve the performance of classifiers is a very significance research direction. We using representation learning technology to map raw data(node of graph) to a low-dimensional feature space. In this space, each raw data obtained a lower dimensional vector representation, we do some simple linear operations for those vectors to produce some virtual data, using those vectors and virtual data to training multi-tag classifier. After that we measured the performance of classifier by F1 score(Macro% F1 and Micro% F1). Our method make Macro F1 rise from 28 % - 450% and make average F1 score rise from 12 % - 224%. By contrast, we trained the classifier directly with the lower dimensional vector, and measured the performance of classifiers. We validate our algorithm on three public data sets, we found that the virtual data helped the classifier greatly improve the F1 score. Therefore, our algorithm is a effective way to improve the performance of classifier. These result suggest that the virtual data generated by simple linear operation, in representation space, still retains the information of the raw data. It's also have great significance to the learning of small sample data sets.",0
"Representation learning, which has recently gained attention for its impressive performance in NLP and complex networks, is becoming an essential technology in machine learning and data mining. A significant research direction involves utilizing representation learning to enhance the performance of classifiers. To achieve this, we utilized representation learning to map raw data (graph nodes) to a low-dimensional feature space, where each raw data is represented by a lower-dimensional vector. Simple linear operations were then performed on these vectors to generate virtual data, which was used to train a multi-tag classifier. We evaluated the classifier's performance using the F1 score (Macro% F1 and Micro% F1). Our approach led to a substantial increase in the Macro F1 score (from 28% to 450%) and the average F1 score (from 12% to 224%). In contrast, when we trained the classifier directly with the lower dimensional vector, the performance was inferior. We tested our algorithm on three public datasets and observed that the virtual data significantly improved the F1 score of the classifier. This indicates that the virtual data, generated by simple linear operations in the representation space, retains the information of the raw data and is particularly useful for learning from small sample data sets. Therefore, our algorithm is an effective way to enhance the performance of classifiers.",1
"Source code representations are key in applying machine learning techniques for processing and analyzing programs. A popular approach in representing source code is neural source code embeddings that represents programs with high-dimensional vectors computed by training deep neural networks on a large volume of programs. Although successful, there is little known about the contents of these vectors and their characteristics. In this paper, we present our preliminary results towards better understanding the contents of code2vec neural source code embeddings. In particular, in a small case study, we use the code2vec embeddings to create binary SVM classifiers and compare their performance with the handcrafted features. Our results suggest that the handcrafted features can perform very close to the highly-dimensional code2vec embeddings, and the information gains are more evenly distributed in the code2vec embeddings compared to the handcrafted features. We also find that the code2vec embeddings are more resilient to the removal of dimensions with low information gains than the handcrafted features. We hope our results serve a stepping stone toward principled analysis and evaluation of these code representations.",0
"The use of machine learning techniques for processing and analyzing programs relies heavily on source code representations. One popular approach is the use of neural source code embeddings, which represents programs using high-dimensional vectors generated by training deep neural networks on a large volume of programs. However, little is known about the contents and characteristics of these vectors. Our paper presents preliminary results aimed at better understanding the contents of code2vec neural source code embeddings. We conducted a small case study using code2vec embeddings to generate binary SVM classifiers and compared their performance to handcrafted features. Our findings suggest that handcrafted features perform similarly to code2vec embeddings, and the information gains in the latter are more evenly distributed. Additionally, we discovered that code2vec embeddings are more resistant to the removal of dimensions with low information gains compared to handcrafted features. Our results are a starting point for a more systematic analysis and evaluation of these code representations.",1
"Recently, there is an increasing demand for automatically detecting anatomical landmarks which provide rich structural information to facilitate subsequent medical image analysis. Current methods related to this task often leverage the power of deep neural networks, while a major challenge in fine tuning such models in medical applications arises from insufficient number of labeled samples. To address this, we propose to regularize the knowledge transfer across source and target tasks through cross-task representation learning. The proposed method is demonstrated for extracting facial anatomical landmarks which facilitate the diagnosis of fetal alcohol syndrome. The source and target tasks in this work are face recognition and landmark detection, respectively. The main idea of the proposed method is to retain the feature representations of the source model on the target task data, and to leverage them as an additional source of supervisory signals for regularizing the target model learning, thereby improving its performance under limited training samples. Concretely, we present two approaches for the proposed representation learning by constraining either final or intermediate model features on the target model. Experimental results on a clinical face image dataset demonstrate that the proposed approach works well with few labeled data, and outperforms other compared approaches.",0
"The demand for automatically detecting anatomical landmarks is increasing to aid in the subsequent analysis of medical images. Deep neural networks are commonly used for this task, but the lack of labeled samples in medical applications presents a challenge in fine-tuning these models. To overcome this, we propose a method that transfers knowledge from source to target tasks through cross-task representation learning. This is demonstrated by extracting facial landmarks to aid in diagnosing fetal alcohol syndrome. The proposed method uses feature representations from the source model as additional supervisory signals for the target model, improving its performance with limited training samples. Two approaches are presented for the proposed representation learning, which constrain either final or intermediate model features on the target model. Experimental results on a clinical face image dataset show that our approach works well with few labeled data and outperforms other compared methods.",1
"We consider the problem of representation learning for temporal interaction graphs where a network of entities with complex interactions over an extended period of time is modeled as a graph with a rich set of node and edge attributes. In particular, an edge between a node-pair within the graph corresponds to a multi-dimensional time-series. To fully capture and model the dynamics of the network, we propose GTEA, a framework of representation learning for temporal interaction graphs with per-edge time-based aggregation. Under GTEA, a Graph Neural Network (GNN) is integrated with a state-of-the-art sequence model, such as LSTM, Transformer and their time-aware variants. The sequence model generates edge embeddings to encode temporal interaction patterns between each pair of nodes, while the GNN-based backbone learns the topological dependencies and relationships among different nodes. GTEA also incorporates a sparsity-inducing self-attention mechanism to distinguish and focus on the more important neighbors of each node during the aggregation process. By capturing temporal interactive dynamics together with multi-dimensional node and edge attributes in a network, GTEA can learn fine-grained representations for a temporal interaction graph to enable or facilitate other downstream data analytic tasks. Experimental results show that GTEA outperforms state-of-the-art schemes including GraphSAGE, APPNP, and TGAT by delivering higher accuracy (100.00%, 98.51%, 98.05% ,79.90%) and macro-F1 score (100.00%, 98.51%, 96.68% ,79.90%) over four large-scale real-world datasets for binary/ multi-class node classification.",0
"We are addressing the challenge of learning representations for temporal interaction graphs, which model complex interactions between entities over an extended period as a graph with rich node and edge attributes. Each edge between a pair of nodes represents a multi-dimensional time-series. To capture the network's dynamics, we propose GTEA, a representation learning framework that uses per-edge time-based aggregation. GTEA combines a Graph Neural Network (GNN) with a state-of-the-art sequence model, such as LSTM or Transformer, to generate edge embeddings that encode temporal interaction patterns. The GNN-based backbone captures topological dependencies and relationships among nodes. GTEA also includes a self-attention mechanism that focuses on important neighbors during aggregation. By learning fine-grained representations for temporal interaction graphs, GTEA facilitates downstream data analytic tasks. Our experiments on four real-world datasets show that GTEA outperforms state-of-the-art schemes, including GraphSAGE, APPNP, and TGAT, in terms of accuracy and macro-F1 score for binary/multi-class node classification.",1
"Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel ""deep adversarial transition learning"" (DATL) framework that bridges the domain gap by projecting the source and target domains into intermediate, transitional spaces through the employment of adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional transitions by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for domain adaptation, mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, transitions generation, and transitions alignment by GANs. Experimental results demonstrate that our method outperforms the state-of-the art on a number of unsupervised domain adaptation benchmarks.",0
"Computer vision's current methods for deep domain adaptation have primarily concentrated on discovering discriminative and domain-invariant features across varying domains. In this paper, we introduce a new framework called ""deep adversarial transition learning"" (DATL). This approach fills in the domain gap by projecting the source and target domains into intermediate, transitional spaces. This is accomplished by utilizing adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Our process involves creating variational auto-encoders (VAE) for both domains. Then, bidirectional transitions are constructed by cross-grafting the VAEs' decoder stacks. Additionally, we use generative adversarial networks (GAN) for domain adaptation, which maps the target domain data to the source domain's known label space. The adaptation process consists of three phases: VAE's feature representation learning, generating transitions, and aligning transitions by GANs. Our experimental results demonstrate that our method exceeds the state-of-the-art on several unsupervised domain adaptation benchmarks.",1
"In the realms of computer vision, it is evident that deep neural networks perform better in a supervised setting with a large amount of labeled data. The representations learned with supervision are not only of high quality but also helps the model in enhancing its accuracy. However, the collection and annotation of a large dataset are costly and time-consuming. To avoid the same, there has been a lot of research going on in the field of unsupervised visual representation learning especially in a self-supervised setting. Amongst the recent advancements in self-supervised methods for visual recognition, in SimCLR Chen et al. shows that good quality representations can indeed be learned without explicit supervision. In SimCLR, the authors maximize the similarity of augmentations of the same image and minimize the similarity of augmentations of different images. A linear classifier trained with the representations learned using this approach yields 76.5% top-1 accuracy on the ImageNet ILSVRC-2012 dataset. In this work, we propose that, with the normalized temperature-scaled cross-entropy (NT-Xent) loss function (as used in SimCLR), it is beneficial to not have images of the same category in the same batch. In an unsupervised setting, the information of images pertaining to the same category is missing. We use the latent space representation of a denoising autoencoder trained on the unlabeled dataset and cluster them with k-means to obtain pseudo labels. With this apriori information we batch images, where no two images from the same category are to be found. We report comparable performance enhancements on the CIFAR10 dataset and a subset of the ImageNet dataset. We refer to our method as G-SimCLR.",0
"It is clear that deep neural networks are more effective in supervised settings with abundant labeled data in the field of computer vision. Supervised learning produces high-quality representations that improve the model's accuracy. However, gathering and annotating large datasets are expensive and time-consuming. As a result, there has been a lot of research into unsupervised visual representation learning, particularly in self-supervised settings. In recent advancements in self-supervised methods for visual recognition, Chen et al. demonstrate that good quality representations can be learned without explicit supervision with their SimCLR approach. In SimCLR, the authors maximize the similarity of augmentations of the same image and minimize the similarity of augmentations of different images. A linear classifier trained with these representations yields 76.5% top-1 accuracy on the ImageNet ILSVRC-2012 dataset. In this paper, we propose that it is beneficial to not have images of the same category in the same batch with the normalized temperature-scaled cross-entropy (NT-Xent) loss function used in SimCLR. In an unsupervised setting, images pertaining to the same category are not available. We use the latent space representation of a denoising autoencoder trained on the unlabeled dataset and cluster them with k-means to obtain pseudo labels. We batch images where no two images from the same category are present. We refer to our approach as G-SimCLR. We report comparable performance improvements on the CIFAR10 dataset and a subset of the ImageNet dataset.",1
"Representation learning has been proven to play an important role in the unprecedented success of machine learning models in numerous tasks, such as machine translation, face recognition and recommendation. The majority of existing representation learning approaches often require a large number of consistent and noise-free labels. However, due to various reasons such as budget constraints and privacy concerns, labels are very limited in many real-world scenarios. Directly applying standard representation learning approaches on small labeled data sets will easily run into over-fitting problems and lead to sub-optimal solutions. Even worse, in some domains such as education, the limited labels are usually annotated by multiple workers with diverse expertise, which yields noises and inconsistency in such crowdsourcing settings. In this paper, we propose a novel framework which aims to learn effective representations from limited data with crowdsourced labels. Specifically, we design a grouping based deep neural network to learn embeddings from a limited number of training samples and present a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels. Furthermore, to expedite the training process, we develop a hard example selection procedure to adaptively pick up training examples that are misclassified by the model. Extensive experiments conducted on three real-world data sets demonstrate the superiority of our framework on learning representations from limited data with crowdsourced labels, comparing with various state-of-the-art baselines. In addition, we provide a comprehensive analysis on each of the main components of our proposed framework and also introduce the promising results it achieved in our real production to fully understand the proposed framework.",0
"Machine learning models have achieved great success in tasks such as machine translation, face recognition, and recommendation due to the important role played by representation learning. However, most of the existing representation learning approaches require numerous noise-free labels, which are often limited in real-world scenarios due to budget constraints and privacy concerns. Since applying standard representation learning approaches to small labeled datasets can lead to over-fitting problems and sub-optimal solutions, a novel framework is proposed in this paper to learn effective representations from limited data with crowdsourced labels. The framework includes a grouping-based deep neural network to learn embeddings from a limited number of training samples, a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels, and a hard example selection procedure to adaptively pick up misclassified training examples. Extensive experiments demonstrate the superiority of the proposed framework over various state-of-the-art baselines. The paper also provides a comprehensive analysis of each component of the framework and introduces the promising results achieved in real production.",1
"Representation learning is a fundamental building block for analyzing entities in a database. While the existing embedding learning methods are effective in various data mining problems, their applicability is often limited because these methods have pre-determined assumptions on the type of semantics captured by the learned embeddings, and the assumptions may not well align with specific downstream tasks. In this work, we propose an embedding learning framework that 1) uses an input format that is agnostic to input data type, 2) is flexible in terms of the relationships that can be embedded into the learned representations, and 3) provides an intuitive pathway to incorporate domain knowledge into the embedding learning process. Our proposed framework utilizes a set of entity-relation-matrices as the input, which quantifies the affinities among different entities in the database. Moreover, a sampling mechanism is carefully designed to establish a direct connection between the input and the information captured by the output embeddings. To complete the representation learning toolbox, we also outline a simple yet effective post-processing technique to properly visualize the learned embeddings. Our empirical results demonstrate that the proposed framework, in conjunction with a set of relevant entity-relation-matrices, outperforms the existing state-of-the-art approaches in various data mining tasks.",0
"Representation learning is crucial for analyzing entities in a database. Although current embedding learning methods are useful in different data mining problems, their usefulness is frequently restricted because they assume the type of semantics captured by the learned embeddings, which may not be suitable for specific downstream tasks. Our approach proposes an embedding learning framework that overcomes these limitations by using an input format that is agnostic to input data type, flexible in terms of relationships that can be embedded into the learned representations, and provides an intuitive pathway to include domain knowledge into the embedding learning process. Our framework uses a set of entity-relation-matrices as the input, which quantifies the affinities among different entities in the database. Additionally, we have designed a sampling mechanism to establish a direct connection between the input and the information captured by the output embeddings. To enhance the representation learning toolbox, we have also outlined a simple yet effective post-processing technique to visualize the learned embeddings appropriately. Our experimental findings demonstrate that our proposed framework, in combination with a relevant set of entity-relation-matrices, outperforms the current state-of-the-art approaches in various data mining tasks.",1
This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.,0
"In this article, an automated framework for acquiring skills in reinforcement learning is introduced. The framework involves identifying hierarchical descriptions of tasks in terms of abstract states and extended actions between them. This identification simplifies and speeds up reinforcement learning algorithms and allows for generalization over multiple tasks without relearning policies. The framework uses ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned with the underlying structure. Skills are defined as sequences of actions that lead to transitions between abstract states, which are generated using connectivity information from PCCA+. These skills can be efficiently reused across various tasks defined over the same model, even without an exact model of the environment, using sample trajectories to construct an approximate estimate. The framework is also scalable to complex tasks with large state spaces by performing state aggregation using the representation learned from an action conditional video prediction network and using the skill acquisition framework on the aggregated state space.",1
"Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",0
"For a long time, axis-aligned decision forests have been the preferred machine learning algorithms used to model tabular data. They are especially useful in learning-to-rank applications due to their exceptional performance and interpretability. However, decision forests have not been able to handle raw structured data such as text and have struggled to learn effective representations for them. Existing methods to address this issue, such as constructing smoothed decision forests, result in non-axis-aligned models that are not interpretable and are not suitable for Transfer Learning. In this study, we propose a new approach to achieve representation learning for decision forests without imposing new restrictions or requiring structural changes. Our method involves using a decision forest on top of a deep neural network, which is trained to learn or fine-tune representations through input perturbation. This approach is applicable to any decision forest and allows the use of any deep neural network for representation learning. We demonstrate the effectiveness of our approach through experiments on synthetic and benchmark classification datasets.",1
"Recently, the interest of graph representation learning has been rapidly increasing in recommender systems. However, most existing studies have focused on improving accuracy, but in real-world systems, the recommendation diversity should be considered as well to improve user experiences. In this paper, we propose the diversity-emphasized node embedding div2vec, which is a random walk-based unsupervised learning method like DeepWalk and node2vec. When generating random walks, DeepWalk and node2vec sample nodes of higher degree more and nodes of lower degree less. On the other hand, div2vec samples nodes with the probability inversely proportional to its degree so that every node can evenly belong to the collection of random walks. This strategy improves the diversity of recommendation models. Offline experiments on the MovieLens dataset showed that our new method improves the recommendation performance in terms of both accuracy and diversity. Moreover, we evaluated the proposed model on two real-world services, WATCHA and LINE Wallet Coupon, and observed the div2vec improves the recommendation quality by diversifying the system.",0
"The field of graph representation learning has recently seen a surge of interest in recommender systems. However, previous research has primarily focused on improving accuracy rather than considering the importance of recommendation diversity in enhancing user experiences in real-world scenarios. This paper introduces div2vec, a novel unsupervised learning method based on random walks similar to DeepWalk and node2vec, that emphasizes diversity in node embedding. Unlike DeepWalk and node2vec, which prioritize high-degree nodes in their random walk sampling, div2vec utilizes a strategy that samples nodes inversely proportional to their degree, ensuring that all nodes have an equal chance of being included in the collection of random walks. Our experiments on the MovieLens dataset indicate that div2vec outperforms existing methods in terms of both accuracy and diversity. Furthermore, we evaluated div2vec on two real-world services, WATCHA and LINE Wallet Coupon, and observed that it enhances recommendation quality by promoting system diversity.",1
"We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture-sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of $58.43\%$. This improves the performance of current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",0
"Our study introduces a novel approach to recognizing emotions from gestures using a generalized zero-shot algorithm. Our objective is to identify emotions that were not encountered during training by associating gestures with novel emotion categories. To achieve this, we employ an adversarial, autoencoder-based representation learning technique that links 3D motion-captured gesture sequences with natural-language perceived emotion terms using word2vec embeddings. This language-semantic embedding generates an emotion label space representation, which we utilize to map the gesture-sequences to the appropriate categorical emotion labels. Our method is trained using a combination of gestures that are annotated with known emotion terms and those that are not annotated with any emotions. We assess the accuracy of our approach on the MPI Emotional Body Expressions Database (EBEDB) and achieve an accuracy of $58.43\%$. Our approach outperforms the current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",1
"Video representation learning has recently attracted attention in computer vision due to its applications for activity and scene forecasting or vision-based planning and control. Video prediction models often learn a latent representation of video which is encoded from input frames and decoded back into images. Even when conditioned on actions, purely deep learning based architectures typically lack a physically interpretable latent space. In this study, we use a differentiable physics engine within an action-conditional video representation network to learn a physical latent representation. We propose supervised and self-supervised learning methods to train our network and identify physical properties. The latter uses spatial transformers to decode physical states back into images. The simulation scenarios in our experiments comprise pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. In experiments we demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences in the simulated scenarios. We evaluate the accuracy of our supervised and self-supervised methods and compare it with a system identification baseline which directly learns from state trajectories. We also demonstrate the ability of our method to predict future video frames from input images and actions.",0
"The use of video representation learning has gained attention in computer vision as it has various applications such as activity and scene forecasting, vision-based planning and control. Video prediction models typically learn a latent representation of video, which is encoded from input frames and decoded back into images. However, deep learning-based architectures lack a physically interpretable latent space, even when conditioned on actions. This study proposes the use of a differentiable physics engine to learn a physical latent representation within an action-conditional video representation network. The network is trained using supervised and self-supervised learning methods to identify physical properties, with the latter using spatial transformers to decode physical states back into images. The experiments involve simulation scenarios such as pushing, sliding and colliding objects, and the observability of physical properties is analyzed. The results demonstrate that the network can learn to encode images and identify physical properties such as mass and friction from videos and action sequences in simulated scenarios. The accuracy of the supervised and self-supervised methods is evaluated and compared with a system identification baseline that directly learns from state trajectories. Additionally, the network's ability to predict future video frames from input images and actions is demonstrated.",1
"The heterogeneous network is a robust data abstraction that can model entities of different types interacting in various ways. Such heterogeneity brings rich semantic information but presents nontrivial challenges in aggregating the heterogeneous relationships between objects - especially those of higher-order indirect relations. Recent graph neural network approaches for representation learning on heterogeneous networks typically employ the attention mechanism, which is often only optimized for predictions based on direct links. Furthermore, even though most deep learning methods can aggregate higher-order information by building deeper models, such a scheme can diminish the degree of interpretability. To overcome these challenges, we explore an architecture - Layer-stacked ATTention Embedding (LATTE) - that automatically decomposes higher-order meta relations at each layer to extract the relevant heterogeneous neighborhood structures for each node. Additionally, by successively stacking layer representations, the learned node embedding offers a more interpretable aggregation scheme for nodes of different types at different neighborhood ranges. We conducted experiments on several benchmark heterogeneous network datasets. In both transductive and inductive node classification tasks, LATTE can achieve state-of-the-art performance compared to existing approaches, all while offering a lightweight model. With extensive experimental analyses and visualizations, the framework can demonstrate the ability to extract informative insights on heterogeneous networks.",0
"The heterogeneous network is a powerful tool for modeling different types of entities and their interactions, but it presents challenges in aggregating relationships between objects, particularly indirect ones. Graph neural network approaches typically use the attention mechanism, which is optimized for direct links and may not handle higher-order information well. Deep learning methods that aggregate information in deeper models can sacrifice interpretability. To address these challenges, we propose a new architecture called Layer-stacked ATTention Embedding (LATTE), which decomposes meta relations to extract relevant neighborhood structures for each node and offers a more interpretable aggregation scheme. We conducted experiments on benchmark datasets and found that LATTE outperforms existing approaches while remaining lightweight. Through analysis and visualization, LATTE can provide informative insights on heterogeneous networks.",1
"We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1\% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro.",0
"Our proposed method for webly-supervised representation learning addresses the scalability issues of both supervised and self-supervised learning, as neither approach can effectively handle the prevalent noise in large-scale real-world data. Unlike existing webly-supervised methods that rely on vanilla supervised learning, our method, Momentum Prototypes (MoPro), utilizes a simple contrastive learning approach to achieve label noise correction and out-of-distribution sample removal while also facilitating representation learning. MoPro achieves superior performance on the WebVision dataset, surpassing both supervised and self-supervised pretrained models. In downstream image classification and detection tasks, MoPro outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and the best self-supervised pretrained model by +17.3 when finetuned on a limited sample size of ImageNet labeled data. Additionally, MoPro shows greater resilience to distribution shifts. Our code and pretrained models are available at https://github.com/salesforce/MoPro.",1
"The diagnosis process of colorectal cancer mainly focuses on the localization and characterization of abnormal growths in the colon tissue known as polyps. Despite recent advances in deep object localization, the localization of polyps remains challenging due to the similarities between tissues, and the high level of artifacts. Recent studies have shown the negative impact of the presence of artifacts in the polyp detection task, and have started to take them into account within the training process. However, the use of prior knowledge related to the spatial interaction of polyps and artifacts has not yet been considered. In this work, we incorporate artifact knowledge in a post-processing step. Our method models this task as an inductive graph representation learning problem, and is composed of training and inference steps. Detected bounding boxes around polyps and artifacts are considered as nodes connected by a defined criterion. The training step generates a node classifier with ground truth bounding boxes. In inference, we use this classifier to analyze a second graph, generated from artifact and polyp predictions given by region proposal networks. We evaluate how the choices in the connectivity and artifacts affect the performance of our method and show that it has the potential to reduce the false positives in the results of a region proposal network.",0
"The primary focus of diagnosing colorectal cancer is identifying and characterizing abnormal growths, or polyps, in the colon tissue. Despite advances in deep object localization, accurately locating polyps remains difficult due to tissue similarities and high levels of artifacts. Recent studies have recognized the negative impact of artifacts on polyp detection and have begun to account for them during training. However, prior knowledge regarding the spatial relationship between polyps and artifacts has not been considered. This study aims to integrate artifact knowledge into a post-processing step using an inductive graph representation learning approach. The method involves a training and inference step, where detected bounding boxes around polyps and artifacts are treated as nodes connected by a set criterion. The training step generates a node classifier with ground truth bounding boxes, while inference involves analyzing a second graph generated from artifact and polyp predictions from region proposal networks. The study evaluates how connectivity and artifacts affect the method's performance and demonstrates its potential to reduce false positives in region proposal network results.",1
"Unsupervised (or self-supervised) graph representation learning is essential to facilitate various graph data mining tasks when external supervision is unavailable. The challenge is to encode the information about the graph structure and the attributes associated with the nodes and edges into a low dimensional space. Most existing unsupervised methods promote similar representations across nodes that are topologically close. Recently, it was shown that leveraging additional graph-level information, e.g., information that is shared among all nodes, encourages the representations to be mindful of the global properties of the graph, which greatly improves their quality. However, in most graphs, there is significantly more structure that can be captured, e.g., nodes tend to belong to (multiple) clusters that represent structurally similar nodes. Motivated by this observation, we propose a graph representation learning method called Graph InfoClust (GIC), that seeks to additionally capture cluster-level information content. These clusters are computed by a differentiable K-means method and are jointly optimized by maximizing the mutual information between nodes of the same clusters. This optimization leads the node representations to capture richer information and nodal interactions, which improves their quality. Experiments show that GIC outperforms state-of-art methods in various downstream tasks (node classification, link prediction, and node clustering) with a 0.9% to 6.1% gain over the best competing approach, on average.",0
"When external supervision is not available, unsupervised or self-supervised graph representation learning is crucial for a variety of graph data mining tasks. The goal is to compress information about the graph's structure and the attributes of its nodes and edges into a low-dimensional space. Most current unsupervised methods produce similar representations for topologically close nodes. However, recent research has demonstrated that incorporating additional graph-level information, such as information shared among all nodes, results in better representations that consider the graph's global properties. Nonetheless, most graphs contain additional structure that can be learned, including nodes that belong to multiple clusters of structurally similar nodes. To address this, we propose Graph InfoClust (GIC), a graph representation learning method that captures cluster-level information by using a differentiable K-means method to compute clusters optimized by maximizing mutual information between nodes in the same cluster. This approach results in richer information and nodal interactions, leading to improved quality. Experimental results show that GIC outperforms state-of-the-art methods in various downstream tasks, with an average gain of 0.9% to 6.1% over the best competing approach.",1
"The random subspace method, known as the pillar of random forests, is good at making precise and robust predictions. However, there is not a straightforward way yet to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than previous higher-order pooling methods, producing good results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS achieves superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with minor extra cost. Code is available at https://github.com/CupidJay/NRS_pytorch.",0
"The random subspace technique is a fundamental aspect of random forests, as it provides accurate and robust predictions. Despite its effectiveness, there is currently no easy way to integrate it with deep learning. Therefore, we introduce Neural Random Subspace (NRS), an innovative random subspace method based on deep learning. Unlike previous forest methods, NRS benefits from end-to-end representation learning driven by data, as well as extensive support from deep learning software and hardware platforms, resulting in faster inference speed and improved accuracy. Additionally, NRS can be encoded as a non-linear component into Convolutional Neural Networks (CNNs), which allows it to efficiently learn non-linear feature representations in CNNs, yielding excellent results with minimal increases in parameters, floating point operations (FLOPs), and real running time. When compared to random subspaces, random forests, and gradient boosting decision trees (GBDTs), NRS outperforms them on 35 machine learning datasets. Furthermore, NRS can be integrated with CNN architectures for both 2D image and 3D point cloud recognition tasks, resulting in consistent improvements with minimal additional costs. The code for NRS can be accessed at https://github.com/CupidJay/NRS_pytorch.",1
"In this work, our objective is to address the problems of generalization and flexibility for text recognition in documents. We introduce a new model that exploits the repetitive nature of characters in languages, and decouples the visual representation learning and linguistic modelling stages. By doing this, we turn text recognition into a shape matching problem, and thereby achieve generalization in appearance and flexibility in classes. We evaluate the new model on both synthetic and real datasets across different alphabets and show that it can handle challenges that traditional architectures are not able to solve without expensive retraining, including: (i) it can generalize to unseen fonts without new exemplars from them; (ii) it can flexibly change the number of classes, simply by changing the exemplars provided; and (iii) it can generalize to new languages and new characters that it has not been trained for by providing a new glyph set. We show significant improvements over state-of-the-art models for all these cases.",0
"The aim of our work is to tackle the issues of text recognition in documents, specifically regarding generalization and flexibility. Our approach involves the development of a novel model that takes advantage of the repetitive nature of characters in languages. This model separates the visual representation learning and linguistic modelling stages, transforming text recognition into a shape matching problem. As a result, our model achieves generalization in appearance and flexibility in classes. We conducted evaluations on synthetic and real datasets with various alphabets and demonstrated that our model can handle challenges that traditional architectures cannot without costly retraining. These challenges include the ability to generalize to unseen fonts, adjust the number of classes with ease, and recognize new languages and characters by providing a new glyph set. Our results show significant improvements over state-of-the-art models in all these cases.",1
"The contemporary process-aware information systems possess the capabilities to record the activities generated during the process execution. To leverage these process specific fine-granular data, process mining has recently emerged as a promising research discipline. As an important branch of process mining, predictive business process management, pursues the objective to generate forward-looking, predictive insights to shape business processes. In this study, we propose a conceptual framework sought to establish and promote understanding of decision-making environment, underlying business processes and nature of the user characteristics for developing explainable business process prediction solutions. Consequently, with regard to the theoretical and practical implications of the framework, this study proposes a novel local post-hoc explanation approach for a deep learning classifier that is expected to facilitate the domain experts in justifying the model decisions. In contrary to alternative popular perturbation-based local explanation approaches, this study defines the local regions from the validation dataset by using the intermediate latent space representations learned by the deep neural networks. To validate the applicability of the proposed explanation method, the real-life process log data delivered by the Volvo IT Belgium's incident management system are used.The adopted deep learning classifier achieves a good performance with the Area Under the ROC Curve of 0.94. The generated local explanations are also visualized and presented with relevant evaluation measures that are expected to increase the users' trust in the black-box-model.",0
"Recent advancements in process-aware information systems have enabled the recording of activities during process execution, leading to the emergence of process mining as a promising research discipline. Predictive business process management, a branch of process mining, aims to generate predictive insights for shaping business processes. In this study, we propose a conceptual framework to promote understanding of the decision-making environment, business processes, and user characteristics for developing explainable business process prediction solutions. We introduce a novel local post-hoc explanation approach for a deep learning classifier, which defines local regions from the validation dataset using intermediate latent space representations learned by the neural networks. We validate the applicability of the proposed method using real-life process log data from Volvo IT Belgium's incident management system. The adopted deep learning classifier achieves a good performance, with an Area Under the ROC Curve of 0.94, and the generated local explanations are visualized and presented with relevant evaluation measures to increase users' trust in the black-box model.",1
"In this paper, we investigate the suitability of state-of-the-art representation learning methods to the analysis of behavioral similarity of moving individuals, based on CDR trajectories. The core of the contribution is a novel methodological framework, mob2vec, centered on the combined use of a recent symbolic trajectory segmentation method for the removal of noise, a novel trajectory generalization method incorporating behavioral information, and an unsupervised technique for the learning of vector representations from sequential data. Mob2vec is the result of an empirical study conducted on real CDR data through an extensive experimentation. As a result, it is shown that mob2vec generates vector representations of CDR trajectories in low dimensional spaces which preserve the similarity of the mobility behavior of individuals.",0
"The objective of this article is to examine the appropriateness of modern representation learning approaches for analyzing the behavioral similarity of mobile individuals using CDR trajectories. The main contribution is a new methodological framework called mob2vec, which integrates a recent symbolic trajectory segmentation technique to eliminate noise, a new trajectory generalization method that includes behavioral information, and an unsupervised approach for learning vector representations from sequential data. Mob2vec is the outcome of a practical investigation using actual CDR data through extensive experimentation. The findings demonstrate that mob2vec produces vector representations of CDR trajectories in low-dimensional spaces that maintain the similarity of individuals' mobility behavior.",1
"Although supervised deep representation learning has attracted enormous attentions across areas of pattern recognition and computer vision, little progress has been made towards unsupervised deep representation learning for image clustering. In this paper, we propose a deep spectral analysis network for unsupervised representation learning and image clustering. While spectral analysis is established with solid theoretical foundations and has been widely applied to unsupervised data mining, its essential weakness lies in the fact that it is difficult to construct a proper affinity matrix and determine the involving Laplacian matrix for a given dataset. In this paper, we propose a SA-Net to overcome these weaknesses and achieve improved image clustering by extending the spectral analysis procedure into a deep learning framework with multiple layers. The SA-Net has the capability to learn deep representations and reveal deep correlations among data samples. Compared with the existing spectral analysis, the SA-Net achieves two advantages: (i) Given the fact that one spectral analysis procedure can only deal with one subset of the given dataset, our proposed SA-Net elegantly integrates multiple parallel and consecutive spectral analysis procedures together to enable interactive learning across different units towards a coordinated clustering model; (ii) Our SA-Net can identify the local similarities among different images at patch level and hence achieves a higher level of robustness against occlusions. Extensive experiments on a number of popular datasets support that our proposed SA-Net outperforms 11 benchmarks across a number of image clustering applications.",0
"Despite the immense attention that supervised deep representation learning has garnered in the fields of pattern recognition and computer vision, unsupervised deep representation learning for image clustering has not seen much progress. This paper introduces a deep spectral analysis network that aims to overcome the weaknesses of traditional spectral analysis, which struggles to construct a suitable affinity matrix and determine the Laplacian matrix for a given dataset. The proposed SA-Net extends spectral analysis into a deep learning framework with multiple layers, allowing for the learning of deep representations and the identification of deep correlations among data samples. Compared to traditional spectral analysis, SA-Net offers two key advantages: it elegantly integrates multiple parallel and consecutive spectral analysis procedures, enabling interactive learning across different units towards a coordinated clustering model; and it identifies local similarities among different images at the patch level, resulting in greater robustness against occlusions. Experiments on several popular datasets demonstrate that SA-Net outperforms 11 benchmarks across various image clustering applications.",1
"Deep representation learning is a crucial procedure in multimedia analysis and attracts increasing attention. Most of the popular techniques rely on convolutional neural network and require a large amount of labeled data in the training procedure. However, it is time consuming or even impossible to obtain the label information in some tasks due to cost limitation. Thus, it is necessary to develop unsupervised deep representation learning techniques. This paper proposes a new network structure for unsupervised deep representation learning based on spectral analysis, which is a popular technique with solid theory foundations. Compared with the existing spectral analysis methods, the proposed network structure has at least three advantages. Firstly, it can identify the local similarities among images in patch level and thus more robust against occlusion. Secondly, through multiple consecutive spectral analysis procedures, the proposed network can learn more clustering-friendly representations and is capable to reveal the deep correlations among data samples. Thirdly, it can elegantly integrate different spectral analysis procedures, so that each spectral analysis procedure can have their individual strengths in dealing with different data sample distributions. Extensive experimental results show the effectiveness of the proposed methods on various image clustering tasks.",0
"There is a growing interest in deep representation learning for multimedia analysis, which typically relies on convolutional neural networks and large amounts of labeled data for training. However, obtaining label information can be expensive or even impossible for some tasks. As a result, it is important to develop unsupervised deep representation learning techniques. This study proposes a new network structure for unsupervised deep representation learning based on spectral analysis, which has a strong theoretical foundation. Compared to existing spectral analysis methods, the proposed network structure has three main advantages. Firstly, it can identify local image similarities in patch level, making it more robust against occlusion. Secondly, it can learn more clustering-friendly representations through multiple consecutive spectral analysis procedures, revealing deep correlations among data samples. Lastly, it can integrate different spectral analysis procedures seamlessly, allowing each procedure to leverage its strengths in handling various data sample distributions. Extensive experiments demonstrate the effectiveness of this approach in various image clustering tasks.",1
"While supervised deep learning has achieved great success in a range of applications, relatively little work has studied the discovery of knowledge from unlabeled data. In this paper, we propose an unsupervised deep learning framework to provide a potential solution for the problem that existing deep learning techniques require large labeled data sets for completing the training process. Our proposed introduces a new principle of joint learning on both deep representations and GMM (Gaussian Mixture Model)-based deep modeling, and thus an integrated objective function is proposed to facilitate the principle. In comparison with the existing work in similar areas, our objective function has two learning targets, which are created to be jointly optimized to achieve the best possible unsupervised learning and knowledge discovery from unlabeled data sets. While maximizing the first target enables the GMM to achieve the best possible modeling of the data representations and each Gaussian component corresponds to a compact cluster, maximizing the second term will enhance the separability of the Gaussian components and hence the inter-cluster distances. As a result, the compactness of clusters is significantly enhanced by reducing the intra-cluster distances, and the separability is improved by increasing the inter-cluster distances. Extensive experimental results show that the propose method can improve the clustering performance compared with benchmark methods.",0
"Although supervised deep learning has been successful in various applications, there has been limited research on discovering knowledge from unlabeled data. This paper proposes an unsupervised deep learning framework to address the issue that current deep learning methods require extensive labeled data sets for training. Our approach implements joint learning principles for deep representations and GMM-based deep modeling, with an integrated objective function to facilitate this process. In comparison to existing work, our objective function has two learning targets that are jointly optimized to maximize unsupervised learning and knowledge discovery from unlabeled data sets. Maximizing the first target enhances the GMM's modeling of data representations, while the second target improves the separability of Gaussian components, resulting in increased inter-cluster distances and reduced intra-cluster distances. Extensive experiments have shown that our proposed method outperforms benchmark methods in clustering performance.",1
"Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.",0
"To understand surgical scenes in robotic surgery, it's crucial to learn how to infer graph representations and perform spatial reasoning in a complex environment. To achieve this, we've developed an approach that generates a scene graph and predicts surgical interactions between instruments and a surgical region of interest. We've integrated an attention link function and a graph parsing network to recognize surgical interactions and used SageConv to embed each node with corresponding neighboring node features. To extract features from complex image representation, we've employed label smoothing weighted loss to avoid over-confident predictions and enhance feature representation. We've evaluated our approach by annotating the bounding box and instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery.",1
"We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",0
"A new method for self-supervised image representation learning has been developed called Bootstrap Your Own Latent (BYOL). This approach utilizes two neural networks, namely the online and target networks, which collaborate and learn from one another. By augmenting an image, the online network is trained to anticipate the target network representation of the same image under a separate augmented view. Concurrently, the target network is updated with a gradual average of the online network. Without relying on negative pairs, BYOL has attained a new state of the art, achieving $74.3\%$ top-1 classification accuracy on ImageNet through linear evaluation using a ResNet-50 architecture, and $79.6\%$ using a larger ResNet. We have demonstrated that BYOL performs at par or better than the current state of the art on transfer and semi-supervised benchmarks. Our GitHub repository includes the implementation and pretrained models.",1
"Transportation systems often rely on understanding the flow of vehicles or pedestrian. From traffic monitoring at the city scale, to commuters in train terminals, recent progress in sensing technology make it possible to use cameras to better understand the demand, i.e., better track moving agents (e.g., vehicles and pedestrians). Whether the cameras are mounted on drones, vehicles, or fixed in the built environments, they inevitably remain scatter. We need to develop the technology to re-identify the same agents across images captured from non-overlapping field-of-views, referred to as the visual re-identification task. State-of-the-art methods learn a neural network based representation trained with the cross-entropy loss function. We argue that such loss function is not suited for the visual re-identification task hence propose to model confidence in the representation learning framework. We show the impact of our confidence-based learning framework with three methods: label smoothing, confidence penalty, and deep variational information bottleneck. They all show a boost in performance validating our claim. Our contribution is generic to any agent of interest, i.e., vehicles or pedestrians, and outperform highly specialized state-of-the-art methods across 5 datasets. The source code and models are shared towards an open science mission.",0
"To better understand the movement of vehicles and pedestrians in transportation systems, cameras are being used for tracking agents. However, the cameras are dispersed across different locations, making it difficult to identify the same agents across images. This is known as the visual re-identification task. State-of-the-art methods use a neural network based on the cross-entropy loss function, but this is not suitable for the visual re-identification task. We propose a confidence-based learning framework that models confidence in the representation learning process. Our framework includes three methods: label smoothing, confidence penalty, and deep variational information bottleneck. We demonstrate the effectiveness of our approach in improving performance across five datasets for any agent of interest, including vehicles and pedestrians. To further promote open science, we are sharing our source code and models.",1
"In deep representational learning, it is often desired to isolate a particular factor (termed {\em content}) from other factors (referred to as {\em style}). What constitutes the content is typically specified by users through explicit labels in the data, while all unlabeled/unknown factors are regarded as style. Recently, it has been shown that such content-labeled data can be effectively exploited by modifying the deep latent factor models (e.g., VAE) such that the style and content are well separated in the latent representations. However, the approach assumes that the content factor is categorical-valued (e.g., subject ID in face image data, or digit class in the MNIST dataset). In certain situations, the content is ordinal-valued, that is, the values the content factor takes are {\em ordered} rather than categorical, making content-labeled VAEs, including the latent space they infer, suboptimal. In this paper, we propose a novel extension of VAE that imposes a partially ordered set (poset) structure in the content latent space, while simultaneously making it aligned with the ordinal content values. To this end, instead of the iid Gaussian latent prior adopted in prior approaches, we introduce a conditional Gaussian spacing prior model. This model admits a tractable joint Gaussian prior, but also effectively places negligible density values on the content latent configurations that violate the poset constraint. To evaluate this model, we consider two specific ordinal structured problems: estimating a subject's age in a face image and elucidating the calorie amount in a food meal image. We demonstrate significant improvements in content-style separation over previous non-ordinal approaches.",0
"The goal of deep representational learning is often to separate a specific factor known as ""content"" from other factors referred to as ""style."" Typically, users specify what constitutes the content by labeling the data explicitly, while all other factors are considered style. Recent studies have shown that by modifying deep latent factor models, such as VAE, to separate style and content well in latent representations, content-labeled data can be effectively utilized. However, this approach assumes that the content factor is categorical, which is not always the case. In some instances, the content is ordinal-valued, which makes content-labeled VAEs suboptimal. In this paper, we propose a novel extension of VAE that includes a partially ordered set (poset) structure in the content latent space, making it aligned with the ordinal content values. We introduce a conditional Gaussian spacing prior model that effectively places negligible density values on the content latent configurations that violate the poset constraint. To evaluate our model, we apply it to two specific ordinal structured problems: estimating a subject's age in a face image and determining the calorie amount in a food meal image. Our results demonstrate significant improvements in content-style separation compared to previous non-ordinal approaches.",1
"Deep neural networks have shown exceptional learning capability and generalizability in the source domain when massive labeled data is provided. However, the well-trained models often fail in the target domain due to the domain shift. Unsupervised domain adaptation aims to improve network performance when applying robust models trained on medical images from source domains to a new target domain. In this work, we present an approach based on the Wasserstein distance guided disentangled representation to achieve 3D multi-domain liver segmentation. Concretely, we embed images onto a shared content space capturing shared feature-level information across domains and domain-specific appearance spaces. The existing mutual information-based representation learning approaches often fail to capture complete representations in multi-domain medical imaging tasks. To mitigate these issues, we utilize Wasserstein distance to learn more complete representation, and introduces a content discriminator to further facilitate the representation disentanglement. Experiments demonstrate that our method outperforms the state-of-the-art on the multi-modality liver segmentation task.",0
"When provided with extensive labeled data, deep neural networks have demonstrated impressive learning and generalization abilities in the source domain. However, these well-trained models often struggle in the target domain due to domain shift. Unsupervised domain adaptation is a method that aims to enhance network performance by using robust models trained on medical images from source domains in a new target domain. In this study, we introduce an approach that employs disentangled representation guided by the Wasserstein distance to achieve 3D multi-domain liver segmentation. Specifically, we embed images onto a shared content space that captures shared feature-level information across domains, as well as domain-specific appearance spaces. Current mutual information-based representation learning methods often fail to generate complete representations in multi-domain medical imaging tasks. To address these challenges, we employ the Wasserstein distance to develop a more complete representation and introduce a content discriminator to further facilitate representation disentanglement. Our experiments demonstrate that our approach outperforms the state-of-the-art in multi-modality liver segmentation tasks.",1
"Graph neural networks (GNN), as a popular methodology for node representation learning on graphs, currently mainly focus on preserving the smoothness and identifiability of node representations. A robust node representation on graphs should further hold the stability property which means a node representation is resistant to slight perturbations on the input. In this paper, we introduce the stability of node representations in addition to the smoothness and identifiability, and develop a novel method called contrastive graph neural networks (CGNN) that learns robust node representations in an unsupervised manner. Specifically, CGNN maintains the stability and identifiability by a contrastive learning objective, while preserving the smoothness with existing GNN models. Furthermore, the proposed method is a generic framework that can be equipped with many other backbone models (e.g. GCN, GraphSage and GAT). Extensive experiments on four benchmarks under both transductive and inductive learning setups demonstrate the effectiveness of our method in comparison with recent supervised and unsupervised models.",0
"Currently, Graph neural networks (GNN) are widely used to learn node representations on graphs, with a focus on preserving smoothness and identifiability. However, a robust node representation should also possess stability, meaning it must be resistant to input perturbations. Our paper introduces the concept of stability to node representations and proposes a novel unsupervised method called contrastive graph neural networks (CGNN) to learn robust node representations. The CGNN approach maintains stability and identifiability through contrastive learning objectives, while preserving smoothness using existing GNN models. Additionally, our method is a versatile framework that can be combined with various backbone models such as GCN, GraphSage, and GAT. Our extensive experiments on four benchmarks demonstrate the effectiveness of our approach in both transductive and inductive learning settings, outperforming recent supervised and unsupervised models.",1
"Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity-aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.",0
"Graph neural networks (GNNs) are a type of machine learning model designed for graphs. One of the reasons existing GNNs are effective is because the node representations they learn are permutation-equivariant, meaning they are able to maintain order regardless of how the nodes are arranged. However, this property can also prevent GNNs from being proximity-aware, which is important for analyzing graphs. Some variants of GNNs have been developed to preserve node proximities, but they do not maintain permutation-equivariance. The challenge is to create a GNN that is both proximity-aware and permutation-equivariant. This paper proposes Stochastic Message Passing (SMP), a GNN that preserves both properties by using stochastic node representations that learn to preserve node proximities. The authors prove that this approach can maintain both properties under certain conditions, and their experiments show that SMP is effective for tasks like node classification and link prediction.",1
"Finding general evaluation metrics for unsupervised representation learning techniques is a challenging open research question, which recently has become more and more necessary due to the increasing interest in unsupervised methods. Even though these methods promise beneficial representation characteristics, most approaches currently suffer from the objective function mismatch. This mismatch states that the performance on a desired target task can decrease when the unsupervised pretext task is learned too long - especially when both tasks are ill-posed. In this work, we build upon the widely used linear evaluation protocol and define new general evaluation metrics to quantitatively capture the objective function mismatch and the more generic metrics mismatch. We discuss the usability and stability of our protocols on a variety of pretext and target tasks and study mismatches in a wide range of experiments. Thereby we disclose dependencies of the objective function mismatch across several pretext and target tasks with respect to the pretext model's representation size, target model complexity, pretext and target augmentations as well as pretext and target task types.",0
"It is a difficult challenge to find universal metrics for assessing unsupervised representation learning methods, which has become increasingly urgent as the demand for such methods grows. Despite the potential benefits of these techniques, many current approaches suffer from a mismatch between the objective of the unsupervised pretext task and the desired target task. If the pretext task is learned for too long, particularly when both tasks are ill-defined, performance on the target task may suffer. In this study, we expand upon the widely-used linear evaluation protocol and develop new metrics to quantitatively measure both the objective function mismatch and the more general metrics mismatch. We test the usability and stability of our protocols across a variety of pretext and target tasks, and examine mismatches in a range of experiments. Our results reveal the dependence of the objective function mismatch on several factors, including the representation size of the pretext model, the complexity of the target model, and the types of augmentations used for both pretext and target tasks.",1
"In this paper, we introduce the MLM (Multiple Languages and Modalities) dataset - a new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. The generation process and inclusion of semantic data provide a resource that further tests the ability for multitask systems to learn relationships between entities. The dataset is designed for researchers and developers who build applications that perform multiple tasks on data encountered on the web and in digital archives. A second version of MLM provides a geo-representative subset of the data with weighted samples for countries of the European Union. We demonstrate the value of the resource in developing novel applications in the digital humanities with a motivating use case and specify a benchmark set of tasks to retrieve modalities and locate entities in the dataset. Evaluation of baseline multitask and single task systems on the full and geo-representative versions of MLM demonstrate the challenges of generalising on diverse data. In addition to the digital humanities, we expect the resource to contribute to research in multimodal representation learning, location estimation, and scene understanding.",0
"The purpose of this paper is to present the MLM (Multiple Languages and Modalities) dataset, which is a new tool that can be used to train and evaluate multitask systems on samples in multiple modalities and three languages. This dataset includes semantic data and is intended for use by researchers and developers who are building applications that perform multiple tasks on data from the web and digital archives. A subset of this dataset, called MLM 2.0, contains geo-representative samples from countries in the European Union. We demonstrate the usefulness of this resource by presenting a motivating use case and a set of benchmark tasks that can be used to retrieve modalities and locate entities in the dataset. We evaluate the performance of baseline multitask and single task systems on both the full and geo-representative versions of MLM, and we highlight the challenges of generalising on diverse data. This dataset has potential applications in the digital humanities, multimodal representation learning, location estimation, and scene understanding.",1
"Graphs or networks are a very convenient way to represent data with lots of interaction. Recently, Machine Learning on Graph data has gained a lot of traction. In particular, vertex classification and missing edge detection have very interesting applications, ranging from drug discovery to recommender systems. To achieve such tasks, tremendous work has been accomplished to learn embedding of nodes and edges into finite-dimension vector spaces. This task is called Graph Representation Learning. However, Graph Representation Learning techniques often display prohibitive time and memory complexities, preventing their use in real-time with business size graphs. In this paper, we address this issue by leveraging a degeneracy property of Graphs - the K-Core Decomposition. We present two techniques taking advantage of this decomposition to reduce the time and memory consumption of walk-based Graph Representation Learning algorithms. We evaluate the performances, expressed in terms of quality of embedding and computational resources, of the proposed techniques on several academic datasets. Our code is available at https://github.com/SBrandeis/kcore-embedding",0
"Representing data with multiple interactions can be done conveniently through graphs or networks. In recent times, there has been a lot of interest in Machine Learning on Graph data, especially in vertex classification and missing edge detection. These techniques have various applications, ranging from drug discovery to recommender systems. However, Graph Representation Learning techniques are time and memory-consuming, making them unsuitable for real-time use with business-sized graphs. In this study, we propose two techniques that leverage the K-Core Decomposition, a degeneracy property of Graphs, to reduce the time and memory consumption of walk-based Graph Representation Learning algorithms. We evaluate the proposed techniques' performance in terms of computational resources and quality of embedding on academic datasets. The code is available at https://github.com/SBrandeis/kcore-embedding.",1
"This paper addresses the problem of self-supervised video representation learning from a new perspective -- by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.",0
"This article takes a fresh approach to the issue of self-supervised video representation learning, focusing on predicting video pace. The idea is based on the observation that people are sensitive to video pace, such as slow motion, which is a popular technique in filmmaking. To train a neural network to understand the underlying video content and learn representative spatio-temporal features, the authors randomly select video clips at different paces and ask the network to identify the pace for each clip. They assume that the network can only perform this task successfully if it has learned the necessary features. They also use contrastive learning to help the model distinguish between different paces by maximizing agreement on similar content. The authors conducted extensive experiments on action recognition and video retrieval tasks with various network architectures, and their approach achieved state-of-the-art performance. The code and pre-trained models can be found at https://github.com/laura-wang/video-pace.",1
"In discrete choice modeling (DCM), model misspecifications may lead to limited predictability and biased parameter estimates. In this paper, we propose a new approach for estimating choice models in which we divide the systematic part of the utility specification into (i) a knowledge-driven part, and (ii) a data-driven one, which learns a new representation from available explanatory variables. Our formulation increases the predictive power of standard DCM without sacrificing their interpretability. We show the effectiveness of our formulation by augmenting the utility specification of the Multinomial Logit (MNL) and the Nested Logit (NL) models with a new non-linear representation arising from a Neural Network (NN), leading to new choice models referred to as the Learning Multinomial Logit (L-MNL) and Learning Nested Logit (L-NL) models. Using multiple publicly available datasets based on revealed and stated preferences, we show that our models outperform the traditional ones, both in terms of predictive performance and accuracy in parameter estimation. All source code of the models are shared to promote open science.",0
"The limited predictability and biased parameter estimates resulting from model misspecifications in discrete choice modeling (DCM) can be addressed through a novel approach proposed in this paper. Our approach involves dividing the systematic part of the utility specification into two parts: a knowledge-driven component and a data-driven one that learns from available explanatory variables. This formulation enhances the predictive ability of standard DCM while maintaining interpretability. We demonstrate the effectiveness of this approach by incorporating a non-linear representation from a Neural Network (NN) into the utility specification of the Multinomial Logit (MNL) and the Nested Logit (NL) models, resulting in new models referred to as the Learning Multinomial Logit (L-MNL) and Learning Nested Logit (L-NL) models. Using various publicly available datasets, we demonstrate that our models perform better than traditional models in terms of predictive accuracy and parameter estimation. We have shared the source code for our models to promote open science.",1
"Representation learning on networks offers a powerful alternative to the oft painstaking process of manual feature engineering, and as a result, has enjoyed considerable success in recent years. However, all the existing representation learning methods are based on the first-order network (FON), that is, the network that only captures the pairwise interactions between the nodes. As a result, these methods may fail to incorporate non-Markovian higher-order dependencies in the network. Thus, the embeddings that are generated may not accurately represent of the underlying phenomena in a network, resulting in inferior performance in different inductive or transductive learning tasks. To address this challenge, this paper presents HONEM, a higher-order network embedding method that captures the non-Markovian higher-order dependencies in a network. HONEM is specifically designed for the higher-order network structure (HON) and outperforms other state-of-the-art methods in node classification, network re-construction, link prediction, and visualization for networks that contain non-Markovian higher-order dependencies.",0
"The process of manual feature engineering can be time-consuming and challenging, making representation learning on networks an attractive alternative that has achieved significant success in recent years. However, current representation learning methods are exclusively based on the first-order network (FON), which only considers pairwise interactions between nodes. This approach may overlook non-Markovian higher-order dependencies within the network, leading to inaccurate embeddings and subpar performance in inductive or transductive learning tasks. To combat this issue, the paper introduces HONEM, a higher-order network embedding method that effectively captures non-Markovian higher-order dependencies within a network. HONEM is specifically tailored to the higher-order network structure (HON) and surpasses other cutting-edge methods in tasks such as node classification, network reconstruction, link prediction, and visualization for networks that contain non-Markovian higher-order dependencies.",1
"Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous graph neural networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world graph data. In this paper, we present a novel cluster-aware graph neural network (CAGNN) model for unsupervised graph representation learning using self-supervised techniques. In CAGNN, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. We further refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over existing baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts.",0
"The goal of unsupervised graph representation learning is to acquire node embeddings that are low-dimensional and preserve both the topological structure and the node attributive features of the graph, without any supervision. Existing graph neural networks (GNN) require a large number of labeled nodes, which may not be available in real-world graph data. In this paper, we propose a new cluster-aware graph neural network (CAGNN) that uses self-supervised techniques for unsupervised graph representation learning. CAGNN clusters the node embeddings and updates the model parameters by predicting the cluster assignments. Additionally, we have observed that graphs frequently contain inter-class edges, which can cause GNN models to aggregate noisy information from neighboring nodes. We further refine the graph topology by enhancing intra-class edges and reducing node connections between different classes based on cluster labels, which more effectively preserves the cluster structures in the embedding space. We have conducted extensive experiments on two benchmark tasks using real-world datasets. The results show that the proposed model performs better than existing baseline methods. Notably, our model achieves more than a 7% improvement in accuracy on node clustering compared to state-of-the-art methods.",1
"Joint image-text embedding extracted from medical images and associated contextual reports is the bedrock for most biomedical vision-and-language (V+L) tasks, including medical visual question answering, clinical image-text retrieval, clinical report auto-generation. In this study, we adopt four pre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn multimodal representation from MIMIC-CXR radiographs and associated reports. The extrinsic evaluation on OpenI dataset shows that in comparison to the pioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models demonstrate performance improvement in the thoracic findings classification task. We conduct an ablation study to analyze the contribution of certain model components and validate the advantage of joint embedding over text-only embedding. We also visualize attention maps to illustrate the attention mechanism of V+L models.",0
"Most biomedical vision-and-language (V+L) tasks rely on joint image-text embedding obtained from medical images and their contextual reports. These tasks include medical visual question answering, clinical image-text retrieval, and clinical report auto-generation. In our research, we utilized four pre-trained V+L models (LXMERT, VisualBERT, UNIER, and PixelBERT) to learn multimodal representation from MIMIC-CXR radiographs and their associated reports. Our evaluation on OpenI dataset revealed that the joint embedding learned by these models yielded better performance than the pioneering CNN-RNN model in the thoracic findings classification task. We conducted an ablation study to analyze the contribution of certain model components and validated the benefit of joint embedding over text-only embedding. Additionally, we employed attention maps to visualize the attention mechanism of V+L models.",1
"Unsupervised learning methods for feature extraction are becoming more and more popular. We combine the popular contrastive learning method (prototypical contrastive learning) and the classic representation learning method (autoencoder) to design an unsupervised feature learning network for hyperspectral classification. Experiments have proved that our two proposed autoencoder networks have good feature learning capabilities by themselves, and the contrastive learning network we designed can better combine the features of the two to learn more representative features. As a result, our method surpasses other comparison methods in the hyperspectral classification experiments, including some supervised methods. Moreover, our method maintains a fast feature extraction speed than baseline methods. In addition, our method reduces the requirements for huge computing resources, separates feature extraction and contrastive learning, and allows more researchers to conduct research and experiments on unsupervised contrastive learning.",0
"The popularity of unsupervised learning methods for feature extraction is on the rise. Our approach involves combining the well-known prototypical contrastive learning method with the classic autoencoder representation learning method to create an unsupervised feature learning network for hyperspectral classification. Our experiments have shown that our autoencoder networks have strong feature learning capabilities on their own, and when combined with the contrastive learning network we designed, they produce more representative features. Our method outperforms other comparison methods, including some supervised methods, in the hyperspectral classification experiments, while maintaining a faster feature extraction speed than baseline methods. Additionally, our method reduces the need for extensive computing resources, separates feature extraction and contrastive learning, and enables more researchers to explore unsupervised contrastive learning through research and experiments.",1
"Due to the hierarchical structure of many machine learning problems, bilevel programming is becoming more and more important recently, however, the complicated correlation between the inner and outer problem makes it extremely challenging to solve. Although several intuitive algorithms based on the automatic differentiation have been proposed and obtained success in some applications, not much attention has been paid to finding the optimal formulation of the bilevel model. Whether there exists a better formulation is still an open problem. In this paper, we propose an improved bilevel model which converges faster and better compared to the current formulation. We provide theoretical guarantee and evaluation results over two tasks: Data Hyper-Cleaning and Hyper Representation Learning. The empirical results show that our model outperforms the current bilevel model with a great margin. \emph{This is a concurrent work with \citet{liu2020generic} and we submitted to ICML 2020. Now we put it on the arxiv for record.}",0
"Bilevel programming has gained importance in machine learning due to the hierarchical structure of many problems. However, the complex relationship between the inner and outer problem poses a significant challenge for solving these problems. Although intuitive algorithms based on automatic differentiation have been successful in some applications, little attention has been paid to optimizing the bilevel model's formulation. This paper proposes an improved bilevel model that exhibits faster and better convergence compared to the current formulation. Theoretical guarantees and evaluation results over two tasks, Data Hyper-Cleaning and Hyper Representation Learning, demonstrate the superiority of our model over the current bilevel model. This work is concurrent with \citet{liu2020generic} and is submitted to ICML 2020. We present it on arxiv for record.",1
"Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.",0
"The study of representation learning on graph structure data has been widely explored due to its vast potential for various applications. However, most previous methods have focused on static graphs, whereas many real-world graphs undergo changes over time. Thus, it is crucial to model such changes to predict the properties of unseen networks. To address this challenge, our proposed solution is SGRNN, a novel neural architecture that utilizes stochastic latent variables to capture the evolution of both node attributes and topology simultaneously. Our method separates deterministic states from stochastic states during the iterative process to avoid interference. Semi-implicit variational inference is integrated into SGRNN, and a non-Gaussian variational distribution is proposed to enhance its performance. Moreover, we propose a simple and interpretable structure to mitigate the KL-vanishing problem in SGRNN, based on the lower bound of KL-divergence. We conducted extensive experiments on real-world datasets to demonstrate the effectiveness of our proposed model, and the code is available at https://github.com/StochasticGRNN/SGRNN.",1
"Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.",0
"In tasks where the fundamental component is a group, set, or sequence, the learning of group representation is a prevalent concern. In the past, researchers have attempted to address this issue by combining the elements in a group based on an indicator. This indicator could either be defined by humans, such as quality and saliency, or generated by a black box, such as an attention score. However, this article offers a more fundamental and understandable perspective. We argue that the most significant indicator for determining whether the group representation can benefit from one of its elements is not the quality or an obscure score, but rather the discriminability with respect to the model. We explicitly design this discriminability using embedded class centroids on a proxy set. We demonstrate that this discriminability knowledge can be distilled by a light-weight distillation network and can be generalized on the unseen target set. This process is referred to as discriminability distillation learning (DDL). DDL can be easily integrated into many group-based recognition tasks without affecting the original training procedures. Extensive experiments on various tasks have confirmed the effectiveness of DDL in terms of both accuracy and efficiency. Additionally, it has significantly advanced the state-of-the-art results on these tasks.",1
"Person re-identification (re-id) aims to match the same person from images taken across multiple cameras. Most existing person re-id methods generally require a large amount of identity labeled data to act as discriminative guideline for representation learning. Difficulty in manually collecting identity labeled data leads to poor adaptability in practical scenarios. To overcome this problem, we propose an unsupervised center-based clustering approach capable of progressively learning and exploiting the underlying re-id discriminative information from temporal continuity within a camera. We call our framework Temporal Continuity based Unsupervised Learning (TCUL). Specifically, TCUL simultaneously does center based clustering of unlabeled (target) dataset and fine-tunes a convolutional neural network (CNN) pre-trained on irrelevant labeled (source) dataset to enhance discriminative capability of the CNN for the target dataset. Furthermore, it exploits temporally continuous nature of images within-camera jointly with spatial similarity of feature maps across-cameras to generate reliable pseudo-labels for training a re-identification model. As the training progresses, number of reliable samples keep on growing adaptively which in turn boosts representation ability of the CNN. Extensive experiments on three large-scale person re-id benchmark datasets are conducted to compare our framework with state-of-the-art techniques, which demonstrate superiority of TCUL over existing methods.",0
"The objective of person re-identification (re-id) is to match the same individual across multiple cameras using images. However, current person re-id methods require a significant amount of identity labeled data for discriminating representation learning, which is challenging to obtain manually, resulting in poor adaptability in practical scenarios. To resolve this limitation, we present an unsupervised center-based clustering method called Temporal Continuity based Unsupervised Learning (TCUL), which progressively learns and exploits re-id discriminative information from temporal continuity within a camera. TCUL conducts center based clustering of unlabeled (target) datasets while fine-tuning a convolutional neural network (CNN) pre-trained on irrelevant labeled (source) datasets to improve its discriminative ability for the target dataset. Additionally, it leverages the temporal continuity of images within-camera and spatial similarity of feature maps across-cameras to generate reliable pseudo-labels for training a re-identification model. The number of reliable samples increases adaptively during training, which enhances the CNN's representation ability. We conducted extensive experiments on three large-scale person re-id benchmark datasets to compare our framework with state-of-the-art techniques, and the results showed that TCUL outperformed existing methods.",1
"In this paper, we show that the performance of a learnt generative model is closely related to the model's ability to accurately represent the inferred \textbf{latent data distribution}, i.e. its topology and structural properties. We propose LaDDer to achieve accurate modelling of the latent data distribution in a variational autoencoder framework and to facilitate better representation learning. The central idea of LaDDer is a meta-embedding concept, which uses multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all the parameters in a unified variational framework. From extensive experiments, we show that our LaDDer model is able to accurately estimate complex latent distribution and results in improvement in the representation quality. We also propose a novel latent space interpolation method that utilises the derived data distribution.",0
"This paper demonstrates the importance of accurately representing the inferred latent data distribution in a learnt generative model to achieve optimal performance. To improve representation learning, we introduce LaDDer, a method that employs a meta-embedding concept in a variational autoencoder framework to achieve accurate modelling of the latent data distribution. Our approach involves using multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all parameters in a unified variational framework. Our experiments show that LaDDer accurately estimates complex latent distributions, resulting in improved representation quality. Additionally, we introduce a novel latent space interpolation method that utilizes the derived data distribution.",1
"Apart from discriminative models for classification and object detection tasks, the application of deep convolutional neural networks to basic research utilizing natural imaging data has been somewhat limited; particularly in cases where a set of interpretable features for downstream analysis is needed, a key requirement for many scientific investigations. We present an algorithm and training paradigm designed specifically to address this: decontextualized hierarchical representation learning (DHRL). By combining a generative model chaining procedure with a ladder network architecture and latent space regularization for inference, DHRL address the limitations of small datasets and encourages a disentangled set of hierarchically organized features. In addition to providing a tractable path for analyzing complex hierarchal patterns using variation inference, this approach is generative and can be directly combined with empirical and theoretical approaches. To highlight the extensibility and usefulness of DHRL, we demonstrate this method in application to a question from evolutionary biology.",0
"The use of deep convolutional neural networks in natural imaging data research has been restricted to discriminative models for classification and object detection tasks. This is particularly evident in situations where interpretable features are required for downstream analysis, which is essential for many scientific investigations. To overcome this limitation, we introduce a training paradigm and algorithm called decontextualized hierarchical representation learning (DHRL). DHRL employs a generative model chaining procedure, ladder network architecture, and latent space regularization for inference. This approach addresses the challenges of limited datasets and encourages a set of hierarchically organized features that are disentangled. DHRL provides a tractable path for analyzing complex hierarchical patterns using variation inference and is generative, making it compatible with empirical and theoretical approaches. To demonstrate the versatility and usefulness of DHRL, we apply it to a question in evolutionary biology.",1
"Recently, the surge in popularity of Internet of Things (IoT), mobile devices, social media, etc. has opened up a large source for graph data. Graph embedding has been proved extremely useful to learn low-dimensional feature representations from graph structured data. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy to prevent inference attacks. That is, adversaries can infer users' sensitive information by analyzing node representations learned from graph embedding algorithms. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. The proposed method preserves the structural information and utility attributes of a graph while concealing users' private attributes from inference attacks. Extensive experiments on real-world graph datasets demonstrate the superior performance of APGE compared to the state-of-the-arts. Our source code can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",0
"The rise of Internet of Things (IoT), mobile devices, social media, and related technologies has led to a wealth of graph data. Graph embedding is a valuable tool for generating low-dimensional feature representations from such data, enabling prediction tasks ranging from node classification to link prediction. However, current graph embedding methods fail to safeguard users' privacy against inference attacks, which exploit node representations to extract sensitive information. This paper proposes Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that employs disentangling and purging mechanisms to ensure that learned node representations do not reveal private information. APGE maintains graph structure and utility attributes while also concealing users' private attributes from inference attacks. Real-world graph datasets were used to demonstrate the superior performance of APGE relative to existing methods. The source code for APGE is available at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",1
"Large scale analysis of source code, and in particular scientific source code, holds the promise of better understanding the data science process, identifying analytical best practices, and providing insights to the builders of scientific toolkits. However, large corpora have remained unanalyzed in depth, as descriptive labels are absent and require expert domain knowledge to generate. We propose a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments. We then evaluate the model on a new classification task for labeling computational notebook cells as stages in the data analysis process from data import to wrangling, exploration, modeling, and evaluation. We show that our model, leveraging only easily-available weak supervision, achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines. Our model enables us to examine a set of 118,000 Jupyter Notebooks to uncover common data analysis patterns. Focusing on notebooks with relationships to academic articles, we conduct the largest ever study of scientific code and find that notebook composition correlates with the citation count of corresponding papers.",0
"The analysis of source code, especially scientific code, on a large scale, has the potential to improve the understanding of the data science process, identify analytical best practices, and offer insights to the creators of scientific toolkits. However, due to the absence of descriptive labels and the need for expert domain knowledge to generate them, large datasets have not been thoroughly analyzed. To address this issue, we introduce a new transformer-based architecture that uses weak supervision to compute joint representations of code from abstract syntax trees and natural language comments. Furthermore, we evaluate the model by classifying computational notebook cells as stages in the data analysis process, achieving a 38% increase in accuracy over expert-supplied heuristics and outperforming a suite of baselines. By examining 118,000 Jupyter Notebooks, we uncover common data analysis patterns and conduct the largest study of scientific code to date. We also find that the composition of notebooks correlates with the citation count of corresponding papers, particularly those related to academic articles.",1
"Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.",0
"Tasks involving visual reasoning, such as visual question answering (VQA), require a combination of visual perception and reasoning about the meaning of the question based on perception. Despite recent progress in this area, improvements have primarily focused on perception (e.g. generating scene graphs) rather than reasoning. While neuro-symbolic models like Neural Module Networks bring compositional reasoning to VQA, they are still tied to visual representation learning, making it difficult to improve and evaluate neural reasoning independently. To tackle this issue, we propose two solutions: (1) a framework to isolate and assess the reasoning aspect of VQA separately from perception, and (2) a new top-down calibration technique that enables the model to answer reasoning questions even with imperfect perception. We introduce a differentiable first-order logic formalism for VQA that explicitly separates question answering from visual perception. Using the challenging GQA dataset, we conduct detailed, disentangled comparisons of well-known VQA models, providing valuable insights into both the models and the task.",1
"High-dimensional latent representations learned by neural network classifiers are notoriously hard to interpret. Especially in medical applications, model developers and domain experts desire a better understanding of how these latent representations relate to the resulting classification performance. We present Projective Latent Interventions (PLIs), a technique for retraining classifiers by back-propagating manual changes made to low-dimensional embeddings of the latent space. The back-propagation is based on parametric approximations of t-distributed stochastic neighbourhood embeddings. PLIs allow domain experts to control the latent decision space in an intuitive way in order to better match their expectations. For instance, the performance for specific pairs of classes can be enhanced by manually separating the class clusters in the embedding. We evaluate our technique on a real-world scenario in fetal ultrasound imaging.",0
"Interpreting high-dimensional latent representations learned by neural network classifiers is challenging, particularly in medical applications where developers and experts seek to understand their connection to classification performance. To address this issue, we introduce Projective Latent Interventions (PLIs), a method for retraining classifiers by modifying low-dimensional embeddings of the latent space via back-propagation. PLIs rely on parametric approximations of t-distributed stochastic neighbourhood embeddings and enable domain experts to intuitively control the latent decision space to better align with their expectations. For instance, enhancing performance for specific class pairs is achievable by manually separating the class clusters in the embedding. We assess PLIs in a fetal ultrasound imaging setting.",1
"Federated learning allows many parties to collaboratively build a model without exposing data. Particularly, vertical federated learning (VFL) enables parties to build a robust shared machine learning model based upon distributed features about the same samples. However, VFL requires all parties to share a sufficient amount of overlapping samples. In reality, the set of overlapping samples may be small, leaving the majority of the non-overlapping data unutilized. In this paper, we propose Federated Multi-View Training (FedMVT), a semi-supervised learning approach that improves the performance of VFL with limited overlapping samples. FedMVT estimates representations for missing features and predicts pseudo-labels for unlabeled samples to expand training set, and trains three classifiers jointly based upon different views of the input to improve model's representation learning. FedMVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on the NUS-WIDE and the CIFAR10. The experimental results demonstrate that FedMVT significantly outperforms vanilla VFL that only utilizes overlapping samples, and improves the performance of the local model in the party that owns labels.",0
"Federated learning is a collaborative method of building a model without revealing data. Vertical federated learning (VFL) allows parties to construct a strong shared machine learning model using distributed features about the same samples. However, VFL necessitates that all parties share enough overlapping samples, which may not be the case in practice, leaving the majority of non-overlapping data unutilized. In the present study, we propose Federated Multi-View Training (FedMVT), a semi-supervised learning technique that enhances the performance of VFL when there are limited overlapping samples. FedMVT predicts pseudo-labels for unlabeled samples and estimates representations for missing features, expanding the training set. It trains three classifiers jointly based on different input views to improve the model's representation learning. FedMVT preserves data privacy by not requiring parties to share original data and model parameters. We conducted experiments on NUS-WIDE and CIFAR10, and the results show that FedMVT outperforms vanilla VFL, which only utilizes overlapping samples, and improves the performance of the local model in the party that owns labels.",1
"With recent developments in smart technologies, there has been a growing focus on the use of artificial intelligence and machine learning for affective computing to further enhance the user experience through emotion recognition. Typically, machine learning models used for affective computing are trained using manually extracted features from biological signals. Such features may not generalize well for large datasets and may be sub-optimal in capturing the information from the raw input data. One approach to address this issue is to use fully supervised deep learning methods to learn latent representations of the biosignals. However, this method requires human supervision to label the data, which may be unavailable or difficult to obtain. In this work we propose an unsupervised framework reduce the reliance on human supervision. The proposed framework utilizes two stacked convolutional autoencoders to learn latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals. These representations are utilized within a random forest model for binary arousal classification. This approach reduces human supervision and enables the aggregation of datasets allowing for higher generalizability. To validate this framework, an aggregated dataset comprised of the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets is created. The results of our proposed method are compared with using convolutional neural networks, as well as methods that employ manual extraction of hand-crafted features. The methodology used for fusing the two modalities is also investigated. Lastly, we show that our method outperforms current state-of-the-art results that have performed arousal detection on the same datasets using ECG and EDA biosignals. The results show the wide-spread applicability for stacked convolutional autoencoders to be used with machine learning for affective computing.",0
"The use of artificial intelligence and machine learning for affective computing has gained attention due to advancements in smart technologies that aim to enhance the user experience through emotion recognition. However, the typical approach of training machine learning models for affective computing using manually extracted features from biological signals may not be optimal for capturing information from raw input data. To address this, fully supervised deep learning methods have been used, but these require human supervision for labeling data, which may not always be available. This work proposes an unsupervised framework that relies less on human supervision by utilizing stacked convolutional autoencoders to learn latent representations from wearable ECG and EDA signals. The resulting representations are used in a random forest model for binary arousal classification, allowing for the aggregation of datasets and higher generalizability. The proposed method is evaluated by creating an aggregated dataset and comparing its results with other methods. The study shows that stacked convolutional autoencoders outperform current state-of-the-art methods and can be widely applied in affective computing.",1
"The drug discovery stage is a vital aspect of the drug development process and forms part of the initial stages of the development pipeline. In recent times, machine learning-based methods are actively being used to model drug-target interactions for rational drug discovery due to the successful application of these methods in other domains. In machine learning approaches, the numerical representation of molecules is critical to the performance of the model. While significant progress has been made in molecular representation engineering, this has resulted in several descriptors for both targets and compounds. Also, the interpretability of model predictions is a vital feature that could have several pharmacological applications. In this study, we propose a self-attention-based multi-view representation learning approach for modeling drug-target interactions. We evaluated our approach using three benchmark kinase datasets and compared the proposed method to some baseline models. Our experimental results demonstrate the ability of our method to achieve competitive prediction performance and offer biologically plausible drug-target interaction interpretations.",0
"The drug discovery phase is a crucial element of drug development and constitutes the early stages of the development process. Machine learning-based techniques have gained popularity in recent times for rational drug discovery, owing to their successful implementation in other fields. The numerical representation of molecules plays a pivotal role in machine learning approaches, and while molecular representation engineering has made significant progress, it has resulted in numerous descriptors for both targets and compounds. It is also essential to have interpretable model predictions for various pharmacological applications. This study proposes a self-attention-based multi-view representation learning approach for modeling drug-target interactions. The proposed method was evaluated on three benchmark kinase datasets, and its performance was compared to some baseline models. Our experimental results demonstrate that the proposed approach achieves competitive prediction performance and provides biologically plausible drug-target interaction interpretations.",1
"A major endeavor of computer vision is to represent, understand and extract structure from 3D data. Towards this goal, unsupervised learning is a powerful and necessary tool. Most current unsupervised methods for 3D shape analysis use datasets that are aligned, require objects to be reconstructed and suffer from deteriorated performance on downstream tasks. To solve these issues, we propose to extend the InfoMax and contrastive learning principles on 3D shapes. We show that we can maximize the mutual information between 3D objects and their ""chunks"" to improve the representations in aligned datasets. Furthermore, we can achieve rotation invariance in SO${(3)}$ group by maximizing the mutual information between the 3D objects and their geometric transformed versions. Finally, we conduct several experiments such as clustering, transfer learning, shape retrieval, and achieve state of art results.",0
"A crucial objective in computer vision is to comprehend, depict, and derive structure from 3D information. To accomplish this, unsupervised learning is an indispensable and potent tool. However, current unsupervised approaches for 3D shape analysis rely on aligned datasets, necessitate object reconstruction, and exhibit reduced performance on downstream tasks. To tackle these challenges, we suggest expanding the InfoMax and contrastive learning principles to 3D shapes. Our study demonstrates that maximizing the mutual information between 3D objects and their ""chunks"" boosts representations in aligned datasets. Additionally, we can obtain rotation invariance in the SO${(3)}$ group by maximizing the mutual information between 3D objects and their geometrically transformed versions. Finally, we perform various experiments, including clustering, transfer learning, and shape retrieval, resulting in state-of-the-art outcomes.",1
"We propose and demonstrate a novel machine learning algorithm that assesses pulmonary edema severity from chest radiographs. While large publicly available datasets of chest radiographs and free-text radiology reports exist, only limited numerical edema severity labels can be extracted from radiology reports. This is a significant challenge in learning such models for image classification. To take advantage of the rich information present in the radiology reports, we develop a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs at inference time. Our experimental results suggest that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained on images only. We also show the use of the text for explaining the image classification by the joint model. To the best of our knowledge, our approach is the first to leverage free-text radiology reports for improving the image model performance in this application. Our code is available at https://github.com/RayRuizhiLiao/joint_chestxray.",0
"Our team has introduced a new machine learning algorithm that evaluates the severity of pulmonary edema based on chest radiographs. Despite the existence of large datasets of chest radiographs and radiology reports, only a limited amount of numerical edema severity labels can be extracted from these reports, making it difficult to train image classification models. To overcome this challenge, we have developed a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs. Our results show that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained solely on images. We have also demonstrated how the use of text can explain the image classification of the joint model. Our approach is the first to utilize free-text radiology reports to enhance the image model performance in this application. Interested parties can access our code on https://github.com/RayRuizhiLiao/joint_chestxray.",1
"Learning about many things can provide numerous benefits to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience---how to adapt the learning system's behavior---to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.",0
"Numerous benefits can be gained by a reinforcement learning system through the acquisition of knowledge about many things. For instance, learning several auxiliary value functions alongside optimizing the environmental reward seems to enhance both exploration and representation learning. The aim of this paper is to determine how to shape the learning system's behavior to optimize the learning of a value function collection. One simple approach is to evaluate an intrinsic reward based on each auxiliary learner's statistics, and subsequently utilize reinforcement learning to maximize the intrinsic reward. However, this approach has been the subject of several decades of research due to the challenges of its implementation. It remains unclear which learning measures would be effective in a setting where environmental reward is sparse or nonexistent. In this paper, we evaluate and compare various intrinsic reward mechanisms within a new bandit-like parallel-learning testbed. We investigate the interaction between reward and prediction learners and highlight the significance of introspective prediction learners who increase their learning rate when progress is feasible and decrease it when it is not. We conduct an extensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our findings indicate that intrinsic rewards based on the amount of learning can produce useful behavior if each individual learner is introspective, thereby highlighting a simple yet potent principle.",1
"In recent years, deep learning-based feature representation methods have shown a promising impact in electroencephalography (EEG)-based brain-computer interface (BCI). Nonetheless, owing to high intra- and inter-subject variabilities, many studies on decoding EEG were designed in a subject-specific manner by using calibration samples, with no concern of its practical use, hampered by time-consuming steps and a large data requirement. To this end, recent studies adopted a transfer learning strategy, especially domain adaptation techniques. Among those, to our knowledge, an adversarial learning has shown its potential in BCIs. In the meantime, it is known that adversarial learning-based domain adaptation methods are prone to negative transfer that disrupts learning generalized feature representations, applicable to diverse domains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel framework that learns class-relevant and subject-invariant feature representations in an information-theoretic manner, without using adversarial learning. To be specific, we devise two operational components in a deep network that explicitly estimate mutual information between feature representations; (1) to decompose features in an intermediate layer into class-relevant and class-irrelevant ones, (2) to enrich class-discriminative feature representation. On two large EEG datasets, we validated the effectiveness of our proposed framework by comparing with several comparative methods in performance. Further, we conducted rigorous analyses by performing an ablation study in regard to the components in our network, explaining our model's decision on input EEG signals via layer-wise relevance propagation, and visualizing the distribution of learned features via t-SNE.",0
"Recently, deep learning techniques have shown promise in electroencephalography (EEG)-based brain-computer interfaces (BCIs). However, the high variability within and between individuals has led to many studies relying on subject-specific calibration samples, which can be time-consuming and require a large amount of data. To overcome these limitations, transfer learning strategies, particularly adversarial learning, have been used. However, adversarial learning techniques can result in negative transfer, hindering the ability to generalize features across different domains. In this study, we propose a novel framework that learns subject-invariant and class-relevant feature representations in an information-theoretic manner, without using adversarial learning. Our deep network uses two operational components to explicitly estimate mutual information between feature representations: (1) to decompose features into class-relevant and class-irrelevant ones, and (2) to enhance class-discriminative feature representation. We evaluated our framework on two large EEG datasets and compared its performance with several comparative methods. We also conducted an ablation study to analyze the components of our network, used layer-wise relevance propagation to explain our model's decision on input EEG signals, and visualized the distribution of learned features using t-SNE.",1
"Recommender systems play a fundamental role in web applications in filtering massive information and matching user interests. While many efforts have been devoted to developing more effective models in various scenarios, the exploration on the explainability of recommender systems is running behind. Explanations could help improve user experience and discover system defects. In this paper, after formally introducing the elements that are related to model explainability, we propose a novel explainable recommendation model through improving the transparency of the representation learning process. Specifically, to overcome the representation entangling problem in traditional models, we revise traditional graph convolution to discriminate information from different layers. Also, each representation vector is factorized into several segments, where each segment relates to one semantic aspect in data. Different from previous work, in our model, factor discovery and representation learning are simultaneously conducted, and we are able to handle extra attribute information and knowledge. In this way, the proposed model can learn interpretable and meaningful representations for users and items. Unlike traditional methods that need to make a trade-off between explainability and effectiveness, the performance of our proposed explainable model is not negatively affected after considering explainability. Finally, comprehensive experiments are conducted to validate the performance of our model as well as explanation faithfulness.",0
"Recommender systems are crucial for web applications as they filter vast amounts of information and match user interests. Although many attempts have been made to create more effective models in various scenarios, research on the explainability of recommender systems lags behind. Providing explanations can enhance user experience and uncover system flaws. This paper introduces the elements related to model explainability and proposes a novel explainable recommendation model by enhancing transparency in the representation learning process. To address the representation entangling issue in conventional models, traditional graph convolution is revised to differentiate information from various layers. Additionally, each representation vector is divided into multiple segments, with each segment linked to one semantic aspect in data. Unlike previous work, our model simultaneously conducts factor discovery and representation learning, and can manage additional attribute information and knowledge. Consequently, the proposed model learns interpretable and meaningful representations for users and items. Unlike traditional methods that sacrifice effectiveness for explainability, our proposed explainable model's performance is not negatively impacted after considering explainability. Comprehensive experiments are carried out to verify the model's performance and explanation faithfulness.",1
"Deep clustering against self-supervised learning is a very important and promising direction for unsupervised visual representation learning since it requires little domain knowledge to design pretext tasks. However, the key component, embedding clustering, limits its extension to the extremely large-scale dataset due to its prerequisite to save the global latent embedding of the entire dataset. In this work, we aim to make this framework more simple and elegant without performance decline. We propose an unsupervised image classification framework without using embedding clustering, which is very similar to standard supervised training manner. For detailed interpretation, we further analyze its relation with deep clustering and contrastive learning. Extensive experiments on ImageNet dataset have been conducted to prove the effectiveness of our method. Furthermore, the experiments on transfer learning benchmarks have verified its generalization to other downstream tasks, including multi-label image classification, object detection, semantic segmentation and few-shot image classification.",0
"Unsupervised visual representation learning is an essential area, and deep clustering is a promising direction compared to self-supervised learning since it requires minimal domain knowledge. However, embedding clustering, a crucial component, limits its scalability to large datasets as it needs to save the global latent embedding of the entire dataset. To address this, we propose a novel unsupervised image classification framework that does not use embedding clustering, making it simpler and more elegant without sacrificing performance. We also analyze its relationship with deep clustering and contrastive learning. Our experiments on the ImageNet dataset demonstrate the effectiveness of our approach, and transfer learning benchmarks show its generalization to other downstream tasks such as multi-label image classification, object detection, semantic segmentation, and few-shot image classification.",1
"Graph representation learning is gaining popularity in a wide range of applications, such as social networks analysis, computational biology, and recommender systems. However, different with positive results from many academic studies, applying graph neural networks (GNNs) in a real-world application is still challenging due to non-stationary environments. The underlying distribution of streaming data changes unexpectedly, resulting in different graph structures (a.k.a., concept drift). Therefore, it is essential to devise a robust graph learning technique so that the model does not overfit to the training graphs. In this work, we present Hop Sampling, a straightforward regularization method that can effectively prevent GNNs from overfishing. The hop sampling randomly selects the number of propagation steps rather than fixing it, and by doing so, it encourages the model to learn meaningful node representation for all intermediate propagation layers and to experience a variety of plausible graphs that are not in the training set. Particularly, we describe the use case of our method in recommender systems, a representative example of the real-world non-stationary case. We evaluated hop sampling on a large-scale real-world LINE dataset and conducted an online A/B/n test in LINE Coupon recommender systems of LINE Wallet Tab. Experimental results demonstrate that the proposed scheme improves the prediction accuracy of GNNs. We observed hop sampling provides 7.97% and 16.93% improvements for NDCG and MAP compared to non-regularized GNN models in our online service. Furthermore, models using hop sampling alleviate the oversmoothing issue in GNNs enabling a deeper model as well as more diversified representation.",0
"The popularity of graph representation learning has grown across various fields, including social networks analysis, computational biology, and recommender systems. Despite positive results from academic studies, using graph neural networks (GNNs) in real-world applications remains challenging due to non-stationary environments. Streaming data distribution changes unexpectedly, causing concept drift and resulting in different graph structures. Therefore, developing a robust graph learning technique that prevents overfitting to training graphs is crucial. This study introduces Hop Sampling, a regularization method that prevents overfishing by randomly selecting the number of propagation steps. This encourages the model to learn meaningful node representation for all intermediate propagation layers and experience a variety of plausible graphs that are not in the training set. The use case of this method in recommender systems is described, and its effectiveness is evaluated on a real-world dataset. The proposed scheme improves the prediction accuracy of GNNs and alleviates the oversmoothing issue, allowing for deeper models and more diversified representation.",1
"This paper presents the novel Riemannian Fusion Network (RFNet), a deep neural architecture for learning spatial and temporal information from Electroencephalogram (EEG) for a number of different EEG-based Brain Computer Interface (BCI) tasks and applications. The spatial information relies on Spatial Covariance Matrices (SCM) of multi-channel EEG, whose space form a Riemannian Manifold due to the Symmetric and Positive Definite structure. We exploit a Riemannian approach to map spatial information onto feature vectors in Euclidean space. The temporal information characterized by features based on differential entropy and logarithm power spectrum density is extracted from different windows through time. Our network then learns the temporal information by employing a deep long short-term memory network with a soft attention mechanism. The output of the attention mechanism is used as the temporal feature vector. To effectively fuse spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets from three popular fields of BCI, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. RFNet approaches the state-of-the-art on one dataset (SEED) and outperforms other methods on the other three datasets (SEED-VIG, BCI-IV 2A, and BCI-IV 2B), setting new state-of-the-art values and showing the robustness of our framework in EEG representation learning.",0
"In this article, we introduce the Riemannian Fusion Network (RFNet), a deep neural architecture for learning both spatial and temporal information from Electroencephalogram (EEG) data for various EEG-based Brain Computer Interface (BCI) tasks and applications. To capture spatial information, we utilize Spatial Covariance Matrices (SCM) of multi-channel EEG, which form a Riemannian Manifold due to their Symmetric and Positive Definite structure. We use a Riemannian approach to map spatial information onto feature vectors in Euclidean space. For temporal information, we extract features based on differential entropy and logarithm power spectrum density from different time windows. Our network then learns temporal information by using a deep long short-term memory network with a soft attention mechanism. The output of the attention mechanism is employed as the temporal feature vector. To effectively fuse spatial and temporal information, we use an effective fusion strategy that learns attention weights applied to embedding-specific features for decision making. Our proposed framework is evaluated on four public datasets from three popular fields of BCI, including emotion recognition, vigilance estimation, and motor imagery classification, with various types of tasks such as binary classification, multi-class classification, and regression. RFNet achieves state-of-the-art performance on one dataset (SEED) and outperforms other methods on the other three datasets (SEED-VIG, BCI-IV 2A, and BCI-IV 2B), demonstrating the robustness of our framework in EEG representation learning.",1
"Face anti-spoofing is crucial to security of face recognition systems. Previous approaches focus on developing discriminative models based on the features extracted from images, which may be still entangled between spoof patterns and real persons. In this paper, motivated by the disentangled representation learning, we propose a novel perspective of face anti-spoofing that disentangles the liveness features and content features from images, and the liveness features is further used for classification. We also put forward a Convolutional Neural Network (CNN) architecture with the process of disentanglement and combination of low-level and high-level supervision to improve the generalization capabilities. We evaluate our method on public benchmark datasets and extensive experimental results demonstrate the effectiveness of our method against the state-of-the-art competitors. Finally, we further visualize some results to help understand the effect and advantage of disentanglement.",0
"The security of face recognition systems relies heavily on face anti-spoofing. Previous methods have focused on creating models that discriminate between features extracted from images, but these models can still confuse spoof patterns with real people. In this study, we propose a new approach to face anti-spoofing that disentangles the liveness and content features from images. We use the liveness features for classification and introduce a Convolutional Neural Network architecture that combines low-level and high-level supervision to enhance generalization capabilities. We assess our approach on public benchmark datasets and observe that it performs better than existing methods. Finally, we present visualizations of our results to help readers understand the benefits of disentanglement.",1
"Knowledge Distillation (KD) based methods adopt the one-way Knowledge Transfer (KT) scheme in which training a lower-capacity student network is guided by a pre-trained high-capacity teacher network. Recently, Deep Mutual Learning (DML) presented a two-way KT strategy, showing that the student network can be also helpful to improve the teacher network. In this paper, we propose Dense Cross-layer Mutual-distillation (DCM), an improved two-way KT method in which the teacher and student networks are trained collaboratively from scratch. To augment knowledge representation learning, well-designed auxiliary classifiers are added to certain hidden layers of both teacher and student networks. To boost KT performance, we introduce dense bidirectional KD operations between the layers appended with classifiers. After training, all auxiliary classifiers are discarded, and thus there are no extra parameters introduced to final models. We test our method on a variety of KT tasks, showing its superiorities over related methods. Code is available at https://github.com/sundw2014/DCM",0
"The approach taken by Knowledge Distillation (KD) methods involves transferring knowledge in a one-way direction, with a highly-capable teacher network guiding the training of a less-capable student network. However, Deep Mutual Learning (DML) has introduced a two-way knowledge transfer strategy that demonstrates the potential for the student network to help improve the teacher network as well. This paper introduces Dense Cross-layer Mutual-distillation (DCM), an enhanced two-way knowledge transfer method that involves collaborative training of both networks from the beginning. To improve the learning of knowledge representation, the networks are equipped with auxiliary classifiers in specific hidden layers, and dense bidirectional KD operations are introduced between the layers that have these classifiers. After training, the auxiliary classifiers are discarded to avoid introducing additional parameters to the final models. The effectiveness of the method is demonstrated through testing on various knowledge transfer tasks, and the code is available at https://github.com/sundw2014/DCM.",1
"Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has been little to no work with these methods on other smaller domains, such as satellite, textural, or biological imagery. We experiment with several popular methods on an unprecedented variety of domains. We discover, among other findings, that Rotation is by far the most semantically meaningful task, with much of the performance of Jigsaw and Instance Discrimination being attributable to the nature of their induced distribution rather than semantic understanding. Additionally, there are several areas, such as fine-grain classification, where all tasks underperform. We quantitatively and qualitatively diagnose the reasons for these failures and successes via novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.",0
"In recent years, self-supervised representation learning has achieved notable success on large internet imagery datasets like ImageNet. However, there has been minimal research on these methods concerning smaller domains such as biological, textural, or satellite imagery. To explore this, we conducted experiments on various domains using popular methods. Our findings reveal that Rotation is the most meaningful task, while Jigsaw and Instance Discrimination perform well due to induced distribution rather than semantic understanding. We also identify areas where all tasks underperform, such as fine-grain classification, and analyze the reasons for these results via novel experiments. Our code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.",1
We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: i) The model is inductive: it can embed new graphs without re-training in the presence of new data; ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.,0
"Our proposed algorithm for unsupervised graph representation learning with attributed graphs offers several advantages that address current limitations in the field. Firstly, our model is inductive, meaning it can embed new graphs without re-training when presented with new data. Secondly, the algorithm considers both micro-structures and macro-structures by examining the attributed graphs at different scales. Finally, our model is end-to-end differentiable, allowing it to be integrated into deep learning pipelines and enabling back-propagation. By combining a coarsening method with strong theoretical guarantees and mutual information maximization, we are able to produce high-quality embeddings. We demonstrated the effectiveness of our algorithm through classification tasks using common benchmarks from the literature, and found that it is competitive with state-of-the-art unsupervised graph representation learning methods.",1
"The advisor-advisee relationship represents direct knowledge heritage, and such relationship may not be readily available from academic libraries and search engines. This work aims to discover advisor-advisee relationships hidden behind scientific collaboration networks. For this purpose, we propose a novel model based on Network Representation Learning (NRL), namely Shifu2, which takes the collaboration network as input and the identified advisor-advisee relationship as output. In contrast to existing NRL models, Shifu2 considers not only the network structure but also the semantic information of nodes and edges. Shifu2 encodes nodes and edges into low-dimensional vectors respectively, both of which are then utilized to identify advisor-advisee relationships. Experimental results illustrate improved stability and effectiveness of the proposed model over state-of-the-art methods. In addition, we generate a large-scale academic genealogy dataset by taking advantage of Shifu2.",0
"The connection between an advisor and their advisee is a valuable source of knowledge that cannot be easily found in academic libraries or search engines. This study aims to uncover these relationships that are hidden in scientific collaboration networks. To achieve this, a new model called Shifu2 based on Network Representation Learning (NRL) is proposed, which uses the collaboration network as input to identify advisor-advisee relationships. Unlike other NRL models, Shifu2 takes into account both the network structure and semantic information of nodes and edges. It encodes nodes and edges into low-dimensional vectors and uses them to identify advisor-advisee relationships. The results of experiments show that Shifu2 is more effective than other methods in identifying these relationships. Additionally, a large-scale academic genealogy dataset is generated using Shifu2.",1
"With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person's clothing may change, and different persons may wearing similar clothes. All these cases can result in an inconsistent ReID performance, revealing a critical problem that current ReID models heavily rely on person's apparels. Therefore, it is critical to learn an apparel-invariant person representation under cases like cloth changing or several persons wearing similar clothes. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth changing images according to the target cloth embedding. It's worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth changing images. We conduct extensive experiments on several datasets comparing with several baselines. Experimental results demonstrate that our proposal can improve the ReID performance of the baseline models.",0
"The advent of deep learning techniques has vastly enhanced the Person Re-Identification (ReID) performance on numerous public datasets. Nonetheless, these datasets are typically gathered in a brief time frame, where the appearance of individuals rarely alters. In real-world scenarios, like a mall, people may change their attire, or multiple people could be dressed similarly, leading to an inconsistent ReID performance. This uncovers a crucial issue that current ReID models are heavily reliant on an individual's clothing. Consequently, it is crucial to acquire an apparel-invariant person representation, especially in cases where clothes change or individuals wear similar outfits. This study approaches the problem from an invariant feature representation learning standpoint. The primary contributions of this work are: (1) proposing the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person in different outfits; (2) developing an unsupervised Apparel-simulation GAN (AS-GAN) to synthesize cloth-changing images based on the target cloth embedding. It is noteworthy that ReID task images were cropped from real-world low-quality CCTV videos, making cloth-changing image synthesis considerably more challenging. We conducted extensive experiments on various datasets, comparing our proposal with several baselines. The experimental outcomes demonstrate that our proposal improves the ReID performance of baseline models.",1
"In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction.",0
"Obtaining manual annotations in medical imaging can be costly and challenging, hindering the scalability of traditional deep learning models. Therefore, it would be advantageous to derive valuable representations from raw data without requiring manual annotations. To address this issue, we suggest a solution for self-supervised representation learning using multi-modal ultrasound video-speech raw data. We assume a strong correlation exists between the ultrasound video and the corresponding narrative speech audio of the sonographer. To develop meaningful representations, the model must identify this correlation and comprehend the underlying anatomical features. We created a framework that models the correspondence between video and audio without human annotations, using cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental results on multi-modal fetal ultrasound video and audio demonstrate that our approach produces robust representations that transfer effectively to downstream tasks, such as standard plane detection and eye-gaze prediction.",1
"We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.",0
"A new measure to assess the transferability of representations learned by classifiers is introduced. The Log Expected Empirical Prediction (LEEP) measure is straightforward and easy to calculate, requiring only one run of the target data set through the source data set classifier. The efficacy of LEEP is demonstrated through empirical evidence and theoretical analysis, revealing its ability to predict both transfer and meta-transfer learning methods' performance and convergence speed, even with limited or unbalanced data. Furthermore, LEEP outperforms other proposed transferability measures, including negative conditional entropy and H scores. Impressively, when transferring from ImageNet to CIFAR100, LEEP achieves up to a 30% improvement in correlation with the actual transfer accuracy, surpassing all competing methods.",1
"Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a \textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.",0
"Significant progress has been made in image and signal representation learning and generation by deep generative models. These models use either the full Euclidean space or a bounded subset as the latent space. However, the flat geometry of this space is often too simplistic to accurately reflect the manifold structure of the data. This paper proposes the use of a multi-chart latent space for improved data representation. The Chart Auto-Encoder (CAE) is proposed, inspired by differential geometry, and a universal approximation theorem is proven on its representation capability. The training data size and network size scale exponentially with the intrinsic dimension of the data manifold. CAE possesses desirable manifold properties that are absent in auto-encoders with a flat latent space, particularly the proximity of data. Extensive experimentation with synthetic and real-life examples demonstrates that CAE provides high-fidelity reconstruction, maintains proximity in the latent space, and generates new data that remains close to the manifold. These experiments highlight the advantages of CAE over existing auto-encoders and their variants, as it preserves both the topology and geometry of the data manifold.",1
"In this paper, we propose an easily trained yet powerful representation learning approach with performance highly competitive to deep neural networks in a digital pathology image segmentation task. The method, called sparse coding driven deep decision tree ensembles that we abbreviate as ScD2TE, provides a new perspective on representation learning. We explore the possibility of stacking several layers based on non-differentiable pairwise modules and generate a densely concatenated architecture holding the characteristics of feature map reuse and end-to-end dense learning. Under this architecture, fast convolutional sparse coding is used to extract multi-level features from the output of each layer. In this way, rich image appearance models together with more contextual information are integrated by learning a series of decision tree ensembles. The appearance and the high-level context features of all the previous layers are seamlessly combined by concatenating them to feed-forward as input, which in turn makes the outputs of subsequent layers more accurate and the whole model efficient to train. Compared with deep neural networks, our proposed ScD2TE does not require back-propagation computation and depends on less hyper-parameters. ScD2TE is able to achieve a fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the superiority of our segmentation technique by evaluating it on the multi-disease state and multi-organ dataset where consistently higher performances were obtained for comparison against several state-of-the-art deep learning methods such as convolutional neural networks (CNN), fully convolutional networks (FCN), etc.",0
"This paper introduces a representation learning approach for digital pathology image segmentation called ScD2TE. It is a sparse coding driven deep decision tree ensemble that can be easily trained and has competitive performance compared to deep neural networks. The approach involves stacking non-differentiable pairwise modules to create a densely concatenated architecture that integrates image appearance models with contextual information. By using fast convolutional sparse coding, multi-level features are extracted from each layer's output, and decision tree ensembles are learned to combine appearance and high-level context features. The ScD2TE model is efficient to train, requires fewer hyper-parameters, and achieves fast end-to-end pixel-wise training in a layer-wise manner. Our evaluation on a multi-disease state and multi-organ dataset showed consistently better performance than state-of-the-art deep learning methods, such as convolutional neural networks and fully convolutional networks.",1
"We propose a self-supervised method to learn feature representations from videos. A standard approach in traditional self-supervised methods uses positive-negative data pairs to train with contrastive learning strategy. In such a case, different modalities of the same video are treated as positives and video clips from a different video are treated as negatives. Because the spatio-temporal information is important for video representation, we extend the negative samples by introducing intra-negative samples, which are transformed from the same anchor video by breaking temporal relations in video clips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn video representations. There are many flexible options in our IIC framework and we conduct experiments by using several different configurations. Evaluations are conducted on video retrieval and video recognition tasks using the learned video representation. Our proposed IIC outperforms current state-of-the-art results by a large margin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, improvements can also be obtained on these two benchmark datasets. Code is available at https://github.com/BestJuly/Inter-intra-video-contrastive-learning.",0
"Our method suggests a self-supervised approach to acquire feature representations from videos. The conventional self-supervised approach uses positive-negative data pairs with a contrastive learning strategy, where different modalities of the same video are considered as positives and video clips from a different video are considered as negatives. Given the importance of spatio-temporal information for video representation, we introduce intra-negative samples, which are obtained by disrupting the temporal relations in video clips from the same anchor video, to extend the negative samples. To train spatio-temporal convolutional networks for video representation, we propose an Inter-Intra Contrastive (IIC) framework that offers flexible options for different configurations. The learned video representation is evaluated on video retrieval and video recognition tasks, and our proposed IIC significantly outperforms the current state-of-the-art results, demonstrating improvements of up to 16.7% and 9.5% points in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. Code is accessible at https://github.com/BestJuly/Inter-intra-video-contrastive-learning.",1
"We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification.",0
"To solve the Cross Model Compatibility (CMC) issue in visual search applications, we suggest a unified representation learning framework. Cross compatibility between embedding models is crucial for visual search systems to recognize and retrieve identities without re-encoding user images. However, current approaches to CMC in face identification fail when the distributions of embedding models change dramatically. Our solution enhances CMC performance with a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize embedding spaces. We conducted extensive experiments that demonstrate our proposed solution outperforms previous approaches by a considerable margin for various challenging visual search scenarios of face recognition and person re-identification.",1
"We propose PiNet, a generalised differentiable attention-based pooling mechanism for utilising graph convolution operations for graph level classification. We demonstrate high sample efficiency and superior performance over other graph neural networks in distinguishing isomorphic graph classes, as well as competitive results with state of the art methods on standard chemo-informatics datasets.",0
"PiNet is our proposed mechanism for utilizing graph convolution operations for graph level classification through a generalised differentiable attention-based pooling approach. Our study showcases a superior performance in distinguishing isomorphic graph classes compared to other graph neural networks, with high sample efficiency. Additionally, we present competitive outcomes with current state of the art methods on standard chemo-informatics datasets.",1
"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https://github.com/bfshi/InfoDrop.",0
"CNNs have a tendency to rely on local texture over global shape for decision-making. Recent research has highlighted the correlation between CNNs' texture-bias and their ability to withstand distribution shift, adversarial perturbation, and random corruption. The aim of this study is to enhance CNNs' robustness universally by mitigating their texture bias. The authors introduce a model-agnostic approach called Informative Dropout (InfoDrop), which is inspired by the human visual system. This method improves interpretability and reduces texture bias by distinguishing between texture and shape based on local self-information in an image and utilizing a Dropout-like algorithm to decorrelate the model output from local texture. The experiments conducted suggest that InfoDrop improves robustness in various scenarios like domain generalization, few-shot classification, image corruption, and adversarial perturbation. This study is one of the first attempts to enhance different types of robustness through a unified model, revealing new insights into the relationship between shape-bias and robustness, and offering novel approaches to trustworthy machine learning algorithms. The code is available at https://github.com/bfshi/InfoDrop.",1
"Deep neural networks often degrade significantly when training data suffer from class imbalance problems. Existing approaches, e.g., re-sampling and re-weighting, commonly address this issue by rearranging the label distribution of training data to train the networks fitting well to the implicit balanced label distribution. However, most of them hinder the representative ability of learned features due to insufficient use of intra/inter-sample information of training data. To address this issue, we propose meta feature modulator (MFM), a meta-learning framework to model the difference between the long-tailed training data and the balanced meta data from the perspective of representation learning. Concretely, we employ learnable hyper-parameters (dubbed modulation parameters) to adaptively scale and shift the intermediate features of classification networks, and the modulation parameters are optimized together with the classification network parameters guided by a small amount of balanced meta data. We further design a modulator network to guide the generation of the modulation parameters, and such a meta-learner can be readily adapted to train the classification network on other long-tailed datasets. Extensive experiments on benchmark vision datasets substantiate the superiority of our approach on long-tailed recognition tasks beyond other state-of-the-art methods.",0
"Class imbalance problems during training can significantly degrade the performance of deep neural networks. Common solutions, such as re-sampling and re-weighting, attempt to address this issue by modifying the label distribution of training data to train the networks to fit a balanced label distribution. However, these methods often limit the representative ability of learned features due to insufficient use of intra/inter-sample information. To tackle this problem, we propose the meta feature modulator (MFM), a meta-learning approach that models the difference between long-tailed training data and balanced meta data from the perspective of representation learning. Our method employs learnable modulation parameters that adaptively scale and shift the intermediate features of classification networks, which are optimized alongside the classification network parameters using a small amount of balanced meta data. We also design a modulator network to guide the generation of the modulation parameters, which can be easily adapted to train the classification network on other long-tailed datasets. Our extensive experiments on benchmark vision datasets demonstrate the superiority of our approach in long-tailed recognition tasks over other state-of-the-art methods.",1
"An advanced conceptual validation framework for multimodal multivariate time series defines a multi-level contextual anomaly detection ranging from an univariate context definition, to a multimodal abstract context representation learnt by an Autoencoder from heterogeneous data (images, time series, sounds, etc.) associated to an industrial process. Each level of the framework is either applicable to historical data and/or live data. The ultimate level is based on causal discovery to identify causal relations in observational data in order to exclude biased data to train machine learning models and provide means to the domain expert to discover unknown causal relations in the underlying process represented by the data sample. A Long Short-Term Memory Autoencoder is successfully evaluated on multivariate time series to validate the learnt representation of abstract contexts associated to multiple assets of a blast furnace. A research roadmap is identified to combine causal discovery and representation learning as an enabler for unsupervised Root Cause Analysis applied to the process industry.",0
"A framework for validating conceptual ideas in the context of multimodal multivariate time series has been developed. This framework includes a multi-level contextual anomaly detection process that ranges from defining univariate context to learning multimodal abstract context representations using an Autoencoder from various data sources associated with industrial processes such as images, time series, and sounds. Each level of the framework can be applied to both historical and live data. The highest level of the framework uses causal discovery to identify causal relationships in observational data, which helps to eliminate biased data when training machine learning models and allows domain experts to discover unknown causal relationships in the underlying process represented by the data sample. The Long Short-Term Memory Autoencoder has been successfully tested on multivariate time series to validate the accuracy of the learned representations of abstract contexts associated with multiple assets of a blast furnace. Finally, a research roadmap is proposed to combine causal discovery and representation learning to enable unsupervised Root Cause Analysis in the process industry.",1
"Temporal cues in videos provide important information for recognizing actions accurately. However, temporal-discriminative features can hardly be extracted without using an annotated large-scale video action dataset for training. This paper proposes a novel Video-based Temporal-Discriminative Learning (VTDL) framework in self-supervised manner. Without labelled data for network pretraining, temporal triplet is generated for each anchor video by using segment of the same or different time interval so as to enhance the capacity for temporal feature representation. Measuring temporal information by time derivative, Temporal Consistent Augmentation (TCA) is designed to ensure that the time derivative (in any order) of the augmented positive is invariant except for a scaling constant. Finally, temporal-discriminative features are learnt by minimizing the distance between each anchor and its augmented positive, while the distance between each anchor and its augmented negative as well as other videos saved in the memory bank is maximized to enrich the representation diversity. In the downstream action recognition task, the proposed method significantly outperforms existing related works. Surprisingly, the proposed self-supervised approach is better than fully-supervised methods on UCF101 and HMDB51 when a small-scale video dataset (with only thousands of videos) is used for pre-training. The code has been made publicly available on https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition.",0
"Videos contain important temporal cues that are crucial for accurate action recognition. However, it is challenging to extract temporal-discriminative features without the use of a large-scale annotated video action dataset for training. This paper presents a new framework called Video-based Temporal-Discriminative Learning (VTDL) that operates in a self-supervised manner. The framework generates temporal triplets for each anchor video without labeled data for network pretraining. These triplets are used to enhance the capacity for temporal feature representation. A method called Temporal Consistent Augmentation (TCA) is designed to measure temporal information by time derivative and ensure that the time derivative of the augmented positive is invariant except for a scaling constant. The framework then learns temporal-discriminative features by minimizing the distance between each anchor and its augmented positive, while maximizing the distance between each anchor and its augmented negative as well as other videos saved in the memory bank to enrich the representation diversity. The proposed method outperforms existing related works in the downstream action recognition task. Surprisingly, the proposed self-supervised approach is better than fully-supervised methods on UCF101 and HMDB51 when a small-scale video dataset with only thousands of videos is used for pre-training. The code for the framework is publicly available on https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition.",1
"The problems of shape classification and part segmentation from 3D point clouds have garnered increasing attention in the last few years. Both of these problems, however, suffer from relatively small training sets, creating the need for statistically efficient methods to learn 3D shape representations. In this paper, we investigate the use of Approximate Convex Decompositions (ACD) as a self-supervisory signal for label-efficient learning of point cloud representations. We show that using ACD to approximate ground truth segmentation provides excellent self-supervision for learning 3D point cloud representations that are highly effective on downstream tasks. We report improvements over the state-of-the-art for unsupervised representation learning on the ModelNet40 shape classification dataset and significant gains in few-shot part segmentation on the ShapeNetPart dataset.Code available at https://github.com/matheusgadelha/PointCloudLearningACD",0
"In recent years, there has been increasing attention given to the problems of shape classification and part segmentation from 3D point clouds. Both of these issues, however, suffer from a shortage of adequate training data, which necessitates the development of statistically efficient approaches for learning 3D shape representations. This study explores the use of Approximate Convex Decompositions (ACD) as a self-supervisory signal to facilitate label-efficient learning of point cloud representations. We demonstrate that utilizing ACD to approximate ground truth segmentation offers excellent self-supervision for learning 3D point cloud representations that are highly effective for downstream tasks. Our results show improvements over the state-of-the-art for unsupervised representation learning on the ModelNet40 shape classification dataset and significant gains in few-shot part segmentation on the ShapeNetPart dataset. Code is available at https://github.com/matheusgadelha/PointCloudLearningACD.",1
"Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: https://europe.naverlabs.com/icmlm.",0
"Developing general-purpose visual features through pretraining has become an essential aspect of addressing various computer vision challenges. Although the ImageNet dataset is extensively annotated and can be used to learn such features, recent methods have explored ways to perform pretraining with noisy, fewer, or no annotations. We suggest that captioned images could be an overlooked source of information for supervising the training of visual representations, as they are easily accessible. Therefore, we introduce image-conditioned masked language modeling (ICMLM) as a proxy task to learn visual representations over image-caption pairs, inspired by recent advancements in language models. ICMLM involves predicting masked words in captions by relying on visual cues. We employ hybrid models with dedicated visual and textual encoders to approach this task, and our results indicate that the visual representations learned through this task transfer effectively to many target tasks. Our experiments demonstrate that image captions can provide global and localized semantic information to visual representations. Visit our project website at https://europe.naverlabs.com/icmlm.",1
"In this paper, we present a new algorithm for semi-supervised representation learning. In this algorithm, we first find a vector representation for the labels of the data points based on their local positions in the space. Then, we map the data to lower-dimensional space using a linear transformation such that the dependency between the transformed data and the assigned labels is maximized. In fact, we try to find a mapping that is as discriminative as possible. The approach will use Hilber-Schmidt Independence Criterion (HSIC) as the dependence measure. We also present a kernelized version of the algorithm, which allows non-linear transformations and provides more flexibility in finding the appropriate mapping. Use of unlabeled data for learning new representation is not always beneficial and there is no algorithm that can deterministically guarantee the improvement of the performance by exploiting unlabeled data. Therefore, we also propose a bound on the performance of the algorithm, which can be used to determine the effectiveness of using the unlabeled data in the algorithm. We demonstrate the ability of the algorithm in finding the transformation using both toy examples and real-world datasets.",0
"This paper introduces a novel algorithm for semi-supervised representation learning. The algorithm begins by generating a vector representation of the labels for each data point based on their local positions in space. Next, a linear transformation is applied to the data, maximizing the dependency between the transformed data and its assigned labels. The goal is to achieve a high level of discrimination. The Hilber-Schmidt Independence Criterion is used as a measure of dependence. Additionally, a kernelized version of the algorithm is presented, which permits non-linear transformations and greater flexibility in selecting the appropriate mapping. It is not always beneficial to use unlabeled data for new representation learning, and there is no algorithm that can guarantee performance improvement by exploiting unlabeled data. Therefore, a performance bound is proposed to determine the effectiveness of using unlabeled data. The algorithm's ability to find the transformation is demonstrated using both toy examples and real-world datasets.",1
"We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively ""de-freeze"" the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales.Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN. Source codes are available at https://github.com/duxingren14/BSD-GAN.",0
"Our team has developed a new approach to training unconditional Generative Adversarial Networks (GANs) called BSD-GAN, which allows for the learning of image representations at multiple scales. This method involves training the network in multiple branches, gradually covering the breadth and depth of the network as training images increase in resolution. Specifically, the noise vector input to the generator network is split into sub-vectors, each of which corresponds to a particular scale and is trained to learn image representations at that scale. During training, we progressively ""de-freeze"" the sub-vectors one by one as higher-resolution images are used for training and more network layers are added. This explicit sub-vector designation allows for the direct manipulation and combination of latent (sub-vector) codes that model different feature scales. Our extensive experiments have shown that this method is effective in scale-disentangled learning of image representations and synthesis of novel image contents, without the need for extra labels and without compromising the quality of the synthesized high-resolution images. Additionally, we have demonstrated several image generation and manipulation applications enabled by BSD-GAN. The source codes for BSD-GAN are available at https://github.com/duxingren14/BSD-GAN.",1
"The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",0
"This paper aims to achieve self-supervised learning from video, specifically for creating representations for action recognition. Our contributions are as follows: Firstly, we propose a new architecture and learning framework called Memory-augmented Dense Predictive Coding (MemDPC) for this task. MemDPC is trained using a predictive attention mechanism over a set of compressed memories, which enables construction of multiple hypotheses efficiently, and any future states can be created through a convex combination of the condensed representations. Secondly, we explore visual-only self-supervised video representation learning from RGB frames, unsupervised optical flow, or both. Finally, we thoroughly evaluate the quality of the learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. Our results demonstrate state-of-the-art or comparable performance across all tasks, with significantly fewer training data than other approaches.",1
"Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words, yet predicting this $known\ $information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks. Formally, we quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation.",0
"Self-supervised representation learning is a method of learning semantic representations without labeled data by solving auxiliary prediction tasks, known as pretext tasks. These tasks are devised using input features, such as predicting missing image patches or words, and help in developing effective representations for downstream prediction tasks. This study proposes a conditional independence-based mechanism to explain how solving certain pretext tasks can significantly reduce the sample complexity of downstream supervised tasks. Specifically, we demonstrate how approximate independence between the components of the pretext task, conditional on the label and latent variables, enables us to learn representations that can solve downstream tasks with minimal sample complexity by training a linear layer on top of the learned representation.",1
"Graph representation learning aims to encode all nodes of a graph into low-dimensional vectors that will serve as input of many compute vision tasks. However, most existing algorithms ignore the existence of inherent data distribution and even noises. This may significantly increase the phenomenon of over-fitting and deteriorate the testing accuracy. In this paper, we propose a Distribution-induced Bidirectional Generative Adversarial Network (named DBGAN) for graph representation learning. Instead of the widely used normal distribution assumption, the prior distribution of latent representation in our DBGAN is estimated in a structure-aware way, which implicitly bridges the graph and feature spaces by prototype learning. Thus discriminative and robust representations are generated for all nodes. Furthermore, to improve their generalization ability while preserving representation ability, the sample-level and distribution-level consistency is well balanced via a bidirectional adversarial learning framework. An extensive group of experiments are then carefully designed and presented, demonstrating that our DBGAN obtains remarkably more favorable trade-off between representation and robustness, and meanwhile is dimension-efficient, over currently available alternatives in various tasks.",0
"The aim of graph representation learning is to encode all nodes of a graph into low-dimensional vectors that can be used as input for various computer vision tasks. However, current algorithms fail to consider the inherent data distribution and noise, which can lead to over-fitting and reduced testing accuracy. To address this, we propose the Distribution-induced Bidirectional Generative Adversarial Network (DBGAN) for graph representation learning. Unlike other algorithms that assume a normal distribution, our DBGAN estimates the prior distribution of latent representation in a structure-aware way, bridging the graph and feature spaces via prototype learning. This results in discriminative and robust representations for all nodes. Additionally, our bidirectional adversarial learning framework balances sample-level and distribution-level consistency, improving generalization ability while preserving representation ability. Through extensive experiments, we show that our DBGAN offers a favorable trade-off between representation and robustness and is more dimension-efficient than current alternatives for various tasks.",1
"Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.",0
"The study of representation learning, which aims to separate the different factors of variation in data, has gained popularity in the field of machine learning. However, most research in this area has focused on computer vision datasets and has not been applicable to music. This paper introduces a new dataset of symbolic music that allows researchers to test the effectiveness of their algorithms on a diverse range of domains and evaluate algorithms specifically designed for music. The dataset contains 1.3 million data points of 2-bar monophonic melodies, each with a unique combination of nine latent factors that span ordinal, categorical, and binary types. This dataset is large enough to train deep networks for disentanglement learning, and benchmarking experiments using popular unsupervised disentanglement algorithms are presented and compared with results obtained from an image-based dataset.",1
"Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.",0
"Recently, self-supervised representation learning methods have outperformed supervised learning methods in downstream tasks such as object detection and image classification. However, it is unclear why this is the case, as these methods train instance classification models by treating each image and its augmented versions as samples of a single class. In this study, we aim to clarify this phenomenon through quantitative experiments. Our findings reveal that approaches like MOCO and PIRL learn representations that are invariant to occlusion but fail to capture viewpoint and category instance invariance, which are crucial for object recognition. Furthermore, we show that these approaches benefit from access to a clean object-centric training dataset like Imagenet. Finally, we propose a novel method that utilizes unstructured videos to learn representations with higher viewpoint invariance. Our results demonstrate that the representations learned through this approach outperform MOCOv2 trained on the same data in terms of the encoded invariances and performance in downstream tasks such as image classification and semantic segmentation.",1
"Graph representation learning based on graph neural networks (GNNs) can greatly improve the performance of downstream tasks, such as node and graph classification. However, the general GNN models do not aggregate node information in a hierarchical manner, and can miss key higher-order structural features of many graphs. The hierarchical aggregation also enables the graph representations to be explainable. In addition, supervised graph representation learning requires labeled data, which is expensive and error-prone. To address these issues, we present an unsupervised graph representation learning method, Unsupervised Hierarchical Graph Representation (UHGR), which can generate hierarchical representations of graphs. Our method focuses on maximizing mutual information between ""local"" and high-level ""global"" representations, which enables us to learn the node embeddings and graph embeddings without any labeled data. To demonstrate the effectiveness of the proposed method, we perform the node and graph classification using the learned node and graph embeddings. The results show that the proposed method achieves comparable results to state-of-the-art supervised methods on several benchmarks. In addition, our visualization of hierarchical representations indicates that our method can capture meaningful and interpretable clusters.",0
"The use of graph neural networks (GNNs) for graph representation learning has been shown to enhance downstream tasks, such as node and graph classification. However, typical GNN models fail to hierarchically aggregate node information, which can lead to a lack of important higher-order structural features in graphs. By implementing hierarchical aggregation, graph representations can be made more understandable. Moreover, supervised graph representation learning requires labeled data, which is both costly and prone to errors. To tackle these issues, we propose Unsupervised Hierarchical Graph Representation (UHGR), an unsupervised method that generates hierarchical representations of graphs. Our approach maximizes mutual information between local and high-level global representations, allowing us to learn node and graph embeddings without labeled data. We evaluate the effectiveness of our method using node and graph classification, and results show that UHGR performs comparably to state-of-the-art supervised methods on various benchmarks. Additionally, our visualization of hierarchical representations reveals meaningful and interpretable clusters.",1
"Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.",0
"Unsupervised pretraining in difficult computer vision tasks has become relevant again with the advent of self-supervised learning. The most effective self-supervised methods involve prediction tasks using features from various perspectives of the data. DeepInfoMax (DIM) is a self-supervised technique that utilizes the internal structure of deep networks to create such perspectives, generating prediction tasks between local and global features. By exploiting similar structure in spatio-temporal networks, we extended DIM to the video domain, resulting in a new approach called Video Deep InfoMax (VDIM). Our results on Kinetics-pretrained action recognition tasks show that using views from both natural-rate and temporally-downsampled sequences matches or exceeds the performance of prior state-of-the-art methods that employ more expensive large-time-scale transformer models. We also explore the impact of data augmentation and fine-tuning methods, achieving the state of the art by a significant margin when training solely on the UCF-101 dataset.",1
"Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.",0
"Our ability to generalize and comprehend at higher levels is based on our perception of objects. However, representation learning research typically focuses on feature learning and often neglects the consideration of multiple objects or treats segmentation as a preprocessing step that is often supervised. Our argument is that it is crucial to learn how to segment and represent objects together. By starting with the assumption that a scene consists of multiple entities, we can learn how to segment images into objects with comprehensible representations. Our method does not require supervision and can also learn to fill in obscured parts, extrapolate to scenes with more objects, and identify novel feature combinations in unseen objects. Furthermore, our system uses iterative variational inference to learn multi-modal posteriors for ambiguous inputs and can be extended to sequences.",1
"Noisy labels are an unavoidable consequence of labeling processes and detecting them is an important step towards preventing performance degradations in Convolutional Neural Networks. Discarding noisy labels avoids a harmful memorization, while the associated image content can still be exploited in a semi-supervised learning (SSL) setup. Clean samples are usually identified using the small loss trick, i.e. they exhibit a low loss. However, we show that different noise distributions make the application of this trick less straightforward and propose to continuously relabel all images to reveal a discriminative loss against multiple distributions. SSL is then applied twice, once to improve the clean-noisy detection and again for training the final model. We design an experimental setup based on ImageNet32/64 for better understanding the consequences of representation learning with differing label noise distributions and find that non-uniform out-of-distribution noise better resembles real-world noise and that in most cases intermediate features are not affected by label noise corruption. Experiments in CIFAR-10/100, ImageNet32/64 and WebVision (real-world noise) demonstrate that the proposed label noise Distribution Robust Pseudo-Labeling (DRPL) approach gives substantial improvements over recent state-of-the-art. Code is available at https://git.io/JJ0PV.",0
"Detecting noisy labels is crucial in preventing performance degradation in Convolutional Neural Networks, as they are inevitable in labeling processes. To avoid harmful memorization, it is recommended to discard noisy labels while still utilizing the associated image content in a semi-supervised learning (SSL) setup. Identifying clean samples is typically done using the small loss trick, but different noise distributions make this trick less effective. To address this, we suggest continuously relabeling all images to reveal a discriminative loss against multiple distributions. SSL is applied twice, once to improve clean-noisy detection and again for training the final model. Our experimental setup based on ImageNet32/64 found that non-uniform out-of-distribution noise better resembles real-world noise, and intermediate features are usually unaffected by label noise corruption. We demonstrate that our label noise Distribution Robust Pseudo-Labeling (DRPL) approach yields significant improvements over recent state-of-the-art in CIFAR-10/100, ImageNet32/64, and WebVision (real-world noise). The code for our approach is available at https://git.io/JJ0PV.",1
"Graph neural networks (GNNs) achieve remarkable success in graph-based semi-supervised node classification, leveraging the information from neighboring nodes to improve the representation learning of target node. The success of GNNs at node classification depends on the assumption that connected nodes tend to have the same label. However, such an assumption does not always work, limiting the performance of GNNs at node classification. In this paper, we propose label-consistency based graph neural network(LC-GNN), leveraging node pairs unconnected but with the same labels to enlarge the receptive field of nodes in GNNs. Experiments on benchmark datasets demonstrate the proposed LC-GNN outperforms traditional GNNs in graph-based semi-supervised node classification.We further show the superiority of LC-GNN in sparse scenarios with only a handful of labeled nodes.",0
"Graph neural networks (GNNs) have achieved remarkable success in semi-supervised node classification on graph-based data by utilizing information from neighboring nodes to improve target node representation learning. However, the performance of GNNs relies on the assumption that connected nodes share the same label, which is not always accurate. This limitation affects GNNs' performance in node classification. To address this, we propose the label-consistency based graph neural network (LC-GNN) that leverages node pairs with the same label but not connected to enlarge the receptive field of nodes in GNNs. Our experiments on benchmark datasets demonstrate that LC-GNN outperforms traditional GNNs in graph-based semi-supervised node classification, particularly in sparse scenarios with only a few labeled nodes.",1
"Several multi-modality representation learning approaches such as LXMERT and ViLBERT have been proposed recently. Such approaches can achieve superior performance due to the high-level semantic information captured during large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual region regression and classification loss, they often suffer from domain gap and noisy label problems, based on the visual features having been pretrained on the Visual Genome dataset. To overcome these issues, we propose unbiased Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual self-supervised loss built upon contrastive learning. We evaluate CVLP on several down-stream tasks, including VQA, GQA and NLVR2 to validate the superiority of contrastive learning on multi-modality representation learning. Our code is available at: https://github.com/ArcherYunDong/CVLP-.",0
"Recently, various approaches to multi-modality representation learning, such as LXMERT and ViLBERT, have been introduced. These approaches have demonstrated exceptional performance due to their ability to capture high-level semantic information during large-scale multimodal pretraining. However, ViLBERT and LXMERT often encounter domain gap and noisy label issues as a result of adopting visual region regression and classification loss that relies on Visual Genome dataset's pretrained visual features. To address these concerns, we propose an unbiased Contrastive Visual-Linguistic Pretraining (CVLP) that employs contrastive learning to construct a visual self-supervised loss. We assess CVLP's effectiveness on several downstream tasks, including VQA, GQA, and NLVR2, to demonstrate the superiority of contrastive learning in multi-modality representation learning. Our code can be accessed at: https://github.com/ArcherYunDong/CVLP-.",1
"Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed \textit{scalogram-signal correspondence learning} based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fully-supervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning.",0
"The vast amount of data produced by smartphones, wearables, and IoT devices cannot be stored in a centralized repository for supervised model learning due to privacy, bandwidth limitations, and the high cost of annotations. While federated learning is a promising framework for decentralized data learning, it typically requires labeled samples, which is not always feasible for on-device data that are often either unlabeled or difficult to annotate through user interaction. To overcome these challenges, we propose a self-supervised approach, called scalogram-signal correspondence learning, which uses wavelet transform to learn useful representations from unlabeled sensor inputs. Our method involves training a deep temporal neural network to determine if a given pair of signal and its complementary viewpoint align with each other, using a contrastive objective. We evaluate the effectiveness of our approach on various public datasets, achieving strong performance in all domains. Our methodology shows promising results in downstream tasks with training a linear classifier over pre-trained networks, demonstrating usefulness in low-data regime, transfer learning, and cross-validation. We also show that our approach outperforms pre-training with autoencoders in central and federated contexts, and improves generalization in semi-supervised settings by reducing the volume of labeled data needed through self-supervised learning.",1
"Invariance (defined in a general sense) has been one of the most effective priors for representation learning. Direct factorization of parametric models is feasible only for a small range of invariances, while regularization approaches, despite improved generality, lead to nonconvex optimization. In this work, we develop a convex representation learning algorithm for a variety of generalized invariances that can be modeled as semi-norms. Novel Euclidean embeddings are introduced for kernel representers in a semi-inner-product space, and approximation bounds are established. This allows invariant representations to be learned efficiently and effectively as confirmed in our experiments, along with accurate predictions.",0
"Representation learning has been greatly aided by the concept of invariance, which has proven to be an effective prior in a broad sense. Although direct factorization of parametric models is possible for only a limited range of invariances, regularization approaches, while more general, can result in nonconvex optimization. This paper presents a novel convex algorithm for learning representations that are invariant in a variety of contexts, modeled as semi-norms. The algorithm introduces new Euclidean embeddings for kernel representers in a semi-inner-product space, and establishes approximation bounds. This enables the efficient and effective learning of invariant representations, as demonstrated in experiments that yielded accurate predictions.",1
"Protein function prediction may be framed as predicting subgraphs (with certain closure properties) of a directed acyclic graph describing the hierarchy of protein functions. Graph neural networks (GNNs), with their built-in inductive bias for relational data, are hence naturally suited for this task. However, in contrast with most GNN applications, the graph is not related to the input, but to the label space. Accordingly, we propose Tail-GNNs, neural networks which naturally compose with the output space of any neural network for multi-task prediction, to provide relationally-reinforced labels. For protein function prediction, we combine a Tail-GNN with a dilated convolutional network which learns representations of the protein sequence, making significant improvement in F_1 score and demonstrating the ability of Tail-GNNs to learn useful representations of labels and exploit them in real-world problem solving.",0
"The prediction of protein function can involve predicting subgraphs of a directed acyclic graph that depicts the hierarchy of protein functions, with certain closure properties. Graph neural networks (GNNs) are ideal for this task due to their inductive bias for relational data. However, unlike most GNN applications where the graph is related to the input, in this case, it is related to the label space. Therefore, we propose Tail-GNNs, neural networks that combine naturally with the output space of any neural network for multi-task prediction, to provide relationally-reinforced labels. To improve protein function prediction, we combine a Tail-GNN with a dilated convolutional network that learns representations of the protein sequence. Our approach significantly enhances the F_1 score, highlighting the effectiveness of Tail-GNNs in learning useful representations of labels and utilizing them in real-world problem-solving contexts.",1
"With promising results of machine learning based models in computer vision, applications on medical imaging data have been increasing exponentially. However, generalizations to complex real-world clinical data is a persistent problem. Deep learning models perform well when trained on standardized datasets from artificial settings, such as clinical trials. However, real-world data is different and translations are yielding varying results. The complexity of real-world applications in healthcare could emanate from a mixture of different data distributions across multiple device domains alongside the inevitable noise sourced from varying image resolutions, human errors, and the lack of manual gradings. In addition, healthcare applications not only suffer from the scarcity of labeled data, but also face limited access to unlabeled data due to HIPAA regulations, patient privacy, ambiguity in data ownership, and challenges in collecting data from different sources. These limitations pose additional challenges to applying deep learning algorithms in healthcare and clinical translations. In this paper, we utilize self-supervised representation learning methods, formulated effectively in transfer learning settings, to address limited data availability. Our experiments verify the importance of diverse real-world data for generalization to clinical settings. We show that by employing a self-supervised approach with transfer learning on a multi-domain real-world dataset, we can achieve 16% relative improvement on a standardized dataset over supervised baselines.",0
"The use of machine learning models in computer vision has shown promising results, leading to a surge in applications for medical imaging data. However, there is a persistent problem with generalizing these models to complex real-world clinical data. Although deep learning models perform well when trained on standardized datasets from clinical trials, the translation to real-world data leads to varying outcomes due to the complexity of healthcare applications. This complexity arises from the diversity of data distributions across multiple device domains, as well as noise from varying image resolutions, human errors, and a lack of manual gradings. In addition, healthcare applications face challenges with limited access to both labeled and unlabeled data due to HIPAA regulations, patient privacy, data ownership ambiguity, and difficulties in data collection from various sources. These limitations present additional obstacles to the implementation of deep learning algorithms in healthcare and clinical translations. To address these limitations, we propose the use of self-supervised representation learning methods in transfer learning settings to overcome limited data availability. Our experiments demonstrate the importance of diverse real-world data for generalization to clinical settings. By utilizing a self-supervised approach with transfer learning on a multi-domain real-world dataset, we achieve a 16% relative improvement on a standardized dataset over supervised baselines.",1
"Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work, we investigate object compositionality as an inductive bias for Generative Adversarial Networks (GANs). We present a minimal modification of a standard generator to incorporate this inductive bias and find that it reliably learns to generate images as compositions of objects. Using this general design as a backbone, we then propose two useful extensions to incorporate dependencies among objects and background. We extensively evaluate our approach on several multi-object image datasets and highlight the merits of incorporating structure for representation learning purposes. In particular, we find that our structured GANs are better at generating multi-object images that are more faithful to the reference distribution. More so, we demonstrate how, by leveraging the structure of the learned generative process, one can `invert' the learned generative model to perform unsupervised instance segmentation. On the challenging CLEVR dataset, it is shown how our approach is able to improve over other recent purely unsupervised object-centric approaches to image generation.",0
"The aim of deep generative models is to recover the process used to produce observed data. These models are useful for creating new samples or extracting representations. In image-based applications, successful approaches rely on specific inductive biases. However, there has been a lack of attention to the bias needed to account for how humans structure visual scenes in terms of objects. This study explores object compositionality as an inductive bias for Generative Adversarial Networks (GANs). The researchers modify a standard generator to incorporate this bias and find that it effectively generates images as compositions of objects. They propose two extensions to incorporate dependencies among objects and background. The team evaluates their approach on several multi-object image datasets and shows the benefits of incorporating structure for representation learning purposes. The structured GANs generate more faithful multi-object images than those generated by other models. Additionally, the researchers demonstrate how the structure of the learned generative process can be used for unsupervised instance segmentation. The study shows that this approach outperforms recent unsupervised object-centric approaches on the challenging CLEVR dataset.",1
"This paper studies semi-supervised object classification in relational data, which is a fundamental problem in relational data modeling. The problem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and graph neural networks (e.g. graph convolutional networks). Statistical relational learning methods can effectively model the dependency of object labels through conditional random fields for collective classification, whereas graph neural networks learn effective object representations for classification through end-to-end training. In this paper, we propose the Graph Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-step, one graph neural network learns effective object representations for approximating the posterior distributions of object labels. In the M-step, another graph neural network is used to model the local label dependency. Experiments on object classification, link classification, and unsupervised node representation learning show that GMNN achieves state-of-the-art results.",0
"The primary focus of this research is on the classification of objects in relational data, a crucial aspect of relational data modeling. Previous studies in both statistical relational learning and graph neural networks have extensively tackled this problem. Statistical relational learning methods employ conditional random fields to model object label dependencies for collective classification, whereas graph neural networks train on end-to-end learning to learn effective object representations for classification. The Graph Markov Neural Network (GMNN) proposed in this paper combines the strengths of both approaches. GMNN models the joint distribution of object labels with a conditional random field, which can be trained using the variational EM algorithm. One graph neural network is used in the E-step to approximate the posterior distributions of object labels, while another graph neural network models the local label dependency in the M-step. The results of experiments on object classification, link classification, and unsupervised node representation learning demonstrate that GMNN outperforms other methods in the field.",1
"Many learning tasks involve multi-modal data streams, where continuous data from different modes convey a comprehensive description about objects. A major challenge in this context is how to efficiently interpret multi-modal information in complex environments. This has motivated numerous studies on learning unsupervised representations from multi-modal data streams. These studies aim to understand higher-level contextual information (e.g., a Twitter message) by jointly learning embeddings for the lower-level semantic units in different modalities (e.g., text, user, and location of a Twitter message). However, these methods directly associate each low-level semantic unit with a continuous embedding vector, which results in high memory requirements. Hence, deploying and continuously learning such models in low-memory devices (e.g., mobile devices) becomes a problem. To address this problem, we present METEOR, a novel MEmory and Time Efficient Online Representation learning technique, which: (1) learns compact representations for multi-modal data by sharing parameters within semantically meaningful groups and preserves the domain-agnostic semantics; (2) can be accelerated using parallel processes to accommodate different stream rates while capturing the temporal changes of the units; and (3) can be easily extended to capture implicit/explicit external knowledge related to multi-modal data streams. We evaluate METEOR using two types of multi-modal data streams (i.e., social media streams and shopping transaction streams) to demonstrate its ability to adapt to different domains. Our results show that METEOR preserves the quality of the representations while reducing memory usage by around 80% compared to the conventional memory-intensive embeddings.",0
"Learning from multi-modal data streams, which provide a comprehensive description of objects through continuous data from different modes, presents a significant challenge in complex environments. To address this, different studies have explored learning unsupervised representations from multi-modal data streams to understand higher-level contextual information by jointly learning embeddings for lower-level semantic units in different modalities. However, these methods associate each low-level semantic unit with a continuous embedding vector, which results in high memory requirements and hinders deployment in low-memory devices. To overcome this problem, we propose METEOR, a novel technique that learns compact representations for multi-modal data by sharing parameters within semantically meaningful groups and preserving domain-agnostic semantics. It can also be accelerated using parallel processes and extended to capture implicit/explicit external knowledge related to multi-modal data streams. We evaluate METEOR using social media streams and shopping transaction streams, showing that it can adapt to different domains, reduce memory usage by around 80%, and preserve the quality of the representations.",1
"Electronic medical record (EMR) data contains historical sequences of visits of patients, and each visit contains rich information, such as patient demographics, hospital utilisation and medical codes, including diagnosis, procedure and medication codes. Most existing EMR embedding methods capture visit-code associations by constructing input visit representations as binary vectors with a static vocabulary of medical codes. With this limited representation, they fail in encapsulating rich attribute information of visits (demographics and utilisation information) and/or codes (e.g., medical code descriptions). Furthermore, current work considers visits of the same patient as discrete-time events and ignores time gaps between them. However, the time gaps between visits depict dynamics of the patient's medical history inducing varying influences on future visits. To address these limitations, we present $\mathtt{MedGraph}$, a supervised EMR embedding method that captures two types of information: (1) the visit-code associations in an attributed bipartite graph, and (2) the temporal sequencing of visits through a point process. $\mathtt{MedGraph}$ produces Gaussian embeddings for visits and codes to model the uncertainty. We evaluate the performance of $\mathtt{MedGraph}$ through an extensive experimental study and show that $\mathtt{MedGraph}$ outperforms state-of-the-art EMR embedding methods in several medical risk prediction tasks.",0
"The data contained in electronic medical records (EMRs) includes a patient's visit history, which holds valuable information such as their demographics, hospital usage, and medical codes including diagnoses, procedures, and medications. However, current EMR embedding methods fail to fully capture rich attribute information of visits and codes due to their limited representation as binary vectors with a static vocabulary. Additionally, these methods consider visits as discrete-time events and do not account for the time gaps between them, which can provide insight into a patient's medical history and influence future visits. To address these limitations, we propose $\mathtt{MedGraph}$, a supervised EMR embedding method that utilizes an attributed bipartite graph to capture visit-code associations and a point process to model the temporal sequencing of visits. $\mathtt{MedGraph}$ produces Gaussian embeddings to account for uncertainty and outperforms existing methods in medical risk prediction tasks, as demonstrated in our extensive experimental study.",1
"With the wide adoption of mobile devices, today's location tracking systems such as satellites, cellular base stations and wireless access points are continuously producing tremendous amounts of location data of moving objects. The ability to discover moving objects that travel together, i.e., traveling companions, from their trajectories is desired by many applications such as intelligent transportation systems and location-based services. Existing algorithms are either based on pattern mining methods that define a particular pattern of traveling companions or based on representation learning methods that learn similar representations for similar trajectories. The former methods suffer from the pairwise point-matching problem and the latter often ignore the temporal proximity between trajectories. In this work, we propose a generic deep representation learning model using autoencoders, namely, ATTN-MEAN, for the discovery of traveling companions. ATTN-MEAN collectively injects spatial and temporal information into its input embeddings using skip-gram, positional encoding techniques, respectively. Besides, our model further encourages trajectories to learn from their neighbours by leveraging the Sort-Tile-Recursive algorithm, mean operation and global attention mechanism. After obtaining the representations from the encoders, we run DBSCAN to cluster the representations to find travelling companion. The corresponding trajectories in the same cluster are considered as traveling companions. Experimental results suggest that ATTN-MEAN performs better than the state-of-the-art algorithms on finding traveling companions.",0
"The use of mobile devices has led to a vast amount of location data being produced by location tracking systems like satellites, cellular base stations, and wireless access points. Finding traveling companions from their trajectories is a desirable capability for applications such as intelligent transportation systems and location-based services. Current methods for achieving this goal either rely on pattern mining or representation learning approaches. However, these methods have their limitations. Pattern mining methods are hindered by the pairwise point-matching problem, while representation learning methods often overlook temporal proximity. In this study, we introduce a novel deep representation learning model called ATTN-MEAN, which utilizes autoencoders to discover traveling companions. By incorporating skip-gram and positional encoding techniques, ATTN-MEAN includes spatial and temporal information in its input embeddings. Additionally, our model leverages the Sort-Tile-Recursive algorithm, mean operation, and global attention mechanism to encourage trajectories to learn from their neighbors. After encoding the representations, we use DBSCAN to cluster them and identify travel companions. Our experimental results indicate that ATTN-MEAN outperforms existing algorithms in detecting traveling companions.",1
"In order to deal with the curse of dimensionality in reinforcement learning (RL), it is common practice to make parametric assumptions where values or policies are functions of some low dimensional feature space. This work focuses on the representation learning question: how can we learn such features? Under the assumption that the underlying (unknown) dynamics correspond to a low rank transition matrix, we show how the representation learning question is related to a particular non-linear matrix decomposition problem. Structurally, we make precise connections between these low rank MDPs and latent variable models, showing how they significantly generalize prior formulations for representation learning in RL. Algorithmically, we develop FLAMBE, which engages in exploration and representation learning for provably efficient RL in low rank transition models.",0
"To address the issue of the curse of dimensionality in reinforcement learning (RL), it is customary to assume that values or policies are dependent on a low-dimensional feature space. This study focuses on how to acquire these features by exploring the representation learning question. Assuming that the unknown dynamics correspond to a low-rank transition matrix, we demonstrate that the representation learning question is related to a specific non-linear matrix decomposition problem. Additionally, we establish precise links between low-rank Markov decision processes (MDPs) and latent variable models, demonstrating that they broaden prior formulations for representation learning in RL. We also introduce FLAMBE, an algorithm that conducts exploration and representation learning for verifiably efficient RL in low-rank transition models.",1
"The advancement of visual tracking has continuously been brought by deep learning models. Typically, supervised learning is employed to train these models with expensive labeled data. In order to reduce the workload of manual annotations and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. The motivation of our unsupervised learning is that a robust tracker should be effective in bidirectional tracking. Specifically, the tracker is able to forward localize a target object in successive frames and backtrace to its initial position in the first frame. Based on such a motivation, in the training process, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch merely using unlabeled videos. We build our framework on a Siamese correlation filter network, and propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy as classic fully supervised trackers while achieving a real-time speed. Furthermore, our unsupervised framework exhibits a potential in leveraging more unlabeled or weakly labeled data to further improve the tracking accuracy.",0
"Deep learning models have been consistently advancing visual tracking. Typically, expensive labeled data is used to train these models through supervised learning. To reduce the manual annotation workload and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. Our motivation for unsupervised learning is that a robust tracker should be effective in bidirectional tracking, meaning it can localize a target object in successive frames and trace back to its initial position in the first frame. During training, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch using only unlabeled videos. Our framework is based on a Siamese correlation filter network, and we propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Our proposed unsupervised tracker achieves baseline accuracy comparable to classic fully supervised trackers while maintaining real-time speed. Additionally, our unsupervised framework has the potential to leverage more unlabeled or weakly labeled data to further improve tracking accuracy.",1
"Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their efficiency in face parsing, which however overlook the correlation among different face regions. The correlation is a critical clue about the facial appearance, pose, expression etc., and should be taken into account for face parsing. To this end, we propose to model and reason the region-wise relations by learning graph representations, and leverage the edge information between regions for optimized abstraction. Specifically, we encode a facial image onto a global graph representation where a collection of pixels (""regions"") with similar features are projected to each vertex. Our model learns and reasons over relations between the regions by propagating information across vertices on the graph. Furthermore, we incorporate the edge information to aggregate the pixel-wise features onto vertices, which emphasizes on the features around edges for fine segmentation along edges. The finally learned graph representation is projected back to pixel grids for parsing. Experiments demonstrate that our model outperforms state-of-the-art methods on the widely used Helen dataset, and also exhibits the superior performance on the large-scale CelebAMask-HQ and LaPa dataset. The code is available at https://github.com/tegusi/EAGRNet.",0
"Recently, there has been a lot of interest in face parsing, which involves assigning a pixel-wise label to each facial component. While previous methods have been successful in this task, they fail to consider the correlation between different facial regions, which is crucial for understanding facial appearance, pose, and expression. To address this issue, we propose a method that models and reasons about region-wise relations using graph representations. Our approach encodes facial images onto a global graph, where pixels with similar features are projected onto vertices. We then use information propagation to reason about relations between regions and incorporate edge information to better capture fine segmentation along edges. Our method outperforms state-of-the-art approaches on the Helen, CelebAMask-HQ, and LaPa datasets. The code is available at https://github.com/tegusi/EAGRNet.",1
"This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.",0
"In this research, a method of knowledge distillation is introduced for foreground object search (FoS). FoS is the process of retrieving compatible foregrounds within a certain category for later image composition, given a background and a rectangle specifying the foreground location and scale. The compatible foregrounds within the same category can be grouped into a small number of patterns, and these instances are called interchangeable foregrounds. To build a pattern-level FoS dataset containing labels of interchangeable foregrounds, a pipeline is presented, followed by the establishment of a benchmark dataset for further training and testing. The proposed method involves training a foreground encoder to learn representations of interchangeable foregrounds and a query encoder to learn query-foreground compatibility. The knowledge distillation framework transfers knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP), as demonstrated by extensive experimental results. The benchmark dataset and code will be released shortly.",1
"Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at https://github.com/xiaobaishu0097/ECCV-VN.git.",0
"The objective of target-driven visual navigation is to guide an agent towards a specified target by relying on the agent's observation. To accomplish this task, it is crucial to develop a strong visual representation and a resilient navigation policy. This paper proposes three techniques to enhance these two components: object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG enhances visual representation by incorporating object relationships, such as category proximity and spatial correlations. Trial-driven IL and TPN strengthen navigation policy by directing the agent to avoid deadlocks, such as looping or being stuck. Trial-driven IL is a form of supervision used in policy network training, while TPN mimics IL supervision in unseen environments during testing. The effectiveness of each technique was validated through experiments in the AI2-Thor simulated environment. When combined, the techniques significantly improved navigation effectiveness and efficiency in unfamiliar environments, resulting in a 22.8% increase in success rate and a 23.5% increase in Success weighted by Path Length (SPL). The code for this research is available at https://github.com/xiaobaishu0097/ECCV-VN.git.",1
Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at \url{https://github.com/facebookresearch/Adversarial-Continual-Learning}.,0
"The objective of continual learning is to acquire new abilities while retaining previously gained ones. Our assumption is that the techniques employed to solve each task in a series share a common structure while also possessing unique properties specific to each task. We have discovered that the shared attributes are less susceptible to being forgotten. To address this, we have devised an innovative approach that employs a hybrid continual learning framework. This framework learns a separate representation for features that are invariant across tasks and those that are specific to each task. Our model leverages architecture growth to prevent the loss of task-specific abilities and an experience replay approach to conserve shared abilities. We have demonstrated the effectiveness of our hybrid approach in preventing forgetfulness and have proven it to be superior to both architecture-based and memory-based approaches in classifying images in a single dataset and a series of multiple datasets. Our code is available on \url{https://github.com/facebookresearch/Adversarial-Continual-Learning}.",1
"We introduce a novel self-supervised learning approach to learn representations of videos that are responsive to changes in the motion dynamics. Our representations can be learned from data without human annotation and provide a substantial boost to the training of neural networks on small labeled data sets for tasks such as action recognition, which require to accurately distinguish the motion of objects. We promote an accurate learning of motion without human annotation by training a neural network to discriminate a video sequence from its temporally transformed versions. To learn to distinguish non-trivial motions, the design of the transformations is based on two principles: 1) To define clusters of motions based on time warps of different magnitude; 2) To ensure that the discrimination is feasible only by observing and analyzing as many image frames as possible. Thus, we introduce the following transformations: forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments show that networks trained with the proposed method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.",0
"Our approach to self-supervised learning introduces a new way to learn video representations that are responsive to changes in motion dynamics. The representations can be learned without human annotation and significantly improve the training of neural networks on small labeled data sets for tasks such as action recognition. To achieve accurate learning of motion without human annotation, we train a neural network to discriminate a video sequence from its temporally transformed versions. We base the design of the transformations on two principles: defining clusters of motions based on time warps of different magnitudes and ensuring that discrimination is feasible only by analyzing as many image frames as possible. Our proposed transformations include forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments demonstrate that networks trained with our method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.",1
"Complex categorical data is often hierarchically coupled with heterogeneous relationships between attributes and attribute values and the couplings between objects. Such value-to-object couplings are heterogeneous with complementary and inconsistent interactions and distributions. Limited research exists on unlabeled categorical data representations, ignores the heterogeneous and hierarchical couplings, underestimates data characteristics and complexities, and overuses redundant information, etc. The deep representation learning of unlabeled categorical data is challenging, overseeing such value-to-object couplings, complementarity and inconsistency, and requiring large data, disentanglement, and high computational power. This work introduces a shallow but powerful UNsupervised heTerogeneous couplIng lEarning (UNTIE) approach for representing coupled categorical data by untying the interactions between couplings and revealing heterogeneous distributions embedded in each type of couplings. UNTIE is efficiently optimized w.r.t. a kernel k-means objective function for unsupervised representation learning of heterogeneous and hierarchical value-to-object couplings. Theoretical analysis shows that UNTIE can represent categorical data with maximal separability while effectively represent heterogeneous couplings and disclose their roles in categorical data. The UNTIE-learned representations make significant performance improvement against the state-of-the-art categorical representations and deep representation models on 25 categorical data sets with diversified characteristics.",0
"Categorical data that is complex often has hierarchical relationships between attributes and their values, as well as couplings between different objects. These couplings are heterogeneous, with interactions and distributions that can be both complementary and inconsistent. Unfortunately, existing research on unlabeled categorical data representations often ignores these complexities, leading to the overuse of redundant information. Additionally, deep representation learning of unlabeled categorical data is challenging due to the need to oversee value-to-object couplings, complementarity and inconsistency, and the requirement for large data, disentanglement, and high computational power. To address these challenges, this work presents a novel approach called UNsupervised heTerogeneous couplIng lEarning (UNTIE), which uses a kernel k-means objective function for unsupervised representation learning of heterogeneous and hierarchical value-to-object couplings. Theoretical analysis demonstrates that UNTIE can effectively represent categorical data while maximizing separability, and the UNTIE-learned representations outperform state-of-the-art categorical representations and deep representation models on 25 categorical data sets.",1
"Graph neural networks have achieved great success in learning node representations for graph tasks such as node classification and link prediction. Graph representation learning requires graph pooling to obtain graph representations from node representations. It is challenging to develop graph pooling methods due to the variable sizes and isomorphic structures of graphs. In this work, we propose to use second-order pooling as graph pooling, which naturally solves the above challenges. In addition, compared to existing graph pooling methods, second-order pooling is able to use information from all nodes and collect second-order statistics, making it more powerful. We show that direct use of second-order pooling with graph neural networks leads to practical problems. To overcome these problems, we propose two novel global graph pooling methods based on second-order pooling; namely, bilinear mapping and attentional second-order pooling. In addition, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. We perform thorough experiments on graph classification tasks to demonstrate the effectiveness and superiority of our proposed methods. Experimental results show that our methods improve the performance significantly and consistently.",0
"The success of graph neural networks in learning node representations for graph tasks like node classification and link prediction is widely acknowledged. However, the process of obtaining graph representations from node representations (known as graph pooling) poses a challenge due to the variable sizes and isomorphic structures of graphs. To address this issue, we suggest employing second-order pooling as graph pooling, which overcomes these challenges and allows for the use of information from all nodes and collection of second-order statistics. However, direct usage of second-order pooling with graph neural networks has its practical challenges. Hence, we introduce two novel global graph pooling techniques, namely bilinear mapping and attentional second-order pooling, to tackle these issues. Furthermore, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. Thorough experiments conducted on graph classification tasks demonstrate the effectiveness and superiority of our proposed methods, which significantly improve performance.",1
"The problem of catastrophic forgetting occurs in deep learning models trained on multiple databases in a sequential manner. Recently, generative replay mechanisms (GRM), have been proposed to reproduce previously learned knowledge aiming to reduce the forgetting. However, such approaches lack an appropriate inference model and therefore can not provide latent representations of data. In this paper, we propose a novel lifelong learning approach, namely the Lifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay network but also learns meaningful latent representations, benefiting representation learning. L-VAEGAN can allow to automatically embed the information associated with different domains into several clusters in the latent space, while also capturing semantically meaningful shared latent variables, across different data domains. The proposed model supports many downstream tasks that traditional generative replay methods can not, including interpolation and inference across different data domains.",0
"The issue of catastrophic forgetting is present in deep learning models that are trained on multiple databases in a sequential manner. To combat this problem, generative replay mechanisms (GRM) have been suggested to replicate previously learned knowledge to reduce forgetting. However, these methods lack a suitable inference model, making it impossible to provide latent representations of data. In this study, we propose a new lifelong learning approach, called Lifelong VAEGAN (L-VAEGAN), which not only introduces a powerful generative replay network but also learns significant latent representations that benefit representation learning. L-VAEGAN can automatically categorize information from different domains into multiple clusters in the latent space while capturing semantically meaningful shared latent variables across various data domains. The proposed model supports several downstream tasks that conventional generative replay methods cannot, including interpolation and inference across distinct data domains.",1
"For person re-identification, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identification. Code is available at https://github.com/ShengcaiLiao/QAConv.",0
"Current deep networks for person re-identification mostly focus on representation learning, which is limited in its ability to handle unforeseen situations without transfer learning. This paper explores an alternative approach to person image matching, which involves formulating it directly in deep feature maps. By treating image matching as the process of finding local correspondences in feature maps, and constructing query-adaptive convolution kernels to achieve local matching, the method becomes more adaptable to unforeseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training, a class memory module is built to cache feature maps of recent samples for each class, enabling computation of image matching losses for metric learning. Compared to popular learning methods, the proposed Query-Adaptive Convolution (QAConv) method yields large improvements (about 10%+ mAP) in direct cross-dataset evaluation, and produces comparable results to many transfer learning methods. Additionally, a model-free temporal cooccurrence-based score weighting method called TLift is introduced, resulting in state-of-the-art performance in cross-dataset person re-identification. The code for this method is available at https://github.com/ShengcaiLiao/QAConv.",1
"Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",0
"The use of network science methods to analyze medical imaging of the human brain is widespread. However, due to the complexity of individual brain network topology, extracting a distinct network representation from multimodal brain networks can be challenging. Current deep learning techniques have shown promise in modeling non-linear cross-modality relationships on graph-structured data. Nevertheless, existing deep brain network methods either disregard intrinsic graph topology or require a shared network basis within a group. To overcome these obstacles, we propose a new deep graph representation learning method (Deep Multimodal Brain Networks - DMBN) that integrates multimodal brain networks. Our approach involves encoding and decoding the graph to decipher the cross-modality relationship. We learn higher-order network mappings from brain structural networks to functional networks in the node domain. The learned network representation is a set of node features that enable the induction of brain saliency maps in a supervised manner. We tested our approach on both synthetic and real image data, and the results demonstrate its superior performance compared to other deep brain network models.",1
"Deep neural networks have gained tremendous success in a broad range of machine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a theoretically proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn genuine class structures that are implicitly embedded in the randomly projected space. Empirical results on 19 real-world datasets show that our learned representations substantially outperform a few state-of-the-art competing methods in both anomaly detection and clustering tasks. Code is available at https://git.io/RDP",0
"Due to its ability to learn semantic-rich features from high-dimensional data, deep neural networks have achieved great success in various machine learning tasks. However, they typically require a large amount of labeled data to learn such features effectively. This limitation makes it difficult for them to be adapted for unsupervised learning tasks like anomaly detection and clustering and restricts their use in critical domains where acquiring a massive amount of labeled data is prohibitively expensive. To overcome this challenge, our work proposes a method to learn features without using any labeled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a proven approach to obtain approximately preserved distances. To accurately predict these random distances, we optimize the representation learner to learn genuine class structures implicitly embedded in the randomly projected space. We demonstrate the effectiveness of our proposed method on 19 real-world datasets, where our learned representations outperform several state-of-the-art competing methods in both anomaly detection and clustering tasks. Our code is available at https://git.io/RDP.",1
"Graph neural networks get significant attention for graph representation and classification in machine learning community. Attention mechanism applied on the neighborhood of a node improves the performance of graph neural networks. Typically, it helps to identify a neighbor node which plays more important role to determine the label of the node under consideration. But in real world scenarios, a particular subset of nodes together, but not the individual pairs in the subset, may be important to determine the label of the graph. To address this problem, we introduce the concept of subgraph attention for graphs. On the other hand, hierarchical graph pooling has been shown to be promising in recent literature. But due to noisy hierarchical structure of real world graphs, not all the hierarchies of a graph play equal role for graph classification. Towards this end, we propose a graph classification algorithm called SubGattPool which jointly learns the subgraph attention and employs two different types of hierarchical attention mechanisms to find the important nodes in a hierarchy and the importance of individual hierarchies in a graph. Experimental evaluation with different types of graph classification algorithms shows that SubGattPool is able to improve the state-of-the-art or remains competitive on multiple publicly available graph classification datasets. We conduct further experiments on both synthetic and real world graph datasets to justify the usefulness of different components of SubGattPool and to show its consistent performance on other downstream tasks.",0
"The machine learning community is placing significant emphasis on graph neural networks for graph representation and classification. By using an attention mechanism on a node's neighborhood, the performance of graph neural networks can be enhanced. This mechanism helps to identify the most vital neighbor node in determining the label of the node being considered. However, in real-world scenarios, the collective importance of a group of nodes, rather than individual pairs, may be necessary to determine the graph's label. To address this issue, we introduce the concept of subgraph attention to graphs. Additionally, hierarchical graph pooling has shown promise in recent literature; however, due to the noisy hierarchical structure of real-world graphs, not all hierarchies play an equal role in graph classification. To solve this problem, we propose the SubGattPool graph classification algorithm, which simultaneously learns subgraph attention and utilizes two distinct types of hierarchical attention mechanisms to identify the critical nodes in a hierarchy and the significance of individual hierarchies in a graph. Experimental evaluations with various graph classification algorithms demonstrate that SubGattPool is either able to improve the state-of-the-art or remain competitive on multiple publicly available graph classification datasets. We conduct further experiments on synthetic and real-world graph datasets to justify the usefulness of different components of SubGattPool and to demonstrate its consistent performance on other downstream tasks.",1
"Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.",0
"Graph neural networks have achieved notable success in graph representation learning. Graph convolutions are a crucial graph operation that performs neighborhood aggregation. However, the performance of neighborhood aggregation methods decreases as the depth increases to enable larger receptive fields, as one layer only considers immediate neighbors. Numerous recent studies have attributed this performance decay to the over-smoothing issue, where repeated propagation makes representations of nodes from different classes indistinguishable. In this study, we analyze this issue systematically and discover that the entanglement of representation transformation and propagation in current graph convolution operations is the primary factor compromising performance significantly. Decoupling these operations allows for deeper graph neural networks to learn graph node representations from larger receptive fields. Our theoretical and empirical analysis leads to the development of Deep Adaptive Graph Neural Network (DAGNN), which incorporates information from large receptive fields adaptively. Our analysis and insights are confirmed by experiments on citation, co-authorship, and co-purchase datasets, demonstrating the superiority of our proposed methods.",1
"Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.",0
"The use of self-supervision has become increasingly popular for training convolutional neural networks (CNNs) in order to achieve more transferable, generalizable, and robust image representation learning. However, it has not been widely explored in graph convolutional networks (GCNs) that operate on graph data. This study presents the first systematic exploration and evaluation of incorporating self-supervision into GCNs. Three mechanisms for implementing self-supervision into GCNs are elaborated upon, and the limitations of pretraining & finetuning and self-training are analyzed. Multi-task learning is then focused on, and three novel self-supervised learning tasks for GCNs are proposed, each with theoretical rationales and numerical comparisons. Finally, multi-task self-supervision is integrated into graph adversarial training. Results show that with appropriately designed task forms and incorporation mechanisms, self-supervision can enhance the generalizability and robustness of GCNs. The codes for this study are available at https://github.com/Shen-Lab/SS-GCNs.",1
"This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.",0
"This paper investigates the challenge of learning deep generative models, which comprise multiple layers of latent variables arranged in top-down structures. These models offer high expressiveness and facilitate learning hierarchical representations. To learn such a generative model, it is necessary to infer the latent variables for each training example based on the posterior distribution of these variables, which is often accomplished using Markov chain Monte Carlo (MCMC). However, MCMC can be time-consuming. In this study, we propose using non-persistent short run MCMC, like finite step Langevin dynamics initialized from the prior distribution of the latent variables, to serve as an approximate inference engine, with the step size of the Langevin dynamics optimized variably by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Results of our experiments indicate that the proposed approach performs better than variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. A significant advantage of our method is its simplicity and automation, obviating the need for an inference model.",1
"Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR$^2$. Code is available at https://github.com/ChenRocks/UNITER.",0
"Most Vision-and-Language (V+L) tasks rely on joint image-text embedding to enable simultaneous processing of multimodality inputs for visual and textual understanding. This study introduces UNITER, a UNiversal Image-TExt Representation, which is learned through large-scale pre-training across four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions) and is capable of powering a range of downstream V+L tasks with joint multimodal embeddings. The pre-training process involves four tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Unlike previous approaches that use joint random masking for both modalities, this study employs conditional masking for pre-training tasks, where masked language/region modeling is conditioned on the full observation of image/text. Additionally, the study proposes WRA using Optimal Transport (OT) to encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis reveals that both conditional masking and OT-based WRA contribute to better pre-training, and an optimal combination of pre-training tasks is identified through a thorough ablation study. The results show that UNITER achieves new state-of-the-art performance across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR$^2$. The code for UNITER is publicly available at https://github.com/ChenRocks/UNITER.",1
"Building in silico models to predict chemical properties and activities is a crucial step in drug discovery. However, limited labeled data often hinders the application of deep learning in this setting. Meanwhile advances in meta-learning have enabled state-of-the-art performances in few-shot learning benchmarks, naturally prompting the question: Can meta-learning improve deep learning performance in low-resource drug discovery projects? In this work, we assess the transferability of graph neural networks initializations learned by the Model-Agnostic Meta-Learning (MAML) algorithm - and its variants FO-MAML and ANIL - for chemical properties and activities tasks. Using the ChEMBL20 dataset to emulate low-resource settings, our benchmark shows that meta-initializations perform comparably to or outperform multi-task pre-training baselines on 16 out of 20 in-distribution tasks and on all out-of-distribution tasks, providing an average improvement in AUPRC of 11.2% and 26.9% respectively. Finally, we observe that meta-initializations consistently result in the best performing models across fine-tuning sets with $k \in \{16, 32, 64, 128, 256\}$ instances.",0
"Developing virtual models to anticipate chemical characteristics and activities is a crucial aspect of drug discovery. Nonetheless, there is often a shortage of labelled data that impedes the application of deep learning in this context. Conversely, the advancements in meta-learning have facilitated top-notch performances in few-shot learning benchmarks, leading to the question: Can meta-learning enhance deep learning performance in drug discovery projects with limited resources? In this study, we evaluate the transferability of graph neural networks initializations acquired via the Model-Agnostic Meta-Learning (MAML) algorithm, along with its variations FO-MAML and ANIL, for chemical properties and activities tasks. By utilizing the ChEMBL20 dataset to mimic low-resource environments, our findings reveal that meta-initializations accomplish similar or superior results compared to multi-task pre-training baselines in 16 out of 20 in-distribution tasks and all out-of-distribution tasks, yielding an average improvement of 11.2% and 26.9% in AUPRC, respectively. Ultimately, we observe that meta-initializations consistently yield the best-performing models across fine-tuning sets that comprise instances of $k \in \{16, 32, 64, 128, 256\}$.",1
"Several animal species (e.g., bats, dolphins, and whales) and even visually impaired humans have the remarkable ability to perform echolocation: a biological sonar used to perceive spatial layout and locate objects in the world. We explore the spatial cues contained in echoes and how they can benefit vision tasks that require spatial reasoning. First we capture echo responses in photo-realistic 3D indoor scene environments. Then we propose a novel interaction-based representation learning framework that learns useful visual features via echolocation. We show that the learned image features are useful for multiple downstream vision tasks requiring spatial reasoning---monocular depth estimation, surface normal estimation, and visual navigation---with results comparable or even better than heavily supervised pre-training. Our work opens a new path for representation learning for embodied agents, where supervision comes from interacting with the physical world.",0
"The ability to use echolocation, a biological sonar, to perceive spatial layout and locate objects is remarkable and observed in various animal species (such as bats, dolphins, and whales) and visually impaired humans. In this study, we investigate the spatial cues present in echoes and explore how they can aid in vision tasks that require spatial reasoning. We begin by recording echo responses in 3D indoor scenes that are photo-realistic. Then, we propose a unique interaction-based representation learning framework that uses echolocation to learn useful visual features. Our results show that the acquired image features are effective for multiple downstream vision tasks, including surface normal estimation, monocular depth estimation, and visual navigation. These outcomes are comparable or better than heavily supervised pre-training methods. Our research paves the way for representation learning for embodied agents, where guidance comes from interacting with the physical world.",1
"Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empirical successes recently, theoretical understanding of its performance was still missing. In a recent work, Arora et al. (2019) provide the first generalisation bounds for CURL, relying on a Rademacher complexity. We extend their framework to the flexible PAC-Bayes setting, allowing us to deal with the non-iid setting. We present PAC-Bayesian generalisation bounds for CURL, which are then used to derive a new representation learning algorithm. Numerical experiments on real-life datasets illustrate that our algorithm achieves competitive accuracy, and yields non-vacuous generalisation bounds.",0
"The latest approach to acquiring a set of features from untagged data is Contrastive unsupervised representation learning (CURL). Although CURL has achieved numerous practical successes, its performance was not comprehensively understood. Arora et al. (2019) recently provided the first set of generalisation bounds for CURL using Rademacher complexity. Our study further extends their framework to the PAC-Bayes setting, enabling us to handle the non-iid setting. We propose PAC-Bayesian generalisation bounds for CURL, which we employ to develop an innovative representation learning algorithm. Moreover, our numerical experiments on real-life datasets demonstrate that our algorithm attains high accuracy and produces meaningful generalisation bounds.",1
"In architecture and computer-aided design, wireframes (i.e., line-based models) are widely used as basic 3D models for design evaluation and fast design iterations. However, unlike a full design file, a wireframe model lacks critical information, such as detailed shape, texture, and materials, needed by a conventional renderer to produce 2D renderings of the objects or scenes. In this paper, we bridge the information gap by generating photo-realistic rendering of indoor scenes from wireframe models in an image translation framework. While existing image synthesis methods can generate visually pleasing images for common objects such as faces and birds, these methods do not explicitly model and preserve essential structural constraints in a wireframe model, such as junctions, parallel lines, and planar surfaces. To this end, we propose a novel model based on a structure-appearance joint representation learned from both images and wireframes. In our model, structural constraints are explicitly enforced by learning a joint representation in a shared encoder network that must support the generation of both images and wireframes. Experiments on a wireframe-scene dataset show that our wireframe-to-image translation model significantly outperforms the state-of-the-art methods in both visual quality and structural integrity of generated images.",0
"Wireframes, which are line-based models, are commonly used in architecture and computer-aided design for design evaluation and quick design iterations. However, wireframe models lack important information, such as shape, texture, and materials, necessary for conventional rendering to create 2D renderings. This paper proposes a solution to this problem by generating photorealistic renderings of indoor scenes from wireframe models using an image translation framework. While current image synthesis methods can create visually appealing images, they do not preserve structural constraints, such as junctions, parallel lines, and planar surfaces, found in wireframe models. To address this, the authors propose a novel model that incorporates both images and wireframes, explicitly enforcing structural constraints through a shared encoder network. Experimental results show that their wireframe-to-image translation model outperforms existing methods in terms of visual quality and structural integrity.",1
"Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDataset for graph classification and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/tudataset.",0
"The interest in (supervised) learning with graph data, particularly utilizing graph neural networks, has been on the rise recently. However, the progress in this field is being held back due to the lack of meaningful benchmark datasets and standardized evaluation procedures. In order to address this issue, we have introduced the TUDataset for graph classification and regression, which includes more than 120 datasets of varying sizes from a broad range of applications. We have also provided Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. This article provides an outline of the datasets, standardized evaluation procedures, and baseline experiments. All datasets can be found on www.graphlearning.io, and the experiments can be fully replicated using the code on www.github.com/chrsmrrs/tudataset.",1
"When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a ""shape curriculum"" for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.",0
"Novice sketchers typically begin with basic and adaptable shapes before advancing to more intricate and precise ones in later training sessions. Our paper presents the development of a ""shape curriculum"" called Curriculum DeepSDF, which facilitates the acquisition of continuous Signed Distance Function (SDF) on shapes. Inspired by human learning, Curriculum DeepSDF arranges the learning process in ascending order of difficulty based on two criteria: surface accuracy and sample complexity. The former requires strict supervision with ground truths, while the latter prioritizes the weights of tough training samples near complex geometry and fine structures. Initially, Curriculum DeepSDF learns to reconstruct coarse shapes and subsequently enhances accuracy while focusing on intricate local details. Empirical results demonstrate that a well-designed curriculum results in significantly better shape reconstructions with the same training data, training epochs, and network architecture as DeepSDF. We anticipate that the use of shape curricula can enhance the training process for various 3D shape representation learning approaches.",1
"In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose to use different orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data.",0
"Our work introduces a novel method for unsupervised image segmentation that focuses on maximizing mutual information between various constructed views of the input. Inspired by autoregressive generative models, we utilize different orderings of the input through masked convolutions to create distinct views of the data. Our model produces two valid output predictions for a given input, which are trained to maximize mutual information. These outputs can either be low-dimensional representations or semantic labels for clustering. During training, we use masked convolutions, but during inference, we use standard convolutions. Our proposed approach surpasses the current state-of-the-art for unsupervised image segmentation and is simple to implement. It can also be applied to other visual tasks and integrated into existing unsupervised learning methods that require varied views of the data.",1
"Determining the traffic scenario space is a major challenge for the homologation and coverage assessment of automated driving functions. In contrast to current approaches that are mainly scenario-based and rely on expert knowledge, we introduce two data driven autoencoding models that learn a latent representation of traffic scenes. First is a CNN based spatio-temporal model that autoencodes a grid of traffic participants' positions. Secondly, we develop a pure temporal RNN based model that auto-encodes a sequence of sets. To handle the unordered set data, we had to incorporate the permutation invariance property. Finally, we show how the latent scenario embeddings can be used for clustering traffic scenarios and similarity retrieval.",0
"A significant obstacle in homologation and coverage assessment of automated driving functions is identifying the traffic scenario space. Current approaches rely on scenario-based and expert knowledge, which is insufficient. To address this, we present two data-driven autoencoding models that learn a latent representation of traffic scenes. The first model is a CNN spatio-temporal model that autoencodes a grid of traffic participants' positions, while the second is a pure temporal RNN model that autoencodes a sequence of sets. To handle unordered set data, we incorporated the permutation invariance property. Finally, we demonstrate how the latent scenario embeddings can be used for traffic scenario clustering and similarity retrieval.",1
"Recently there was an increasing interest in applications of graph neural networks in non-Euclidean geometry; however, are non-Euclidean representations always useful for graph learning tasks? For different problems such as node classification and link prediction we compute hyperbolic embeddings and conclude that for tasks that require global prediction consistency it might be useful to use non-Euclidean embeddings, while for other tasks Euclidean models are superior. To do so we first fix an issue of the existing models associated with the optimization process at zero curvature. Current hyperbolic models deal with gradients at the origin in ad-hoc manner, which is inefficient and can lead to numerical instabilities. We solve the instabilities of kappa-Stereographic model at zero curvature cases and evaluate the approach of embedding graphs into the manifold in several graph representation learning tasks.",0
"Lately, there has been an upsurge of interest in the use of graph neural networks in non-Euclidean geometry. However, it remains unclear whether non-Euclidean representations are always advantageous for graph learning tasks. For instance, when dealing with node classification and link prediction, we have found that hyperbolic embeddings are suitable for tasks that necessitate global prediction consistency, while Euclidean models outperform in other tasks. To achieve this, we first addressed a flaw in existing hyperbolic models concerning the optimization process at zero curvature. Currently, the ad-hoc approach used to deal with gradients at the origin can cause numerical instabilities and inefficiencies. We resolved these instabilities in the kappa-Stereographic model for zero curvature cases and proceeded to evaluate our graph embedding approach in various graph representation learning tasks.",1
"Dictionary learning is a classic representation learning method that has been widely applied in signal processing and data analytics. In this paper, we investigate a family of $\ell_p$-norm ($p>2,p \in \mathbb{N}$) maximization approaches for the complete dictionary learning problem from theoretical and algorithmic aspects. Specifically, we prove that the global maximizers of these formulations are very close to the true dictionary with high probability, even when Gaussian noise is present. Based on the generalized power method (GPM), an efficient algorithm is then developed for the $\ell_p$-based formulations. We further show the efficacy of the developed algorithm: for the population GPM algorithm over the sphere constraint, it first quickly enters the neighborhood of a global maximizer, and then converges linearly in this region. Extensive experiments will demonstrate that the $\ell_p$-based approaches enjoy a higher computational efficiency and better robustness than conventional approaches and $p=3$ performs the best.",0
"The method of dictionary learning has been extensively used in data analytics and signal processing as a means of representation learning. This study focuses on a group of $\ell_p$-norm ($p>2,p \in \mathbb{N}$) maximization techniques for the complete dictionary learning problem, examining both theoretical and algorithmic aspects. The research shows that the global maximizers of these formulations are highly likely to be very close to the true dictionary, even in the presence of Gaussian noise. An efficient algorithm based on the generalized power method (GPM) is then developed, specifically for the $\ell_p$-based formulations. The study also demonstrates the effectiveness of the developed algorithm, revealing that for the population GPM algorithm over the sphere constraint, it quickly enters the neighborhood of a global maximizer, and then converges linearly in this region. Extensive experiments highlight the superior computational efficiency and robustness of the $\ell_p$-based approaches compared to conventional approaches, with $p=3$ performing the best.",1
"Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by 2.4% and 1.3%, respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed.",0
"Researchers are exploring the use of Two-Dimensional Convolutional Neural Network (2D CNN) for video characterization, inspired by its success in image recognition. However, the application of 2D CNN to analyze videos has a limitation where identical 2D CNN kernels are used for different frames of a video, leading to repetitive information extraction and neglecting critical frame variations. To address this issue, the paper proposes two solutions: 1) a Progressive Enhancement Module (PEM) that filters discriminative channels of features from different frames sequentially to avoid repeated information extraction, and 2) a Temporal Diversity Loss (TD Loss) that forces the kernels to capture variations among frames instead of image regions with similar appearance. The effectiveness of the proposed method is demonstrated by its visible improvements over the best competitor by 2.4% and 1.3% on benchmark temporal reasoning datasets Something-Something V1 and V2, respectively. Furthermore, performance improvements over the 2D-CNN-based state-of-the-art on the large-scale dataset Kinetics are also observed.",1
"Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored. To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e.,CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis .",0
"Medical images contain a wealth of information about human anatomy, including recurring patterns that offer great potential for powerful semantic representation learning. However, there is a lack of research into how to effectively harness this rich semantic information for self-supervised learning. In order to address this gap, we have developed a pre-trained 3D model called Semantic Genesis, which uses self-discovery, self-classification, and self-restoration to learn semantically enriched visual representation of anatomy in medical images. We compared Semantic Genesis to other publicly available pre-trained models on six different medical tasks, including classification and segmentation using CT, MRI, and X-ray images. Our experiments demonstrate that Semantic Genesis outperforms other 3D models and ImageNet-based transfer learning in 2D, thanks to our novel self-supervised learning framework that leverages consistent anatomical patterns in medical images. The code and pre-trained Semantic Genesis model are available at https://github.com/JLiangLab/SemanticGenesis.",1
"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",0
"Analyzing data that is structured as a graph requires the fundamental use of graph representation learning in today's world. In this study, we propose a new framework for unsupervised graph representation learning that leverages a contrastive objective at the node level, inspired by the recent success of contrastive methods. We generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To ensure diverse node contexts for the contrastive objective, we propose a hybrid scheme that generates graph views on both structure and attribute levels. In addition, we provide theoretical justification for our motivation from two perspectives, mutual information and the classical triplet loss. We conduct empirical experiments on a variety of real-world datasets for both transductive and inductive learning tasks. Our experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art methods by large margins, despite its simplicity. Furthermore, our unsupervised method even outperforms its supervised counterparts on transductive tasks, demonstrating its great potential for real-world applications.",1
"Time series prediction is an important problem in machine learning. Previous methods for time series prediction did not involve additional information. With a lot of dynamic knowledge graphs available, we can use this additional information to predict the time series better. Recently, there has been a focus on the application of deep representation learning on dynamic graphs. These methods predict the structure of the graph by reasoning over the interactions in the graph at previous time steps. In this paper, we propose a new framework to incorporate the information from dynamic knowledge graphs for time series prediction. We show that if the information contained in the graph and the time series data are closely related, then this inter-dependence can be used to predict the time series with improved accuracy. Our framework, DArtNet, learns a static embedding for every node in the graph as well as a dynamic embedding which is dependent on the dynamic attribute value (time-series). Then it captures the information from the neighborhood by taking a relation specific mean and encodes the history information using RNN. We jointly train the model link prediction and attribute prediction. We evaluate our method on five specially curated datasets for this problem and show a consistent improvement in time series prediction results. We release the data and code of model DArtNet for future research at https://github.com/INK-USC/DArtNet .",0
"Machine learning faces the significant task of time series prediction. Prior techniques for this process did not utilize supplementary data. However, with the abundance of dynamic knowledge graphs available, we can harness this additional information to improve time series prediction. Recently, there has been a focus on applying deep representation learning on dynamic graphs. These methods predict graph structure by analyzing previous interactions within the graph. This paper introduces a new framework, DArtNet, that incorporates dynamic knowledge graphs to enhance time series prediction accuracy. By utilizing the interdependence between the graph information and time series data, DArtNet produces improved results. DArtNet generates both static and dynamic embeddings for each node in the graph and captures information from the neighborhood by taking a relation-specific mean. Additionally, it encodes historical information using RNN. The model is jointly trained for link prediction and attribute prediction and demonstrates consistent improvement in time series prediction on five specially curated datasets. We provide the data and code for DArtNet to encourage further research at https://github.com/INK-USC/DArtNet.",1
"Despite their strong modeling capacities, Convolutional Neural Networks (CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale variance, multi-scale feature fusion from different layers or filters attracts great attention among existing solutions, while the more granular kernel space is overlooked. We bridge this regret by exploiting multi-scale features in a finer granularity. The proposed convolution operation, named Poly-Scale Convolution (PSConv), mixes up a spectrum of dilation rates and tactfully allocate them in the individual convolutional kernels of each filter regarding a single convolutional layer. Specifically, dilation rates vary cyclically along the axes of input and output channels of the filters, aggregating features over a wide range of scales in a neat style. PSConv could be a drop-in replacement of the vanilla convolution in many prevailing CNN backbones, allowing better representation learning without introducing additional parameters and computational complexities. Comprehensive experiments on the ImageNet and MS COCO benchmarks validate the superior performance of PSConv. Code and models are available at https://github.com/d-li14/PSConv.",0
"Although Convolutional Neural Networks (CNNs) have strong modeling capabilities, they are often sensitive to scale. Existing solutions to improve CNN robustness to scale variance focus on multi-scale feature fusion from different layers or filters, but neglect the more detailed kernel space. We address this issue by utilizing multi-scale features at a finer granularity. Our proposed Poly-Scale Convolution (PSConv) convolution operation mixes a range of dilation rates and skillfully allocates them in the individual convolutional kernels of each filter in a single convolutional layer. Dilation rates vary cyclically along the axes of input and output channels of the filters, effectively aggregating features over a wide range of scales. PSConv can be easily integrated into existing CNN backbones without introducing additional parameters or computational complexities, leading to improved representation learning. We conducted comprehensive experiments on ImageNet and MS COCO benchmarks, demonstrating the superior performance of PSConv. Code and models are available at https://github.com/d-li14/PSConv.",1
"We present a multi-relational temporal Knowledge Graph based on the daily interactions between artifacts in GitHub, one of the largest social coding platforms. Such representation enables posing many user-activity and project management questions as link prediction and time queries over the knowledge graph. In particular, we introduce two new datasets for i) interpolated time-conditioned link prediction and ii) extrapolated time-conditioned link/time prediction queries, each with distinguished properties. Our experiments on these datasets highlight the potential of adapting knowledge graphs to answer broad software engineering questions. Meanwhile, it also reveals the unsatisfactory performance of existing temporal models on extrapolated queries and time prediction queries in general. To overcome these shortcomings, we introduce an extension to current temporal models using relative temporal information with regards to past events.",0
"Introducing a multi-relational temporal Knowledge Graph, which is constructed based on the daily interactions between artifacts in GitHub, one of the biggest social coding platforms. This representation allows for various user-activity and project management questions, such as link prediction and time queries, to be posed over the knowledge graph. We present two new datasets for interpolated time-conditioned link prediction and extrapolated time-conditioned link/time prediction queries with unique characteristics. Our experiments on these datasets demonstrate the potential of adapting knowledge graphs to address a wide range of software engineering inquiries. However, our findings also reveal the unsatisfactory performance of existing temporal models on extrapolated queries and time prediction queries in general. As a solution to overcome these limitations, we propose an extension to current temporal models that employs relative temporal information concerning past events.",1
"Multi-label classification is the challenging task of predicting the presence and absence of multiple targets, involving representation learning and label correlation modeling. We propose a novel framework for multi-label classification, Multivariate Probit Variational AutoEncoder (MPVAE), that effectively learns latent embedding spaces as well as label correlations. MPVAE learns and aligns two probabilistic embedding spaces for labels and features respectively. The decoder of MPVAE takes in the samples from the embedding spaces and models the joint distribution of output targets under a Multivariate Probit model by learning a shared covariance matrix. We show that MPVAE outperforms the existing state-of-the-art methods on a variety of application domains, using public real-world datasets. MPVAE is further shown to remain robust under noisy settings. Lastly, we demonstrate the interpretability of the learned covariance by a case study on a bird observation dataset.",0
"Multi-label classification is a difficult task that involves predicting multiple targets' presence or absence and requires representation learning and label correlation modeling. Our proposed solution for this challenge is the Multivariate Probit Variational AutoEncoder (MPVAE) framework, which efficiently learns latent embedding spaces and label correlations. MPVAE aligns two probabilistic embedding spaces for labels and features, respectively, and the decoder models the joint distribution of output targets using a Multivariate Probit model. By learning a shared covariance matrix, MPVAE outperforms existing state-of-the-art methods across a variety of domains and remains robust in noisy settings. Additionally, we demonstrate the interpretability of the learned covariance through a case study on a bird observation dataset.",1
"We consider the problem of semi-supervised 3D action recognition which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning. Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on NTU and N-UCLA datasets. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.",0
"The problem of semi-supervised 3D action recognition has received little attention and poses a major challenge in effectively learning motion representations from unlabeled data. While self-supervised learning has proven to be effective in the image domain, there are few approaches that apply to 3D action recognition. Moreover, directly applying SSL for semi-supervised learning results in misaligned representations. To address these issues, we propose a novel framework called Adversarial Self-Supervised Learning (ASSL) that tightly couples SSL and the semi-supervised scheme. ASSL improves the discrimination capability of learned representations by exploring the data relations within a neighborhood and aligns the feature distributions of labeled and unlabeled samples using adversarial regularization. We demonstrate the effectiveness of ASSL in semi-supervised 3D action recognition through extensive experiments on NTU and N-UCLA datasets, which confirm its superiority over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.",1
"Anomaly detection-based spoof attack detection is a recent development in face Presentation Attack Detection (fPAD), where a spoof detector is learned using only non-attacked images of users. These detectors are of practical importance as they are shown to generalize well to new attack types. In this paper, we present a deep-learning solution for anomaly detection-based spoof attack detection where both classifier and feature representations are learned together end-to-end. First, we introduce a pseudo-negative class during training in the absence of attacked images. The pseudo-negative class is modeled using a Gaussian distribution whose mean is calculated by a weighted running mean. Secondly, we use pairwise confusion loss to further regularize the training process. The proposed approach benefits from the representation learning power of the CNNs and learns better features for fPAD task as shown in our ablation study. We perform extensive experiments on four publicly available datasets: Replay-Attack, Rose-Youtu, OULU-NPU and Spoof in Wild to show the effectiveness of the proposed approach over the previous methods. Code is available at: \url{https://github.com/yashasvi97/IJCB2020_anomaly}",0
"Spoof attack detection in face Presentation Attack Detection (fPAD) has seen recent advancements with the implementation of anomaly detection-based techniques. Such techniques employ a spoof detector that is trained using non-attacked images of users, and are valuable as they demonstrate good generalization abilities to new attack types. In this study, we present an end-to-end deep learning solution for anomaly detection-based spoof attack detection. We incorporate a pseudo-negative class during training, modeled using a Gaussian distribution with a weighted running mean. Furthermore, we use pairwise confusion loss to further regulate the training process. Our approach leverages the representation learning power of CNNs, resulting in improved features for the fPAD task. We conduct extensive experiments on four publicly available datasets, including Replay-Attack, Rose-Youtu, OULU-NPU, and Spoof in Wild, demonstrating the effectiveness of our proposed method over previous approaches. The code is available at: \url{https://github.com/yashasvi97/IJCB2020_anomaly}.",1
"In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose a novel framework connecting Wasserstein GANs with a classifier, allowing a principled mechanism for producing good negative distributions for contrastive learning, which is currently a challenging problem. Our full objective is cast as a subspace learning problem on the Grassmann manifold and solved via Riemannian optimization. To empirically study our formulation, we provide experiments on the task of human action recognition in video sequences. Our results demonstrate competitive performance against challenging baselines.",0
"The focus of this paper is to investigate the challenge of obtaining streamlined representations of sequential data that effectively capture its implicit spatio-temporal cues. We approach this problem through contrastive representation learning and introduce a new objective using optimal transport, which seeks to extract informative cues from the data. The formulation aims to produce a low-dimensional representation of the data that takes into account temporal order and minimizes data distortion, while maximizing the distance between the data and an adversarial distribution under the Wasserstein distance. To create the adversarial distribution, we propose a novel framework that combines Wasserstein GANs with a classifier. The full objective is solved using Riemannian optimization on the Grassmann manifold. To evaluate our approach, we conduct experiments on human action recognition in video sequences and our results show promising performance compared to challenging baselines.",1
"Deep auto-encoders (DAEs) have achieved great success in learning data representations via the powerful representability of neural networks. But most DAEs only focus on the most dominant structures which are able to reconstruct the data from a latent space and neglect rich latent structural information. In this work, we propose a new representation learning method that explicitly models and leverages sample relations, which in turn is used as supervision to guide the representation learning. Different from previous work, our framework well preserves the relations between samples. Since the prediction of pairwise relations themselves is a fundamental problem, our model adaptively learns them from data. This provides much flexibility to encode real data manifold. The important role of relation and representation learning is evaluated on the clustering task. Extensive experiments on benchmark data sets demonstrate the superiority of our approach. By seeking to embed samples into subspace, we further show that our method can address the large-scale and out-of-sample problem.",0
"Auto-encoders with deep architectures (DAEs) have been successful in learning data representations through the powerful neural networks they employ. However, most DAEs focus only on the most prominent structures that can reconstruct data from a latent space, disregarding valuable latent structural information. In this study, we introduce a novel approach to learning representations that explicitly models and exploits sample relations. This approach serves as supervision to guide representation learning and, unlike previous work, maintains the relationships between samples. Our model adaptively learns pairwise relations from data, providing greater flexibility to encode real data manifold. We evaluated the critical role of relation and representation learning in the clustering task, and our method outperformed existing approaches in experiments on benchmark data sets. We further demonstrated that our approach can address the large-scale and out-of-sample problem by embedding samples into subspaces.",1
"The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) for text, DNA, and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series data mining tool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series data mining methods on top of matrix profile, many time series data mining tasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series data mining methods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series data mining problems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series data mining projects.",0
"In the past 10 years, there has been extensive research into all-pairs-similarity-search (or self-join) for various types of data such as text and DNA. Despite this, little progress has been made in addressing this problem for time series subsequences. In this thesis, we have introduced the matrix profile, a time series data mining tool that can efficiently solve the all-pairs-similarity-search problem and store the output in an accessible manner. This algorithm is parameter-free, exact, scalable, and applicable for both single and multidimensional time series. By using the matrix profile as a foundation, various time series data mining tasks such as motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering can be efficiently accomplished. The matrix profile is a versatile and reusable data structure that can be shared by different time series data mining methods. We have demonstrated the effectiveness of the matrix profile in solving various time series data mining problems in diverse fields such as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We envision the matrix profile as a starting point for many more time series data mining projects.",1
"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods.",0
"The structure of biomedical data in graphs is distinct from that of standard knowledge graph benchmark tasks. Biomedical data possesses long-range dependencies, which are described as logical rules. To address this, we introduce a new approach that merges these rules with a neural multi-hop reasoning technique that employs reinforcement learning. We examine drug repurposing as a real-world task and formulate it as a link prediction issue. Our empirical study on the Hetionet biomedical knowledge graph demonstrates that our method is superior to various baseline methods.",1
"Recently deep neural networks demonstrate competitive performances in classification and regression tasks for many temporal or sequential data. However, it is still hard to understand the classification mechanisms of temporal deep neural networks. In this paper, we propose two new frameworks to visualize temporal representations learned from deep neural networks. Given input data and output, our algorithm interprets the decision of temporal neural network by extracting highly activated periods and visualizes a sub-sequence of input data which contributes to activate the units. Furthermore, we characterize such sub-sequences with clustering and calculate the uncertainty of the suggested type and actual data. We also suggest Layer-wise Relevance from the output of a unit, not from the final output, with backward Monte-Carlo dropout to show the relevance scores of each input point to activate units with providing a visual representation of the uncertainty about this impact.",0
"Deep neural networks have shown to be effective in classification and regression tasks for various temporal or sequential data. However, the classification mechanisms of these networks are still difficult to comprehend. This article introduces two new frameworks that enable the visualization of temporal representations learned from deep neural networks. Our algorithm interprets the decision of the temporal neural network by extracting highly activated periods and visualizing a sub-sequence of input data that contributes to the activation of the units. We also cluster such sub-sequences and calculate the uncertainty of the suggested type and actual data. Additionally, we suggest a Layer-wise Relevance approach that shows the relevance scores of each input point to activate units based on the output of a unit, not from the final output, using backward Monte-Carlo dropout. This method provides a visual representation of the uncertainty regarding the impact of each input point.",1
"State-of-the-art single depth image-based 3D hand pose estimation methods are based on dense predictions, including voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. Despite the good performance, those methods have a few issues in nature, such as the poor trade-off between accuracy and efficiency, and plain feature representation learning with local convolutions. In this paper, a novel pixel-wise prediction-based method is proposed to address the above issues. The key ideas are two-fold: a) explicitly modeling the dependencies among joints and the relations between the pixels and the joints for better local feature representation learning; b) unifying the dense pixel-wise offset predictions and direct joint regression for end-to-end training. Specifically, we first propose a graph convolutional network (GCN) based joint graph reasoning module to model the complex dependencies among joints and augment the representation capability of each pixel. Then we densely estimate all pixels' offsets to joints in both image plane and depth space and calculate the joints' positions by a weighted average over all pixels' predictions, totally discarding the complex postprocessing operations. The proposed model is implemented with an efficient 2D fully convolutional network (FCN) backbone and has only about 1.4M parameters. Extensive experiments on multiple 3D hand pose estimation benchmarks demonstrate that the proposed method achieves new state-of-the-art accuracy while running very efficiently with around a speed of 110fps on a single NVIDIA 1080Ti GPU.",0
"Cutting-edge methods for estimating 3D hand pose from single depth images rely on dense predictions, such as voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. However, these methods suffer from issues like a poor balance between accuracy and efficiency, and simplistic feature representation learning with local convolutions. This paper presents a new approach to tackle these challenges, utilizing pixel-wise prediction-based methods. The approach involves two key ideas: (a) explicitly modeling dependencies among joints and the relations between joints and pixels for improved local feature representation learning, and (b) combining dense pixel-wise offset predictions with direct joint regression for end-to-end training. The model includes a graph convolutional network (GCN) based joint graph reasoning module to model complex joint dependencies and augment pixel representation capability. It then densely estimates all pixel offsets to joints in both image and depth space, computing joint positions via a weighted average of all pixel predictions, without the need for complex postprocessing. This efficient 2D fully convolutional network (FCN) backbone has roughly 1.4M parameters, and achieves new state-of-the-art accuracy on multiple 3D hand pose estimation benchmarks, while running at a speed of about 110fps on a single NVIDIA 1080Ti GPU.",1
"Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.",0
"It has been challenging to train an agent to solve control tasks directly from high-dimensional images using model-free reinforcement learning (RL). One promising strategy is to learn a latent representation along with the control policy. However, creating a high-capacity encoder with a limited reward signal results in poor performance and is sample inefficient. Past research has shown that auxiliary losses, such as image reconstruction, can help with efficient representation learning. Nevertheless, integrating the reconstruction loss into an off-policy learning algorithm can lead to training instability. We examine the root causes and attribute the divergence to variational autoencoders used in prior studies. Based on these discoveries, we present effective methods for improving training stability. As a result, we propose a straightforward approach that matches the performance of state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Moreover, our approach shows robustness to observational noise, surpassing current methods in this scenario. The code, findings, and videos are available anonymously at https://sites.google.com/view/sac-ae/home.",1
"We present a systematic investigation using graph neural networks (GNNs) to model organic chemical reactions. To do so, we prepared a dataset collection of four ubiquitous reactions from the organic chemistry literature. We evaluate seven different GNN architectures for classification tasks pertaining to the identification of experimental reagents and conditions. We find that models are able to identify specific graph features that affect reaction conditions and lead to accurate predictions. The results herein show great promise in advancing molecular machine learning.",0
"Our study involves the use of graph neural networks (GNNs) to model organic chemical reactions in a systematic manner. For this purpose, we compiled a dataset comprising four widely used reactions from the organic chemistry domain. We assessed the performance of seven distinct GNN architectures for classification tasks related to identifying experimental reagents and conditions. Our findings indicate that these models can pinpoint particular graph features that impact reaction conditions and make precise predictions. Overall, these results are a significant step forward in enhancing molecular machine learning.",1
"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting [2, 10]. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning ""simple"" classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.",0
"Our investigation into the causes of adversarial vulnerability in deep neural networks has led us to identify two main factors: bad data and poorly trained models. Despite achieving zero training error with SGD, deep neural networks are still susceptible to adversarial attacks due to label noise, which is a significant contributor to their vulnerability. We have found evidence of label noise in datasets like MNIST and CIFAR, and although removing these noisy labels helps, it is not enough to achieve full adversarial robustness. This is because standard training procedures favor simple classification boundaries, which are less robust than more complex ones. Adversarial training, on the other hand, produces more complex decision boundaries, which we believe is partly due to sub-optimal representation learning. Our simple toy examples demonstrate how the choice of representation can impact adversarial robustness.",1
"This paper addresses a fundamental challenge in 3D medical image processing: how to deal with imaging thickness. For anisotropic medical volumes, there is a significant performance gap between thin-slice (mostly 1mm) and thick-slice (mostly 5mm) volumes. Prior arts tend to use 3D approaches for the thin-slice and 2D approaches for the thick-slice, respectively. We aim at a unified approach for both thin- and thick-slice medical volumes. Inspired by recent advances in video analysis, we propose AlignShift, a novel parameter-free operator to convert theoretically any 2D pretrained network into thickness-aware 3D network. Remarkably, the converted networks behave like 3D for the thin-slice, nevertheless degenerate to 2D for the thick-slice adaptively. The unified thickness-aware representation learning is achieved by shifting and fusing aligned ""virtual slices"" as per the input imaging thickness. Extensive experiments on public large-scale DeepLesion benchmark, consisting of 32K lesions for universal lesion detection, validate the effectiveness of our method, which outperforms previous state of the art by considerable margins without whistles and bells. More importantly, to our knowledge, this is the first method that bridges the performance gap between thin- and thick-slice volumes by a unified framework. To improve research reproducibility, our code in PyTorch is open source at https://github.com/M3DV/AlignShift.",0
"This article focuses on a major challenge in 3D medical image processing, which is how to handle variations in imaging thickness. In cases of anisotropic medical volumes, the performance gap between thin-slice volumes (typically 1mm) and thick-slice volumes (typically 5mm) is significant. The current approach is to use 3D methods for thin-slice volumes and 2D methods for thick-slice volumes. The authors of this paper aim to develop a unified method that can be used for both types of volumes. They propose a new operator called AlignShift, which can convert any 2D pretrained network into a thickness-aware 3D network without requiring any parameters. This approach involves aligning and fusing ""virtual slices"" depending on the input imaging thickness. The authors conducted extensive experiments using the DeepLesion benchmark, which contains 32K lesions for universal lesion detection. Their results show that their method outperforms the previous state of the art significantly, without any additional features. Most importantly, the authors claim that their method is the first to bridge the performance gap between thin- and thick-slice volumes using a unified framework. The authors have made their PyTorch code open source on GitHub to enhance research reproducibility.",1
"Audio representation learning based on deep neural networks (DNNs) emerged as an alternative approach to hand-crafted features. For achieving high performance, DNNs often need a large amount of annotated data which can be difficult and costly to obtain. In this paper, we propose a method for learning audio representations, aligning the learned latent representations of audio and associated tags. Aligning is done by maximizing the agreement of the latent representations of audio and tags, using a contrastive loss. The result is an audio embedding model which reflects acoustic and semantic characteristics of sounds. We evaluate the quality of our embedding model, measuring its performance as a feature extractor on three different tasks (namely, sound event recognition, and music genre and musical instrument classification), and investigate what type of characteristics the model captures. Our results are promising, sometimes in par with the state-of-the-art in the considered tasks and the embeddings produced with our method are well correlated with some acoustic descriptors.",0
"The use of deep neural networks (DNNs) for audio representation learning has become a popular alternative to manually-crafted features. However, DNNs require a large amount of annotated data, which can be expensive and challenging to obtain. This study proposes a novel approach to learning audio representations by aligning the latent representations of audio and associated tags, using a contrastive loss to maximize agreement. The resulting audio embedding model reflects both acoustic and semantic characteristics of sounds. The model's effectiveness was evaluated as a feature extractor in three different tasks, including sound event recognition, music genre classification, and musical instrument classification. The results were promising, with the embeddings produced by our method often matching or exceeding the state-of-the-art in these tasks. Additionally, our study found that the model effectively captures some acoustic descriptors, providing further evidence of its quality.",1
"Messenger advertisements (ads) give direct and personal user experience yielding high conversion rates and sales. However, people are skeptical about ads and sometimes perceive them as spam, which eventually leads to a decrease in user satisfaction. Targeted advertising, which serves ads to individuals who may exhibit interest in a particular advertising message, is strongly required. The key to the success of precise user targeting lies in learning the accurate user and ad representation in the embedding space. Most of the previous studies have limited the representation learning in the Euclidean space, but recent studies have suggested hyperbolic manifold learning for the distinct projection of complex network properties emerging from real-world datasets such as social networks, recommender systems, and advertising. We propose a framework that can effectively learn the hierarchical structure in users and ads on the hyperbolic space, and extend to the Multi-Manifold Learning. Our method constructs multiple hyperbolic manifolds with learnable curvatures and maps the representation of user and ad to each manifold. The origin of each manifold is set as the centroid of each user cluster. The user preference for each ad is estimated using the distance between two entities in the hyperbolic space, and the final prediction is determined by aggregating the values calculated from the learned multiple manifolds. We evaluate our method on public benchmark datasets and a large-scale commercial messenger system LINE, and demonstrate its effectiveness through improved performance.",0
"Although messenger ads have high conversion rates and sales due to their direct and personal user experience, they are often viewed as spam and can decrease user satisfaction. Thus, targeted advertising is necessary to serve ads to individuals who may be interested in a particular message. The success of precise user targeting depends on learning accurate user and ad representation in the embedding space. While previous studies have limited representation learning to the Euclidean space, recent studies recommend hyperbolic manifold learning for complex network properties emerging from real-world datasets. To address this, we propose a framework that can effectively learn the hierarchical structure in users and ads on the hyperbolic space and extend to Multi-Manifold Learning. Our method constructs multiple hyperbolic manifolds with learnable curvatures and maps the representation of the user and ad to each manifold. User preference for each ad is estimated using the distance between two entities in the hyperbolic space, and the final prediction is determined by aggregating the values calculated from the learned multiple manifolds. We evaluate our method on public benchmark datasets and a large-scale commercial messenger system LINE, demonstrating improved performance.",1
"Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions typically lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear ICA, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.",0
"The property of identifiability is considered advantageous in statistical modeling as it implies that the model's true parameters can be estimated with precision, given adequate computational resources and data. Our focus is on identifying whether nonlinear data representations, which are optimal for downstream tasks, possess this property of identifiability in the context of representation learning. While deep neural networks, when parameterized, tend to lack identifiability in parameter space due to their overparameterization, we propose to address this issue by leveraging nonlinear ICA advancements. We show that a broad range of discriminative models are indeed identifiable in function space, with a linear indeterminacy. Our findings indicate that many models applied in various domains, such as text, images, and audio, achieve this sense of identifiability, which was the state-of-the-art at the time of publication. We provide empirical evidence and derive sufficient conditions for linear identifiability, using both simulated and real-world data.",1
"Fair representation learning aims to encode invariant representation with respect to the protected attribute, such as gender or age. In this paper, we design Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair representation learning. This network disentangles latent space into three subspaces with a decorrelation loss that encourages each subspace to contain independent information: 1) target attribute information, 2) protected attribute information, 3) mutual attribute information. After the representation learning, this disentangled representation is leveraged for fairer downstream classification by excluding the subspace with the protected attribute information. We demonstrate the effectiveness of our model through extensive experiments on CelebA and UTK Face datasets. Our method outperforms the previous state-of-the-art method by large margins in terms of equal opportunity and equalized odds.",0
"The objective of fair representation learning is to create an invariant representation that does not discriminate against a protected attribute such as age or gender. To achieve this goal, we have developed a Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) in this study. The FD-VAE separates the latent space into three subspaces using a decorrelation loss, which encourages each subspace to contain independent information. These subspaces comprise 1) target attribute information, 2) protected attribute information, and 3) mutual attribute information. After the representation learning phase, the disentangled representation is used for fairer downstream classification by excluding the subspace with the protected attribute information. Our experiments on CelebA and UTK Face datasets demonstrate that our model is highly effective, outperforming the previous state-of-the-art approach in terms of equal opportunity and equalized odds.",1
"We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking Graph Neural Networks. The dataset consists of nodes corresponding to Computer Science articles, with edges based on hyperlinks and 10 classes representing different branches of the field. We use the dataset to evaluate semi-supervised node classification and single-relation link prediction models. Our experiments show that these methods perform well on a new domain, with structural properties different from earlier benchmarks. The dataset is publicly available, along with the implementation of the data pipeline and the benchmark experiments, at https://github.com/pmernyei/wiki-cs-dataset .",0
"Introducing Wiki-CS, an innovative dataset originating from Wikipedia that serves as a benchmark for Graph Neural Networks. The dataset comprises nodes that correspond to Computer Science articles and edges based on hyperlinks, featuring 10 classes that represent distinct branches of the field. We leverage the dataset to assess semi-supervised node classification and single-relation link prediction models. Our findings indicate that these approaches deliver satisfactory results in a new domain, which exhibits structural attributes distinct from prior benchmarks. The dataset, along with the data pipeline implementation and benchmark experiments, is available to the public at https://github.com/pmernyei/wiki-cs-dataset.",1
"Multi-view learning is a learning task in which data is described by several concurrent representations. Its main challenge is most often to exploit the complementarities between these representations to help solve a classification/regression task. This is a challenge that can be met nowadays if there is a large amount of data available for learning. However, this is not necessarily true for all real-world problems, where data are sometimes scarce (e.g. problems related to the medical environment). In these situations, an effective strategy is to use intermediate representations based on the dissimilarities between instances. This work presents new ways of constructing these dissimilarity representations, learning them from data with Random Forest classifiers. More precisely, two methods are proposed, which modify the Random Forest proximity measure, to adapt it to the context of High Dimension Low Sample Size (HDLSS) multi-view classification problems. The second method, based on an Instance Hardness measurement, is significantly more accurate than other state-of-the-art measurements including the original RF Proximity measurement and the Large Margin Nearest Neighbor (LMNN) metric learning measurement.",0
"Multi-view learning involves using multiple concurrent representations to describe data. The main challenge is to effectively use the complementarities between these representations to solve classification/regression tasks. This can be difficult when there is limited data available, such as in medical environments. To address this issue, this study proposes using dissimilarity representations based on the differences between instances. The researchers developed two methods for constructing these representations using Random Forest classifiers. These methods modify the proximity measure to suit High Dimension Low Sample Size (HDLSS) multi-view classification problems. The second method, which uses an Instance Hardness measurement, outperforms other state-of-the-art measurements like the original RF Proximity measurement and the Large Margin Nearest Neighbor (LMNN) metric learning measurement.",1
"Learning discriminative powerful representations is a crucial step for machine learning systems. Introducing invariance against arbitrary nuisance or sensitive attributes while performing well on specific tasks is an important problem in representation learning. This is mostly approached by purging the sensitive information from learned representations. In this paper, we propose a novel disentanglement approach to invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints as a proxy for independence. We explicitly enforce the meaningful representation to be agnostic to sensitive information by entropy maximization. The proposed approach is evaluated on five publicly available datasets and compared with state of the art methods for learning fairness and invariance achieving the state of the art performance on three datasets and comparable performance on the rest. Further, we perform an ablative study to evaluate the effect of each component.",0
"Developing discriminative and powerful representations is a crucial aspect of machine learning systems. To ensure that these representations are effective, it is important to make them invariant to arbitrary nuisance or sensitive attributes while still performing well on specific tasks. The most common approach to achieving this is to eliminate sensitive information from the learned representations. In this paper, we introduce a novel disentanglement approach to solve the invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints to ensure independence. Additionally, we enforce the meaningful representation to be agnostic to sensitive information by maximizing entropy. We evaluate our proposed approach on five publicly available datasets and compare it to state-of-the-art methods for learning fairness and invariance. Our approach achieves state-of-the-art performance on three datasets and comparable performance on the remaining two. Moreover, we conduct an ablative study to assess the impact of each component.",1
"Studies on acquiring appropriate continuous representations of discrete objects, such as graphs and knowledge base data, have been conducted by many researchers in the field of machine learning. In this study, we introduce Nested SubSpace (NSS) arrangement, a comprehensive framework for representation learning. We show that existing embedding techniques can be regarded as special cases of the NSS arrangement. Based on the concept of the NSS arrangement, we implement a Disk-ANChor ARrangement (DANCAR), a representation learning method specialized to reproducing general graphs. Numerical experiments have shown that DANCAR has successfully embedded WordNet in ${\mathbb R}^{20}$ with an F1 score of 0.993 in the reconstruction task. DANCAR is also suitable for visualization in understanding the characteristics of graphs.",0
"Numerous machine learning researchers have studied the acquisition of continuous representations for discrete objects, including graphs and knowledge base data. Our research presents a comprehensive framework for representation learning called Nested SubSpace (NSS) arrangement. We demonstrate that existing embedding techniques are special cases of NSS arrangement. Using NSS arrangement, we develop Disk-ANChor ARrangement (DANCAR), a representation learning method that focuses on reproducing general graphs. Numerical experiments show that DANCAR successfully embeds WordNet in ${\mathbb R}^{20}$ with an F1 score of 0.993 in the reconstruction task. Additionally, DANCAR is suitable for visualizing and understanding graph characteristics.",1
"The problem of linking functional connectomics to behavior is extremely challenging due to the complex interactions between the two distinct, but related, data domains. We propose a coupled manifold optimization framework which projects fMRI data onto a low dimensional matrix manifold common to the cohort. The patient specific loadings simultaneously map onto a behavioral measure of interest via a second, non-linear, manifold. By leveraging the kernel trick, we can optimize over a potentially infinite dimensional space without explicitly computing the embeddings. As opposed to conventional manifold learning, which assumes a fixed input representation, our framework directly optimizes for embedding directions that predict behavior. Our optimization algorithm combines proximal gradient descent with the trust region method, which has good convergence guarantees. We validate our framework on resting state fMRI from fifty-eight patients with Autism Spectrum Disorder using three distinct measures of clinical severity. Our method outperforms traditional representation learning techniques in a cross validated setting, thus demonstrating the predictive power of our coupled objective.",0
"Connecting functional connectomics to behavior poses a significant challenge due to the intricate interactions between the two interrelated yet distinct data domains. To overcome this challenge, we propose a framework for coupled manifold optimization that projects fMRI data onto a common low-dimensional matrix manifold for the cohort. Patient-specific loadings are mapped onto a behavioral measure of interest through a second non-linear manifold, leveraging the kernel trick to optimize over an infinite dimensional space without explicitly computing embeddings. Rather than assuming a fixed input representation, our framework directly optimizes embedding directions that predict behavior. We use a combination of proximal gradient descent and the trust region method for optimization, which guarantees good convergence. We validate our framework on resting state fMRI data from 58 patients with Autism Spectrum Disorder, using three distinct clinical severity measures. Our coupled objective outperforms traditional representation learning techniques in a cross-validated setting, demonstrating its predictive power.",1
"Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification.",0
"Is it possible to group images into meaningful clusters without ground-truth annotations? Unsupervised image classification is a vital but unresolved challenge in computer vision. Previous attempts have aimed to tackle this issue in an end-to-end manner. However, in this study, we propose a two-step approach, separating feature learning from clustering. First, we use a self-supervised task to obtain semantically meaningful features for representation learning. Second, we apply the features as a prior in a learnable clustering method to eliminate the dependence on low-level features in current end-to-end approaches. Our experimental results show that our method outperforms the state-of-the-art techniques by a significant margin, particularly +26.6% on CIFAR10, +25.0% on CIFAR100-20, and +21.3% on STL10 in terms of classification accuracy. Moreover, our method is the first to perform well on a large-scale dataset for image classification, including ImageNet, outperforming several semi-supervised learning methods in the low-data regime without requiring ground-truth annotations. The code is available at https://github.com/wvangansbeke/Unsupervised-Classification.",1
"Adversarial Training (AT) is proposed to alleviate the adversarial vulnerability of machine learning models by extracting only robust features from the input, which, however, inevitably leads to severe accuracy reduction as it discards the non-robust yet useful features. This motivates us to preserve both robust and non-robust features and separate them with disentangled representation learning. Our proposed Adversarial Asymmetric Training (AAT) algorithm can reliably disentangle robust and non-robust representations without additional supervision on robustness. Empirical results show our method does not only successfully preserve accuracy by combining two representations, but also achieve much better disentanglement than previous work.",0
"To address the issue of adversarial vulnerability in machine learning models, Adversarial Training (AT) has been suggested. However, AT only retains robust features from the input, leading to a significant drop in accuracy as it discards non-robust yet valuable features. This has prompted us to develop a method that preserves both types of features through disentangled representation learning. Our proposed Adversarial Asymmetric Training (AAT) algorithm effectively separates robust and non-robust representations without the need for additional robustness supervision. Our method not only maintains accuracy by combining both representations but also achieves superior disentanglement compared to previous approaches, as demonstrated by empirical results.",1
"Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.",0
"The task of visual question answering involves answering questions about an image using a deep understanding of language and object association. This requires techniques from both computer vision and natural language processing. Our proposed method involves context-driven, sequential reasoning using the objects and their relationships within the image. We begin by creating a scene graph that describes the objects, their attributes, and relationships. An agent then uses this graph to generate paths and derive answers. Our method was tested on the GQA dataset with manually curated scene graphs, and achieved nearly human-level performance.",1
"Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",0
"The use of graph representation learning has become a valuable approach to tackling real-world issues. Recent developments in this field have proven beneficial for various downstream graph learning tasks, such as node classification, similarity search, and graph classification. However, previous studies on graph representation learning have mainly focused on specific domains and trained models dedicated to each graph dataset, which are often not transferable to other datasets. To address this issue, we drew inspiration from recent advances in pre-training from natural language processing and computer vision and created Graph Contrastive Coding (GCC), a self-supervised graph neural network pre-training framework. Our aim was to capture universal network topological properties across multiple networks. We implemented GCC's pre-training task as subgraph instance discrimination within and across networks, utilizing contrastive learning to enable graph neural networks to learn intrinsic and transferable structural representations. We conducted extensive experiments on ten graph datasets and three graph learning tasks, and the results showed that GCC pre-trained on a diverse set of datasets achieved competitive or better performance than task-specific and trained-from-scratch models. This highlights the potential of the pre-training and fine-tuning paradigm for graph representation learning.",1
"Financial transactions constitute connections between entities and through these connections a large scale heterogeneous weighted graph is formulated. In this labyrinth of interactions that are continuously updated, there exists a variety of similarity-based patterns that can provide insights into the dynamics of the financial system. With the current work, we propose the application of Graph Representation Learning in a scalable dynamic setting as a means of capturing these patterns in a meaningful and robust way. We proceed to perform a rigorous qualitative analysis of the latent trajectories to extract real world insights from the proposed representations and their evolution over time that is to our knowledge the first of its kind in the financial sector. Shifts in the latent space are associated with known economic events and in particular the impact of the recent Covid-19 pandemic to consumer patterns. Capturing such patterns indicates the value added to financial modeling through the incorporation of latent graph representations.",0
"Connections between entities form financial transactions, which create a large and complex weighted graph. This graph is continuously updated and contains a variety of similarity-based patterns that can provide insights into the financial system's dynamics. To capture these patterns in a meaningful and robust way, we propose applying Graph Representation Learning in a scalable dynamic setting. Through a rigorous qualitative analysis of latent trajectories, we can extract real-world insights from the proposed representations and their evolution over time. This is the first study of its kind in the financial sector. We found that shifts in the latent space correspond to known economic events, such as the impact of the Covid-19 pandemic on consumer patterns. Incorporating latent graph representations adds value to financial modeling by capturing these patterns.",1
"Most existing works on disentangled representation learning are solely built upon an marginal independence assumption: all factors in disentangled representations should be statistically independent. This assumption is necessary but definitely not sufficient for the disentangled representations without additional inductive biases in the modeling process, which is shown theoretically in recent studies. We argue in this work that disentangled representations should be characterized by their relation with observable data. In particular, we formulate such a relation through the concept of mutual information: the mutual information between each factor of the disentangled representations and data should be invariant conditioned on values of the other factors. Together with the widely accepted independence assumption, we further bridge it with the conditional independence of factors in representations conditioned on data. Moreover, we note that conditional independence of latent variables has been imposed on most VAE-type models and InfoGAN due to the artificial choice of factorized approximate posterior $q(\rvz|\rvx)$ in the encoders. Such an arrangement of encoders introduces a crucial inductive bias for disentangled representations. To demonstrate the importance of our proposed assumption and the related inductive bias, we show in experiments that violating the assumption leads to decline of disentanglement among factors in the learned representations.",0
"Previous research on disentangled representation learning has primarily relied on the assumption of marginal independence, which states that all factors in disentangled representations should be statistically independent. However, recent studies have shown that this assumption is not sufficient for achieving disentangled representations without additional inductive biases in the modeling process. In this paper, we argue that disentangled representations should be characterized by their relation with observable data, which we formalize using the concept of mutual information. Specifically, we propose that the mutual information between each factor of the disentangled representations and data should remain invariant, conditioned on values of the other factors. We combine this assumption with the widely accepted independence assumption and bridge it with the conditional independence of factors in representations conditioned on data. Additionally, we note that most VAE-type models and InfoGAN impose conditional independence of latent variables due to the factorized approximate posterior in the encoders, which introduces a crucial inductive bias for disentangled representations. We demonstrate the importance of our proposed assumption and related inductive bias through experiments that show a decline in disentanglement among factors in the learned representations when the assumption is violated.",1
"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",0
"The article introduces SimCLR, a framework for contrastive learning of visual representations that simplifies existing self-supervised learning algorithms. This approach does not require specialized architectures or a memory bank. The authors systematically investigate the key components of the framework to understand how the contrastive prediction tasks learn useful representations. They find that the composition of data augmentations is critical, a learnable nonlinear transformation between the representation and the contrastive loss improves representations, and contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, SimCLR outperforms previous methods for self-supervised and semi-supervised learning on ImageNet. The framework achieves 76.5% top-1 accuracy, a 7% relative improvement over previous state-of-the-art, and matches the performance of a supervised ResNet-50. It also outperforms AlexNet with 100X fewer labels, achieving 85.8% top-5 accuracy when fine-tuned on only 1% of the labels.",1
"Graph Neural Networks (GNNs) are versatile, powerful machine learning methods that enable graph structure and feature representation learning, and have applications across many domains. For applications critically requiring interpretation, attention-based GNNs have been leveraged. However, these approaches either rely on specific model architectures or lack a joint consideration of graph structure and node features in their interpretation. Here we present a model-agnostic framework for interpreting important graph structure and node features, Graph neural networks Including SparSe inTerpretability (GISST). With any GNN model, GISST combines an attention mechanism and sparsity regularization to yield an important subgraph and node feature subset related to any graph-based task. Through a single self-attention layer, a GISST model learns an importance probability for each node feature and edge in the input graph. By including these importance probabilities in the model loss function, the probabilities are optimized end-to-end and tied to the task-specific performance. Furthermore, GISST sparsifies these importance probabilities with entropy and L1 regularization to reduce noise in the input graph topology and node features. Our GISST models achieve superior node feature and edge explanation precision in synthetic datasets, as compared to alternative interpretation approaches. Moreover, our GISST models are able to identify important graph structure in real-world datasets. We demonstrate in theory that edge feature importance and multiple edge types can be considered by incorporating them into the GISST edge probability computation. By jointly accounting for topology, node features, and edge features, GISST inherently provides simple and relevant interpretations for any GNN models and tasks.",0
"Graph Neural Networks (GNNs) are a powerful and versatile machine learning technique that can learn graph structure and feature representation, with applications across many domains. Attention-based GNNs have been used in applications that require interpretation, but these approaches either rely on specific model architectures or lack a joint consideration of graph structure and node features in their interpretation. To address this, we propose a model-agnostic framework called Graph neural networks Including SparSe inTerpretability (GISST), which can interpret important graph structure and node features for any GNN model. GISST combines an attention mechanism and sparsity regularization to identify an important subgraph and node feature subset related to any graph-based task. The model learns an importance probability for each node feature and edge in the input graph and optimizes these probabilities end-to-end to tie them to the task-specific performance. Additionally, GISST sparsifies these importance probabilities with entropy and L1 regularization to reduce noise in the input graph topology and node features. Our GISST models achieve superior node feature and edge explanation precision in synthetic datasets and can identify important graph structure in real-world datasets. We also demonstrate in theory that GISST can consider edge feature importance and multiple edge types. By jointly considering topology, node features, and edge features, GISST provides simple and relevant interpretations for any GNN models and tasks.",1
"In self-supervised visual representation learning, a feature extractor is trained on a ""pretext task"" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such ""shortcut"" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a ""lens"" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.",0
"The process of self-supervised visual representation learning involves training a feature extractor on a ""pretext task"" that can be completed without human annotation. However, the feature extractor often relies on low-level visual features such as color aberrations or watermarks, leading to poor semantic representations. Efforts have been made to identify and minimize the impact of these ""shortcut"" features. In this study, we introduce a general framework for reducing the impact of shortcut features. Our approach assumes that the features used to solve the pretext task are vulnerable to an adversary trained to make the task harder. We validate this assumption by training a ""lens"" network to make small image modifications that reduce performance in the pretext task. Our results demonstrate that representations learned with modified images are superior to those learned without modifications. Furthermore, the lens modifications reveal how the choice of pretext task and dataset affects the features learned by self-supervision.",1
"Deep clustering has increasingly been demonstrating superiority over conventional shallow clustering algorithms. Deep clustering algorithms usually combine representation learning with deep neural networks to achieve this performance, typically optimizing a clustering and non-clustering loss. In such cases, an autoencoder is typically connected with a clustering network, and the final clustering is jointly learned by both the autoencoder and clustering network. Instead, we propose to learn an autoencoded embedding and then search this further for the underlying manifold. For simplicity, we then cluster this with a shallow clustering algorithm, rather than a deeper network. We study a number of local and global manifold learning methods on both the raw data and autoencoded embedding, concluding that UMAP in our framework is best able to find the most clusterable manifold in the embedding, suggesting local manifold learning on an autoencoded embedding is effective for discovering higher quality discovering clusters. We quantitatively show across a range of image and time-series datasets that our method has competitive performance against the latest deep clustering algorithms, including out-performing current state-of-the-art on several. We postulate that these results show a promising research direction for deep clustering. The code can be found at https://github.com/rymc/n2d",0
"Conventional shallow clustering algorithms have been surpassed by deep clustering in recent times. This is because deep clustering algorithms make use of deep neural networks and representation learning to attain better performance. Typically, a clustering and non-clustering loss is optimized. The autoencoder is connected to a clustering network, and both jointly learn the final clustering. However, we suggest learning an autoencoded embedding and then searching for the underlying manifold. This is then clustered with a shallow clustering algorithm instead of a deeper network for simplicity. We studied several local and global manifold learning methods on both the raw data and autoencoded embedding. Our conclusion was that UMAP is the most effective in discovering higher quality clusters in the embedding. We quantitatively show that our method competes with the latest deep clustering algorithms in a range of image and time-series datasets. In fact, it outperforms the current state-of-the-art on some. We believe that this is a promising research direction for deep clustering. The code is available at https://github.com/rymc/n2d.",1
"Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet difficulties in modelling the ``parts'' and their ``interactions'' still persist in terms of graph classification, where graph-level representations are usually obtained by squeezing the whole graph into a single vector through graph pooling. From complex systems point of view, mixing all the parts of a system together can affect both model interpretability and predictive performance, because properties of a complex system arise largely from the interaction among its components. We analyze the intrinsic difficulty in graph classification under the unified concept of ``resolution dilemmas'' with learning theoretic recovery guarantees, and propose ``SLIM'', an inductive neural network model for Structural Landmarking and Interaction Modelling. It turns out, that by solving the resolution dilemmas, and leveraging explicit interacting relation between component parts of a graph to explain its complexity, SLIM is more interpretable, accurate, and offers new insight in graph representation learning.",0
"The use of graph neural networks shows great potential for learning and inference with data that is structured like a graph. However, challenges still exist when it comes to modelling the individual parts and how they interact with each other for graph classification. In this scenario, graph-level representations are typically obtained by compressing the entire graph into a single vector through graph pooling. This approach can have a negative impact on both the interpretability and predictive performance of a model, as the properties of a complex system are largely influenced by the interactions between its components. To address this issue, we propose the use of SLIM, an inductive neural network model that combines Structural Landmarking and Interaction Modelling. By tackling the resolution dilemmas and explicitly considering the interactions between the component parts of a graph, SLIM offers a more accurate and interpretable approach to graph representation learning that provides new insights.",1
"This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.",0
"The aim of this paper is to explore unsupervised learning of representations for action recognition in videos. Previous attempts to train networks using future prediction or other domain-specific objectives have only had limited success, whereas simpler discrimination-based methods have recently shown greater success in image representation learning. The authors propose adapting two top-performing discrimination-based methods - instance recognition and local aggregation - to the video domain. The latter involves clustering videos in the feature space of a network and updating it to respect the clusters with a non-parametric classification loss. While this approach shows promise, the learned representations tend to group videos based on appearance rather than motion patterns. To address this limitation, the authors turn to heuristic-based IDT descriptors, which were designed to encode motion patterns in videos. Clustering videos based on these descriptors leads to better results on UCF101 and HMDB51 action recognition benchmarks, and qualitative analysis shows that the learned representations successfully capture video dynamics.",1
"With the explosion of digital data in recent years, continuously learning new tasks from a stream of data without forgetting previously acquired knowledge has become increasingly important. In this paper, we propose a new continual learning (CL) setting, namely ``continual representation learning'', which focuses on learning better representation in a continuous way. We also provide two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant. In contrast to requiring the model to recognize more learned classes, we aim to learn feature representation that can be better generalized to not only previously unseen images but also unseen classes/identities. For the new setting, we propose a novel approach that performs the knowledge distillation over a large number of identities by applying the neighbourhood selection and consistency relaxation strategies to improve scalability and flexibility of the continual learning model. We demonstrate that existing CL methods can improve the representation in the new setting, and our method achieves better results than the competitors.",0
"The importance of continuously learning new tasks from a stream of data without losing previously acquired knowledge has grown as digital data has exploded in recent years. This paper presents a novel approach to continual learning called ""continual representation learning,"" which focuses on learning better representation in a continuous manner. Additionally, we introduce two multi-step benchmarks for biometric identification that prioritize the visual appearance of different classes. Rather than requiring the model to recognize more learned classes, the goal is to learn a feature representation that can be better generalized to not only previously unseen images but also unseen classes or identities. To achieve this, we propose a new approach that performs knowledge distillation over a large number of identities using neighborhood selection and consistency relaxation strategies to improve the scalability and flexibility of the continual learning model. We demonstrate that our method outperforms competitors and existing CL methods in the new setting by improving the representation.",1
"Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: https://github.com/DSE-MSU/DeepRobust). The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",0
"Graph Neural Networks (GNNs) are effective for representation learning in graphs, but recent research has revealed their vulnerability to adversarial attacks. These attacks can easily deceive GNNs in predicting downstream tasks, jeopardizing their use in safety-critical applications. Hence, it is crucial to develop robust algorithms to defend against adversarial attacks. One approach is to clean the perturbed graph since real-world graphs share intrinsic properties such as sparsity and low-rankness, and adjacent nodes have similar features. In this paper, we propose Pro-GNN, a general framework that jointly learns a structural graph and a robust GNN model from the perturbed graph by leveraging these properties. Our experiments on real-world graphs demonstrate that Pro-GNN outperforms state-of-the-art defense methods even under heavy perturbations. We provide the implementation of Pro-GNN in our DeepRobust repository for adversarial attacks and defenses, and the specific experimental settings to reproduce our results are available at https://github.com/ChandlerBang/Pro-GNN.",1
"Unsupervised multi-object scene decomposition is a fast-emerging problem in representation learning. Despite significant progress in static scenes, such models are unable to leverage important dynamic cues present in video. We propose a novel spatio-temporal iterative inference framework that is powerful enough to jointly model complex multi-object representations and explicit temporal dependencies between latent variables across frames. This is achieved by leveraging 2D-LSTM, temporally conditioned inference and generation within the iterative amortized inference for posterior refinement. Our method improves the overall quality of decompositions, encodes information about the objects' dynamics, and can be used to predict trajectories of each object separately. Additionally, we show that our model has a high accuracy even without color information. We demonstrate the decomposition, segmentation, and prediction capabilities of our model and show that it outperforms the state-of-the-art on several benchmark datasets, one of which was curated for this work and will be made publicly available.",0
"Representation learning faces a new challenge in unsupervised multi-object scene decomposition. While there has been notable progress in analyzing static scenes, these models do not account for dynamic elements present in videos. Our solution is a unique spatio-temporal iterative inference framework. This framework is capable of simultaneously modeling intricate multi-object representations, as well as explicit temporal connections between latent variables across frames. We achieve this through the application of 2D-LSTM, temporal conditioning, and generation within the iterative amortized inference for posterior refinement. Our approach improves the quality of decompositions, incorporates data on object dynamics, and predicts individual object trajectories. Even without color information, our model is highly accurate. We exhibit the capabilities of our model in decomposition, segmentation, and prediction, and demonstrate that it performs better than current benchmarks on multiple datasets. Additionally, we curated a benchmark dataset for our work and will make it publicly available.",1
"Autoencoders have been widely used for dimensional reduction and feature extraction. Various types of autoencoders have been proposed by introducing regularization terms. Most of these regularizations improve representation learning by constraining the weights in the encoder part, which maps input into hidden nodes and affects the generation of features. In this study, we show that a constraint to the decoder can also significantly improve its performance because the decoder determines how the latent variables contribute to the reconstruction of input. Inspired by the structural modal analysis method in mechanical engineering, a new modal autoencoder (MAE) is proposed by othogonalising the columns of the readout weight matrix. The new regularization helps to disentangle explanatory factors of variation and forces the MAE to extract fundamental modes in data. The learned representations are functionally independent in the reconstruction of input and perform better in consecutive classification tasks. The results were validated on the MNIST variations and USPS classification benchmark suite. Comparative experiments clearly show that the new algorithm has a surprising advantage. The new MAE introduces a very simple training principle for autoencoders and could be promising for the pre-training of deep neural networks.",0
"Dimensional reduction and feature extraction are common applications of autoencoders. To improve their performance, various regularization techniques have been proposed, most of which focus on the weights in the encoder part. However, our study demonstrates that regularizing the decoder can also enhance its capabilities, as it determines how the latent variables contribute to input reconstruction. Inspired by structural modal analysis in mechanical engineering, we introduce a new modal autoencoder (MAE) that orthogonally constrains the readout weight matrix columns. This regularization facilitates the extraction of fundamental data modes and disentangles explanatory factors of variation. The learned representations exhibit functional independence and perform better in classification tasks, as evidenced by our experiments on MNIST and USPS benchmark suites. Our algorithm has a significant advantage over competing methods and offers a simple training principle for autoencoders, making it a promising pre-training tool for deep neural networks.",1
"Representation learning models for graphs are a successful family of techniques that project nodes into feature spaces that can be exploited by other machine learning algorithms. Since many real-world networks are inherently dynamic, with interactions among nodes changing over time, these techniques can be defined both for static and for time-varying graphs. Here, we build upon the fact that the skip-gram embedding approach implicitly performs a matrix factorization, and we extend it to perform implicit tensor factorization on different tensor representations of time-varying graphs. We show that higher-order skip-gram with negative sampling (HOSGNS) is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. We empirically evaluate our approach using time-resolved face-to-face proximity data, showing that the learned time-varying graph representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction, and to predict the outcome of dynamical processes such as disease spreading. The source code and data are publicly available at https://github.com/simonepiaggesi/hosgns.",0
"Graph representation learning models have been successful in projecting nodes into feature spaces that can be utilized by other machine learning algorithms. These techniques have been applied to both static and time-varying graphs, as many real-world networks are dynamic with node interactions changing over time. In this study, we utilized the skip-gram embedding approach to perform matrix factorization and extended it to implicit tensor factorization on different tensor representations of time-varying graphs. Our higher-order skip-gram with negative sampling (HOSGNS) approach is able to disentangle the roles of nodes and time with significantly fewer parameters than other methods. We evaluated our approach on time-resolved face-to-face proximity data and found that our learned time-varying graph representations outperformed state-of-the-art methods when used for downstream tasks such as network reconstruction and predicting the outcome of dynamical processes like disease spreading. Our source code and data are publicly available at https://github.com/simonepiaggesi/hosgns.",1
"Neural networks used for multi-interaction trajectory reconstruction lack the ability to estimate the uncertainty in their outputs, which would be useful to better analyse and understand the systems they model. In this paper we extend the Factorised Neural Relational Inference model to output both a mean and a standard deviation for each component of the phase space vector, which together with an appropriate loss function, can account for uncertainty. A variety of loss functions are investigated including ideas from convexification and a Bayesian treatment of the problem. We show that the physical meaning of the variables is important when considering the uncertainty and demonstrate the existence of pathological local minima that are difficult to avoid during training.",0
"The current neural networks employed for reconstructing trajectories involving multiple interactions do not possess the capability to determine the uncertainty associated with their outputs. This information would prove beneficial in comprehending and analyzing the systems being modeled. Our paper introduces an extension to the Factorised Neural Relational Inference model that can output both the mean and standard deviation for each element of the phase space vector. A suitable loss function is also proposed to incorporate the uncertainty. We explore different loss functions, including those based on convexification and Bayesian methods. Our study highlights the significance of the variables' physical interpretation when considering uncertainty and reveals the existence of challenging local minima during training.",1
"Graph representation learning has been extensively studied in recent years. Despite its potential in generating continuous embeddings for various networks, both the effectiveness and efficiency to infer high-quality representations toward large corpus of nodes are still challenging. Sampling is a critical point to achieve the performance goals. Prior arts usually focus on sampling positive node pairs, while the strategy for negative sampling is left insufficiently explored. To bridge the gap, we systematically analyze the role of negative sampling from the perspectives of both objective and risk, theoretically demonstrating that negative sampling is as important as positive sampling in determining the optimization objective and the resulted variance. To the best of our knowledge, we are the first to derive the theory and quantify that the negative sampling distribution should be positively but sub-linearly correlated to their positive sampling distribution. With the guidance of the theory, we propose MCNS, approximating the positive distribution with self-contrast approximation and accelerating negative sampling by Metropolis-Hastings. We evaluate our method on 5 datasets that cover extensive downstream graph learning tasks, including link prediction, node classification and personalized recommendation, on a total of 19 experimental settings. These relatively comprehensive experimental results demonstrate its robustness and superiorities.",0
"In recent years, there has been considerable research into graph representation learning. Although it has the potential to create continuous embeddings for various networks, it remains challenging to infer high-quality representations for large numbers of nodes, both effectively and efficiently. The key to achieving performance goals lies in sampling. However, previous studies have tended to focus on sampling positive node pairs, while negative sampling strategies have not been explored enough. To address this issue, we conducted a thorough analysis of negative sampling from both an objective and risk perspective, and demonstrated theoretically that negative sampling is just as important as positive sampling in determining the optimization objective and the resulting variance. We are the first to derive a theory and quantify that the negative sampling distribution should be positively but sub-linearly correlated with the positive sampling distribution. Using this theory as a guide, we propose a new method called MCNS, which approximates the positive distribution with self-contrast approximation and speeds up negative sampling by using Metropolis-Hastings. We tested our method on five datasets that cover a range of downstream graph learning tasks, including link prediction, node classification, and personalized recommendation, across a total of 19 experimental settings. Our comprehensive experimental results demonstrate the robustness and superiority of our method.",1
"In this article we introduce theory and algorithms for learning discrete representations that take on a lattice that is embedded in an Euclidean space. Lattice representations possess an interesting combination of properties: a) they can be computed explicitly using lattice quantization, yet they can be learned efficiently using the ideas we introduce in this paper, b) they are highly related to Gaussian Variational Autoencoders, allowing designers familiar with the latter to easily produce discrete representations from their models and c) since lattices satisfy the axioms of a group, their adoption can lead into a way of learning simple algebras for modeling binary operations between objects through symbolic formalisms, yet learn these structures also formally using differentiation techniques. This article will focus on laying the groundwork for exploring and exploiting the first two properties, including a new mathematical result linking expressions used during training and inference time and experimental validation on two popular datasets.",0
"This article presents a theory and algorithms for acquiring discrete representations that are embedded in an Euclidean space as a lattice. The lattice representations possess unique features: firstly, they can be explicitly computed through lattice quantization but can also be efficiently learned through our novel ideas presented in this paper; secondly, they are closely related to Gaussian Variational Autoencoders, making it easy for designers who are familiar with the latter to generate discrete representations from their models; thirdly, as lattices satisfy the axioms of a group, their adoption can facilitate the learning of simple algebras for modeling binary operations between objects through symbolic formalisms, and their structures can also be learned formally using differentiation techniques. This paper will focus on establishing the foundation for exploring and leveraging the first two properties, including a new mathematical finding that links expressions used during training and inference time, as well as experimental validation conducted on two prominent datasets.",1
"Continual learning aims to learn tasks sequentially, with (often severe) constraints on the storage of old learning samples, without suffering from catastrophic forgetting. In this work, we propose prescient continual learning, a novel experimental setting, to incorporate existing information about the classes, prior to any training data. Usually, each task in a traditional continual learning setting evaluates the model on present and past classes, the latter with a limited number of training samples. Our setting adds future classes, with no training samples at all. We introduce Ghost Model, a representation-learning-based model for continual learning using ideas from zero-shot learning. A generative model of the representation space in concert with a careful adjustment of the losses allows us to exploit insights from future classes to constraint the spatial arrangement of the past and current classes. Quantitative results on the AwA2 and aP\&Y datasets and detailed visualizations showcase the interest of this new setting and the method we propose to address it.",0
"The goal of continual learning is to learn tasks one after another while facing serious constraints on the storage of previous learning data, all without experiencing catastrophic forgetting. This study introduces a new experimental setting called prescient continual learning, which takes into account prior information about classes before any training data is provided. In traditional continual learning, each task evaluates the model on past and present classes, with limited training samples for the former. However, our setting includes future classes that have no training samples at all. To address this challenge, we propose the Ghost Model, a representation-learning-based model that draws from zero-shot learning ideas. By combining a generative model of the representation space with careful loss adjustments, we utilize insights from future classes to control the spatial arrangement of past and present classes. Quantitative results on the AwA2 and aP\&Y datasets and detailed visualizations demonstrate the value of this new setting and the proposed method for tackling it.",1
"Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.",0
"Theoretical analysis suggests that neural networks can be comprehended by treating the number of channels as infinite, resulting in Gaussian process (GP) distributed outputs. However, this approach lacks a critical aspect of real neural networks - the fixed kernel, determined solely by network hyperparameters, prohibits any form of representation learning. As a result, the networks are less flexible, leading to inferior performance compared to finite networks. Analytic results demonstrate the prior over representations and representation learning in finite deep linear networks. The representations in state-of-the-art architectures closely resemble those suggested by deep linear results, rather than infinite networks. This motivates the introduction of a new type of network: infinite networks with bottlenecks, which maintain the theoretical tractability of infinite networks while enabling representation learning.",1
"Existing approaches for graph neural networks commonly suffer from the oversmoothing issue, regardless of how neighborhoods are aggregated. Most methods also focus on transductive scenarios for fixed graphs, leading to poor generalization for unseen graphs. To address these issues, we propose a new graph neural network that considers both edge-based neighborhood relationships and node-based entity features, i.e. Graph Entities with Step Mixture via random walk (GESM). GESM employs a mixture of various steps through random walk to alleviate the oversmoothing problem, attention to dynamically reflect interrelations depending on node information, and structure-based regularization to enhance embedding representation. With intensive experiments, we show that the proposed GESM achieves state-of-the-art or comparable performances on eight benchmark graph datasets comprising transductive and inductive learning tasks. Furthermore, we empirically demonstrate the significance of considering global information.",0
"Graph neural networks currently face the issue of oversmoothing, regardless of the method used to aggregate neighborhoods. Additionally, most approaches only work well for fixed graphs in transductive scenarios, causing poor generalization for unseen graphs. To combat these problems, we introduce Graph Entities with Step Mixture via random walk (GESM), a novel graph neural network that takes into account both edge-based neighborhood relationships and node-based entity features. GESM uses a mixture of various steps through random walk to reduce oversmoothing, pays attention to interrelations based on node information, and includes structure-based regularization to improve embedding representation. Through numerous experiments, we demonstrate that GESM outperforms or equals state-of-the-art methods on eight benchmark graph datasets that include both transductive and inductive learning tasks. Additionally, we show the importance of considering global information.",1
"A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations. Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space. Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control. In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space. We call this model control-aware representation learning (CARL). We derive a loss function for CARL that has close connection to the prediction, consistency, and curvature (PCC) principle for representation learning. We derive three implementations of CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance. Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",0
"Efficient control of dynamical systems from high-dimensional sensory observations is a major challenge in modern reinforcement learning (RL). To address this challenge, a promising approach is learning controllable embedding (LCE), which involves embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it for control. However, two important questions in this area are how to learn a representation that is suitable for the control problem and how to achieve an end-to-end framework for representation learning and control. In this paper, we take steps towards answering these questions by introducing a control-aware representation learning (CARL) model that formulates a LCE model to learn suitable representations for policy iteration style algorithms in the latent space. We derive a loss function for CARL that is closely connected to the prediction, consistency, and curvature (PCC) principle for representation learning and implement three versions of CARL. The offline implementation replaces the locally-linear control algorithm with a RL algorithm, model-based soft actor-critic, resulting in significant improvement. The online CARL interweaves representation learning and control, demonstrating further gains in performance. Lastly, we propose value-guided CARL, which optimizes a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms through extensive experiments on benchmark tasks and compare them with several LCE baselines.",1
"Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure ""expressiveness"" of embedding is conceptually clean; it leads to simpler proofs than \cite{oono2019graph} and can handle more non-linearities.",0
"Success has been achieved by Graph Neural Networks (GNNs) on data with graph structure. However, it has been observed that the performance of GNNs does not improve with increasing layers, known as over-smoothing, mostly analyzed in linear cases. In this study, we expand on previous findings \cite{oono2019graph} to further examine this effect in the general GNN architecture. We demonstrate that under conditions set by the spectrum of augmented normalized Laplacian, the weight matrix causes the Dirichlet energy of embeddings to converge to zero, leading to loss of discriminative power. Using Dirichlet energy as a measure of embedding ""expressiveness"" is conceptually clear, simplifying proofs compared to \cite{oono2019graph} and accommodating more non-linearities.",1
"Massive open online courses are becoming a modish way for education, which provides a large-scale and open-access learning opportunity for students to grasp the knowledge. To attract students' interest, the recommendation system is applied by MOOCs providers to recommend courses to students. However, as a course usually consists of a number of video lectures, with each one covering some specific knowledge concepts, directly recommending courses overlook students'interest to some specific knowledge concepts. To fill this gap, in this paper, we study the problem of knowledge concept recommendation. We propose an end-to-end graph neural network-based approach calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other recommendation problems, it suffers from sparsity issues. To address this issue, we leverage both content information and context information to learn the representation of entities via graph convolution network. In addition to students and knowledge concepts, we consider other types of entities (e.g., courses, videos, teachers) and construct a heterogeneous information network to capture the corresponding fruitful semantic relationships among different types of entities and incorporate them into the representation learning process. Specifically, we use meta-path on the HIN to guide the propagation of students' preferences. With the help of these meta-paths, the students' preference distribution with respect to a candidate knowledge concept can be captured. Furthermore, we propose an attention mechanism to adaptively fuse the context information from different meta-paths, in order to capture the different interests of different students. The promising experiment results show that the proposedACKRecis able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs.",0
"Massive open online courses are a trendy method of education that offers a vast and accessible learning opportunity for students to acquire knowledge. MOOC providers apply a recommendation system to attract students, but simply recommending courses overlooks their interest in specific knowledge concepts. To address this gap, we present a study on knowledge concept recommendation in MOOCs. Our proposed solution, the Attentional Heterogeneous Graph Convolutional Deep Knowledge Recommender (ACKRec), is an end-to-end graph neural network-based approach. Like other recommendation problems, sparsity is an issue, so we use both content and context information to learn entity representations through graph convolution network. Our approach incorporates a heterogeneous information network that considers various entities such as courses, videos, teachers, and students, and captures fruitful semantic relationships among them. We employ meta-paths to guide the propagation of students' preferences, allowing us to capture their preference distribution with respect to a candidate knowledge concept. Furthermore, we propose an attention mechanism that fuses context information from different meta-paths to adaptively capture the different interests of diverse students. Our experiment results demonstrate that ACKRec can effectively recommend knowledge concepts to students pursuing online learning in MOOCs.",1
"Image segmentation is a fundamental and challenging problem in computer vision with applications spanning multiple areas, such as medical imaging, remote sensing, and autonomous vehicles. Recently, convolutional neural networks (CNNs) have gained traction in the design of automated segmentation pipelines. Although CNN-based models are adept at learning abstract features from raw image data, their performance is dependent on the availability and size of suitable training datasets. Additionally, these models are often unable to capture the details of object boundaries and generalize poorly to unseen classes. In this thesis, we devise novel methodologies that address these issues and establish robust representation learning frameworks for fully-automatic semantic segmentation in medical imaging and mainstream computer vision. In particular, our contributions include (1) state-of-the-art 2D and 3D image segmentation networks for computer vision and medical image analysis, (2) an end-to-end trainable image segmentation framework that unifies CNNs and active contour models with learnable parameters for fast and robust object delineation, (3) a novel approach for disentangling edge and texture processing in segmentation networks, and (4) a novel few-shot learning model in both supervised settings and semi-supervised settings where synergies between latent and image spaces are leveraged to learn to segment images given limited training data.",0
"Computer vision faces a significant challenge in image segmentation, which has a wide range of applications in areas such as medical imaging, remote sensing, and autonomous vehicles. Convolutional neural networks (CNNs) have recently been used to design automated segmentation pipelines, but their performance is highly dependent on the availability and size of appropriate training datasets. Moreover, they often fail to capture object boundaries and perform poorly when dealing with unseen classes. To address these issues, this thesis proposes new methodologies for fully-automatic semantic segmentation in medical imaging and mainstream computer vision. The contributions include state-of-the-art 2D and 3D image segmentation networks, an end-to-end trainable framework that combines CNNs and active contour models with learnable parameters for fast and robust object delineation, a novel approach for disentangling edge and texture processing in segmentation networks, and a few-shot learning model that leverages synergies between latent and image spaces to learn to segment images in both supervised and semi-supervised settings with limited training data.",1
"Learning interpretable and disentangled representations is a crucial yet challenging task in representation learning. In this work, we focus on semi-supervised disentanglement learning and extend work by Locatello et al. (2019) by introducing another source of supervision that we denote as label replacement. Specifically, during training, we replace the inferred representation associated with a data point with its ground-truth representation whenever it is available. Our extension is theoretically inspired by our proposed general framework of semi-supervised disentanglement learning in the context of VAEs which naturally motivates the supervised terms commonly used in existing semi-supervised VAEs (but not for disentanglement learning). Extensive experiments on synthetic and real datasets demonstrate both quantitatively and qualitatively the ability of our extension to significantly and consistently improve disentanglement with very limited supervision.",0
"Developing interpretable and disentangled representations is an essential yet difficult task in the field of representation learning. This study concentrates on semi-supervised disentanglement learning and builds upon the work of Locatello et al. (2019) by introducing a new form of supervision referred to as label replacement. During training, we replace the inferred representation of a data point with its ground-truth representation, if available. Our extension is based on a theoretical framework for semi-supervised disentanglement learning in VAEs, which motivates the use of supervised terms commonly employed in semi-supervised VAEs. Our experiments on both synthetic and real datasets demonstrate that our approach significantly and consistently improves disentanglement with limited supervision, both quantitatively and qualitatively.",1
"The objective of this paper is self-supervised representation learning, with the goal of solving semi-supervised video object segmentation (a.k.a. dense tracking). We make the following contributions: (i) we propose to improve the existing self-supervised approach, with a simple, yet more effective memory mechanism for long-term correspondence matching, which resolves the challenge caused by the dis-appearance and reappearance of objects; (ii) by augmenting the self-supervised approach with an online adaptation module, our method successfully alleviates tracker drifts caused by spatial-temporal discontinuity, e.g. occlusions or dis-occlusions, fast motions; (iii) we explore the efficiency of self-supervised representation learning for dense tracking, surprisingly, we show that a powerful tracking model can be trained with as few as 100 raw video clips (equivalent to a duration of 11mins), indicating that low-level statistics have already been effective for tracking tasks; (iv) we demonstrate state-of-the-art results among the self-supervised approaches on DAVIS-2017 and YouTube-VOS, as well as surpassing most of methods trained with millions of manual segmentation annotations, further bridging the gap between self-supervised and supervised learning. Codes are released to foster any further research (https://github.com/fangruizhu/self_sup_semiVOS).",0
"This paper focuses on self-supervised representation learning to address the challenge of semi-supervised video object segmentation, also known as dense tracking. The paper proposes several contributions, including: (i) an improved self-supervised approach that employs a more effective memory mechanism for long-term correspondence matching, which addresses the issue of object dis-appearance and reappearance; (ii) an online adaptation module that mitigates tracker drifts caused by spatial-temporal discontinuity such as occlusions or dis-occlusions, and fast motions; (iii) an investigation into the efficiency of self-supervised representation learning for dense tracking, which shows that a powerful tracking model can be trained with as few as 100 raw video clips, indicating the effectiveness of low-level statistics for tracking tasks; and (iv) state-of-the-art results on DAVIS-2017 and YouTube-VOS, surpassing most methods trained with millions of manual segmentation annotations, thereby bridging the gap between self-supervised and supervised learning. The authors have made their codes available to encourage further research. (https://github.com/fangruizhu/self_sup_semiVOS)",1
"Hypergraphs provide a natural representation for many real world datasets. We propose a novel framework, HNHN, for hypergraph representation learning. HNHN is a hypergraph convolution network with nonlinear activation functions applied to both hypernodes and hyperedges, combined with a normalization scheme that can flexibly adjust the importance of high-cardinality hyperedges and high-degree vertices depending on the dataset. We demonstrate improved performance of HNHN in both classification accuracy and speed on real world datasets when compared to state of the art methods.",0
A lot of real world datasets can be naturally represented through hypergraphs. Our proposal is a new framework called HNHN for hypergraph representation learning. HNHN is basically a hypergraph convolution network that uses nonlinear activation functions on hypernodes and hyperedges. It also has a normalization scheme that can adjust the significance of high-cardinality hyperedges and high-degree vertices based on the dataset. We have shown that HNHN performs better than state-of-the-art methods in terms of classification accuracy and speed when tested on real world datasets.,1
"We present a hierarchical neural message passing architecture for learning on molecular graphs. Our model takes in two complementary graph representations: the raw molecular graph representation and its associated junction tree, where nodes represent meaningful clusters in the original graph, e.g., rings or bridged compounds. We then proceed to learn a molecule's representation by passing messages inside each graph, and exchange messages between the two representations using a coarse-to-fine and fine-to-coarse information flow. Our method is able to overcome some of the restrictions known from classical GNNs, like detecting cycles, while still being very efficient to train. We validate its performance on the ZINC dataset and datasets stemming from the MoleculeNet benchmark collection.",0
"We introduce a hierarchical neural message passing structure for molecular graph learning. Our approach utilizes two graph representations: the initial raw molecular graph representation and the corresponding junction tree, in which the nodes depict significant clusters in the original graph, such as bridged compounds or rings. We then employ message passing within each graph and exchange messages between the two representations using a coarse-to-fine and fine-to-coarse information flow to learn a molecule's representation. Our technique overcomes some limitations of traditional GNNs, such as cycle detection, while remaining highly efficient for training. To demonstrate its efficacy, we validate our method on the ZINC dataset and MoleculeNet benchmark collection datasets.",1
"Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models have some limitations in different applications. For example, a VAE easily suffers from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve better disentangling and reconstruction quality than the existing methods. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality of generated images compared to the original VAE.",0
"The use of Variational Autoencoders (VAE) and their variants has become widespread in several applications such as image generation, dialog generation, and disentangled representation learning. Despite their popularity, VAE models have limitations in various applications. For instance, KL vanishing presents a problem in language modeling, while disentangling may lead to low reconstruction quality. To solve these issues, we suggest a new approach referred to as ControlVAE. It is a controllable variational autoencoder framework that involves a controller inspired by automatic control theory. By combining the basic VAE with a controller, we aim to improve the performance of resulting generative models. Our approach adopts a novel non-linear PI controller, which is a variant of the proportional-integral-derivative (PID) control. The controller automatically tunes the hyperparameter (weight) added in the VAE objective. During model training, we use the output KL-divergence as feedback to adjust the model's performance. We evaluate our framework using three applications, including language modeling, disentangled representation learning, and image generation. Our results indicate that ControlVAE outperforms existing methods in terms of disentangling and reconstruction quality. In language modeling, our approach not only eliminates KL-vanishing but also improves the diversity of generated text. Additionally, ControlVAE enhances the reconstruction quality of generated images compared to the original VAE.",1
"We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.",0
"Our proposed solution offers a lightweight approach to recovering 3D pose from multiple images obtained through spatially calibrated cameras. By leveraging recent developments in interpretable representation learning and utilizing 3D geometry, we combine the input images to create a unified latent representation of pose that is independent of camera viewpoints. This enables efficient reasoning about 3D pose across various views, without the need for computationally intensive volumetric grids. Our architecture conditions the learned representation on camera projection operators to generate precise per-view 2D detections, which can be effortlessly converted to 3D using a differentiable Direct Linear Transform (DLT) layer. We have developed a novel implementation of DLT that performs significantly faster on GPU architectures than standard SVD-based triangulation techniques. Our method has been tested on two large-scale human pose datasets (H36M and Total Capture) and has outperformed or achieved comparable results to state-of-the-art volumetric methods, while also providing real-time performance.",1
"In self-supervised spatio-temporal representation learning, the temporal resolution and long-short term characteristics are not yet fully explored, which limits representation capabilities of learned models. In this paper, we propose a novel self-supervised method, referred to as video Playback Rate Perception (PRP), to learn spatio-temporal representation in a simple-yet-effective way. PRP roots in a dilated sampling strategy, which produces self-supervision signals about video playback rates for representation model learning. PRP is implemented with a feature encoder, a classification module, and a reconstructing decoder, to achieve spatio-temporal semantic retention in a collaborative discrimination-generation manner. The discriminative perception model follows a feature encoder to prefer perceiving low temporal resolution and long-term representation by classifying fast-forward rates. The generative perception model acts as a feature decoder to focus on comprehending high temporal resolution and short-term representation by introducing a motion-attention mechanism. PRP is applied on typical video target tasks including action recognition and video retrieval. Experiments show that PRP outperforms state-of-the-art self-supervised models with significant margins. Code is available at github.com/yuanyao366/PRP",0
"The representation capabilities of learned models in self-supervised spatio-temporal representation learning are limited due to the under-exploration of temporal resolution and long-short term characteristics. This paper introduces a new self-supervised method, the video Playback Rate Perception (PRP), that effectively learns spatio-temporal representation. PRP uses a dilated sampling strategy to generate self-supervision signals about video playback rates for representation model learning. It consists of a feature encoder, a classification module, and a reconstructing decoder that collaboratively discriminate and generate to achieve spatio-temporal semantic retention. The discriminative perception model classifies fast-forward rates to prefer perceiving low temporal resolution and long-term representation, while the generative perception model introduces a motion-attention mechanism to focus on comprehending high temporal resolution and short-term representation. PRP is evaluated on action recognition and video retrieval tasks, where it outperforms state-of-the-art self-supervised models with significant margins. The code for PRP is available on github.com/yuanyao366/PRP.",1
"Graph neural networks have attracted wide attentions to enable representation learning of graph data in recent works. In complement to graph convolution operators, graph pooling is crucial for extracting hierarchical representation of graph data. However, most recent graph pooling methods still fail to efficiently exploit the geometry of graph data. In this paper, we propose a novel graph pooling strategy that leverages node proximity to improve the hierarchical representation learning of graph data with their multi-hop topology. Node proximity is obtained by harmonizing the kernel representation of topology information and node features. Implicit structure-aware kernel representation of topology information allows efficient graph pooling without explicit eigendecomposition of the graph Laplacian. Similarities of node signals are adaptively evaluated with the combination of the affine transformation and kernel trick using the Gaussian RBF function. Experimental results demonstrate that the proposed graph pooling strategy is able to achieve state-of-the-art performance on a collection of public graph classification benchmark datasets.",0
"Recent studies have shown that graph neural networks are effective for learning representations of graph data. Along with graph convolution operators, graph pooling is essential for creating a hierarchical representation of graph data. However, current graph pooling methods do not efficiently utilize the geometry of graph data. To address this issue, this research proposes a novel graph pooling approach that utilizes node proximity to improve hierarchical representation learning. Node proximity is obtained by combining topology information and node features. The proposed approach uses an implicit structure-aware kernel representation of topology information, which enables efficient graph pooling without explicit eigendecomposition of the graph Laplacian. Additionally, the similarities of node signals are evaluated using an affine transformation and kernel trick with the Gaussian RBF function. Experimental results show that the proposed approach outperforms existing methods on various graph classification benchmark datasets.",1
"Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering (ODC) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, i.e., samples memory to store samples labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that ODC stabilizes the training process and boosts the performance effectively. Code: https://github.com/open-mmlab/OpenSelfSup.",0
"Unsupervised representation learning has seen impressive results through the use of joint clustering and feature learning methods. However, alternating between feature clustering and network parameter updates during training has led to unstable learning of visual representations. To address this issue, we introduce Online Deep Clustering (ODC), which performs clustering and network updates simultaneously instead of alternatingly. Our approach focuses on ensuring that cluster centroids evolve steadily to maintain a stable classifier. We accomplish this by maintaining two dynamic memory modules - samples memory for storing sample labels and features, and centroids memory for centroids evolution. We break down global clustering into steady memory updates and batch-wise label re-assignment, and integrate this process into network update iterations. By doing so, the labels and network evolve together instead of in an alternating fashion. Our experiments show that ODC effectively stabilizes the training process and improves performance. Our code can be found on GitHub at https://github.com/open-mmlab/OpenSelfSup.",1
"In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting is NP-complete and requires more global inference to oversee the whole graph. To make it scalable for large-scale graphs and patterns, we propose a learning framework which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. A mutagenic compound dataset, MUTAG, is also used to evaluate neural models and demonstrate the success of transfer learning. While the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in linear time compared to the exponential time of the original NP-complete problem. Experimental results show that learning based subgraph isomorphism counting can speed up the traditional algorithm, VF2, 10-1,000 times with acceptable errors. Domain adaptation based on fine-tuning also shows the usefulness of our approach in real-world applications.",0
"The focus of this research is a novel graph learning problem: the ability to count subgraph isomorphisms. Unlike other traditional graph learning problems, such as node classification and link prediction, subgraph isomorphism counting is a complex NP-complete task that requires global inference to analyze the entire graph. To make this task feasible for large-scale graphs and patterns, we propose a learning framework that incorporates various representation learning architectures. This framework iteratively attends to pattern and target data graphs to memorize subgraph isomorphisms for global counting. We evaluate our approach using both small graphs (with <= 1,024 subgraph isomorphisms each) and large graphs (with <= 4,096 subgraph isomorphisms each) and use the MUTAG dataset to demonstrate the effectiveness of transfer learning. Although the learning-based approach is not exact, we are able to generalize and count large patterns and data graphs in linear time, in contrast to the exponential time required for the original NP-complete problem. Our experimental results show that our approach can speed up the traditional VF2 algorithm by 10-1,000 times with acceptable errors. Furthermore, our domain adaptation approach based on fine-tuning indicates the practical usefulness of our approach in real-world applications.",1
"Graph Representation Learning (GRL) has experienced significant progress as a means to extract structural information in a meaningful way for subsequent learning tasks. Current approaches including shallow embeddings and Graph Neural Networks have mostly been tested with node classification and link prediction tasks. In this work, we provide an application oriented perspective to a set of popular embedding approaches and evaluate their representational power with respect to real-world graph properties. We implement an extensive empirical data-driven framework to challenge existing norms regarding the expressive power of embedding approaches in graphs with varying patterns along with a theoretical analysis of the limitations we discovered in this process. Our results suggest that ""one-to-fit-all"" GRL approaches are hard to define in real-world scenarios and as new methods are being introduced they should be explicit about their ability to capture graph properties and their applicability in datasets with non-trivial structural differences.",0
"Significant progress has been made in Graph Representation Learning (GRL) to extract structural information in a meaningful way for subsequent learning tasks. However, current approaches such as shallow embeddings and Graph Neural Networks have mainly been tested with node classification and link prediction tasks. This study takes an application-oriented perspective to evaluate popular embedding approaches and their representational power with respect to real-world graph properties. An empirical data-driven framework is implemented to challenge existing norms and a theoretical analysis sheds light on the limitations discovered. The results indicate that a ""one-to-fit-all"" GRL approach is difficult to define in real-world scenarios and new methods should explicitly state their ability to capture graph properties and their applicability in datasets with non-trivial structural differences.",1
"Robust representation learning of temporal dynamic interactions is an important problem in robotic learning in general and automated unsupervised learning in particular. Temporal dynamic interactions can be described by (multiple) geometric trajectories in a suitable space over which unsupervised learning techniques may be applied to extract useful features from raw and high-dimensional data measurements. Taking a geometric approach to robust representation learning for temporal dynamic interactions, it is necessary to develop suitable metrics and a systematic methodology for comparison and for assessing the stability of an unsupervised learning method with respect to its tuning parameters. Such metrics must account for the (geometric) constraints in the physical world as well as the uncertainty associated with the learned patterns. In this paper we introduce a model-free metric based on the Procrustes distance for robust representation learning of interactions, and an optimal transport based distance metric for comparing between distributions of interaction primitives. These distance metrics can serve as an objective for assessing the stability of an interaction learning algorithm. They are also used for comparing the outcomes produced by different algorithms. Moreover, they may also be adopted as an objective function to obtain clusters and representative interaction primitives. These concepts and techniques will be introduced, along with mathematical properties, while their usefulness will be demonstrated in unsupervised learning of vehicle-to-vechicle interactions extracted from the Safety Pilot database, the world's largest database for connected vehicles.",0
"The problem of learning robust representations for temporal dynamic interactions is crucial in robotic and unsupervised learning. To achieve this, geometric trajectories are utilized to extract valuable features from raw and high-dimensional data measurements. Developing appropriate metrics and a systematic methodology is essential to compare and assess the stability of unsupervised learning methods. These metrics should consider the physical world's constraints and the uncertainty associated with learned patterns. This paper presents two distance metrics, the Procrustes distance and optimal transport-based distance metric, to measure the stability of interaction learning algorithms and compare outcomes. These metrics can also be used to obtain clusters and representative interaction primitives. The effectiveness of these concepts and techniques is demonstrated using the Safety Pilot database, the world's largest database for connected vehicles. Mathematical properties are discussed to support the presented approach.",1
"Over the last few years, graph autoencoders (AE) and variational autoencoders (VAE) emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on multi-layer graph convolutional networks (GCN) encoders to learn vector space representations of nodes. In this paper, we show that GCN encoders are actually unnecessarily complex for many applications. We propose to replace them by significantly simpler and more interpretable linear models w.r.t. the direct neighborhood (one-hop) adjacency matrix of the graph, involving fewer operations, fewer parameters and no activation function. For the two aforementioned tasks, we show that this simpler approach consistently reaches competitive performances w.r.t. GCN-based graph AE and VAE for numerous real-world graphs, including all benchmark datasets commonly used to evaluate graph AE and VAE. Based on these results, we also question the relevance of repeatedly using these datasets to compare complex graph AE and VAE.",0
"In recent years, graph autoencoders and variational autoencoders have become popular methods for node embedding, showing promising results in tasks such as link prediction and node clustering. These methods rely on multi-layer graph convolutional networks to learn node representations, but we argue that these networks are overly complex for many applications. Instead, we propose using simpler linear models based on the one-hop adjacency matrix, which require fewer operations and parameters and no activation function. Our experiments show that this simpler approach performs competitively with GCN-based methods on a variety of real-world graphs, including all common benchmark datasets. This raises questions about the usefulness of repeatedly evaluating graph AE and VAE on these datasets.",1
"Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",0
"Utilizing unlabelled data is made possible by self-supervised learning, which yields feature representations applicable to downstream tasks like classification, object detection, segmentation, and anomaly detection. While self-supervised learning has been utilized for classification, object detection, and segmentation, anomaly detection has not received enough emphasis. Our study addresses the issue of anomaly detection in images and videos and presents a novel technique for visual anomaly detection in videos. We evaluated numerous self-supervised methods, including seminal and state-of-the-art ones, for anomaly detection across various image datasets. We then employed the best-performing image-based self-supervised representation learning method for video anomaly detection to demonstrate the significance of spatial features in visual anomaly detection in videos. Furthermore, we proposed a straightforward self-supervision approach that learns temporal coherence across video frames without using optical flow information. Our method identifies the frame indices of a disordered video sequence, allowing it to learn the spatiotemporal features of the video. Our intuitive approach shows superior performance for visual anomaly detection in images and videos compared to numerous existing methods, as demonstrated on UCF101 and ILSVRC2015 video datasets.",1
"Organizations that own data face increasing legal liability for its discriminatory use against protected demographic groups, extending to contractual transactions involving third parties access and use of the data. This is problematic, since the original data owner cannot ex-ante anticipate all its future uses by downstream users. This paper explores the upstream ability to preemptively remove the correlations between features and sensitive attributes by mapping features to a fair representation space. Our main result shows that the fairness measured by the demographic parity of the representation distribution can be certified from a finite sample if and only if the chi-squared mutual information between features and representations is finite. Empirically, we find that smoothing the representation distribution provides generalization guarantees of fairness certificates, which improves upon existing fair representation learning approaches. Moreover, we do not observe that smoothing the representation distribution degrades the accuracy of downstream tasks compared to state-of-the-art methods in fair representation learning.",0
"As organizations possess data, they are increasingly exposed to legal responsibility for any discriminatory actions taken against protected demographic groups. This liability extends to transactions with third parties who access and use the data. The issue arises as the original data owner cannot predict all the future uses of the data by downstream users. This study examines the possibility of preemptively eliminating the links between features and sensitive attributes by mapping features to an impartial representation space. The primary finding of this research is that fairness, as measured by the demographic parity of the representation distribution, can be certified from a finite sample only if the chi-squared mutual information between features and representations is finite. We have discovered that smoothing the representation distribution offers generalization guarantees of fairness certificates, which enhances current fair representation learning methods. Additionally, we have not observed any degradation in the accuracy of downstream tasks with the smoothing of the representation distribution compared to state-of-the-art fair representation learning approaches.",1
"To learn intrinsic low-dimensional structures from high-dimensional data that most discriminate between classes, we propose the principle of Maximal Coding Rate Reduction ($\text{MCR}^2$), an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class. We clarify its relationships with most existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees for learning diverse and discriminative features. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions and can learn intrinsic representations in supervised, self-supervised, and unsupervised settings in a unified manner. Empirically, the representations learned using this principle alone are significantly more robust to label corruptions in classification than those using cross-entropy, and can lead to state-of-the-art results in clustering mixed data from self-learned invariant features.",0
"Our proposed principle, Maximal Coding Rate Reduction ($\text{MCR}^2$), aims to uncover the intrinsic low-dimensional structures of high-dimensional data that best differentiate between classes. This information-theoretic measure maximizes the coding rate difference between the entire dataset and the sum of each class, and can effectively learn diverse and discriminative features. We establish its relationship with existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions, and can be used for supervised, self-supervised, and unsupervised learning. By using $\text{MCR}^2$ alone, the representations learned are more robust to label corruptions in classification than those using cross-entropy, and can achieve state-of-the-art results in clustering mixed data from self-learned invariant features.",1
"Many tasks in computer vision are often calibrated and evaluated relative to human perception. In this paper, we propose to directly approximate the perceptual function performed by human observers completing a visual detection task. Specifically, we present a novel methodology for learning to detect image transformations visible to human observers through approximating perceptual thresholds. To do this, we carry out a subjective two-alternative forced-choice study to estimate perceptual thresholds of human observers detecting local exposure shifts in images. We then leverage transformation equivariant representation learning to overcome issues of limited perceptual data. This representation is then used to train a dense convolutional classifier capable of detecting local suprathreshold exposure shifts - a distortion common to image composites. In this context, our model can approximate perceptual thresholds with an average error of 0.1148 exposure stops between empirical and predicted thresholds. It can also be trained to detect a range of different local transformations.",0
"Tasks in computer vision often rely on human perception as a benchmark for calibration and evaluation. This paper proposes a novel approach to approximating the perceptual function that human observers perform when completing a visual detection task. The methodology involves learning to detect image transformations that are visible to human observers by approximating their perceptual thresholds. The approach involves a subjective two-alternative forced-choice study to estimate these thresholds. To overcome the challenge of limited perceptual data, transformation equivariant representation learning is used. This representation is then used to train a dense convolutional classifier that can detect local suprathreshold exposure shifts, a common distortion in image composites. The model can approximate perceptual thresholds with an average error of 0.1148 exposure stops and can be trained to detect a range of different local transformations.",1
"With the dramatic increase of dimensions in the data representation, extracting latent low-dimensional features becomes of the utmost importance for efficient classification. Aiming at the problems of unclear margin representation and difficulty in revealing the data manifold structure in most of the existing linear discriminant methods, we propose a new discriminant feature extraction framework, namely Robust Locality-Aware Regression (RLAR). In our model, we introduce a retargeted regression to perform the marginal representation learning adaptively instead of using the general average inter-class margin. Besides, we formulate a new strategy for enhancing the local intra-class compactness of the data manifold, which can achieve the joint learning of locality-aware graph structure and desirable projection matrix. To alleviate the disturbance of outliers and prevent overfitting, we measure the regression term and locality-aware term together with the regularization term by the L2,1 norm. Further, forcing the row sparsity on the projection matrix through the L2,1 norm achieves the cooperation of feature selection and feature extraction. Then, we derive an effective iterative algorithm for solving the proposed model. The experimental results over a range of UCI data sets and other benchmark databases demonstrate that the proposed RLAR outperforms some state-of-the-art approaches.",0
"The extraction of latent low-dimensional features is critical for efficient classification as the representation of data dimensions increases dramatically. However, existing linear discriminant methods face issues with unclear margin representation and revealing the data manifold structure. To address these problems, we propose a new discriminant feature extraction framework called Robust Locality-Aware Regression (RLAR). In our model, we use retargeted regression to learn marginal representation adaptively instead of using the general average inter-class margin. Additionally, we formulate a new strategy to enhance the local intra-class compactness of the data manifold, achieving joint learning of locality-aware graph structure and desirable projection matrix. To counter outliers and prevent overfitting, we measure the regression term and locality-aware term with the regularization term through the L2,1 norm. We also use the L2,1 norm to enforce row sparsity on the projection matrix, enabling feature selection and extraction cooperation. We then derive an effective iterative algorithm to solve the proposed model. Our experiments on several benchmark databases and UCI data sets show that RLAR outperforms some state-of-the-art approaches.",1
"The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field.",0
"Recently, there has been a surge of interest in the deep learning community regarding the adaptive processing of graph data, which has been a longstanding research topic. Unfortunately, this increase in research has led to a lack of organization and attention to earlier literature. To address this, this paper serves as a tutorial introduction to the field of deep learning for graphs. Rather than focusing solely on the latest literature, this paper aims to provide a consistent and gradual introduction to the main concepts and architectural aspects of the field. The paper takes a top-down approach, introducing a generalized formulation of graph representation learning through a local and iterative approach to structured information processing. It also provides an overview of the basic building blocks that can be used to design novel and effective neural models for graphs. Additionally, the paper discusses interesting research challenges and applications in the field.",1
"In recent years, we have witnessed a surge of interest in multi-view representation learning, which is concerned with the problem of learning representations of multi-view data. When facing multiple views that are highly related but sightly different from each other, most of existing multi-view methods might fail to fully integrate multi-view information. Besides, correlations between features from multiple views always vary seriously, which makes multi-view representation challenging. Therefore, how to learn appropriate embedding from multi-view information is still an open problem but challenging. To handle this issue, this paper proposes a novel multi-view learning method, named Multi-view Low-rank Preserving Embedding (MvLPE). It integrates different views into one centroid view by minimizing the disagreement term, based on distance or similarity matrix among instances, between the centroid view and each view meanwhile maintaining low-rank reconstruction relations among samples for each view, which could make more full use of compatible and complementary information from multi-view features. Unlike existing methods with additive parameters, the proposed method could automatically allocate a suitable weight for each view in multi-view information fusion. However, MvLPE couldn't be directly solved, which makes the proposed MvLPE difficult to obtain an analytic solution. To this end, we approximate this solution based on stationary hypothesis and normalization post-processing to efficiently obtain the optimal solution. Furthermore, an iterative alternating strategy is provided to solve this multi-view representation problem. The experiments on six benchmark datasets demonstrate that the proposed method outperforms its counterparts while achieving very competitive performance.",0
"The past few years have seen a growing interest in multi-view representation learning, which involves learning representations of data from multiple views. Existing multi-view methods may struggle to fully integrate information from multiple related but slightly different views, and the varying correlations between features from different views make multi-view representation challenging. Consequently, learning an appropriate embedding from multi-view information remains an open and challenging problem. This paper proposes a new multi-view learning method, Multi-view Low-rank Preserving Embedding (MvLPE), which integrates different views into a centroid view by minimizing the disagreement term between the centroid view and each view's distance or similarity matrix while maintaining low-rank reconstruction relations among samples for each view. This approach makes better use of compatible and complementary information from multi-view features, and unlike existing methods that use additive parameters, MvLPE automatically allocates suitable weights for each view in information fusion. However, obtaining an analytic solution for MvLPE is challenging, so this paper approximates the solution based on the stationary hypothesis and normalization post-processing, and provides an iterative alternating strategy to solve the multi-view representation problem. Experiments on six benchmark datasets demonstrate that MvLPE outperforms its counterparts while achieving very competitive performance.",1
"Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.",0
"The Graph Convolutional Networks (GCNs) have attracted considerable attention due to their ability to learn representations of graphs. However, unlike Convolutional Neural Networks (CNNs), which can utilize deep layer stacking, GCNs face challenges such as vanishing gradients, over-smoothing, and over-fitting when going deeper, leading to limited representation power on large-scale graphs. This study introduces DeeperGCN, which effectively and reliably trains very deep GCNs. To unify various message aggregation operations, such as mean and max, we define differentiable generalized aggregation functions. Additionally, we suggest a novel normalization layer known as MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) demonstrate that DeeperGCN outperforms state-of-the-art approaches in node and graph property prediction tasks on large-scale graphs. For more information, please visit https://www.deepgcns.org.",1
"As a crucial component in intelligent transportation systems, traffic flow prediction has recently attracted widespread research interest in the field of artificial intelligence (AI) with the increasing availability of massive traffic mobility data. Its key challenge lies in how to integrate diverse factors (such as temporal rules and spatial dependencies) to infer the evolution trend of traffic flow. To address this problem, we propose a unified neural network called Attentive Traffic Flow Machine (ATFM), which can effectively learn the spatial-temporal feature representations of traffic flow with an attention mechanism. In particular, our ATFM is composed of two progressive Convolutional Long Short-Term Memory (ConvLSTM \cite{xingjian2015convolutional}) units connected with a convolutional layer. Specifically, the first ConvLSTM unit takes normal traffic flow features as input and generates a hidden state at each time-step, which is further fed into the connected convolutional layer for spatial attention map inference. The second ConvLSTM unit aims at learning the dynamic spatial-temporal representations from the attentionally weighted traffic flow features. Further, we develop two deep learning frameworks based on ATFM to predict citywide short-term/long-term traffic flow by adaptively incorporating the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks well demonstrate the superiority of the proposed method for traffic flow prediction. Moreover, to verify the generalization of our method, we also apply the customized framework to forecast the passenger pickup/dropoff demands in traffic prediction and show its superior performance. Our code and data are available at {\color{blue}\url{https://github.com/liulingbo918/ATFM}}.",0
"With the increasing availability of vast traffic mobility data, traffic flow prediction has become a vital element in intelligent transportation systems and has attracted extensive research interest in the field of artificial intelligence (AI). The primary challenge in this field is integrating various factors such as temporal rules and spatial dependencies to infer traffic flow's evolution trend. To address this issue, we propose the Attentive Traffic Flow Machine (ATFM), a unified neural network that uses an attention mechanism to effectively learn the spatial-temporal feature representations of traffic flow. Our ATFM comprises two progressive Convolutional Long Short-Term Memory (ConvLSTM) units connected to a convolutional layer. The first unit takes normal traffic flow features as input and generates a hidden state at each time-step, which is then fed into the connected convolutional layer for spatial attention map inference. The second unit aims to learn dynamic spatial-temporal features from the attentionally weighted traffic flow features. We have developed two deep learning frameworks based on ATFM that adaptively incorporate sequential and periodic data and other external influences to predict citywide short-term/long-term traffic flow. The proposed method's superiority has been demonstrated through extensive experiments on two standard benchmarks. Furthermore, we have applied the customized framework to forecast passenger pickup/dropoff demands, and our method has shown superior performance. Our code and data are available at {\color{blue}\url{https://github.com/liulingbo918/ATFM}}.",1
"Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch.",0
"Deep learning models have made significant progress in video person re-identification (Re-ID) in recent years. A crucial aspect of Re-ID is constructing effective and robust video feature representations for complex scenarios. Part-based methods use spatial and temporal attention to extract local features that are representative. However, these methods neglect correlations between different parts. To address this, we propose an innovative adaptive graph representation learning scheme for video person Re-ID. This scheme enables contextual interactions between relevant regional features by exploiting pose alignment and feature affinity connections to construct an adaptive structure-aware adjacency graph. We refine regional features iteratively by performing feature propagation on the adjacency graph, taking into account neighbor nodes' information for part feature representation. To learn compact and discriminative representations, we introduce a novel temporal resolution-aware regularization that enforces consistency among different temporal resolutions for the same identities. We evaluate our proposed method extensively on four benchmarks, including iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID. The experimental results demonstrate the effectiveness of our approach, achieving competitive performance. The code is available at https://github.com/weleen/AGRL.pytorch.",1
"In real-world classification problems, pairwise supervision (i.e., a pair of patterns with a binary label indicating whether they belong to the same class or not) can often be obtained at a lower cost than ordinary class labels. Similarity learning is a general framework to utilize such pairwise supervision to elicit useful representations by inferring the relationship between two data points, which encompasses various important preprocessing tasks such as metric learning, kernel learning, graph embedding, and contrastive representation learning. Although elicited representations are expected to perform well in downstream tasks such as classification, little theoretical insight has been given in the literature so far. In this paper, we reveal that a specific formulation of similarity learning is strongly related to the objective of binary classification, which spurs us to learn a binary classifier without ordinary class labels---by fitting the product of real-valued prediction functions of pairwise patterns to their similarity. Our formulation of similarity learning does not only generalize many existing ones, but also admits an excess risk bound showing an explicit connection to classification. Finally, we empirically demonstrate the practical usefulness of the proposed method on benchmark datasets.",0
"Obtaining pairwise supervision in real-world classification problems is often less costly than obtaining ordinary class labels. The similarity learning framework uses this pairwise supervision to infer the relationship between two data points, which includes important preprocessing tasks such as metric learning, kernel learning, graph embedding, and contrastive representation learning. While these elicited representations are expected to perform well in downstream tasks like classification, there is little theoretical understanding of this process. This paper shows that a specific formulation of similarity learning is strongly related to the objective of binary classification, prompting us to learn a binary classifier without ordinary class labels. We achieve this by fitting real-valued prediction functions of pairwise patterns to their similarity. Our formulation generalizes many existing ones and admits an excess risk bound that has an explicit connection to classification. Finally, we demonstrate the practical usefulness of our proposed method on benchmark datasets.",1
"The recent GRAPH-BERT model introduces a new approach to learning graph representations merely based on the attention mechanism. GRAPH-BERT provides an opportunity for transferring pre-trained models and learned graph representations across different tasks within the same graph dataset. In this paper, we will further investigate the graph-to-graph transfer of a universal GRAPH-BERT for graph representation learning across different graph datasets, and our proposed model is also referred to as the G5 for simplicity. Many challenges exist in learning G5 to adapt the distinct input and output configurations for each graph data source, as well as the information distributions differences. G5 introduces a pluggable model architecture: (a) each data source will be pre-processed with a unique input representation learning component; (b) each output application task will also have a specific functional component; and (c) all such diverse input and output components will all be conjuncted with a universal GRAPH-BERT core component via an input size unification layer and an output representation fusion layer, respectively.   The G5 model removes the last obstacle for cross-graph representation learning and transfer. For the graph sources with very sparse training data, the G5 model pre-trained on other graphs can still be utilized for representation learning with necessary fine-tuning. What's more, the architecture of G5 also allows us to learn a supervised functional classifier for data sources without any training data at all. Such a problem is also named as the Apocalypse Learning task in this paper. Two different label reasoning strategies, i.e., Cross-Source Classification Consistency Maximization (CCCM) and Cross-Source Dynamic Routing (CDR), are introduced in this paper to address the problem.",0
"The attention-based approach of the GRAPH-BERT model has revolutionized graph representation learning. It enables pre-trained models and learned graph representations to be transferred across various tasks in the same graph dataset. The present study delves further into the graph-to-graph transfer of a universal GRAPH-BERT, which we refer to as G5. However, adapting G5 to different input and output configurations and information distribution differences across distinct graph data sources presents numerous challenges. To overcome these challenges, we propose a pluggable model architecture for G5 that includes unique input representation learning components and specific functional components for each data source and output application task, respectively. All these diverse input and output components are integrated with a universal GRAPH-BERT core component through an input size unification layer and an output representation fusion layer. The G5 model resolves the last hurdle for cross-graph representation learning and transfer and can be utilized for representation learning even in graph sources with sparse training data. Additionally, the G5 architecture allows us to learn a supervised functional classifier for data sources without any training data, which is also referred to as the Apocalypse Learning task in this study. To address this issue, we introduce two label reasoning strategies: Cross-Source Classification Consistency Maximization (CCCM) and Cross-Source Dynamic Routing (CDR).",1
"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",0
"Using a weighted combination of hard targets and the uniform distribution over labels as soft targets can enhance the generalization and learning speed of a multi-class neural network. This technique, known as label smoothing, has been widely applied in various state-of-the-art models such as image classification, language translation, and speech recognition. Although label smoothing is commonly used, its mechanism is not yet fully understood. In this study, we demonstrate that label smoothing not only enhances generalization but also improves model calibration, leading to significant improvement in beam-search. However, when a teacher network is trained with label smoothing, knowledge distillation into a student network becomes less effective. Our visualization of the penultimate layer's representations reveals that label smoothing causes training examples from the same class to form tight clusters, resulting in a loss of information about the similarities between instances of different classes. This information is crucial for distillation but does not affect the generalization or calibration of the model's predictions.",1
"This paper tackles face recognition in videos employing metric learning methods and similarity ranking models. The paper compares the use of the Siamese network with contrastive loss and Triplet Network with triplet loss implementing the following architectures: Google/Inception architecture, 3D Convolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent Neural Network. We make use of still images and sequences from videos for training the networks and compare the performances implementing the above architectures. The dataset used was the YouTube Face Database designed for investigating the problem of face recognition in videos. The contribution of this paper is two-fold: to begin, the experiments have established 3-D Convolutional networks and 2-D LSTMs with the contrastive loss on image sequences do not outperform Google/Inception architecture with contrastive loss in top $n$ rank face retrievals with still images. However, the 3-D Convolution networks and 2-D LSTM with triplet Loss outperform the Google/Inception with triplet loss in top $n$ rank face retrievals on the dataset; second, a Support Vector Machine (SVM) was used in conjunction with the CNNs' learned feature representations for facial identification. The results show that feature representation learned with triplet loss is significantly better for n-shot facial identification compared to contrastive loss. The most useful feature representations for facial identification are from the 2-D LSTM with triplet loss. The experiments show that learning spatio-temporal features from video sequences is beneficial for facial recognition in videos.",0
"The focus of this research paper is on the recognition of faces in videos by utilizing metric learning methods and similarity ranking models. The paper aims to compare the efficacy of Siamese network with contrastive loss and Triplet Network with triplet loss by implementing Google/Inception architecture, 3D Convolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent Neural Network. The training of networks was done using still images and sequences from videos, and the performances of the above architectures were compared using the YouTube Face Database. The paper makes a two-fold contribution: firstly, the experiments show that 3-D Convolutional networks and 2-D LSTMs with the contrastive loss on image sequences do not perform better than Google/Inception architecture with contrastive loss in top $n$ rank face retrievals with still images. However, 3-D Convolution networks and 2-D LSTM with triplet Loss outperform Google/Inception with triplet loss in top $n$ rank face retrievals on the dataset. Secondly, a Support Vector Machine (SVM) was integrated with the CNNs' learned feature representations for facial identification. The results indicate that feature representation learned with triplet loss is significantly better for n-shot facial identification compared to contrastive loss. The 2-D LSTM with triplet loss provides the most useful feature representations for facial identification. The experiments demonstrate that learning spatio-temporal features from video sequences is advantageous for facial recognition in videos.",1
"We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl",0
"We propose a novel method for acquiring node and graph level representations using self-supervised learning by comparing structural perspectives of graphs. Our research demonstrates that, unlike visual representation learning, the use of more than two perspectives or contrasting multi-scale encodings does not enhance performance. Instead, the best results are achieved by contrasting encodings from first-order neighbors and a graph diffusion. Our self-supervised approach attains new state-of-the-art outcomes in learning on all eight node and graph classification benchmarks under the linear evaluation protocol. For instance, our approach achieves accuracies of 86.8% and 84.5% on the Cora (node) and Reddit-Binary (graph) classification benchmarks, respectively, which represent a 5.5% and 2.4% improvement compared to the previous state-of-the-art. Moreover, our approach outperforms supervised baselines in four out of eight benchmarks. The source code for our approach is available at: https://github.com/kavehhassani/mvgrl.",1
"Canonical Correlation Analysis (CCA) is a statistical technique used to extract common information from multiple data sources or views. It has been used in various representation learning problems, such as dimensionality reduction, word embedding, and clustering. Recent work has given CCA probabilistic footing in a deep learning context and uses a variational lower bound for the data log likelihood to estimate model parameters. Alternatively, adversarial techniques have arisen in recent years as a powerful alternative to variational Bayesian methods in autoencoders. In this work, we explore straightforward adversarial alternatives to recent work in Deep Variational CCA (VCCA and VCCA-Private) we call ACCA and ACCA-Private and show how these approaches offer a stronger and more flexible way to match the approximate posteriors coming from encoders to much larger classes of priors than the VCCA and VCCA-Private models. This allows new priors for what constitutes a good representation, such as disentangling underlying factors of variation, to be more directly pursued. We offer further analysis on the multi-level disentangling properties of VCCA-Private and ACCA-Private through the use of a newly designed dataset we call Tangled MNIST. We also design a validation criteria for these models that is theoretically grounded, task-agnostic, and works well in practice. Lastly, we fill a minor research gap by deriving an additional variational lower bound for VCCA that allows the representation to use view-specific information from both input views.",0
"Canonical Correlation Analysis (CCA) is a statistical method that extracts shared information from multiple data sources or perspectives. CCA has been utilized in various representation learning issues, such as clustering, word embedding, and dimensionality reduction. Recent research has given CCA a probabilistic foundation in the context of deep learning by using a variational lower bound to estimate model parameters for data log likelihood. In contrast, adversarial methods have emerged as a potent substitute for variational Bayesian techniques in autoencoders. This study examines straightforward adversarial alternatives to recent Deep Variational CCA (VCCA and VCCA-Private) work, referred to as ACCA and ACCA-Private. We demonstrate that these methods provide a more robust and flexible approach to matching encoder-generated approximate posteriors to a broader range of priors than the VCCA and VCCA-Private models, enabling the pursuit of new priors for what constitutes a good representation. We further investigate the multi-level disentangling properties of VCCA-Private and ACCA-Private using a new dataset called Tangled MNIST and propose a theoretically grounded and task-agnostic validation criterion for these models. Finally, we address a minor research gap by deriving an additional variational lower bound for VCCA that allows the representation to utilize view-specific information from both input views.",1
"Disentangled representation learning has recently attracted a significant amount of attention, particularly in the field of image representation learning. However, learning the disentangled representations behind a graph remains largely unexplored, especially for the attributed graph with both node and edge features. Disentanglement learning for graph generation has substantial new challenges including 1) the lack of graph deconvolution operations to jointly decode node and edge attributes; and 2) the difficulty in enforcing the disentanglement among latent factors that respectively influence: i) only nodes, ii) only edges, and iii) joint patterns between them. To address these challenges, we propose a new disentanglement enhancement framework for deep generative models for attributed graphs. In particular, a novel variational objective is proposed to disentangle the above three types of latent factors, with novel architecture for node and edge deconvolutions. Moreover, within each type, individual-factor-wise disentanglement is further enhanced, which is shown to be a generalization of the existing framework for images. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed model and its extensions.",0
"The concept of disentangled representation learning has gained a lot of attention in recent times, particularly in image representation learning. However, the exploration of learning disentangled representations in graphs, especially those with both node and edge features, remains largely underdeveloped. Disentanglement learning for graph generation presents significant new challenges, such as the lack of graph deconvolution operations to jointly decode node and edge attributes and the difficulty in enforcing disentanglement among latent factors that affect only nodes, only edges, and joint patterns between them. To address these challenges, we propose a disentanglement enhancement framework for deep generative models for attributed graphs. Our approach involves a novel variational objective to disentangle the three types of latent factors mentioned above, which is supported by a new architecture for node and edge deconvolutions. Furthermore, we enhance individual-factor-wise disentanglement within each type, which is a generalization of the existing image framework. Our proposed model and its extensions are evaluated using qualitative and quantitative experiments on both real-world and synthetic datasets, demonstrating their effectiveness.",1
"Generating interpretable visualizations from complex data is a common problem in many applications. Two key ingredients for tackling this issue are clustering and representation learning. However, current methods do not yet successfully combine the strengths of these two approaches. Existing representation learning models which rely on latent topological structure such as self-organising maps, exhibit markedly lower clustering performance compared to recent deep clustering methods. To close this performance gap, we (a) present a novel way to fit self-organizing maps with probabilistic cluster assignments (PSOM), (b) propose a new deep architecture for probabilistic clustering (DPSOM) using a VAE, and (c) extend our architecture for time-series clustering (T-DPSOM), which also allows forecasting in the latent space using LSTMs. We show that DPSOM achieves superior clustering performance compared to current deep clustering methods on MNIST/Fashion-MNIST, while maintaining the favourable visualization properties of SOMs. On medical time series, we show that T-DPSOM outperforms baseline methods in time series clustering and time series forecasting, while providing interpretable visualizations of patient state trajectories and uncertainty estimation.",0
"The task of creating understandable visual representations from complex data is a common challenge across various fields. To address this issue, clustering and representation learning are two key components that have been utilized. However, current methodologies have not successfully merged the strengths of these approaches. Existing representation learning models, which rely on latent topological structures such as self-organizing maps, demonstrate lower clustering performance compared to recent deep clustering techniques. To bridge this performance gap, we introduce a new approach, namely (a) a novel method to fit self-organizing maps using probabilistic cluster assignments (PSOM), (b) a new deep architecture for probabilistic clustering (DPSOM) that integrates a VAE, and (c) an extension of our architecture for time-series clustering (T-DPSOM) that includes forecasting in the latent space using LSTMs. We demonstrate that DPSOM outperforms current deep clustering methods on MNIST/Fashion-MNIST, while also retaining the favorable visualization characteristics of SOMs. Moreover, we illustrate that T-DPSOM surpasses baseline methods in time-series clustering and forecasting for medical data, while providing interpretable visualizations of patient state trajectories and uncertainty estimation.",1
"We present a novel architecture named Neural Physicist (NeurPhy) to learn physical dynamics directly from image sequences using deep neural networks. For any physical system, given the global system parameters, the time evolution of states is governed by the underlying physical laws. How to learn meaningful system representations in an end-to-end way and estimate accurate state transition dynamics facilitating long-term prediction have been long-standing challenges. In this paper, by leveraging recent progresses in representation learning and state space models (SSMs), we propose NeurPhy, which uses variational auto-encoder (VAE) to extract underlying Markovian dynamic state at each time step, neural process (NP) to extract the global system parameters, and a non-linear non-recurrent stochastic state space model to learn the physical dynamic transition. We apply NeurPhy to two physical experimental environments, i.e., damped pendulum and planetary orbits motion, and achieve promising results. Our model can not only extract the physically meaningful state representations, but also learn the state transition dynamics enabling long-term predictions for unseen image sequences. Furthermore, from the manifold dimension of the latent state space, we can easily identify the degree of freedom (DoF) of the underlying physical systems.",0
"The article introduces a new architecture called Neural Physicist (NeurPhy) that employs deep neural networks to learn physical dynamics from image sequences. The challenges of learning meaningful system representations and accurate state transition dynamics have been long-standing issues, especially for end-to-end learning. The paper proposes NeurPhy, which uses variational auto-encoder (VAE), neural process (NP), and a non-linear non-recurrent stochastic state space model to solve these problems. The model was tested on two physical experimental environments and achieved promising results. The NeurPhy model is capable of extracting physically meaningful state representations and predicting state transitions for unseen image sequences. Additionally, the model can identify the degree of freedom (DoF) of the underlying physical systems based on the manifold dimension of the latent state space.",1
"Variational autoencoders (VAEs) are a powerful class of deep generative latent variable model for unsupervised representation learning on high-dimensional data. To ensure computational tractability, VAEs are often implemented with a univariate standard Gaussian prior and a mean-field Gaussian variational posterior distribution. This results in a vector-valued latent variables that are agnostic to the original data structure which might be highly correlated across and within multiple dimensions. We propose a tensor-variate extension to the VAE framework, the tensor-variate Gaussian process prior variational autoencoder (tvGP-VAE), which replaces the standard univariate Gaussian prior and posterior distributions with tensor-variate Gaussian processes. The tvGP-VAE is able to explicitly model correlation structures via the use of kernel functions over the dimensions of tensor-valued latent variables. Using spatiotemporally correlated image time series as an example, we show that the choice of which correlation structures to explicitly represent in the latent space has a significant impact on model performance in terms of reconstruction.",0
"The article discusses Variational autoencoders (VAEs) which are a type of deep generative latent variable model used for unsupervised representation learning for high-dimensional data. VAEs are generally implemented with a standard Gaussian prior and a Gaussian variational posterior distribution, resulting in vector-valued latent variables that do not consider the original data structure. To address this limitation, the authors propose a tensor-variate extension to the VAE framework called the tensor-variate Gaussian process prior variational autoencoder (tvGP-VAE). The tvGP-VAE uses tensor-variate Gaussian processes to replace the standard Gaussian prior and posterior distributions, allowing for explicit modeling of correlation structures through kernel functions over the dimensions of tensor-valued latent variables. The authors demonstrate the effectiveness of their approach by applying it to spatiotemporally correlated image time series and showing that the choice of which correlation structures to represent in the latent space significantly affects the model's reconstruction performance.",1
"Deep representation learning on non-Euclidean data types, such as graphs, has gained significant attention in recent years. Invent of graph neural networks has improved the state-of-the-art for both node and the entire graph representation in a vector space. However, for the entire graph representation, most of the existing graph neural networks are trained on a graph classification loss in a supervised way. But obtaining labels of a large number of graphs is expensive for real world applications. Thus, we aim to propose an unsupervised graph neural network to generate a vector representation of an entire graph in this paper. For this purpose, we combine the idea of hierarchical graph neural networks and mutual information maximization into a single framework. We also propose and use the concept of periphery representation of a graph and show its usefulness in the proposed algorithm which is referred as GraPHmax. We conduct thorough experiments on several real-world graph datasets and compare the performance of GraPHmax with a diverse set of both supervised and unsupervised baseline algorithms. Experimental results show that we are able to improve the state-of-the-art for multiple graph level tasks on several real-world datasets, while remain competitive on the others.",0
"In recent years, there has been a significant focus on deep representation learning for non-Euclidean data types, specifically graphs. Graph neural networks have been introduced and have improved the state-of-the-art for node and entire graph representation in a vector space. However, existing graph neural networks for entire graph representation are mostly trained in a supervised way using graph classification loss, which is expensive for real-world applications due to the need for a large number of graph labels. To address this, we propose an unsupervised graph neural network called GraPHmax that generates a vector representation of an entire graph. Our approach combines hierarchical graph neural networks and mutual information maximization into a single framework, and introduces the concept of periphery representation of a graph. We conduct thorough experiments on various real-world graph datasets and compare the performance of GraPHmax with both supervised and unsupervised baseline algorithms. Our results demonstrate that GraPHmax improves the state-of-the-art for multiple graph level tasks on several real-world datasets and remains competitive on others.",1
"Graph Attention Network (GAT) and GraphSAGE are neural network architectures that operate on graph-structured data and have been widely studied for link prediction and node classification. One challenge raised by GraphSAGE is how to smartly combine neighbour features based on graph structure. GAT handles this problem through attention, however the challenge with GAT is its scalability over large and dense graphs. In this work, we proposed a new architecture to address these issues that is more efficient and is capable of incorporating different edge type information. It generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings. Experiments achieved comparable or better results on several graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",0
"Neural network architectures, such as Graph Attention Network (GAT) and GraphSAGE, have gained popularity for their ability to operate on graph-structured data and perform link prediction and node classification. GraphSAGE faces the challenge of combining neighbour features based on graph structure, while GAT tackles this issue through attention. However, scaling GAT over large and dense graphs poses a challenge. To address these problems, we propose a novel architecture that is more efficient and can incorporate different edge type information. Our approach generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We evaluate our method in both transductive and inductive settings and obtain comparable or better results compared to several graph benchmarks, including Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",1
"The remarkable success of machine learning has fostered a growing number of cloud-based intelligent services for mobile users. Such a service requires a user to send data, e.g. image, voice and video, to the provider, which presents a serious challenge to user privacy. To address this, prior works either obfuscate the data, e.g. add noise and remove identity information, or send representations extracted from the data, e.g. anonymized features. They struggle to balance between the service utility and data privacy because obfuscated data reduces utility and extracted representation may still reveal sensitive information.   This work departs from prior works in methodology: we leverage adversarial learning to a better balance between privacy and utility. We design a \textit{representation encoder} that generates the feature representations to optimize against the privacy disclosure risk of sensitive information (a measure of privacy) by the \textit{privacy adversaries}, and concurrently optimize with the task inference accuracy (a measure of utility) by the \textit{utility discriminator}. The result is the privacy adversarial network (\systemname), a novel deep model with the new training algorithm, that can automatically learn representations from the raw data.   Intuitively, PAN adversarially forces the extracted representations to only convey the information required by the target task. Surprisingly, this constitutes an implicit regularization that actually improves task accuracy. As a result, PAN achieves better utility and better privacy at the same time! We report extensive experiments on six popular datasets and demonstrate the superiority of \systemname compared with alternative methods reported in prior work.",0
"The success of machine learning has led to an increase in cloud-based intelligent services for mobile users. However, these services require users to send data, such as images, voice recordings, and videos, to the provider, which poses a challenge to user privacy. Previous approaches to address this issue have either obscured the data by adding noise or removing identity information, or sent anonymized features extracted from the data. However, these methods struggle to balance between service utility and data privacy, as obscured data reduces utility and extracted features may still reveal sensitive information. This study uses adversarial learning to better balance between privacy and utility. The researchers design a representation encoder that generates feature representations to optimize against the privacy disclosure risk by privacy adversaries, while optimizing task inference accuracy by a utility discriminator. The result is the privacy adversarial network, a novel deep model that automatically learns representations from raw data. This approach forces the extracted representations to only convey the information required by the target task, improving task accuracy and achieving better utility and privacy simultaneously. Extensive experiments on six popular datasets demonstrate the superiority of this approach compared to prior work.",1
"Graph neural networks (GNNs) have attracted much attention because of their excellent performance on tasks such as node classification. However, there is inadequate understanding on how and why GNNs work, especially for node representation learning. This paper aims to provide a theoretical framework to understand GNNs, specifically, spectral graph convolutional networks and graph attention networks, from graph signal denoising perspectives. Our framework shows that GNNs are implicitly solving graph signal denoising problems: spectral graph convolutions work as denoising node features, while graph attentions work as denoising edge weights. We also show that a linear self-attention mechanism is able to compete with the state-of-the-art graph attention methods. Our theoretical results further lead to two new models, GSDN-F and GSDN-EF, which work effectively for graphs with noisy node features and/or noisy edges. We validate our theoretical findings and also the effectiveness of our new models by experiments on benchmark datasets. The source code is available at \url{https://github.com/fuguoji/GSDN}.",0
"The exceptional performance of Graph neural networks (GNNs) on node classification tasks has made them a popular topic of discussion. However, there is a lack of understanding about how and why GNNs work, especially for node representation learning. To address this issue, this paper presents a theoretical framework to comprehend GNNs, specifically spectral graph convolutional networks and graph attention networks, from the perspective of graph signal denoising. Our framework reveals that GNNs are implicitly solving graph signal denoising problems by using spectral graph convolutions to denoise node features and graph attentions to denoise edge weights. Additionally, our theoretical findings suggest that a linear self-attention mechanism is as effective as state-of-the-art graph attention methods. Furthermore, based on our theoretical results, we introduce two new models, GSDN-F and GSDN-EF, which are effective for graphs with noisy node features and/or noisy edges. We validate our theoretical findings and the effectiveness of our new models through experiments on benchmark datasets. The source code for our work is available at \url{https://github.com/fuguoji/GSDN}.",1
"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.",0
"Having a grasp on disentangled representations of natural language is crucial for various NLP tasks, like personalized dialogue systems, style transfer, and conditional text generation. While similar studies have been done for other types of data, like images and videos, the discrete nature of natural language makes it more complex to disentangle textual representations (e.g. manipulating the data space is not easily done). To tackle this, we present a new method inspired by information theory that effectively generates disentangled representations of text without supervision on semantics. We derive a novel mutual information upper bound to measure dependence between content and style, and by minimizing it, our method induces style and content embeddings into two independent low-dimensional spaces. Our experiments on conditional text generation and text-style transfer demonstrate the exceptional quality of our disentangled representation in preserving both content and style.",1
"In recent years, several unsupervised, ""contrastive"" learning algorithms in vision have been shown to learn representations that perform remarkably well on transfer tasks. We show that this family of algorithms maximizes a lower bound on the mutual information between two or more ""views"" of an image where typical views come from a composition of image augmentations. Our bound generalizes the InfoNCE objective to support negative sampling from a restricted region of ""difficult"" contrasts. We find that the choice of negative samples and views are critical to the success of these algorithms. Reformulating previous learning objectives in terms of mutual information also simplifies and stabilizes them. In practice, our new objectives yield representations that outperform those learned with previous approaches for transfer to classification, bounding box detection, instance segmentation, and keypoint detection. % experiments show that choosing more difficult negative samples results in a stronger representation, outperforming those learned with IR, LA, and CMC in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unifying comparison of approaches to contrastive learning and uncovers the choices that impact representation learning.",0
"Several unsupervised learning algorithms in vision have recently shown exceptional performance on transfer tasks. These algorithms, known as ""contrastive"" learning algorithms, aim to maximize the mutual information between multiple ""views"" of an image, which are generated through image augmentations. Our research demonstrates that this family of algorithms optimizes a lower bound on mutual information that supports negative sampling from a limited area of ""difficult"" contrasts. The selection of negative samples and views is crucial to the success of these algorithms, and our new learning objectives, based on mutual information, simplify and stabilize previous approaches. Our experiments reveal that the use of more challenging negative samples leads to stronger representations that outperform those obtained through other methods in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unified framework for comparing contrastive learning approaches and highlights the choices that affect representation learning.",1
"It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",0
"In numerous 3D computer vision applications, there has been an increasing demand for effective representation learning on point clouds. Convolutional neural networks (CNNs) have been successful in dealing with data that has a Euclidean structure, such as images, but point clouds are irregular and unordered, which poses a challenge. Point neural networks have been developed with isotropic filters or weighting matrices to address this inconsistency; however, their representation power is limited. This paper proposes a permutable anisotropic convolutional operation (PAI-Conv), which calculates soft-permutation matrices for each point using dot-product attention with kernel points on a sphere's surface and performs shared anisotropic filters. PAI-Conv can be seen as the transformer for point clouds, similar to its use in natural language processing (NLP), and is robust when combined with efficient random point sampling. The experiments conducted on point clouds show that PAI-Conv achieves competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",1
"The recent success of self-supervised learning can be largely attributed to content-preserving transformations, which can be used to easily induce invariances. While transformations generate positive sample pairs in contrastive loss training, most recent work focuses on developing new objective formulations, and pays relatively little attention to the transformations themselves. In this paper, we introduce the framework of Generalized Data Transformations to (1) reduce several recent self-supervised learning objectives to a single formulation for ease of comparison, analysis, and extension, (2) allow a choice between being invariant or distinctive to data transformations, obtaining different supervisory signals, and (3) derive the conditions that combinations of transformations must obey in order to lead to well-posed learning objectives. This framework allows both invariance and distinctiveness to be injected into representations simultaneously, and lets us systematically explore novel contrastive objectives. We apply it to study multi-modal self-supervision for audio-visual representation learning from unlabelled videos, improving the state-of-the-art by a large margin, and even surpassing supervised pretraining. We demonstrate results on a variety of downstream video and audio classification and retrieval tasks, on datasets such as HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve new state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101.",0
"The success of self-supervised learning is largely due to content-preserving transformations, which easily induce invariances and generate positive sample pairs in contrastive loss training. However, recent research has focused on developing new objective formulations rather than exploring the transformations themselves. This paper introduces the Generalized Data Transformations framework, which reduces recent self-supervised learning objectives to a single formulation for ease of comparison, analysis, and extension. It allows for a choice between invariance and distinctiveness to data transformations, resulting in different supervisory signals and deriving conditions for well-posed learning objectives. The framework enables both invariance and distinctiveness to be injected into representations simultaneously, allowing for the exploration of novel contrastive objectives. The framework is applied to multi-modal self-supervision for audio-visual representation learning from unlabelled videos, achieving state-of-the-art results and surpassing supervised pretraining on various downstream video and audio classification and retrieval tasks. New state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101 are achieved.",1
"Deep clustering algorithms combine representation learning and clustering by jointly optimizing a clustering loss and a non-clustering loss. In such methods, a deep neural network is used for representation learning together with a clustering network. Instead of following this framework to improve clustering performance, we propose a simpler approach of optimizing the entanglement of the learned latent code representation of an autoencoder. We define entanglement as how close pairs of points from the same class or structure are, relative to pairs of points from different classes or structures. To measure the entanglement of data points, we use the soft nearest neighbor loss, and expand it by introducing an annealing temperature factor. Using our proposed approach, the test clustering accuracy was 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, outperforming our baseline models.",0
"Rather than utilizing deep clustering algorithms that combine representation learning and clustering, which involves optimizing both a clustering and non-clustering loss, we suggest a simpler method to enhance clustering performance. Our approach involves optimizing the entanglement of the latent code representation learned through an autoencoder, where entanglement refers to the proximity of pairs of points from the same class or structure compared to those from different classes or structures. To assess entanglement, we apply the soft nearest neighbor loss and introduce an annealing temperature factor. Our proposed approach achieved superior results to our baseline models, with test clustering accuracy of 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset.",1
"Humans are able to create rich representations of their external reality. Their internal representations allow for cross-modality inference, where available perceptions can induce the perceptual experience of missing input modalities. In this paper, we contribute the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model for representation learning. Inspired by human cognitive models, the MHVAE is able to learn modality-specific distributions, of an arbitrary number of modalities, and a joint-modality distribution, responsible for cross-modality inference. We formally derive the model's evidence lower bound and propose a novel methodology to approximate the joint-modality posterior based on modality-specific representation dropout. We evaluate the MHVAE on standard multimodal datasets. Our model performs on par with other state-of-the-art generative models regarding joint-modality reconstruction from arbitrary input modalities and cross-modality inference.",0
"The ability of humans to create intricate representations of their external environment is due to their internal representations, which enable them to make inferences across various sensory inputs. In this research, we introduce the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical model that can learn distributions for multiple modalities as well as a joint-modality distribution for cross-modality inference. Drawing inspiration from human cognition, the MHVAE can handle an arbitrary number of modalities and uses a new technique to approximate the joint-modality posterior. We assess the MHVAE on common multimodal datasets and find that it performs comparably to other state-of-the-art generative models with respect to joint-modality reconstruction and cross-modality inference.",1
"We consider representation learning (hypothesis class $\mathcal{H} = \mathcal{F}\circ\mathcal{G}$) where training and test distributions can be different. Recent studies provide hints and failure examples for domain invariant representation learning, a common approach for this problem, but the explanations provided are somewhat different and do not provide a unified picture. In this paper, we provide new decompositions of risk which give finer-grained explanations and clarify potential generalization issues. For Single-Source Domain Adaptation, we give an exact decomposition (an equality) of the target risk, via a natural hybrid argument, as sum of three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift. We derive a similar decomposition for the Multi-Source case. These decompositions reveal factors (2) and (3) as the precise reasons for failure to generalize. For example, we demonstrate that domain adversarial neural networks (DANN) attempt to regularize for (3) but miss (2), while a recent technique Invariant Risk Minimization (IRM) attempts to account for (2) but does not consider (3). We also verify our observations experimentally.",0
"Representation learning is examined in this study, with the hypothesis class $\mathcal{H} = \mathcal{F}\circ\mathcal{G}$, in which the training and test distributions may vary. Previous research has highlighted both successful and unsuccessful domain invariant representation learning approaches, but these explanations lack a cohesive understanding. This paper introduces new risk decompositions that offer more detailed explanations and address generalization issues. For Single-Source Domain Adaptation, the target risk is broken into three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift, using a hybrid argument. A similar decomposition is derived for the Multi-Source case. These decompositions identify factors (2) and (3) as reasons for the inability to generalize. For instance, domain adversarial neural networks (DANN) aim to regulate (3) but miss (2), while Invariant Risk Minimization (IRM) considers (2) but disregards (3). Experimental results support these observations.",1
"In recent years, semi-supervised learning (SSL) has shown tremendous success in leveraging unlabeled data to improve the performance of deep learning models, which significantly reduces the demand for large amounts of labeled data. Many SSL techniques have been proposed and have shown promising performance on famous datasets such as ImageNet and CIFAR-10. However, some exiting techniques (especially data augmentation based) are not suitable for industrial applications empirically. Therefore, this work proposes the pseudo-representation labeling, a simple and flexible framework that utilizes pseudo-labeling techniques to iteratively label a small amount of unlabeled data and use them as training data. In addition, our framework is integrated with self-supervised representation learning such that the classifier gains benefits from representation learning of both labeled and unlabeled data. This framework can be implemented without being limited at the specific model structure, but a general technique to improve the existing model. Compared with the existing approaches, the pseudo-representation labeling is more intuitive and can effectively solve practical problems in the real world. Empirically, it outperforms the current state-of-the-art semi-supervised learning methods in industrial types of classification problems such as the WM-811K wafer map and the MIT-BIH Arrhythmia dataset.",0
"Semi-supervised learning has become increasingly successful in recent years by utilizing unlabeled data to enhance deep learning models, reducing the need for large amounts of labeled data. While many SSL techniques have been introduced and proven effective on well-known datasets like ImageNet and CIFAR-10, some techniques, particularly those based on data augmentation, are not practical for industrial use. This study proposes a flexible and straightforward framework called pseudo-representation labeling, which employs pseudo-labeling techniques to label a small amount of unlabeled data iteratively for use as training data. Additionally, the framework integrates self-supervised representation learning to benefit from both labeled and unlabeled data. The framework is not model-specific and serves as a general technique to enhance existing models. It is more straightforward to use and solves practical real-world issues more effectively than current methods. Empirically, it outperforms the state-of-the-art SSL approaches for industrial classification problems like the WM-811K wafer map and the MIT-BIH Arrhythmia dataset.",1
"There is an increasing interest in image-to-image translation with applications ranging from generating maps from satellite images to creating entire clothes' images from only contours. In the present work, we investigate image-to-image translation using Generative Adversarial Networks (GANs) for generating new data, taking as a case study the morphing of giraffes images into bird images. Morphing a giraffe into a bird is a challenging task, as they have different scales, textures, and morphology. An unsupervised cross-domain translator entitled InstaGAN was trained on giraffes and birds, along with their respective masks, to learn translation between both domains. A dataset of synthetic bird images was generated using translation from originally giraffe images while preserving the original spatial arrangement and background. It is important to stress that the generated birds do not exist, being only the result of a latent representation learned by InstaGAN. Two subsets of common literature datasets were used for training the GAN and generating the translated images: COCO and Caltech-UCSD Birds 200-2011. To evaluate the realness and quality of the generated images and masks, qualitative and quantitative analyses were made. For the quantitative analysis, a pre-trained Mask R-CNN was used for the detection and segmentation of birds on Pascal VOC, Caltech-UCSD Birds 200-2011, and our new dataset entitled FakeSet. The generated dataset achieved detection and segmentation results close to the real datasets, suggesting that the generated images are realistic enough to be detected and segmented by a state-of-the-art deep neural network.",0
"The use of image-to-image translation has become increasingly popular in a variety of applications, ranging from creating satellite maps to generating full clothing images from mere outlines. In this study, we explore the use of Generative Adversarial Networks (GANs) for image-to-image translation, focusing on the challenging task of morphing giraffe images into bird images. Due to their differing scales, textures, and morphology, this task presents a significant challenge. To overcome this, we trained an unsupervised cross-domain translator called InstaGAN on giraffes and birds, as well as their corresponding masks, to learn how to translate between the two domains. We generated a set of synthetic bird images by translating giraffe images while preserving the original spatial arrangement and background. Notably, these generated bird images do not actually exist and are only the result of the latent representation learned by InstaGAN. We used two subsets of common literature datasets, COCO and Caltech-UCSD Birds 200-2011, for training the GAN and generating translated images. We conducted qualitative and quantitative analyses to evaluate the realism and quality of the generated images and masks. For the quantitative analysis, we used a pre-trained Mask R-CNN to detect and segment birds on Pascal VOC, Caltech-UCSD Birds 200-2011, and our new dataset, FakeSet. Our generated dataset achieved detection and segmentation results that were comparable to those of the real datasets, suggesting that the generated images are realistic enough to be detected and segmented by a state-of-the-art deep neural network.",1
"Geometric representation learning has recently shown great promise in several machine learning settings, ranging from relational learning to language processing and generative models. In this work, we consider the problem of performing manifold-valued regression onto an hyperbolic space as an intermediate component for a number of relevant machine learning applications. In particular, by formulating the problem of predicting nodes of a tree as a manifold regression task in the hyperbolic space, we propose a novel perspective on two challenging tasks: 1) hierarchical classification via label embeddings and 2) taxonomy extension of hyperbolic representations. To address the regression problem we consider previous methods as well as proposing two novel approaches that are computationally more advantageous: a parametric deep learning model that is informed by the geodesics of the target space and a non-parametric kernel-method for which we also prove excess risk bounds. Our experiments show that the strategy of leveraging the hyperbolic geometry is promising. In particular, in the taxonomy expansion setting, we find that the hyperbolic-based estimators significantly outperform methods performing regression in the ambient Euclidean space.",0
"The use of geometric representation learning has shown great potential in various machine learning applications, including relational learning, language processing, and generative models. This study focuses on the task of manifold-valued regression onto a hyperbolic space, which can be used as an intermediate component for different machine learning tasks. By framing the problem of predicting tree nodes as a manifold regression task in hyperbolic space, we introduce a new perspective on hierarchical classification via label embeddings and taxonomy extension of hyperbolic representations. We explore different methods to address the regression problem, including two novel approaches, a parametric deep learning model based on geodesics, and a non-parametric kernel-method with proven excess risk bounds. Our experiments demonstrate that leveraging hyperbolic geometry is a promising strategy, with hyperbolic-based estimators outperforming methods performing regression in ambient Euclidean space, particularly in the taxonomy expansion setting.",1
"This work analyses the impact of self-supervised pre-training on document images in the context of document image classification. While previous approaches explore the effect of self-supervision on natural images, we show that patch-based pre-training performs poorly on document images because of their different structural properties and poor intra-sample semantic information. We propose two context-aware alternatives to improve performance on the Tobacco-3482 image classification task. We also propose a novel method for self-supervision, which makes use of the inherent multi-modality of documents (image and text), which performs better than other popular self-supervised methods, including supervised ImageNet pre-training, on document image classification scenarios with a limited amount of data.",0
"The impact of self-supervised pre-training on document images for document image classification is analyzed in this work. While previous studies have examined the effects of self-supervision on natural images, this work demonstrates that patch-based pre-training is inadequate for document images due to their distinct structural characteristics and insufficient intra-sample semantic information. To enhance performance on the Tobacco-3482 image classification task, two context-aware alternatives are proposed. Additionally, a novel self-supervision approach that leverages the inherent multi-modality of documents (image and text) is proposed, and it outperforms other popular self-supervised methods, including supervised ImageNet pre-training, in limited-data document image classification scenarios.",1
"With the advent of agriculture 3.0 and 4.0, researchers are increasingly focusing on the development of innovative smart farming and precision agriculture technologies by introducing automation and robotics into the agricultural processes. Autonomous agricultural field machines have been gaining significant attention from farmers and industries to reduce costs, human workload, and required resources. Nevertheless, achieving sufficient autonomous navigation capabilities requires the simultaneous cooperation of different processes; localization, mapping, and path planning are just some of the steps that aim at providing to the machine the right set of skills to operate in semi-structured and unstructured environments. In this context, this study presents a low-cost local motion planner for autonomous navigation in vineyards based only on an RGB-D camera, low range hardware, and a dual layer control algorithm. The first algorithm exploits the disparity map and its depth representation to generate a proportional control for the robotic platform. Concurrently, a second back-up algorithm, based on representations learning and resilient to illumination variations, can take control of the machine in case of a momentaneous failure of the first block. Moreover, due to the double nature of the system, after initial training of the deep learning model with an initial dataset, the strict synergy between the two algorithms opens the possibility of exploiting new automatically labeled data, coming from the field, to extend the existing model knowledge. The machine learning algorithm has been trained and tested, using transfer learning, with acquired images during different field surveys in the North region of Italy and then optimized for on-device inference with model pruning and quantization. Finally, the overall system has been validated with a customized robot platform in the relevant environment.",0
"Researchers are increasingly focusing on developing innovative smart farming and precision agriculture technologies by introducing automation and robotics into agricultural processes, with the advent of agriculture 3.0 and 4.0. Farmers and industries are showing significant interest in autonomous agricultural field machines due to their potential to reduce costs, human workload, and required resources. However, achieving sufficient autonomous navigation capabilities requires the cooperation of different processes, such as localization, mapping, and path planning, to enable the machine to operate in semi-structured and unstructured environments. In this study, a low-cost local motion planner for autonomous navigation in vineyards is presented, which uses only an RGB-D camera, low range hardware, and a dual layer control algorithm. The first algorithm uses the disparity map and its depth representation to generate proportional control for the robotic platform, while the second back-up algorithm, based on representations learning, can take control in case of a momentary failure of the first block. The system's double nature allows for the possibility of exploiting new automatically labeled data from the field to extend the existing model knowledge. The machine learning algorithm is trained and tested using transfer learning with acquired images during different field surveys in the North region of Italy, optimized for on-device inference with model pruning and quantization, and validated with a customized robot platform in the relevant environment.",1
"Contrastive learning (CL) is an emerging analysis approach that aims to discover unique patterns in one dataset relative to another. By applying this approach to network analysis, we can reveal unique characteristics in one network by contrasting with another. For example, with networks of protein interactions obtained from normal and cancer tissues, we can discover unique types of interactions in cancer tissues. However, existing CL methods cannot be directly applied to networks. To address this issue, we introduce a novel approach called contrastive network representation learning (cNRL). This approach embeds network nodes into a low-dimensional space that reveals the uniqueness of one network compared to another. Within this approach, we also design a method, named i-cNRL, that offers interpretability in the learned results, allowing for understanding which specific patterns are found in one network but not the other. We demonstrate the capability of i-cNRL with multiple network models and real-world datasets. Furthermore, we provide quantitative and qualitative comparisons across i-cNRL and other potential cNRL algorithm designs.",0
"Emerging analysis approach, Contrastive Learning (CL), seeks to identify unique patterns in one dataset in comparison to another. When applied to network analysis, CL can expose unique characteristics in one network by contrasting it with another. For instance, when comparing networks of protein interactions from normal and cancer tissues, unique interactions in cancer tissues can be discovered. However, existing CL methods are not directly applicable to networks. To address this, we introduce a new approach, contrastive network representation learning (cNRL), which embeds network nodes into a low-dimensional space that highlights the uniqueness of one network relative to the other. Additionally, we have designed a method, i-cNRL, that offers interpretability, making it possible to understand the specific patterns identified in one network but not the other. The effectiveness of i-cNRL is demonstrated using various network models and real-world datasets. Furthermore, we provide both quantitative and qualitative comparisons between i-cNRL and other potential cNRL algorithm designs.",1
"Heterogeneous face recognition (HFR) refers to matching face images acquired from different domains with wide applications in security scenarios. This paper presents a deep neural network approach namely Multi-Margin based Decorrelation Learning (MMDL) to extract decorrelation representations in a hyperspherical space for cross-domain face images. The proposed framework can be divided into two components: heterogeneous representation network and decorrelation representation learning. First, we employ a large scale of accessible visual face images to train heterogeneous representation network. The decorrelation layer projects the output of the first component into decorrelation latent subspace and obtains decorrelation representation. In addition, we design a multi-margin loss (MML), which consists of quadruplet margin loss (QML) and heterogeneous angular margin loss (HAML), to constrain the proposed framework. Experimental results on two challenging heterogeneous face databases show that our approach achieves superior performance on both verification and recognition tasks, comparing with state-of-the-art methods.",0
"The use of Heterogeneous Face Recognition (HFR) involves matching face images obtained from various sources in security scenarios. This paper introduces a deep neural network technique called Multi-Margin based Decorrelation Learning (MMDL) that extracts decorrelation representations in a hyperspherical space for cross-domain face images. The proposed approach comprises two main components: the heterogeneous representation network and the decorrelation representation learning. Firstly, we train the heterogeneous representation network using a vast quantity of visual face images. Then, the decorrelation layer converts the output of the first component into decorrelation latent subspace to acquire decorrelation representation. Moreover, we incorporate a multi-margin loss (MML) that consists of quadruplet margin loss (QML) and heterogeneous angular margin loss (HAML) to constrain the proposed framework. The results of our experiments on two challenging heterogeneous face databases reveal that our approach performs better than state-of-the-art methods in both verification and recognition tasks.",1
"Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter-conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",0
"A promising approach to understanding the real world without incurring annotation costs is the comprehension of three-dimensional (3D) geometries from two-dimensional (2D) images without labeled information. Our new generative model, RGBD-GAN, achieves unsupervised 3D representation learning from 2D images. This method allows for camera parameter-conditional image generation and depth image generation without the need for 3D annotations, such as camera poses or depth. To ensure the consistency of two RGBD images generated from different camera parameters, we use an explicit 3D consistency loss in addition to the ordinal GAN objective. This loss is effective for any type of image generator, such as DCGAN and StyleGAN, to be conditioned on camera parameters. Our experiments confirmed that the proposed method could learn 3D representations from 2D images using various generator architectures.",1
"Recent years have witnessed the emergence and development of graph neural networks (GNNs), which have been shown as a powerful approach for graph representation learning in many tasks, such as node classification and graph classification. The research on the robustness of these models has also started to attract attentions in the machine learning field. However, most of the existing work in this area focus on the GNNs for node-level tasks, while little work has been done to study the robustness of the GNNs for the graph classification task. In this paper, we aim to explore the vulnerability of the Hierarchical Graph Pooling (HGP) Neural Networks, which are advanced GNNs that perform very well in the graph classification in terms of prediction accuracy. We propose an adversarial attack framework for this task. Specifically, we design a surrogate model that consists of convolutional and pooling operators to generate adversarial samples to fool the hierarchical GNN-based graph classification models. We set the preserved nodes by the pooling operator as our attack targets, and then we perturb the attack targets slightly to fool the pooling operator in hierarchical GNNs so that they will select the wrong nodes to preserve. We show the adversarial samples generated from multiple datasets by our surrogate model have enough transferability to attack current state-of-art graph classification models. Furthermore, we conduct the robust train on the target models and demonstrate that the retrained graph classification models are able to better defend against the attack from the adversarial samples. To the best of our knowledge, this is the first work on the adversarial attack against hierarchical GNN-based graph classification models.",0
"Graph neural networks (GNNs) have emerged as a powerful method for graph representation learning in recent years, particularly for tasks like node and graph classification. Despite this, research on the robustness of these models has only recently garnered attention in the field of machine learning. While existing work on GNNs has focused primarily on node-level tasks, there has been little exploration of their robustness for graph classification tasks. In this paper, we seek to address this gap by examining the vulnerability of Hierarchical Graph Pooling (HGP) Neural Networks, an advanced GNN model that performs well in graph classification. To achieve this, we propose an adversarial attack framework that leverages a surrogate model consisting of convolutional and pooling operators to generate adversarial samples that can fool the hierarchical GNN-based graph classification models. By perturbing the attack targets slightly, we aim to trick the pooling operator in the hierarchical GNNs into selecting the wrong nodes to preserve. Our experiments demonstrate that the adversarial samples generated by our surrogate model can successfully attack current state-of-the-art graph classification models, and that our retrained models are better able to defend against such attacks. This is the first work to explore adversarial attacks on hierarchical GNN-based graph classification models.",1
"We address the challenging problem of RGB image-based head pose estimation. We first reformulate head pose representation learning to constrain it to a bounded space. Head pose represented as vector projection or vector angles shows helpful to improving performance. Further, a ranking loss combined with MSE regression loss is proposed. The ranking loss supervises a neural network with paired samples of the same person and penalises incorrect ordering of pose prediction. Analysis on this new loss function suggests it contributes to a better local feature extractor, where features are generalised to Abstract Landmarks which are pose-related features instead of pose-irrelevant information such as identity, age, and lighting. Extensive experiments show that our method significantly outperforms the current state-of-the-art schemes on public datasets: AFLW2000 and BIWI. Our model achieves significant improvements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 to 3.66 and from 4.0 to 3.71 respectively. Source code will be made available at: https://github.com/seathiefwang/RankHeadPose.",0
"We are tackling the complex task of estimating head pose based on RGB images. To improve performance, we have redefined how head pose is represented and learned, constraining it to a limited space. Vector projection or angles are effective in this regard. Additionally, we have introduced a ranking loss combined with MSE regression loss, which evaluates paired samples of the same person and punishes inaccurate pose prediction ordering. Our analysis indicates that this approach enhances feature extraction by generalizing features to Abstract Landmarks, which are pose-related rather than identity, age, or lighting. Our method has been tested extensively and has outperformed existing state-of-the-art solutions on public datasets such as AFLW2000 and BIWI. Our model has improved the previous SOTA MAE on AFLW2000 and BIWI from 4.50 to 3.66 and from 4.0 to 3.71 respectively. To access our source code, please visit: https://github.com/seathiefwang/RankHeadPose.",1
"In many machine learning tasks, learning a good representation of the data can be the key to building a well-performant solution. This is because most learning algorithms operate with the features in order to find models for the data. For instance, classification performance can improve if the data is mapped to a space where classes are easily separated, and regression can be facilitated by finding a manifold of data in the feature space. As a general rule, features are transformed by means of statistical methods such as principal component analysis, or manifold learning techniques such as Isomap or locally linear embedding. From a plethora of representation learning methods, one of the most versatile tools is the autoencoder. In this paper we aim to demonstrate how to influence its learned representations to achieve the desired learning behavior. To this end, we present a series of learning tasks: data embedding for visualization, image denoising, semantic hashing, detection of abnormal behaviors and instance generation. We model them from the representation learning perspective, following the state of the art methodologies in each field. A solution is proposed for each task employing autoencoders as the only learning method. The theoretical developments are put into practice using a selection of datasets for the different problems and implementing each solution, followed by a discussion of the results in each case study and a brief explanation of other six learning applications. We also explore the current challenges and approaches to explainability in the context of autoencoders. All of this helps conclude that, thanks to alterations in their structure as well as their objective function, autoencoders may be the core of a possible solution to many problems which can be modeled as a transformation of the feature space.",0
"A crucial aspect of developing an effective solution in many machine learning tasks involves acquiring a solid understanding of the data. This requires identifying a suitable representation of the data since most learning algorithms operate on the features to derive models. Creating a mapping of the data to a space where classes are easily separable can enhance classification performance, while regression can be simplified by identifying a manifold of data. Commonly, statistical methods, such as principal component analysis, or manifold learning techniques, such as Isomap or locally linear embedding, are employed to transform features. The autoencoder is among the most versatile representation learning approaches available. In this paper, we aim to demonstrate how to influence learned representations to achieve desired learning behavior, presenting a series of learning tasks, including data embedding, image denoising, semantic hashing, detection of abnormal behaviors, and instance generation. We utilize autoencoders exclusively as the learning method to address each task, employing state-of-the-art methodologies for each field. Theoretical developments are put into practice using different datasets, and each solution is discussed, followed by a brief explanation of six other learning applications. Additionally, we explore the current challenges and approaches to explainability concerning autoencoders. Overall, our study demonstrates that autoencoders may be the core of a potential solution to many problems that may be modeled as a transformation of the feature space, thanks to their structural modifications and objective function.",1
"Human-Object Interaction (HOI) detection lies at the core of action understanding. Besides 2D information such as human/object appearance and locations, 3D pose is also usually utilized in HOI learning since its view-independence. However, rough 3D body joints just carry sparse body information and are not sufficient to understand complex interactions. Thus, we need detailed 3D body shape to go further. Meanwhile, the interacted object in 3D is also not fully studied in HOI learning. In light of these, we propose a detailed 2D-3D joint representation learning method. First, we utilize the single-view human body capture method to obtain detailed 3D body, face and hand shapes. Next, we estimate the 3D object location and size with reference to the 2D human-object spatial configuration and object category priors. Finally, a joint learning framework and cross-modal consistency tasks are proposed to learn the joint HOI representation. To better evaluate the 2D ambiguity processing capacity of models, we propose a new benchmark named Ambiguous-HOI consisting of hard ambiguous images. Extensive experiments in large-scale HOI benchmark and Ambiguous-HOI show impressive effectiveness of our method. Code and data are available at https://github.com/DirtyHarryLYL/DJ-RN.",0
"The detection of Human-Object Interaction (HOI) is crucial in understanding actions. In addition to 2D information such as human/object appearance and locations, 3D pose is often used in HOI learning because of its view-independence. However, 3D body joints only provide limited information and are insufficient in comprehending complex interactions. Hence, a detailed 3D body shape is necessary. Moreover, the interacted object in 3D is not fully explored in HOI learning. To address these issues, we propose a method for learning a detailed 2D-3D joint representation. Firstly, we use a single-view human body capture method to obtain a detailed 3D body, face, and hand shapes. Secondly, we estimate the 3D object location and size based on the 2D human-object spatial configuration and object category priors. Finally, we propose a joint learning framework and cross-modal consistency tasks to learn the joint HOI representation. We also introduce a new benchmark called Ambiguous-HOI to evaluate the 2D ambiguity processing capabilities of models on hard ambiguous images. Extensive experiments on large-scale HOI benchmark and Ambiguous-HOI demonstrate the impressive effectiveness of our method. The code and data are available at https://github.com/DirtyHarryLYL/DJ-RN.",1
"Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.",0
"Firms can gain valuable insights on cannibalization and product line optimization by analyzing competition and market structure at the product level rather than the brand level. However, analyzing product-level competition for the millions of products available on e-commerce platforms poses a computational challenge. To address this issue, we introduce Product2Vec, a method that utilizes the representation learning algorithm Word2Vec to study product-level competition. Product2Vec generates low-dimensional embeddings for every product that retain important product information. To make these embeddings useful for firm strategic decision making, we propose two modifications to Word2Vec. First, we create complementarity and exchangeability measures to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand and estimate price elasticities. To ensure accurate price elasticity estimates, we remove the influence of price from the product vectors. Our approach is faster and produces more accurate demand forecasts and price elasticities compared to state-of-the-art models.",1
"Previous researches of sketches often considered sketches in pixel format and leveraged CNN based models in the sketch understanding. Fundamentally, a sketch is stored as a sequence of data points, a vector format representation, rather than the photo-realistic image of pixels. SketchRNN studied a generative neural representation for sketches of vector format by Long Short Term Memory networks (LSTM). Unfortunately, the representation learned by SketchRNN is primarily for the generation tasks, rather than the other tasks of recognition and retrieval of sketches. To this end and inspired by the recent BERT model, we present a model of learning Sketch Bidirectional Encoder Representation from Transformer (Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed components and pre-training algorithms, including the newly designed sketch embedding networks, and the self-supervised learning of sketch gestalt. Particularly, towards the pre-training task, we present a novel Sketch Gestalt Model (SGM) to help train the Sketch-BERT. Experimentally, we show that the learned representation of Sketch-BERT can help and improve the performance of the downstream tasks of sketch recognition, sketch retrieval, and sketch gestalt.",0
"Previous research on sketches has primarily focused on pixel-based sketches and employed CNN-based models to understand them. However, sketches are fundamentally stored as a sequence of data points in vector format, rather than as photo-realistic pixel images. SketchRNN used Long Short Term Memory networks (LSTM) to study a generative neural representation for vector format sketches. Unfortunately, the representation learned by SketchRNN is mainly geared towards generation tasks and not recognition or retrieval of sketches. To address this, we propose a Sketch Bidirectional Encoder Representation from Transformer (Sketch-BERT) model inspired by the recent BERT model. We generalize BERT to the sketch domain using novel components and pre-training algorithms, including newly designed sketch embedding networks and self-supervised learning of sketch gestalt. We introduce a novel Sketch Gestalt Model (SGM) to aid in the pre-training task of Sketch-BERT. Our experiments demonstrate that the learned representation of Sketch-BERT can enhance the performance of downstream tasks such as sketch recognition, sketch retrieval, and sketch gestalt.",1
"PredNet, a deep predictive coding network developed by Lotter et al., combines a biologically inspired architecture based on the propagation of prediction error with self-supervised representation learning in video. While the architecture has drawn a lot of attention and various extensions of the model exist, there is a lack of a critical analysis. We fill in the gap by evaluating PredNet both as an implementation of the predictive coding theory and as a self-supervised video prediction model using a challenging video action classification dataset. We design an extended model to test if conditioning future frame predictions on the action class of the video improves the model performance. We show that PredNet does not yet completely follow the principles of predictive coding. The proposed top-down conditioning leads to a performance gain on synthetic data, but does not scale up to the more complex real-world action classification dataset. Our analysis is aimed at guiding future research on similar architectures based on the predictive coding theory.",0
"Lotter et al. developed a deep predictive coding network called PredNet, which uses a biologically inspired architecture that propagates prediction error and self-supervised representation learning in video. Although the model has gained attention and has various extensions, there is a lack of critical analysis. This study aims to fill this gap by evaluating PredNet's implementation of the predictive coding theory and self-supervised video prediction model using a challenging video action classification dataset. An extended model is designed to test if conditioning future frame predictions on the action class of the video improves model performance. Results show that PredNet does not fully adhere to the principles of predictive coding. While the proposed top-down conditioning improves performance on synthetic data, it does not scale up to the more complex real-world action classification dataset. This analysis aims to guide future research on similar architectures based on the predictive coding theory.",1
"In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial representations of speech, applicable to unsupervised voice conversion and reaching state-of-the-art performance on unit discovery tasks. For unsupervised representation learning, they became viable alternatives to continuous latent variable models such as the Variational Auto-Encoder (VAE). However, training deep discrete variable models is challenging, due to the inherent non-differentiability of the discretization operation. In this paper we focus on VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par with its continuous counterparts. It quantizes encoder outputs with on-line $k$-means clustering. We show that the codebook learning can suffer from poor initialization and non-stationarity of clustered encoder outputs. We demonstrate that these can be successfully overcome by increasing the learning rate for the codebook and periodic date-dependent codeword re-initialization. As a result, we achieve more robust training across different tasks, and significantly increase the usage of latent codewords even for large codebooks. This has practical benefit, for instance, in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.",0
"The objective of this study is to exhibit effective techniques for training discrete representation using Vector-Quantized Variational Auto-Encoder models (VQ-VAEs) that are reliable and efficient. Discrete latent variable models have proven to learn complex speech representations that are relevant to unsupervised voice conversion and unit discovery tasks, surpassing state-of-the-art performance. In unsupervised representation learning, they are considered as viable substitutes for continuous latent variable models like the Variational Auto-Encoder (VAE). Nonetheless, training deep discrete variable models is difficult due to the non-differentiability of the discretization operation. This paper focuses on VQ-VAE, which is a state-of-the-art discrete bottleneck model that performs just as well as its continuous counterparts. Encoder outputs are quantized using on-line $k$-means clustering. However, we have observed that codebook learning can be adversely impacted by poor initialization and non-stationarity of clustered encoder outputs. To address this issue, we have increased the learning rate for the codebook and introduced periodic date-dependent codeword re-initialization. This has resulted in more robust training across different tasks and has significantly increased the usage of latent codewords even for large codebooks. This is particularly useful in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.",1
"Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes",0
"Developing improved perception systems with low sample complexity is necessary to expand the abilities of robotics to complex, unstructured real-world environments. Existing methods for dealing with high-dimensional state spaces are either model-free or model-based with reconstruction objectives. However, the former is limited in its efficiency with samples, hindering its application in real-world situations. The latter has low sample complexity, but it learns latent spaces that must reconstruct all details of the scene, including unnecessary components. In this study, we introduce MIRO, an information-theoretic representational learning algorithm for model-based reinforcement learning. Our approach maximizes mutual information with future information while capturing all required information for planning. Our approach proves more robust than reconstruction objectives in scenarios with distractors and cluttered scenes.",1
"When observing a phenomenon, severe cases or anomalies are often characterised by deviation from the expected data distribution. However, non-deviating data samples may also implicitly lead to severe outcomes. In the case of unsupervised severe weather detection, these data samples can lead to mispredictions, since the predictors of severe weather are often not directly observed as features. We posit that incorporating external or auxiliary information, such as the outcome of an external task or an observation, can improve the decision boundaries of an unsupervised detection algorithm. In this paper, we increase the effectiveness of a clustering method to detect cases of severe weather by learning augmented and linearly separable latent representations.We evaluate our solution against three individual cases of severe weather, namely windstorms, floods and tornado outbreaks.",0
"When analyzing a phenomenon, deviations from the expected data distribution are often used to identify severe cases or anomalies. However, it's important to note that non-deviating data samples can also lead to severe outcomes. In unsupervised severe weather detection, this can result in mispredictions as the predictors of severe weather are not always directly observable as features. To improve the accuracy of unsupervised detection algorithms, we propose incorporating external or auxiliary information, such as an external task or observation, to enhance decision boundaries. In this research, we enhance the effectiveness of a clustering method for detecting severe weather cases by learning linearly separable latent representations through augmentation. Our approach is evaluated on three types of severe weather - windstorms, floods, and tornado outbreaks.",1
"Designing agent that can autonomously discover and learn a diversity of structures and skills in unknown changing environments is key for lifelong machine learning. A central challenge is how to learn incrementally representations in order to progressively build a map of the discovered structures and re-use it to further explore. To address this challenge, we identify and target several key functionalities. First, we aim to build lasting representations and avoid catastrophic forgetting throughout the exploration process. Secondly we aim to learn a diversity of representations allowing to discover a ""diversity of diversity"" of structures (and associated skills) in complex high-dimensional environments. Thirdly, we target representations that can structure the agent discoveries in a coarse-to-fine manner. Finally, we target the reuse of such representations to drive exploration toward an ""interesting"" type of diversity, for instance leveraging human guidance. Current approaches in state representation learning rely generally on monolithic architectures which do not enable all these functionalities. Therefore, we present a novel technique to progressively construct a Hierarchy of Observation Latent Models for Exploration Stratification, called HOLMES. This technique couples the use of a dynamic modular model architecture for representation learning with intrinsically-motivated goal exploration processes (IMGEPs). The paper shows results in the domain of automated discovery of diverse self-organized patterns, considering as testbed the experimental framework from Reinke et al. (2019).",0
"The key to lifelong machine learning is designing agents that can independently discover and learn various structures and skills in unknown and changing environments. The main challenge is to incrementally learn representations to build a map of discovered structures for future exploration. To tackle this challenge, we aim to achieve several key functionalities. Firstly, we want to create lasting representations that avoid catastrophic forgetting during exploration. Secondly, we aim to learn diverse representations to discover a range of structures and associated skills in complex environments. Thirdly, we target representations that can organize agent discoveries in a coarse-to-fine manner. Finally, we aim to reuse these representations to guide exploration towards interesting types of diversity, such as utilizing human guidance. However, existing approaches in state representation learning do not enable all these functionalities. Therefore, we introduce a new technique called HOLMES, which progressively constructs a Hierarchy of Observation Latent Models for Exploration Stratification. This technique combines a dynamic modular model architecture with intrinsically-motivated goal exploration processes (IMGEPs). The paper showcases results from using HOLMES in the automated discovery of diverse self-organized patterns, using Reinke et al.'s (2019) experimental framework as a testbed.",1
"Most video person re-identification (re-ID) methods are mainly based on supervised learning, which requires cross-camera ID labeling. Since the cost of labeling increases dramatically as the number of cameras increases, it is difficult to apply the re-identification algorithm to a large camera network. In this paper, we address the scalability issue by presenting deep representation learning without ID information across multiple cameras. Technically, we train neural networks to generate both ID-discriminative and camera-invariant features. To achieve the ID discrimination ability of the embedding features, we maximize feature distances between different person IDs within a camera by using a metric learning approach. At the same time, considering each camera as a different domain, we apply adversarial learning across multiple camera domains for generating camera-invariant features. We also propose a part-aware adaptation module, which effectively performs multi-camera domain invariant feature learning in different spatial regions. We carry out comprehensive experiments on three public re-ID datasets (i.e., PRID-2011, iLIDS-VID, and MARS). Our method outperforms state-of-the-art methods by a large margin of about 20\% in terms of rank-1 accuracy on the large-scale MARS dataset.",0
"The majority of video person re-identification (re-ID) methods rely on supervised learning, which necessitates the labeling of IDs across various cameras. This labeling process becomes prohibitively expensive as the number of cameras increases, making it difficult to implement re-identification algorithms on a large camera network. In this article, we tackle the scalability problem by introducing deep representation learning that does not require ID information across multiple cameras. Our method involves training neural networks to generate both ID-discriminative and camera-invariant features. To ensure that the embedding features can distinguish between different person IDs, we adopt a metric learning approach that maximizes feature distances within a camera. We also use adversarial learning to produce camera-invariant features across multiple camera domains, treating each camera as a distinct domain. Additionally, we propose a part-aware adaptation module that effectively performs multi-camera domain invariant feature learning in various spatial regions. We conduct extensive experiments on three public re-ID datasets (PRID-2011, iLIDS-VID, and MARS) and demonstrate that our approach outperforms state-of-the-art methods by approximately 20\% in terms of rank-1 accuracy on the large-scale MARS dataset.",1
"In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks. The number of channels in the bottleneck, on the other hand, is secondary in importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Copying does not occur, despite training the CAE for 1,000 epochs on a tiny ($\approx$ 600 images) dataset. We believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs.",0
"The focus of this paper is to undertake a comprehensive exploration of the bottleneck in convolutional autoencoders (CAEs). These autoencoders, particularly the convolutional kind, are widely used in deep learning for a range of purposes such as compression, outlier detection, and representation learning. However, the impact of the bottleneck shape on the CAE's emergent properties is not well understood. Our research indicates that a larger bottleneck size improves generalization, resulting in better performance of latent codes in downstream transfer learning tasks. We also discovered that the number of channels in the bottleneck is of secondary importance. Additionally, we challenge the conventional belief that CAEs learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Our experiments show that even after training the CAE on a small dataset for 1,000 epochs, no copying occurs. Our results have practical implications and can lead to enhancements in models that rely on CAEs.",1
"Graph convolutional networks (GCNs) are a widely used method for graph representation learning. To elucidate the capabilities and limitations of GCNs, we investigate their power, as a function of their number of layers, to distinguish between different random graph models (corresponding to different class-conditional distributions in a classification problem) on the basis of the embeddings of their sample graphs. In particular, the graph models that we consider arise from graphons, which are the most general possible parameterizations of infinite exchangeable graph models and which are the central objects of study in the theory of dense graph limits. We give a precise characterization of the set of pairs of graphons that are indistinguishable by a GCN with nonlinear activation functions coming from a certain broad class if its depth is at least logarithmic in the size of the sample graph. This characterization is in terms of a degree profile closeness property. Outside this class, a very simple GCN architecture suffices for distinguishability. We then exhibit a concrete, infinite class of graphons arising from stochastic block models that are well-separated in terms of cut distance and are indistinguishable by a GCN. These results theoretically match empirical observations of several prior works. To prove our results, we exploit a connection to random walks on graphs. Finally, we give empirical results on synthetic and real graph classification datasets, indicating that indistinguishable graph distributions arise in practice.",0
"The use of graph convolutional networks (GCNs) for learning graph representations is widespread. Our study investigates the effectiveness of GCNs, in relation to the number of layers, in distinguishing between various random graph models based on their embeddings. These graph models are derived from graphons, which are the most general parameterizations of infinite exchangeable graph models and are central to dense graph limits theory. We provide a precise characterization of GCN's ability to distinguish between graphons, based on a degree profile closeness property, showing that a simple GCN architecture suffices for distinguishability beyond a certain depth. We also demonstrate that a class of graphons from stochastic block models are well-separated in terms of cut distance and are indistinguishable by GCN. Our results are supported by empirical observations from previous studies, which we prove using a connection to random walks on graphs. Additionally, our empirical results on synthetic and real graph classification datasets demonstrate the existence of indistinguishable graph distributions in practice.",1
"As many algorithms depend on a suitable representation of data, learning unique features is considered a crucial task. Although supervised techniques using deep neural networks have boosted the performance of representation learning, the need for a large set of labeled data limits the application of such methods. As an example, high-quality delineations of regions of interest in the field of pathology is a tedious and time-consuming task due to the large image dimensions. In this work, we explored the performance of a deep neural network and triplet loss in the area of representation learning. We investigated the notion of similarity and dissimilarity in pathology whole-slide images and compared different setups from unsupervised and semi-supervised to supervised learning in our experiments. Additionally, different approaches were tested, applying few-shot learning on two publicly available pathology image datasets. We achieved high accuracy and generalization when the learned representations were applied to two different pathology datasets.",0
"The acquisition of a suitable representation of data is imperative for many algorithms, making the learning of unique features a critical task. While supervised techniques utilizing deep neural networks have enhanced the performance of representation learning, their application is constrained by the need for a large set of labeled data. For instance, generating high-quality region of interest delineations in pathology is arduous and time-consuming due to the large image dimensions. In this study, we evaluated the efficacy of deep neural networks and triplet loss in representation learning, exploring the concepts of similarity and dissimilarity in whole-slide pathology images. We compared various setups, including unsupervised, semi-supervised, and supervised learning, and assessed different approaches utilizing few-shot learning on two publicly available pathology image datasets. The learned representations exhibited high accuracy and generalization when applied to the two distinct pathology datasets.",1
"Autoencoders are techniques for data representation learning based on artificial neural networks. Differently to other feature learning methods which may be focused on finding specific transformations of the feature space, they can be adapted to fulfill many purposes, such as data visualization, denoising, anomaly detection and semantic hashing. This work presents these applications and provides details on how autoencoders can perform them, including code samples making use of an R package with an easy-to-use interface for autoencoder design and training, \texttt{ruta}. Along the way, the explanations on how each learning task has been achieved are provided with the aim to help the reader design their own autoencoders for these or other objectives.",0
"Autoencoders are artificial neural network-based techniques used for learning data representation. Unlike other feature learning approaches that focus on specific transformations of the feature space, autoencoders can be adapted for various purposes, including data visualization, denoising, anomaly detection, and semantic hashing. This article explores these applications and offers insights into how autoencoders can accomplish them. It also includes code samples utilizing the R package, \texttt{ruta}, which provides an easy-to-use interface for autoencoder design and training. Additionally, the article explains how each learning task has been achieved to assist readers in designing their own autoencoders for similar or different objectives.",1
"In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.",0
"The purpose of this research is to introduce plan2vec, which is an unsupervised method of learning representations inspired by reinforcement learning. Plan2vec creates a graph on an image dataset using nearest-neighbor distances and then uses this local metric to create a global embedding by distilling a path-integral over planned paths. When applied to control, plan2vec allows for learning goal-conditioned value estimates that are accurate over long time horizons while remaining computationally and sample efficient. The effectiveness of plan2vec is demonstrated on one simulated and two challenging real-world image datasets. The experimental results reveal that plan2vec can amortize planning costs, allowing for reactive planning that is linear in memory and computational complexity instead of being exhaustive over the entire state space.",1
"Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at https://www.github.com/danielgordon10/vince",0
"Various tasks have been accomplished by recent unsupervised representation learning techniques using a single image. These techniques focus on instance discrimination, which involves distinguishing between augmented versions of the same image and a large number of unrelated images. Through this method, networks can learn to extract meaningful representations while disregarding augmentation noise. However, previous works mainly relied on artificial data augmentation techniques like cropping and color jitter, which only superficially affect images and are not aligned with how objects actually change. The authors argue that videos offer a natural augmentation that provides entirely new perspectives on objects, showing deformation and connecting visually distinct but semantically similar concepts. They propose a new method called Video Noise Contrastive Estimation, which uses unlabeled videos to develop robust and transferable single image representations. The method outperforms recent unsupervised single image techniques and fully supervised ImageNet pretraining across various temporal and non-temporal tasks. The Random Related Video Views dataset and code are available at https://www.github.com/danielgordon10/vince.",1
"Building a scalable machine learning system for unsupervised anomaly detection via representation learning is highly desirable. One of the prevalent methods is using a reconstruction error from variational autoencoder (VAE) via maximizing the evidence lower bound. We revisit VAE from the perspective of information theory to provide some theoretical foundations on using the reconstruction error, and finally arrive at a simpler and more effective model for anomaly detection. In addition, to enhance the effectiveness of detecting anomalies, we incorporate a practical model uncertainty measure into the metric. We show empirically the competitive performance of our approach on benchmark datasets.",0
It is highly desirable to create a machine learning system that can detect anomalies without supervision and can be scaled up. One of the most commonly used methods is maximizing the evidence lower bound by using a variational autoencoder (VAE) to calculate a reconstruction error. We have revisited VAE from an information theory perspective to establish some theoretical foundations for using the reconstruction error. We have developed a simpler and more effective model for anomaly detection and incorporated a practical model uncertainty measure into the metric to enhance its effectiveness. Our approach has demonstrated competitive performance on benchmark datasets through empirical evidence.,1
"Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.",0
"The efficiency of deep neural network training for vision can be improved by transferring pre-trained representations, which also simplifies hyperparameter tuning. Our research revisits the approach of pre-training on large supervised datasets and fine-tuning on a target task. We present a scaling up of pre-training, known as Big Transfer (BiT), using a straightforward recipe that combines a few carefully selected components and utilizes a simple heuristic for transferring. BiT delivers strong performance on more than 20 datasets, even across a wide range of data regimes, from 1 example per class to 1M total examples. Notably, BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). Furthermore, on small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. Our study includes a detailed analysis of the key components contributing to high transfer performance.",1
"Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.",0
"The significance of Mutual Information (MI) in representation learning is undeniable, but its computation becomes difficult in continuous and high-dimensional scenarios. Fortunately, recent developments have resulted in scalable and feasible MI estimators that help identify useful representations. However, current approaches fail to offer an accurate estimation of MI with low-variance when MI is large. Consequently, we suggest that estimating the gradients of MI directly is more desirable for representation learning than estimating MI itself. Hence, we introduce the Mutual Information Gradient Estimator (MIGE), which is based on score estimation of implicit distributions. MIGE provides a tight and smooth gradient estimation of MI in large-MI and high-dimensional settings. We have applied MIGE in unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method, resulting in significant performance improvement in learning useful representation, as indicated by our experimental results.",1
"Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically jotted down by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in electronic health records. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset. Code avaialble at https://github.com/sajaddarabi/TAPER-EHR",0
"The task of representing electronic health records in an effective manner is challenging, particularly with the widespread availability of such data. These records are irregular and include various modalities, such as medical codes and notes, which are written by medical staff and may be influenced by the patient's medical conditions. The notes often contain valuable information about patients beyond the structured data within the electronic health records. To address these challenges, we utilize transformer networks and the BERT language model to create a unified vector representation of these data streams. This approach successfully encodes a patient's visit data into a single distributed representation, which can be utilized for downstream tasks. Our model demonstrates exceptional performance and generalization on mortality, readmission, and length of stay tasks using the MIMIC-III ICU dataset, which is publicly available. The code for our approach is accessible at https://github.com/sajaddarabi/TAPER-EHR.",1
"To solve the problem of the overwhelming size of Deep Neural Networks (DNN) several compression schemes have been proposed, one of them is teacher-student. Teacher-student tries to transfer knowledge from a complex teacher network to a simple student network. In this paper, we propose a novel method called a teacher-class network consisting of a single teacher and multiple student networks (i.e. class of students). Instead of transferring knowledge to one student only, the proposed method transfers a chunk of knowledge about the entire solution to each student. Our students are not trained for problem-specific logits, they are trained to mimic knowledge (dense representation) learned by the teacher network. Thus unlike the logits-based single student approach, the combined knowledge learned by the class of students can be used to solve other problems as well. These students can be designed to satisfy a given budget, e.g. for comparative purposes we kept the collective parameters of all the students less than or equivalent to that of a single student in the teacher-student approach . These small student networks are trained independently, making it possible to train and deploy models on memory deficient devices as well as on parallel processing systems such as data centers. The proposed teacher-class architecture is evaluated on several benchmark datasets including MNIST, FashionMNIST, IMDB Movie Reviews and CAMVid on multiple tasks including classification, sentiment classification and segmentation. Our approach outperforms the state-of-the-art single student approach in terms of accuracy as well as computational cost and in many cases it achieves an accuracy equivalent to the teacher network while having 10-30 times fewer parameters.",0
"Numerous compression schemes have been proposed to address the issue of the unwieldy size of Deep Neural Networks (DNN), including the teacher-student approach, which aims to transfer knowledge from a complex teacher network to a simpler student network. However, we introduce a novel method called the teacher-class network, consisting of one teacher and several student networks, which transfers a portion of knowledge about the entire solution to each student. Unlike the single-student approach, our students are not trained for problem-specific logits, but rather to mimic the teacher network's dense representation. As a result, the collective knowledge learned by the class of students can be used to tackle various problems. Our small student networks can be designed to meet a specific budget, such that their combined parameters are less than or equal to those of a single student in the teacher-student approach. These small student networks are trained independently, allowing for deployment on memory-deficient devices and parallel processing systems. We evaluated our teacher-class architecture on multiple tasks, including classification, sentiment classification, and segmentation, using several benchmark datasets such as MNIST, FashionMNIST, IMDB Movie Reviews, and CAMVid. Our approach outperforms the state-of-the-art single-student approach in terms of accuracy and computational cost, achieving an accuracy equivalent to the teacher network while having 10-30 times fewer parameters in many cases.",1
"The real-world data usually exhibits heterogeneous properties such as modalities, views, or resources, which brings some unique challenges wherein the key is Heterogeneous Representation Learning (HRL) termed in this paper. This brief survey covers the topic of HRL, centered around several major learning settings and real-world applications. First of all, from the mathematical perspective, we present a unified learning framework which is able to model most existing learning settings with the heterogeneous inputs. After that, we conduct a comprehensive discussion on the HRL framework by reviewing some selected learning problems along with the mathematics perspectives, including multi-view learning, heterogeneous transfer learning, Learning using privileged information and heterogeneous multi-task learning. For each learning task, we also discuss some applications under these learning problems and instantiates the terms in the mathematical framework. Finally, we highlight the challenges that are less-touched in HRL and present future research directions. To the best of our knowledge, there is no such framework to unify these heterogeneous problems, and this survey would benefit the community.",0
"This paper introduces the concept of Heterogeneous Representation Learning (HRL) and its importance in dealing with the diverse properties of real-world data. The survey focuses on various learning settings and applications of HRL. A unified learning framework is presented from a mathematical perspective, which can model different learning settings with heterogeneous inputs. The survey then reviews selected learning problems and their applications, including multi-view learning, heterogeneous transfer learning, Learning using privileged information, and heterogeneous multi-task learning. The mathematical perspective of each learning task is discussed along with its applications. Finally, the challenges that are yet to be addressed in HRL are highlighted, and future research directions are presented. This comprehensive survey is unique in its unified approach to heterogeneous problems and will be useful for the research community.",1
"Learning a good representation is an essential component for deep reinforcement learning (RL). Representation learning is especially important in multitask and partially observable settings where building a representation of the unknown environment is crucial to solve the tasks. Here we introduce Prediction of Bootstrap Latents (PBL), a simple and flexible self-supervised representation learning algorithm for multitask deep RL. PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics. Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics. In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more. We show in our experiments that PBL delivers across-the-board improved performance over state of the art deep RL agents in the DMLab-30 and Atari-57 multitask setting.",0
"Deep reinforcement learning (RL) heavily relies on having a good representation, which is especially crucial in multitask and partially observable environments where an accurate understanding of the unknown surroundings is necessary for successful task completion. To address this need, we present Prediction of Bootstrap Latents (PBL), a self-supervised representation learning approach that is simple and adaptable for multitask deep RL. PBL utilizes multistep predictive representations of future observations to capture structured information about environment dynamics, training its representation by predicting latent embeddings of future observations. These embeddings are themselves trained to predict the aforementioned representations, creating a bootstrapping effect that enhances the agent's comprehension of the environment's key aspects. Furthermore, PBL's prediction tasks are entirely defined in the latent space, which allows for the use of multimodal observations, such as pixel images, language instructions, and rewards. Our experiments demonstrate that PBL outperforms state-of-the-art deep RL agents in the DMLab-30 and Atari-57 multitask settings, providing across-the-board enhancements in performance.",1
"Recent breakthroughs in representation learning of unseen classes and examples have been made in deep metric learning by training at the same time the image representations and a corresponding metric with deep networks. Recent contributions mostly address the training part (loss functions, sampling strategies, etc.), while a few works focus on improving the discriminative power of the image representation. In this paper, we propose DIABLO, a dictionary-based attention method for image embedding. DIABLO produces richer representations by aggregating only visually-related features together while being easier to train than other attention-based methods in deep metric learning. This is experimentally confirmed on four deep metric learning datasets (Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval) for which DIABLO shows state-of-the-art performances.",0
"Deep metric learning has recently achieved breakthroughs in representation learning, particularly in the context of unseen classes and examples. This has been achieved by simultaneously training image representations and a corresponding metric with deep networks. While most recent contributions have focused on the training aspect, such as loss functions and sampling strategies, a few works aim to improve the discriminative power of image representations. This paper proposes a new dictionary-based attention method for image embedding, called DIABLO, which aggregates visually-related features to produce richer representations. Compared to other attention-based methods, DIABLO is easier to train and achieves state-of-the-art performance on four deep metric learning datasets, including Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval.",1
"The wide-spread adoption of representation learning technologies in clinical decision making strongly emphasizes the need for characterizing model reliability and enabling rigorous introspection of model behavior. While the former need is often addressed by incorporating uncertainty quantification strategies, the latter challenge is addressed using a broad class of interpretability techniques. In this paper, we argue that these two objectives are not necessarily disparate and propose to utilize prediction calibration to meet both objectives. More specifically, our approach is comprised of a calibration-driven learning method, which is also used to design an interpretability technique based on counterfactual reasoning. Furthermore, we introduce \textit{reliability plots}, a holistic evaluation mechanism for model reliability. Using a lesion classification problem with dermoscopy images, we demonstrate the effectiveness of our approach and infer interesting insights about the model behavior.",0
"The use of representation learning technologies in clinical decision making highlights the importance of ensuring model reliability and understanding model behavior. While uncertainty quantification strategies address the former, interpretability techniques deal with the latter. This paper proposes a prediction calibration approach that can achieve both objectives. The approach includes a learning method that uses calibration and a counterfactual reasoning-based interpretability technique. Additionally, the paper introduces reliability plots to evaluate model reliability comprehensively. The effectiveness of the approach is demonstrated through a lesion classification problem with dermoscopy images, providing valuable insights into the model's behavior.",1
"Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.",0
"In various real-life scenarios such as social networks, ontologies, computational finance, and biology, graphs are a common occurrence. Initially, machine learning models for graphs were primarily designed for static graphs. However, with the emergence of evolving graphs in many applications, this poses significant challenges for learning and inference since there are changes in nodes, attributes, and edges over time. This paper surveys recent developments in representation learning for dynamic graphs, including dynamic knowledge graphs. The paper categorizes existing models based on an encoder-decoder perspective and the techniques employed, and evaluates the approaches in each category. Additionally, the survey reviews various popular datasets and applications and suggests future research directions.",1
"We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (e.g., using Faster R-CNN), like any other unsupervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while others are by using the conventional likelihood: P(Y|X). This is also the core reason why VC R-CNN can learn ""sense-making"" knowledge like chair can be sat -- while not just ""common"" co-occurrences such as chair is likely to exist if table is observed. We extensively apply VC R-CNN features in prevailing models of three popular tasks: Image Captioning, VQA, and VCR, and observe consistent performance boosts across them, achieving many new state-of-the-arts. Code and feature are available at https://github.com/Wangt-CN/VC-R-CNN.",0
"Introducing a new method for unsupervised feature representation learning, we present Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN) as an improved visual region encoder. This method is aimed at high-level tasks like captioning and VQA. With the help of Faster R-CNN, a set of detected object regions in an image is given, and the training objective of VC R-CNN is to predict the contextual objects of a region using causal intervention (P(Y|do(X))) unlike other unsupervised feature learning methods (P(Y|X)) like word2vec. This key difference helps VC R-CNN to learn ""sense-making"" knowledge like ""chair can be sat"" rather than just ""common"" co-occurrences like ""chair is likely to exist if table is observed."" We have applied VC R-CNN features extensively in the prevailing models of three popular tasks: Image Captioning, VQA, and VCR, and have achieved consistent performance boosts across them. Our code and feature are available at https://github.com/Wangt-CN/VC-R-CNN.",1
"It is always demanding to learn robust visual representation for various learning problems; however, this learning and maintenance process usually suffers from noise, incompleteness or knowledge domain mismatch. Thus, robust representation learning by removing noisy features or samples, complementing incomplete data, and mitigating the distribution difference becomes the key. Along this line of research, low-rank modeling has been widely-applied to solving representation learning challenges. This survey covers the topic from a knowledge flow perspective in terms of: (1) robust knowledge recovery, (2) robust knowledge transfer, and (3) robust knowledge fusion, centered around several major applications. First of all, we deliver a unified formulation for robust knowledge discovery given single dataset. Second, we discuss robust knowledge transfer and fusion given multiple datasets with different knowledge flows, followed by practical challenges, model variations, and remarks. Finally, we highlight future research of robust knowledge discovery for incomplete, unbalance, large-scale data analysis. This would benefit AI community from literature review to future direction.",0
"Learning robust visual representation for various learning problems is always challenging, and the learning and maintenance process is often hindered by noise, incompleteness, or knowledge domain mismatch. Therefore, the key to robust representation learning is to eliminate noisy features or samples, supplement incomplete data, and mitigate distribution differences. Low-rank modeling has been extensively used to address representation learning challenges. This survey examines the topic from a knowledge flow perspective, focusing on robust knowledge recovery, transfer, and fusion in reference to various major applications. Firstly, we provide a unified formulation for robust knowledge discovery from a single dataset. Secondly, we discuss robust knowledge transfer and fusion from multiple datasets with different knowledge flows, and consider practical challenges, model variations, and remarks. Finally, we highlight the future research directions of robust knowledge discovery for incomplete, unbalanced, and large-scale data analysis, which would benefit the AI community from literature review to future directions.",1
"Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence between variational approximate posterior and the true posterior, however, iVAE has to maximize the evidence lower bound (ELBO) of the marginal likelihood, leading to suboptimal solutions in both theory and practice. In contrast, we propose an identifiable framework for estimating latent representations using a flow-based model (iFlow). Our approach directly maximizes the marginal likelihood, allowing for theoretical guarantees on identifiability, thereby dispensing with variational approximations. We derive its optimization objective in analytical form, making it possible to train iFlow in an end-to-end manner. Simulations on synthetic data validate the correctness and effectiveness of our proposed method and demonstrate its practical advantages over other existing methods.",0
"Representation learning aims to identify the true underlying representations that generate observed data. However, most deep generative models fail to achieve identifiability, which is crucial for recovering the true latent sources. Recent research proposes identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. However, iVAE is limited by the intractability of KL divergence and leads to suboptimal solutions. In contrast, we propose an identifiable framework called iFlow, which directly maximizes the marginal likelihood and provides theoretical guarantees on identifiability, without the need for variational approximations. Our method can be trained end-to-end due to its analytical optimization objective. Synthetic data simulations confirm the effectiveness and practical advantages of our proposed method over other existing methods.",1
"Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing end-to-end rPPG and heart rate (HR) measurement methods from facial videos are vulnerable to the less-constrained scenarios (e.g., with head movement and bad illumination). In this letter, we explore the reason why existing end-to-end networks perform poorly in challenging conditions and establish a strong end-to-end baseline (AutoHR) for remote HR measurement with neural architecture search (NAS). The proposed method includes three parts: 1) a powerful searched backbone with novel Temporal Difference Convolution (TDC), intending to capture intrinsic rPPG-aware clues between frames; 2) a hybrid loss function considering constraints from both time and frequency domains; and 3) spatio-temporal data augmentation strategies for better representation learning. Comprehensive experiments are performed on three benchmark datasets to show our superior performance on both intra- and cross-dataset testing.",0
"The potential of remote photoplethysmography (rPPG) is significant in various fields, including remote healthcare. However, current rPPG and heart rate (HR) measurement methods using facial videos are limited in less-constrained scenarios due to factors such as head movement and poor illumination. In this letter, we investigate why existing end-to-end networks struggle in challenging conditions and introduce a robust end-to-end baseline (AutoHR) for remote HR measurement using neural architecture search (NAS). Our approach includes a powerful searched backbone with a novel Temporal Difference Convolution (TDC) to capture intrinsic rPPG-aware clues between frames, a hybrid loss function that considers constraints from both time and frequency domains, and spatio-temporal data augmentation strategies for improved representation learning. We conduct extensive experiments on three benchmark datasets and demonstrate our superior performance in both intra- and cross-dataset testing.",1
"Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.",0
"Extensive research has been conducted on image classification; however, there has been a limited exploration of unconventional external guidance beyond traditional image-label pairs for training. Our research introduces a set of methods that utilize information about the embedded semantic hierarchy in class labels. We first incorporate label-hierarchy knowledge into an arbitrary CNN-based classifier and demonstrate through empirical evidence that the availability of external semantic information, combined with visual semantics from images, enhances overall performance. Additionally, we take a step further and model the label-label and label-image interactions explicitly, using order-preserving embeddings governed by Euclidean and hyperbolic geometries, which are prevalent in natural language. We tailor these models to hierarchical image classification and representation learning. All models are empirically validated using the hierarchical ETHEC dataset.",1
"Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task training regime. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.",0
"The majority of research on vision and language concentrates on a limited number of separate tasks and the corresponding data sets, which are frequently examined in isolation. Nonetheless, the skills required for visually-grounded language comprehension, which enable these tasks to be successful, have a significant degree of overlap. This study explores the associations between vision and language tasks by creating a comprehensive, multi-task training program. Our strategy culminates in a single model for twelve datasets from four broad categories of tasks, such as visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. This approach reduces the number of parameters from about three billion to 270 million, while simultaneously enhancing performance by an average of 2.05 points across tasks, compared to independently trained single-task models. We use our multi-task framework to conduct a thorough analysis of the impact of training diverse tasks jointly. In addition, we demonstrate that fine-tuning task-specific models from our single multi-task model can result in further improvements, achieving performance that matches or surpasses the current state-of-the-art.",1
"The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. In this paper, we focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG's topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets.",0
"In the development of real-world artificial intelligence systems, the ability to learn new classes incrementally is of utmost importance. This paper focuses on the practical and challenging problem of few-shot class-incremental learning (FSCIL), where CNN models must learn new classes from very few labelled samples without forgetting the previously learned ones. To solve this problem, we use a neural gas (NG) network to represent knowledge, which can learn and preserve the topology of the feature manifold formed by different classes. Based on this, we propose the TOPology-Preserving knowledge InCrementer (TOPIC) framework, which stabilizes NG's topology to prevent forgetting of old classes and enhances representation learning for few-shot new classes by growing and adapting NG to new training samples. Our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets, as demonstrated by comprehensive experimental results.",1
"Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",0
"The study of representation learning for graph structured data has mainly focused on static graph settings, with little attention paid to modeling dynamic graphs. This paper presents a novel hierarchical variational model that uses additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN), allowing for the capture of both topology and node attribute changes in dynamic graphs. The authors argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs and the uncertainty of node latent representation. The introduction of semi-implicit variational inference for this new VGRNN architecture (SI-VGRNN) demonstrates that flexible non-Gaussian latent representations can further improve dynamic graph analytic tasks. The authors conducted experiments with multiple real-world dynamic graph datasets, which showed that SI-VGRNN and VGRNN consistently outperformed existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",1
"In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.",0
"Our work presents a learning-based strategy for chip placement, which is a time-consuming and intricate stage in the chip design process. Unlike previous approaches, our approach has the capacity to learn from previous experience and enhance over time. As we train over more chip blocks, our method becomes more proficient in generating optimized placements for new chip blocks. We treat placement as a Reinforcement Learning (RL) problem and train an agent to position the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to new blocks, we ground representation learning in the supervised task of predicting placement quality. By devising a neural architecture that can accurately predict reward across a range of netlists and placements, we create rich feature embeddings of the input netlists. We use this architecture as the encoder of our policy and value networks to facilitate transfer learning. Our aim is to minimize PPA (power, performance, and area), and we demonstrate that our method can generate placements that are comparable or better than human experts, and existing approaches require human experts and take several weeks. We achieve these results in under six hours on modern accelerator netlists.",1
"Point cloud is a principal data structure adopted for 3D geometric information encoding. Unlike other conventional visual data, such as images and videos, these irregular points describe the complex shape features of 3D objects, which makes shape feature learning an essential component of point cloud analysis. To this end, a shape-oriented message passing scheme dubbed ShapeConv is proposed to focus on the representation learning of the underlying shape formed by each local neighboring point. Despite this intra-shape relationship learning, ShapeConv is also designed to incorporate the contextual effects from the inter-shape relationship through capturing the long-ranged dependencies between local underlying shapes. This shape-oriented operator is stacked into our hierarchical learning architecture, namely Shape-Oriented Convolutional Neural Network (SOCNN), developed for point cloud analysis. Extensive experiments have been performed to evaluate its significance in the tasks of point cloud classification and part segmentation.",0
"The main data structure utilized for encoding 3D geometric information is the point cloud. Unlike traditional visual data like images and videos, these irregular points are used to describe the complex shape features of 3D objects. Consequently, shape feature learning is crucial for analyzing point clouds. In order to achieve this, a message passing scheme called ShapeConv has been introduced to focus on learning the representation of each local neighboring point's underlying shape. Furthermore, ShapeConv is designed to incorporate contextual effects from inter-shape relationships by capturing long-ranged dependencies between local underlying shapes. This shape-oriented operator is incorporated into a hierarchical learning structure called Shape-Oriented Convolutional Neural Network (SOCNN) developed for point cloud analysis. The effectiveness of SOCNN has been evaluated through various experiments in point cloud classification and part segmentation.",1
"The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. Machine learning algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. Since the performance of a learning algorithm not only depends on the size and nature of the data but also on the quality of underlying representation, deep neural networks can learn non-linear mappings that allow transforming GVs data into more clustering and classification friendly representations than manual feature selection. In this paper, we proposed convolutional embedded networks in which we combine two DNN architectures called convolutional embedded clustering and convolutional autoencoder classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the 1000 genomes and Simons genome diversity projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index of 0.915, the normalized mutual information of 0.92, and the clustering accuracy of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees(GBT) and SHAP. Overall, our approach is transparent and faster than the baseline methods, and scalable for 5% to 100% of the full human genome.",0
"By studying genetic variants, it is possible to identify groups that are predisposed to common diseases and explain differences in disease susceptibility and patient reactions to drugs. Machine learning algorithms are increasingly being used to understand the complex phenotypic traits of interacting genetic variants. Deep neural networks can learn non-linear mappings that allow for more clustering and classification-friendly representations of genetic variant data than manual feature selection. This paper proposed the use of convolutional embedded networks, which combine two DNN architectures, convolutional embedded clustering and convolutional autoencoder classifier, to cluster individuals and predict geographic ethnicity based on genetic variants, respectively. The approach was tested on 95 million genetic variants from the 1000 genomes and Simons genome diversity projects, and it outperformed state-of-the-art approaches such as VariantSpark and ADMIXTURE. The CEC algorithm clustered targeted population groups in 22 hours with high accuracy, while the CAE classifier predicted the geographic ethnicity of unknown samples with a high F1 and Mathews correlation coefficient score. The approach is transparent, faster than baseline methods, and scalable for a range of genetic variant data. To interpret the predictions, significant biomarkers were identified using gradient boosted trees and SHAP.",1
"Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models",0
"The objective of grounding referring expressions in images is to identify the specific object instance mentioned in a referring expression within an image. This task requires a comprehensive understanding of both natural language and image content and is crucial for various visual tasks related to human-computer interaction. The primary challenge in this language-to-vision matching problem is to extract all the relevant information from both the image and referring expression, including object and relationship information, and align cross-modal semantic concepts using contextual information. However, current methods for grounding referring expressions are inadequate in extracting multi-order relationships accurately and linking them to the corresponding objects and their contextual information in the image. To address this issue, we introduce the Cross-Modal Relationship Extractor (CMRE), which applies a cross-modal attention mechanism to highlight relevant objects and relationships based on the given expression and represent them as a language-guided visual relation graph. Additionally, we propose the Gated Graph Convolutional Network (GGCN) to fuse multimodal information and propagate it throughout the structured relation graph to compute multimodal semantic contexts. Our experimental results on three standard benchmark datasets demonstrate that our Cross-Modal Relationship Inference Network, comprising CMRE and GGCN, outperforms all existing state-of-the-art methods. The code for our approach is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models.",1
"Most current studies on survey analysis and risk tolerance modelling lack professional knowledge and domain-specific models. Given the effectiveness of generative adversarial learning in cross-domain information, we design an Asymmetric cross-Domain Generative Adversarial Network (ADGAN) for domain scale inequality. ADGAN utilizes the information-sufficient domain to provide extra information to improve the representation learning on the information-insufficient domain via domain alignment. We provide data analysis and user model on two data sources: Consumer Consumption Information and Survey Information. We further test ADGAN on a real-world dataset with view embedding structures and show ADGAN can better deal with the class imbalance and unqualified data space than state-of-the-art, demonstrating the effectiveness of leveraging asymmetrical domain information.",0
"Many current studies regarding survey analysis and risk tolerance modelling do not possess the necessary expertise and specific models for the field. To address this issue, we have developed an Asymmetric cross-Domain Generative Adversarial Network (ADGAN) that utilizes generative adversarial learning to improve representation learning on domains with insufficient information by aligning them with information-rich domains. Our approach has been applied to two data sources, Consumer Consumption Information and Survey Information, and has been tested on a real-world dataset using view embedding structures. ADGAN has shown better performance in dealing with class imbalance and unqualified data space than current state-of-the-art methods, demonstrating the effectiveness of using asymmetrical domain information.",1
"Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This paper presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.",0
"Different identities or poses are reflected in the various shapes of human bodies, but the underlying structure of the body's shape has certain similarities that allow it to be represented in a low-dimensional space. In this paper, we introduce a network architecture similar to an autoencoder that can learn disentangled embeddings for both shape and pose, specifically for the 3D human body. Our approach is inspired by recent advances in deformation-based latent representation learning. To improve the accuracy of reconstruction, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the neural network to learn from. Our learned embedding not only produces superior reconstruction accuracy but also provides great flexibility in 3D human body generation through interpolation, bilinear interpolation, and latent space sampling. The extensive experiments we conducted demonstrate the power of our learned 3D human body embedding in various applications.",1
"Recent studies show that deep neural networks are vulnerable to adversarial examples which can be generated via certain types of transformations. Being robust to a desired family of adversarial attacks is then equivalent to being invariant to a family of transformations. Learning invariant representations then naturally emerges as an important goal to achieve which we explore in this paper within specific application contexts. Specifically, we propose a cyclically-trained adversarial network to learn a mapping from image space to latent representation space and back such that the latent representation is invariant to a specified factor of variation (e.g., identity). The learned mapping assures that the synthesized image is not only realistic, but has the same values for unspecified factors (e.g., pose and illumination) as the original image and a desired value of the specified factor. Unlike disentangled representation learning, which requires two latent spaces, one for specified and another for unspecified factors, invariant representation learning needs only one such space. We encourage invariance to a specified factor by applying adversarial training using a variational autoencoder in the image space as opposed to the latent space. We strengthen this invariance by introducing a cyclic training process (forward and backward cycle). We also propose a new method to evaluate conditional generative networks. It compares how well different factors of variation can be predicted from the synthesized, as opposed to real, images. In quantitative terms, our approach attains state-of-the-art performance in experiments spanning three datasets with factors such as identity, pose, illumination or style. Our method produces sharp, high-quality synthetic images with little visible artefacts compared to previous approaches.",0
"Recent research indicates that adversarial examples can compromise the integrity of deep neural networks through specific transformations. Consequently, being resistant to targeted adversarial attacks is equivalent to being unaffected by certain transformations. Therefore, it is crucial to develop invariant representations, which is the focus of this paper in the context of specific applications. We suggest a cyclically-trained adversarial network that maps image space to latent representation space and back, resulting in an invariant latent representation for a specified factor of variation. Unlike disentangled representation learning, our method only requires one latent space. We use adversarial training in the image space to encourage invariance to a specified factor and cyclic training to enhance the invariance. Additionally, we propose a new method for evaluating conditional generative networks based on how well they predict different factors of variation. Our approach achieves state-of-the-art performance on three datasets, producing high-quality synthetic images with minimal visual artifacts.",1
"Most neural networks utilize the same amount of compute for every example independent of the inherent complexity of the input. Further, methods that adapt the amount of computation to the example focus on finding a fixed inference-time computational graph per example, ignoring any external computational budgets or varying inference time limitations. In this work, we utilize conditional computation to make neural sequence models (Transformer) more efficient and computation-aware during inference. We first modify the Transformer architecture, making each set of operations conditionally executable depending on the output of a learned control network. We then train this model in a multi-task setting, where each task corresponds to a particular computation budget. This allows us to train a single model that can be controlled to operate on different points of the computation-quality trade-off curve, depending on the available computation budget at inference time. We evaluate our approach on two tasks: (i) WMT English-French Translation and (ii) Unsupervised representation learning (BERT). Our experiments demonstrate that the proposed Conditional Computation Transformer (CCT) is competitive with vanilla Transformers when allowed to utilize its full computational budget, while improving significantly over computationally equivalent baselines when operating on smaller computational budgets.",0
"Many neural networks have a fixed amount of compute for every example, regardless of the input's complexity. Even methods that adjust the computation for each example only consider a fixed inference-time computational graph and ignore outside computational budgets or varying inference time constraints. This study introduces conditional computation to enhance the efficiency and computation-awareness of neural sequence models, specifically Transformers, during inference. The Transformer architecture is modified so that each operation set can be conditionally executed based on the output of a learned control network. The model is then trained in a multi-task setting, where each task corresponds to a specific computation budget. This allows for a single model that can be controlled to operate on different points of the computation-quality trade-off curve depending on the available computation budget. The approach is evaluated on two tasks: WMT English-French Translation and Unsupervised representation learning (BERT). The results show that the proposed Conditional Computation Transformer (CCT) is competitive with vanilla Transformers when given full computational budget, and significantly improves over computationally equivalent baselines when using smaller computational budgets.",1
"The goal of few-shot learning is to recognize new visual concepts with just a few amount of labeled samples in each class. Recent effective metric-based few-shot approaches employ neural networks to learn a feature similarity comparison between query and support examples. However, the importance of feature embedding, i.e., exploring the relationship among training samples, is neglected. In this work, we present a simple yet powerful baseline for few-shot classification by emphasizing the importance of feature embedding. Specifically, we revisit the classical triplet network from deep metric learning, and extend it into a deep K-tuplet network for few-shot learning, utilizing the relationship among the input samples to learn a general representation learning via episode-training. Once trained, our network is able to extract discriminative features for unseen novel categories and can be seamlessly incorporated with a non-linear distance metric function to facilitate the few-shot classification. Our result on the miniImageNet benchmark outperforms other metric-based few-shot classification methods. More importantly, when evaluated on completely different datasets (Caltech-101, CUB-200, Stanford Dogs and Cars) using the model trained with miniImageNet, our method significantly outperforms prior methods, demonstrating its superior capability to generalize to unseen classes.",0
"Few-shot learning aims to recognize new visual concepts by using a small number of labeled samples in each class. Although current metric-based few-shot approaches utilize neural networks to compare feature similarity between query and support examples, they overlook the significance of feature embedding, which explores the relationship among training samples. To address this issue, we propose a straightforward yet effective baseline for few-shot classification that emphasizes the importance of feature embedding. We modify the classical triplet network from deep metric learning and convert it into a deep K-tuplet network for few-shot learning. Our approach leverages the relationship among input samples to learn a general representation learning using episode-training, enabling the extraction of discriminative features for novel categories. Additionally, our model can seamlessly integrate with a non-linear distance metric function to facilitate few-shot classification. We achieve superior performance on the miniImageNet benchmark compared to other metric-based few-shot classification methods. Furthermore, our method surpasses previous methods when evaluated on different datasets such as Caltech-101, CUB-200, Stanford Dogs, and Cars, demonstrating its ability to generalize to unseen classes.",1
"Adversarial perturbations are noise-like patterns that can subtly change the data, while failing an otherwise accurate classifier. In this paper, we propose to use such perturbations within a novel contrastive learning setup to build negative samples, which are then used to produce improved video representations. To this end, given a well-trained deep model for per-frame video recognition, we first generate adversarial noise adapted to this model. Positive and negative bags are produced using the original data features from the full video sequence and their perturbed counterparts, respectively. Unlike the classic contrastive learning methods, we develop a binary classification problem that learns a set of discriminative hyperplanes -- as a subspace -- that will separate the two bags from each other. This subspace is then used as a descriptor for the video, dubbed \emph{discriminative subspace pooling}. As the perturbed features belong to data classes that are likely to be confused with the original features, the discriminative subspace will characterize parts of the feature space that are more representative of the original data, and thus may provide robust video representations. To learn such descriptors, we formulate a subspace learning objective on the Stiefel manifold and resort to Riemannian optimization methods for solving it efficiently. We provide experiments on several video datasets and demonstrate state-of-the-art results.",0
"The paper proposes the use of adversarial perturbations, which are patterns that subtly alter data without affecting an accurate classifier, in a new contrastive learning framework to create negative samples. This approach aims to improve video representations by generating positive and negative bags using the original data features and their perturbed counterparts, respectively. Unlike traditional contrastive learning methods, the authors develop a binary classification problem that learns discriminative hyperplanes to separate the two bags from each other, resulting in a subspace descriptor for the video called ""discriminative subspace pooling."" By using perturbed features from classes that are likely to be confused with the original features, the discriminative subspace can capture more representative parts of the feature space and provide robust video representations. The authors use a subspace learning objective on the Stiefel manifold and Riemannian optimization methods to efficiently solve the problem and provide state-of-the-art results on several video datasets.",1
"How to utilize deep learning methods for graph classification tasks has attracted considerable research attention in the past few years. Regarding graph classification tasks, the graphs to be classified may have various graph sizes (i.e., different number of nodes and edges) and have various graph properties (e.g., average node degree, diameter, and clustering coefficient). The diverse property of graphs has imposed significant challenges on existing graph learning techniques since diverse graphs have different best-fit hyperparameters. It is difficult to learn graph features from a set of diverse graphs by a unified graph neural network. This motivates us to use a multiplex structure in a diverse way and utilize a priori properties of graphs to guide the learning. In this paper, we propose MxPool, which concurrently uses multiple graph convolution/pooling networks to build a hierarchical learning structure for graph representation learning tasks. Our experiments on numerous graph classification benchmarks show that our MxPool has superiority over other state-of-the-art graph representation learning methods.",0
"Over the past few years, there has been a considerable amount of research focused on using deep learning techniques for graph classification tasks. These tasks involve classifying graphs with varying sizes and properties, such as different numbers of nodes and edges, as well as varying average node degrees, diameters, and clustering coefficients. However, these diverse graph properties present significant challenges for existing graph learning techniques, as each graph requires different hyperparameters for optimal performance. This makes it challenging to learn graph features using a single graph neural network. To address this issue, we propose MxPool, which utilizes a multiplex structure to guide learning based on a priori graph properties. Our approach uses multiple graph convolution/pooling networks to establish a hierarchical learning structure for graph representation learning tasks. Our experiments on several graph classification benchmarks show that MxPool outperforms other state-of-the-art graph representation learning methods.",1
"The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.",0
"Intelligent decision-making systems require the ability to predict, anticipate, and reason about future outcomes. Recently, deep learning has been successful in computer vision, leading to the emergence of deep-learning-based video prediction as a promising research area. Video prediction is a self-supervised learning task that offers a suitable framework for representation learning, as it can extract meaningful representations of patterns in natural videos. This paper reviews deep learning methods for prediction in video sequences, starting with an introduction to video prediction fundamentals and necessary background concepts, as well as commonly used datasets. The authors then analyze existing video prediction models based on a proposed taxonomy, highlighting their contributions and significance in the field. Experimental results are presented to assess the state of the art quantitatively. The paper concludes with general conclusions, open research challenges, and future research directions.",1
"One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of 86.07% mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.",0
"Learning a robust and discriminative visual representation for vehicle re-identification (re-id) poses a fundamental challenge due to significant intra-class variations across different camera views. Limited training images and viewpoints in existing vehicle datasets further complicate the issue. To address this, we propose a two-stage progressive approach to learn a more robust visual representation from a unique large-scale vehicle dataset called VehicleNet, which we built by combining four public vehicle datasets. In the first stage, we train with conventional classification loss to learn a generic representation for all domains, which relaxes the need for full alignment between training and testing domains. In the second stage, we fine-tune the trained model based purely on the target vehicle set, minimizing the distribution discrepancy between VehicleNet and any target domain. We evaluate our approach through extensive experiments and achieve state-of-the-art accuracy of 86.07% mAP on the private test set of AICity Challenge, as well as competitive results on VeRi-776 and VehicleID datasets. Our proposed VehicleNet dataset and the learned robust representations hold potential for advancing vehicle re-id in real-world environments.",1
"Key features of mental illnesses are reflected in speech. Our research focuses on designing a multimodal deep learning structure that automatically extracts salient features from recorded speech samples for predicting various mental disorders including depression, bipolar, and schizophrenia. We adopt a variety of pre-trained models to extract embeddings from both audio and text segments. We use several state-of-the-art embedding techniques including BERT, FastText, and Doc2VecC for the text representation learning and WaveNet and VGG-ish models for audio encoding. We also leverage huge auxiliary emotion-labeled text and audio corpora to train emotion-specific embeddings and use transfer learning in order to address the problem of insufficient annotated multimodal data available. All these embeddings are then combined into a joint representation in a multimodal fusion layer and finally a recurrent neural network is used to predict the mental disorder. Our results show that mental disorders can be predicted with acceptable accuracy through multimodal analysis of clinical interviews.",0
"Speech can reflect key features of mental illnesses. Our research aims to create a multimodal deep learning structure that can automatically extract significant features from recorded speech samples to predict various mental disorders such as schizophrenia, bipolar, and depression. To achieve this, we utilize various pre-trained models that can extract embeddings from both audio and text. These models include BERT, FastText, Doc2VecC, WaveNet, and VGG-ish. We also employ emotion-labeled text and audio data to train emotion-specific embeddings and transfer learning to overcome the lack of annotated multimodal data. The embeddings are then merged into a joint representation in a multimodal fusion layer and a recurrent neural network is used to predict the mental disorder. Our research demonstrates that mental disorders can be predicted with acceptable accuracy through multimodal analysis of clinical interviews.",1
"Supervised deep learning requires a large amount of training samples with annotations (e.g. label class for classification task, pixel- or voxel-wised label map for segmentation tasks), which are expensive and time-consuming to obtain. During the training of a deep neural network, the annotated samples are fed into the network in a mini-batch way, where they are often regarded of equal importance. However, some of the samples may become less informative during training, as the magnitude of the gradient start to vanish for these samples. In the meantime, other samples of higher utility or hardness may be more demanded for the training process to proceed and require more exploitation. To address the challenges of expensive annotations and loss of sample informativeness, here we propose a novel training framework which adaptively selects informative samples that are fed to the training process. The adaptive selection or sampling is performed based on a hardness-aware strategy in the latent space constructed by a generative model. To evaluate the proposed training framework, we perform experiments on three different datasets, including MNIST and CIFAR-10 for image classification task and a medical image dataset IVUS for biophysical simulation task. On all three datasets, the proposed framework outperforms a random sampling method, which demonstrates the effectiveness of proposed framework.",0
"Obtaining a large number of annotated samples for supervised deep learning is a time-consuming and costly process. During the training of a deep neural network, all annotated samples are treated equally and fed into the network in mini-batches. However, as training progresses, some samples may become less informative, leading to gradient vanishing, while others may be more useful and require further exploitation. To overcome these challenges, a novel training framework is proposed that selects informative samples for training based on their hardness in the latent space constructed by a generative model. This adaptive selection strategy is evaluated on three datasets, including MNIST, CIFAR-10, and IVUS, and is found to outperform the random sampling method.",1
"Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.",0
"Visual search, recommendation, and identification rely heavily on deep representation learning, which is a widely accepted approach. However, retrieving representations from a large database poses computational challenges. To address this, approximate methods have been explored, including locality sensitive hashing, product quantization, and PCA, which aim to learn compact representations. In this study, we propose a different approach. Instead of learning compact representations, we suggest learning high dimensional and sparse representations with similar representational capacity to dense embeddings. This is possible due to sparse matrix multiplication operations, which are faster than dense multiplication. Our novel approach involves using a carefully constructed regularization function to learn distributed sparse embeddings that minimize the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments demonstrate that our method is competitive with other existing methods and offers a similar or better speed-vs-accuracy tradeoff on practical datasets.",1
"This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. Intuitively, we seek to expand the distribution of the tail classes by transferring from the head classes, so as to alleviate the distortion of the feature space. To this end, we propose to construct each feature into a ""feature cloud"". If a sample belongs to a tail class, the corresponding feature cloud will have relatively large distribution range, in compensation to its lack of diversity. It allows each tail sample to push the samples from other classes far away, recovering the intra-class diversity of tail classes. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.",0
"The focus of this paper is on acquiring profound features from long-tailed data. It has been observed that the distribution patterns of head classes and tail classes differ in the deep feature space. The spatial span of head classes is relatively large, while tail classes have a significantly smaller spatial span due to the absence of intra-class diversity. This imbalance in distribution between the two sets of classes distorts the overall feature space, thereby compromising the discriminative ability of the acquired features. The idea is to extend the distribution of tail classes by transferring from head classes to reduce feature space distortion. The proposed approach involves constructing each feature into a ""feature cloud"". If a sample is from a tail class, the corresponding feature cloud will have a larger distribution range to compensate for the lack of diversity. This enables each tail sample to push the samples from other classes further away, thereby restoring the intra-class diversity of tail classes. Comprehensive experimentation on person re-identification and face recognition tasks validates the efficacy of the proposed method.",1
"We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We show that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradient. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights.",0
"We tackle the difficult challenge of achieving deep representation learning, which involves adapting a pre-trained deep network to varying tasks in an efficient manner. Our proposal involves exploring gradient-based features that are the gradients of the model parameters in relation to a task-specific loss given an input sample. Our innovative approach involves designing a linear model that incorporates both gradient and activation of the pre-trained network. Through our research, we discovered that our model provides a local linear approximation to an underlying deep model and we discuss the important theoretical insights we gained. In addition, we developed an effective algorithm for training and inference of our model without the need for computing the actual gradient. Our approach was tested across different tasks in representation-learning and various datasets, using different network architectures. We achieved impressive results in all settings, which were consistent with our theoretical insights.",1
"In this paper, we propose FairNN a neural network that performs joint feature representation and classification for fairness-aware learning. Our approach optimizes a multi-objective loss function in which (a) learns a fair representation by suppressing protected attributes (b) maintains the information content by minimizing a reconstruction loss and (c) allows for solving a classification task in a fair manner by minimizing the classification error and respecting the equalized odds-based fairness regularized. Our experiments on a variety of datasets demonstrate that such a joint approach is superior to separate treatment of unfairness in representation learning or supervised learning. Additionally, our regularizers can be adaptively weighted to balance the different components of the loss function, thus allowing for a very general framework for conjoint fair representation learning and decision making.",0
"FairNN, a neural network that combines feature representation and classification for fairness-aware learning, is proposed in this paper. Our method optimizes a multi-objective loss function that achieves three objectives: (a) a fair representation is learned by suppressing protected attributes, (b) information content is maintained by minimizing reconstruction loss, and (c) classification tasks are solved in a fair manner by minimizing classification error while respecting the equalized odds-based fairness regularization. Our experiments on various datasets demonstrate that this joint approach is superior to separate approaches for unfairness in representation learning or supervised learning. Furthermore, our regularizers can be weighted to balance the loss function's different components, allowing for a general framework for fair representation learning and decision making.",1
"Recent advancements in learning Discrete Representations as opposed to continuous ones have led to state of art results in tasks that involve Language, Audio and Vision. Some latent factors such as words, phonemes and shapes are better represented by discrete latent variables as opposed to continuous. Vector Quantized Variational Autoencoders (VQVAE) have produced remarkable results in multiple domains. VQVAE learns a prior distribution $z_e$ along with its mapping to a discrete number of $K$ vectors (Vector Quantization). We propose applying VQ along the feature axis. We hypothesize that by doing so, we are learning a mapping between the codebook vectors and the marginal distribution of the prior feature space. Our approach leads to 33\% improvement as compared to prevous discrete models and has similar performance to state of the art auto-regressive models (e.g. PixelSNAIL). We evaluate our approach on a static prior using an artificial toy dataset (blobs). We further evaluate our approach on benchmarks for CIFAR-10 and ImageNet.",0
"The recent progress in learning Discrete Representations, instead of continuous ones, has resulted in the achievement of state-of-the-art outcomes in tasks involving Language, Audio, and Vision. Discrete latent variables are better suited for representing certain latent factors such as words, phonemes, and shapes. Vector Quantized Variational Autoencoders (VQVAE) have demonstrated exceptional performance across multiple domains. VQVAE is capable of learning a prior distribution $z_e$ and its mapping to a discrete number of $K$ vectors (Vector Quantization). In this regard, we propose utilizing VQ along the feature axis. Our hypothesis is that by doing so, we can effectively learn a mapping between the codebook vectors and the marginal distribution of the prior feature space. Our proposed approach has shown a 33% improvement over the previous discrete models and has performed comparably to state-of-the-art auto-regressive models (e.g. PixelSNAIL). We have evaluated our approach on a static prior using an artificial toy dataset (blobs) and further tested it on benchmarks for CIFAR-10 and ImageNet.",1
"We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object.   We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features.   In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance in K-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects.   Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.",0
"Our research focuses on utilizing Minimax distances to extract features that capture unknown underlying patterns and structures in data in a nonparametric manner. We have developed a flexible and computationally efficient framework that can be applied to various machine learning techniques for numerical data. Our study includes the computation of pairwise Minimax distances for all object pairs and the calculation of Minimax distances for all objects to and from a fixed (test) object. We efficiently compute pairwise Minimax distances using the equivalence of Minimax distances over a graph and a minimum spanning tree constructed on it. We perform an embedding of pairwise Minimax distances into a new vector space, where their squared Euclidean distances in the new space are equal to their pairwise Minimax distances in the original space. In addition, we explore the case of multiple pairwise Minimax matrices and propose an embedding method that involves summing up centered matrices and performing an eigenvalue decomposition. We also investigate computing Minimax distances from a fixed (test) object, which is useful for K-nearest neighbor search. Our framework is applicable to any base distance measure, and we analyze the edges selected by Minimax distances to detect outlier objects. Finally, we conduct experiments to demonstrate the effectiveness of our framework in each setting.",1
"Person re-identification (Re-ID) in real-world scenarios usually suffers from various degradation factors, e.g., low-resolution, weak illumination, blurring and adverse weather. On the one hand, these degradations lead to severe discriminative information loss, which significantly obstructs identity representation learning; on the other hand, the feature mismatch problem caused by low-level visual variations greatly reduces retrieval performance. An intuitive solution to this problem is to utilize low-level image restoration methods to improve the image quality. However, existing restoration methods cannot directly serve to real-world Re-ID due to various limitations, e.g., the requirements of reference samples, domain gap between synthesis and reality, and incompatibility between low-level and high-level methods. In this paper, to solve the above problem, we propose a degradation invariance learning framework for real-world person Re-ID. By introducing a self-supervised disentangled representation learning strategy, our method is able to simultaneously extract identity-related robust features and remove real-world degradations without extra supervision. We use low-resolution images as the main demonstration, and experiments show that our approach is able to achieve state-of-the-art performance on several Re-ID benchmarks. In addition, our framework can be easily extended to other real-world degradation factors, such as weak illumination, with only a few modifications.",0
"Re-identification of individuals in real-world situations often faces various issues that degrade the quality of the images, such as low-resolution, weak lighting, blurring, and unfavorable weather conditions. These degradations result in significant loss of discerning information, which hinders the learning of identity representation, and the mismatch of features caused by visual variations reduces retrieval performance. A possible solution to this problem is to enhance image quality using low-level restoration methods. However, these methods are not directly applicable to real-world Re-ID due to several limitations, such as reference sample requirements, domain gaps between synthesis and reality, and incompatibility between low-level and high-level techniques. To address this problem, we propose a degradation invariance learning framework for real-world person Re-ID in this paper. Our approach utilizes a self-supervised disentangled representation learning strategy to extract robust features related to identity and remove real-world degradations without additional supervision. We demonstrate the effectiveness of our approach on low-resolution images and achieve state-of-the-art performance on several Re-ID benchmarks. Furthermore, our framework can be easily adapted to other real-world degradation factors, such as weak lighting, with minimal adjustments.",1
"Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., answering queries such as (US, has president, ?, 2012), we propose a solution inspired by the canonical decomposition of tensors of order 4. We introduce new regularization schemes and present an extension of ComplEx (Trouillon et al., 2016) that achieves state-of-the-art performance. Additionally, we propose a new dataset for knowledge base completion constructed from Wikidata, larger than previous benchmarks by an order of magnitude, as a new reference for evaluating temporal and non-temporal link prediction methods.",0
"Representation learning and link prediction algorithms commonly used for relational data are typically designed for static data. However, the data they are applied to often changes over time, such as friend graphs in social networks or user interactions with items in recommender systems. Knowledge bases also contain time-sensitive facts, like (US, has president, B. Obama, [2009-2017]). To address link prediction under temporal constraints, like answering queries such as (US, has president, ?, 2012), we propose a solution based on the canonical decomposition of tensors of order 4. We introduce new regularization schemes and extend ComplEx (Trouillon et al., 2016) to achieve the best performance. Additionally, we introduce a new dataset for knowledge base completion sourced from Wikidata, which is larger than previous benchmarks and serves as a new reference for evaluating temporal and non-temporal link prediction methods.",1
"This paper presents a framework for the analysis of changes in visual streams: ordered sequences of images, possibly separated by significant time gaps. We propose a new approach to incorporating unlabeled data into training to generate natural language descriptions of change. We also develop a framework for estimating the time of change in visual stream. We use learned representations for change evidence and consistency of perceived change, and combine these in a regularized graph cut based change detector. Experimental evaluation on visual stream datasets, which we release as part of our contribution, shows that representation learning driven by natural language descriptions significantly improves change detection accuracy, compared to methods that do not rely on language.",0
"In this article, a framework is introduced for evaluating changes in visual streams that consist of a sequence of images. The authors suggest a novel method for integrating unlabeled data to create descriptive language about changes and also establish a process for estimating the time of change in a visual stream. Learned representations for consistency of perceived change and change evidence are incorporated and combined in a regularized graph cut-based change detector. The authors release visual stream datasets to support their contribution and demonstrate that representation learning guided by natural language descriptions enhances change detection accuracy compared to methods that do not rely on language.",1
"We consider the problem of learning low-dimensional representations for large-scale Markov chains. We formulate the task of representation learning as that of mapping the state space of the model to a low-dimensional state space, called the kernel space. The kernel space contains a set of meta states which are desired to be representative of only a small subset of original states. To promote this structural property, we constrain the number of nonzero entries of the mappings between the state space and the kernel space. By imposing the desired characteristics of the representation, we cast the problem as a constrained nonnegative matrix factorization. To compute the solution, we propose an efficient block coordinate gradient descent and theoretically analyze its convergence properties.",0
"Our focus is on acquiring low-dimensional representations for large-scale Markov chains. Our approach involves mapping the model's state space to a low-dimensional state space called the kernel space. The kernel space consists of a group of meta states that represent a small portion of the original states. To ensure this structural feature, we limit the number of nonzero entries in the mappings between the two spaces. We accomplish this by constraining the nonnegative matrix factorization. Our proposed solution to the problem is an efficient block coordinate gradient descent, and we have analyzed its convergence properties.",1
"Unsupervised representation learning holds the promise of exploiting large amounts of unlabeled data to learn general representations. A promising technique for unsupervised learning is the framework of Variational Auto-encoders (VAEs). However, unsupervised representations learned by VAEs are significantly outperformed by those learned by supervised learning for recognition. Our hypothesis is that to learn useful representations for recognition the model needs to be encouraged to learn about repeating and consistent patterns in data. Drawing inspiration from the mid-level representation discovery work, we propose PatchVAE, that reasons about images at patch level. Our key contribution is a bottleneck formulation that encourages mid-level style representations in the VAE framework. Our experiments demonstrate that representations learned by our method perform much better on the recognition tasks compared to those learned by vanilla VAEs.",0
"The potential of unsupervised representation learning is to utilize vast amounts of unlabeled data for acquiring general representations. Variational Auto-encoders (VAEs) is a promising form of unsupervised learning; however, supervised learning surpasses the unsupervised representations learned by VAEs in recognition. Our assumption is that for learning useful recognition representations, the model must comprehend recurring and coherent data patterns. Inspired by mid-level representation discovery, we introduce PatchVAE, which evaluates images on the patch level. Our significant contribution is a bottleneck formulation that encourages mid-level style representations within the VAE framework. Our experiments demonstrate that our approach outperforms vanilla VAEs in recognition tasks.",1
"The cognitive framework of conceptual spaces proposes to represent concepts as regions in psychological similarity spaces. These similarity spaces are typically obtained through multidimensional scaling (MDS), which converts human dissimilarity ratings for a fixed set of stimuli into a spatial representation. One can distinguish metric MDS (which assumes that the dissimilarity ratings are interval or ratio scaled) from nonmetric MDS (which only assumes an ordinal scale). In our first study, we show that despite its additional assumptions, metric MDS does not necessarily yield better solutions than nonmetric MDS. In this chapter, we furthermore propose to learn a mapping from raw stimuli into the similarity space using artificial neural networks (ANNs) in order to generalize the similarity space to unseen inputs. In our second study, we show that a linear regression from the activation vectors of a convolutional ANN to similarity spaces obtained by MDS can be successful and that the results are sensitive to the number of dimensions of the similarity space.",0
"The theory of conceptual spaces suggests that concepts can be represented as areas in psychological similarity spaces. These spaces are created through multidimensional scaling (MDS), which translates human dissimilarity ratings for a set of stimuli into a spatial representation. MDS can be metric, assuming the ratings are interval or ratio scaled, or nonmetric, assuming only an ordinal scale. Our initial study demonstrates that metric MDS does not necessarily offer better solutions than nonmetric MDS. This chapter proposes using artificial neural networks (ANNs) to learn a mapping from raw stimuli to similarity spaces, allowing for generalization to new inputs. Our second study finds that linear regression from the activation vectors of a convolutional ANN to MDS similarity spaces can be successful, with results varying based on the number of dimensions in the similarity space.",1
"The vulnerability of automated fingerprint recognition systems to presentation attacks (PA), i.e., spoof or altered fingers, has been a growing concern, warranting the development of accurate and efficient presentation attack detection (PAD) methods. However, one major limitation of the existing PAD solutions is their poor generalization to new PA materials and fingerprint sensors, not used in training. In this study, we propose a robust PAD solution with improved cross-material and cross-sensor generalization. Specifically, we build on top of any CNN-based architecture trained for fingerprint spoof detection combined with cross-material spoof generalization using a style transfer network wrapper. We also incorporate adversarial representation learning (ARL) in deep neural networks (DNN) to learn sensor and material invariant representations for PAD. Experimental results on LivDet 2015 and 2017 public domain datasets exhibit the effectiveness of the proposed approach.",0
"The susceptibility of automated fingerprint recognition systems to presentation attacks, such as fake or altered fingers, has become a growing concern. This has led to the need for accurate and efficient presentation attack detection (PAD) methods. However, the current PAD solutions have a significant drawback in their inability to generalize to new PA materials and fingerprint sensors that were not included in the training phase. As a solution, our study proposes a robust PAD methodology that improves cross-material and cross-sensor generalization. We achieve this by utilizing a CNN-based architecture trained for fingerprint spoof detection, combined with a style transfer network wrapper for cross-material spoof generalization. Additionally, we incorporate adversarial representation learning (ARL) in deep neural networks (DNN) to obtain sensor and material invariant representations for PAD. Our experimental results on public domain datasets, LivDet 2015 and 2017, demonstrate the effectiveness of our proposed approach.",1
"Similarity learning has gained a lot of attention from researches in recent years and tons of successful approaches have been recently proposed. However, the majority of the state-of-the-art similarity learning methods consider only a binary similarity. In this paper we introduce a new loss function called Continuous Histogram Loss (CHL) which generalizes recently proposed Histogram loss to multiple-valued similarities, i.e. allowing the acceptable values of similarity to be continuously distributed within some range. The novel loss function is computed by aggregating pairwise distances and similarities into 2D histograms in a differentiable manner and then computing the probability of condition that pairwise distances will not decrease as the similarities increase. The novel loss is capable of solving a wider range of tasks including similarity learning, representation learning and data visualization.",0
"In recent years, researchers have taken a keen interest in similarity learning, leading to numerous successful approaches. However, most of the current state-of-the-art methods for similarity learning only consider binary similarity. This paper presents a new loss function, called Continuous Histogram Loss (CHL), which extends the previously proposed Histogram loss to multiple-valued similarities. This new loss function allows for a continuous distribution of acceptable similarity values within a given range. CHL is computed by aggregating pairwise distances and similarities into differentiable 2D histograms, and then calculating the probability that the pairwise distances will not decrease as the similarities increase. This novel loss function can address a wider range of tasks, including similarity learning, representation learning, and data visualization.",1
"Deep Reinforcement Learning (DRL) has been successfully applied in several research domains such as robot navigation and automated video game playing. However, these methods require excessive computation and interaction with the environment, so enhancements on sample efficiency are required. The main reason for this requirement is that sparse and delayed rewards do not provide an effective supervision for representation learning of deep neural networks. In this study, Proximal Policy Optimization (PPO) algorithm is augmented with Generative Adversarial Networks (GANs) to increase the sample efficiency by enforcing the network to learn efficient representations without depending on sparse and delayed rewards as supervision. The results show that an increased performance can be obtained by jointly training a DRL agent with a GAN discriminator.   ----   Derin Pekistirmeli Ogrenme, robot navigasyonu ve otomatiklestirilmis video oyunu oynama gibi arastirma alanlarinda basariyla uygulanmaktadir. Ancak, kullanilan yontemler ortam ile fazla miktarda etkilesim ve hesaplama gerektirmekte ve bu nedenle de ornek verimliligi yonunden iyilestirmelere ihtiyac duyulmaktadir. Bu gereksinimin en onemli nedeni, gecikmeli ve seyrek odul sinyallerinin derin yapay sinir aglarinin etkili betimlemeler ogrenebilmesi icin yeterli bir denetim saglayamamasidir. Bu calismada, Proksimal Politika Optimizasyonu algoritmasi Uretici Cekismeli Aglar (UCA) ile desteklenerek derin yapay sinir aglarinin seyrek ve gecikmeli odul sinyallerine bagimli olmaksizin etkili betimlemeler ogrenmesi tesvik edilmektedir. Elde edilen sonuclar onerilen algoritmanin ornek verimliliginde artis elde ettigini gostermektedir.",0
"Deep Reinforcement Learning (DRL) has been applied successfully in various research areas, such as robot navigation and automated video game playing. However, these methods require a significant amount of computation and interaction with the environment, making improvements in sample efficiency necessary. This need arises from the fact that sparse and delayed rewards are not effective for supervising the representation learning of deep neural networks. In this study, the Proximal Policy Optimization (PPO) algorithm is combined with Generative Adversarial Networks (GANs) to increase sample efficiency by enabling the network to learn efficient representations without relying on sparse and delayed rewards for supervision. The results indicate that joint training of a DRL agent with a GAN discriminator can lead to improved performance.",1
"A discrete system's heterogeneity is measured by the R\'enyi heterogeneity family of indices (also known as Hill numbers or Hannah--Kay indices), whose units are {the numbers equivalent}. Unfortunately, numbers equivalent heterogeneity measures for non-categorical data require {a priori} (A) categorical partitioning and (B) pairwise distance measurement on the observable data space, thereby precluding application to problems with ill-defined categories or where semantically relevant features must be learned as abstractions from some data. We thus introduce representational R\'enyi heterogeneity (RRH), which transforms an observable domain onto a latent space upon which the R\'enyi heterogeneity is both tractable and semantically relevant. This method requires neither {a priori} binning nor definition of a distance function on the observable space. We show that RRH can generalize existing biodiversity and economic equality indices. Compared with existing indices on a beta-mixture distribution, we show that RRH responds more appropriately to changes in mixture component separation and weighting. Finally, we demonstrate the measurement of RRH in a set of natural images, with respect to abstract representations learned by a deep neural network. The RRH approach will further enable heterogeneity measurement in disciplines whose data do not easily conform to the assumptions of existing indices.",0
"The R\'enyi heterogeneity family of indices (also known as Hill numbers or Hannah-Kay indices) is used to measure the heterogeneity of a discrete system. However, non-categorical data requires categorical partitioning and pairwise distance measurement on the observable data space, making it difficult to apply to problems with ill-defined categories or where semantically relevant features must be learned as abstractions from some data. To address this issue, we propose the use of representational R\'enyi heterogeneity (RRH), which transforms the observable domain to a latent space that is both tractable and semantically relevant. This method does not require prior binning or definition of a distance function on the observable space. We demonstrate that RRH can generalize existing biodiversity and economic equality indices and responds more appropriately to changes in mixture component separation and weighting compared to existing indices on a beta-mixture distribution. Finally, we show how to measure RRH in natural images using abstract representations learned by a deep neural network. This approach will enable heterogeneity measurement in disciplines with data that do not easily conform to the assumptions of existing indices.",1
"A good clustering algorithm can discover natural groupings in data. These groupings, if used wisely, provide a form of weak supervision for learning representations. In this work, we present Clustering-based Contrastive Learning (CCL), a new clustering-based representation learning approach that uses labels obtained from clustering along with video constraints to learn discriminative face features. We demonstrate our method on the challenging task of learning representations for video face clustering. Through several ablation studies, we analyze the impact of creating pair-wise positive and negative labels from different sources. Experiments on three challenging video face clustering datasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new state-of-the-art on all datasets.",0
"The identification of natural groupings in data is facilitated by a good clustering algorithm. These groupings can serve as a type of vague supervision for developing representations, if properly utilized. Our research introduces Clustering-based Contrastive Learning (CCL), a novel clustering-based representation learning technique that employs video constraints and labels derived from clustering to acquire discriminative facial features. We apply our approach to the complex task of generating representations for video face clustering. We investigate the impact of generating positive and negative pair-wise labels from various sources through several ablation studies. Our experiments on three challenging video face clustering datasets: BBT-0101, BF-0502, and ACCIO demonstrate that CCL achieves new state-of-the-art results for all datasets.",1
"Semantic segmentation is an essential step for electron microscopy (EM) image analysis. Although supervised models have achieved significant progress, the need for labor intensive pixel-wise annotation is a major limitation. To complicate matters further, supervised learning models may not generalize well on a novel dataset due to domain shift. In this study, we introduce an adversarial-prediction guided multi-task network to learn the adaptation of a well-trained model for use on a novel unlabeled target domain. Since no label is available on target domain, we learn an encoding representation not only for the supervised segmentation on source domain but also for unsupervised reconstruction of the target data. To improve the discriminative ability with geometrical cues, we further guide the representation learning by multi-level adversarial learning in semantic prediction space. Comparisons and ablation study on public benchmark demonstrated state-of-the-art performance and effectiveness of our approach.",0
"The analysis of electron microscopy (EM) images requires semantic segmentation, which is a crucial step. Despite the significant progress made by supervised models, the need for pixel-wise annotation is a challenging and time-consuming task. Additionally, supervised learning models may not be effective on a new dataset due to domain shift. This study introduces an adversarial-prediction guided multi-task network to tackle this issue by adapting a trained model for use on an unlabeled target domain. The encoding representation is learned for both the supervised segmentation on the source domain and the unsupervised reconstruction of the target data. Furthermore, the multi-level adversarial learning in semantic prediction space improves the discriminative ability with geometrical cues. Our approach demonstrated state-of-the-art performance and efficiency, as evidenced by comparisons and ablation study on the public benchmark.",1
"Face presentation attack detection (PAD) has been an urgent problem to be solved in the face recognition systems. Conventional approaches usually assume the testing and training are within the same domain; as a result, they may not generalize well into unseen scenarios because the representations learned for PAD may overfit to the subjects in the training set. In light of this, we propose an efficient disentangled representation learning for cross-domain face PAD. Our approach consists of disentangled representation learning (DR-Net) and multi-domain learning (MD-Net). DR-Net learns a pair of encoders via generative models that can disentangle PAD informative features from subject discriminative features. The disentangled features from different domains are fed to MD-Net which learns domain-independent features for the final cross-domain face PAD task. Extensive experiments on several public datasets validate the effectiveness of the proposed approach for cross-domain PAD.",0
"Detecting face presentation attacks (PAD) is a pressing issue in face recognition systems. Traditional methods assume that testing and training occur in the same domain, which may lead to poor performance in unseen scenarios due to overfitting to subjects in the training set. To address this, we present a disentangled representation learning approach for cross-domain face PAD. Our approach, which includes disentangled representation learning (DR-Net) and multi-domain learning (MD-Net), utilizes generative models to separate PAD informative features from subject discriminative features. DR-Net then feeds the disentangled features from different domains to MD-Net, which learns domain-independent features for the final cross-domain face PAD task. Our approach is validated through extensive experiments on various public datasets, demonstrating its effectiveness in cross-domain PAD.",1
"Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as <= 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework.",0
"The difficulty in collecting large and diverse data makes it challenging to train accurate and robust gaze models, hindering its importance in various applications. This is due to the high cost of annotating 3D gaze and the use of different setups in existing datasets. To overcome this issue, our paper proposes a novel approach for learning a low-dimensional gaze representation without annotations. Our approach utilizes a gaze redirection network and the difference in gaze representation between the input and target images as the redirection variable. We introduce a redirection loss in the image domain, facilitating joint training of both the redirection and gaze representation networks. Additionally, we propose a warping field regularization that provides explicit physical meaning to the gaze representations and prevents redirection distortions. Our framework demonstrates promising results in few-shot gaze estimation, cross-dataset gaze estimation, gaze network pretraining, and head pose estimation tasks. This is the first work to propose such an approach to the best of our knowledge.",1
"Although Shannon theory states that it is asymptotically optimal to separate the source and channel coding as two independent processes, in many practical communication scenarios this decomposition is limited by the finite bit-length and computational power for decoding. Recently, neural joint source-channel coding (NECST) is proposed to sidestep this problem. While it leverages the advancements of amortized inference and deep learning to improve the encoding and decoding process, it still cannot always achieve compelling results in terms of compression and error correction performance due to the limited robustness of its learned coding networks. In this paper, motivated by the inherent connections between neural joint source-channel coding and discrete representation learning, we propose a novel regularization method called Infomax Adversarial-Bit-Flip (IABF) to improve the stability and robustness of the neural joint source-channel coding scheme. More specifically, on the encoder side, we propose to explicitly maximize the mutual information between the codeword and data; while on the decoder side, the amortized reconstruction is regularized within an adversarial framework. Extensive experiments conducted on various real-world datasets evidence that our IABF can achieve state-of-the-art performances on both compression and error correction benchmarks and outperform the baselines by a significant margin.",0
"Despite the asymptotic optimality of separating source and channel coding as two independent processes according to Shannon theory, practical communication scenarios often limit this approach due to finite bit-length and computational power for decoding. To address this issue, recent advancements in amortized inference and deep learning have led to the proposal of neural joint source-channel coding (NECST). However, the learned coding networks of NECST often lack the necessary robustness to consistently achieve optimal compression and error correction performance. This paper introduces a novel regularization method called Infomax Adversarial-Bit-Flip (IABF) that aims to improve the stability and robustness of neural joint source-channel coding by leveraging the connections between this approach and discrete representation learning. Specifically, IABF proposes to explicitly maximize the mutual information between the codeword and data on the encoder side while using an adversarial framework to regularize the amortized reconstruction on the decoder side. Experimental results using various real-world datasets demonstrate that IABF achieves state-of-the-art performances on both compression and error correction benchmarks, outperforming existing baselines by a significant margin.",1
"In this paper, we study a new representation-learning task, which we termed as disassembling object representations. Given an image featuring multiple objects, the goal of disassembling is to acquire a latent representation, of which each part corresponds to one category of objects. Disassembling thus finds its application in a wide domain such as image editing and few- or zero-shot learning, as it enables category-specific modularity in the learned representations. To this end, we propose an unsupervised approach to achieving disassembling, named Unsupervised Disassembling Object Representation (UDOR). UDOR follows a double auto-encoder architecture, in which a fuzzy classification and an object-removing operation are imposed. The fuzzy classification constrains each part of the latent representation to encode features of up to one object category, while the object-removing, combined with a generative adversarial network, enforces the modularity of the representations and integrity of the reconstructed image. Furthermore, we devise two metrics to respectively measure the modularity of disassembled representations and the visual integrity of reconstructed images. Experimental results demonstrate that the proposed UDOR, despited unsupervised, achieves truly encouraging results on par with those of supervised methods.",0
"The aim of this study is to explore a new task in representation-learning called disassembling object representations. This involves obtaining a latent representation from an image containing multiple objects, where each part of the representation corresponds to a specific object category. Disassembling has potential applications in fields such as image editing and few- or zero-shot learning, allowing for category-specific modularity in learned representations. To achieve this, we introduce an unsupervised approach called Unsupervised Disassembling Object Representation (UDOR), which uses a double auto-encoder architecture with a fuzzy classification and object-removing operation. The fuzzy classification ensures that each part of the latent representation encodes features of only one object category, while the object-removing, combined with a generative adversarial network, maintains the modularity of the representations and the integrity of the reconstructed image. Additionally, we create two metrics to measure the modularity of disassembled representations and the visual integrity of reconstructed images. Experimental results show that UDOR achieves impressive results comparable to those of supervised methods.",1
"Accurately predicting drug-target binding affinity (DTA) in silico is a key task in drug discovery. Most of the conventional DTA prediction methods are simulation-based, which rely heavily on domain knowledge or the assumption of having the 3D structure of the targets, which are often difficult to obtain. Meanwhile, traditional machine learning-based methods apply various features and descriptors, and simply depend on the similarities between drug-target pairs. Recently, with the increasing amount of affinity data available and the success of deep representation learning models on various domains, deep learning techniques have been applied to DTA prediction. However, these methods consider either label/one-hot encodings or the topological structure of molecules, without considering the local chemical context of amino acids and SMILES sequences. Motivated by this, we propose a novel end-to-end learning framework, called DeepGS, which uses deep neural networks to extract the local chemical context from amino acids and SMILES sequences, as well as the molecular structure from the drugs. To assist the operations on the symbolic data, we propose to use advanced embedding techniques (i.e., Smi2Vec and Prot2Vec) to encode the amino acids and SMILES sequences to a distributed representation. Meanwhile, we suggest a new molecular structure modeling approach that works well under our framework. We have conducted extensive experiments to compare our proposed method with state-of-the-art models including KronRLS, SimBoost, DeepDTA and DeepCPI. Extensive experimental results demonstrate the superiorities and competitiveness of DeepGS.",0
"In drug discovery, accurately predicting the binding affinity between drug and target is crucial. However, conventional simulation-based methods heavily rely on domain knowledge or the availability of 3D target structures, which can be difficult to obtain. Machine learning-based methods, on the other hand, simply use features and descriptors to measure similarity between drug-target pairs. With the increasing availability of affinity data and success of deep learning models, deep learning techniques have been applied to DTA prediction. However, these methods often overlook the local chemical context of amino acids and SMILES sequences. To address this, we propose DeepGS, an end-to-end learning framework that uses deep neural networks to extract local chemical context from amino acids and SMILES sequences, as well as molecular structure from drugs. We also suggest using advanced embedding techniques to encode amino acids and SMILES sequences to a distributed representation. Our proposed method was compared to state-of-the-art models and showed superior performance.",1
"We propose an algorithm, guided variational autoencoder (Guided-VAE), that is able to learn a controllable generative model by performing latent representation disentanglement learning. The learning objective is achieved by providing signals to the latent encoding/embedding in VAE without changing its main backbone architecture, hence retaining the desirable properties of the VAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE and observe enhanced modeling and controlling capability over the vanilla VAE. In the unsupervised strategy, we guide the VAE learning by introducing a lightweight decoder that learns latent geometric transformation and principal components; in the supervised strategy, we use an adversarial excitation and inhibition mechanism to encourage the disentanglement of the latent variables. Guided-VAE enjoys its transparency and simplicity for the general representation learning task, as well as disentanglement learning. On a number of experiments for representation learning, improved synthesis/sampling, better disentanglement for classification, and reduced classification errors in meta-learning have been observed.",0
"Our proposed algorithm, Guided-VAE, is capable of learning a controllable generative model through latent representation disentanglement learning. This is achieved by providing signals to the latent encoding/embedding in VAE, without altering its main architecture. As a result, we retain the desirable properties of the VAE. We have designed both an unsupervised and a supervised strategy in Guided-VAE, which have shown enhanced modeling and controlling capabilities compared to vanilla VAE. In the unsupervised strategy, we introduce a lightweight decoder that learns latent geometric transformation and principal components. In the supervised strategy, we use an adversarial excitation and inhibition mechanism to encourage disentanglement of the latent variables. Guided-VAE is transparent and simple for general representation learning tasks, as well as disentanglement learning. In various experiments for representation learning, improved synthesis/sampling, better disentanglement for classification, and reduced classification errors in meta-learning have been observed.",1
"Recent work in graph neural networks (GNNs) has led to improvements in molecular activity and property prediction tasks. Unfortunately, GNNs often fail to capture the relative importance of interactions between molecular substructures, in part due to the absence of efficient intermediate pooling steps. To address these issues, we propose LaPool (Laplacian Pooling), a novel, data-driven, and interpretable hierarchical graph pooling method that takes into account both node features and graph structure to improve molecular representation. We benchmark LaPool on molecular graph prediction and understanding tasks and show that it outperforms recent GNNs. Interestingly, LaPool also remains competitive on non-molecular tasks. Both quantitative and qualitative assessments are done to demonstrate LaPool's improved interpretability and highlight its potential benefits in drug design. Finally, we demonstrate LaPool's utility for the generation of valid and novel molecules by incorporating it into an adversarial autoencoder.",0
"Graph neural networks (GNNs) have recently demonstrated advancements in predicting molecular activity and properties. However, their limitations in capturing the importance of molecular substructure interactions due to the lack of efficient intermediate pooling steps have hindered their performance. To overcome these issues, we introduce LaPool, a novel, data-driven, and interpretable hierarchical graph pooling approach that considers both node features and graph structure to enhance molecular representation. We evaluate LaPool on molecular graph prediction and understanding tasks and outperform recent GNNs. Interestingly, LaPool also maintains competitiveness in non-molecular tasks. We conduct both quantitative and qualitative evaluations to demonstrate LaPool's interpretability and highlight its potential benefits in drug design. Finally, we apply LaPool to an adversarial autoencoder for the generation of valid and new molecules.",1
"As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to confidently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model. Present state-of-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.",0
"Humans tend to focus on the main features of an image and disregard any insignificant details. However, Deep Neural Networks have been found to misclassify images that have been corrupted with tiny, imperceptible changes. This study aims to address this issue by training networks to use the higher bit planes to form a rough impression and then refine their prediction using the lower bit planes. By ensuring consistency across differently quantized images, the networks become more robust against adversarial attacks without the need for expensive adversarial training. This approach is faster and more similar to the way humans learn. While adversarial training remains the most effective method, this study provides a new way to achieve robustness without explicitly using adversarial samples.",1
"In neuroscience, understanding inter-individual differences has recently emerged as a major challenge, for which functional magnetic resonance imaging (fMRI) has proven invaluable. For this, neuroscientists rely on basic methods such as univariate linear correlations between single brain features and a score that quantifies either the severity of a disease or the subject's performance in a cognitive task. However, to this date, task-fMRI and resting-state fMRI have been exploited separately for this question, because of the lack of methods to effectively combine them. In this paper, we introduce a novel machine learning method which allows combining the activation-and connectivity-based information respectively measured through these two fMRI protocols to identify markers of individual differences in the functional organization of the brain. It combines a multi-view deep autoencoder which is designed to fuse the two fMRI modalities into a joint representation space within which a predictive model is trained to guess a scalar score that characterizes the patient. Our experimental results demonstrate the ability of the proposed method to outperform competitive approaches and to produce interpretable and biologically plausible results.",0
"The challenge of comprehending differences between individuals in neuroscience has recently become prominent, and functional magnetic resonance imaging (fMRI) has proven to be invaluable in addressing this issue. Currently, neuroscientists use basic methods such as univariate linear correlations between individual brain features and a score that measures the severity of a disease or the subject's performance in a cognitive task. However, task-fMRI and resting-state fMRI have been separately utilized due to the lack of effective methods to combine them. This paper introduces a new machine learning approach that combines activation and connectivity-based information from both fMRI protocols to identify markers of individual differences in brain function. This method employs a multi-view deep autoencoder to fuse the two fMRI modalities into a joint representation space, within which a predictive model is trained to predict a scalar score characterizing the patient. The experimental outcomes show that this method outperforms other approaches and produces biologically plausible and interpretable results.",1
"Deep learning is recognized to be capable of discovering deep features for representation learning and pattern recognition without requiring elegant feature engineering techniques by taking advantage of human ingenuity and prior knowledge. Thus it has triggered enormous research activities in machine learning and pattern recognition. One of the most important challenge of deep learning is to figure out relations between a feature and the depth of deep neural networks (deep nets for short) to reflect the necessity of depth. Our purpose is to quantify this feature-depth correspondence in feature extraction and generalization. We present the adaptivity of features to depths and vice-verse via showing a depth-parameter trade-off in extracting both single feature and composite features. Based on these results, we prove that implementing the classical empirical risk minimization on deep nets can achieve the optimal generalization performance for numerous learning tasks. Our theoretical results are verified by a series of numerical experiments including toy simulations and a real application of earthquake seismic intensity prediction.",0
"The use of deep learning has been widely recognized for its ability to uncover deep features for both representation learning and pattern recognition. This is achieved without the need for complex feature engineering techniques, which would typically rely on human ingenuity and prior knowledge. As a result, deep learning has sparked extensive research efforts in the fields of machine learning and pattern recognition. However, a significant challenge remains in understanding the relationship between features and the depth of deep neural networks (deep nets). This will enable the reflection of the importance of depth. Our goal is to quantify this feature-depth relationship in feature extraction and generalization. We demonstrate the adaptivity of features to depths and vice versa by presenting a depth-parameter trade-off in extracting both single features and composite features. Our results show that implementing classical empirical risk minimization on deep nets can achieve optimal generalization performance for a wide range of learning tasks. To validate our theoretical findings, we conducted a series of numerical experiments, including toy simulations and a real-world application for earthquake seismic intensity prediction.",1
"Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive. In this paper, we propose to ""look into object"" (explicitly yet intrinsically model the object structure) through incorporating self-supervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our look-into-object approach (LIO) achieves large performance gain on a number of benchmarks, including generic object recognition (ImageNet) and fine-grained object recognition tasks (CUB, Cars, Aircraft). We also show that this learning paradigm is highly generalizable to other tasks such as object detection and segmentation (MS COCO). Project page: https://github.com/JDAI-CV/LIO.",0
"The majority of object recognition methods concentrate on learning visual patterns that differentiate objects, rather than considering the object's overall structure. However, modeling structure is essential, but it requires a lot of manual annotations, which can be time-consuming. To address this issue, our paper proposes an innovative approach called ""look into object,"" which incorporates self-supervision into the traditional framework to intrinsically model the object structure. This method significantly improves the recognition backbone for more robust representation learning without any additional annotation or inference time. We introduce two modules: object-extent learning and spatial context learning, which can be easily integrated into any backbone network during training and removed at inference time. Our look-into-object approach (LIO) has achieved significant performance improvements on various benchmarks, including ImageNet, CUB, Cars, Aircraft, MS COCO, and other tasks such as object detection and segmentation. For more information, please visit our project page at https://github.com/JDAI-CV/LIO.",1
"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.",0
"Although Generative Adversarial Networks (GANs) have been successful in generating images, they pose challenges when it comes to processing real images. Commonly used methods for inverting a target image back to the latent space are back-propagation or learning an additional encoder, but the results from both approaches are unsatisfactory. In this study, we introduce a new method called mGANprior that utilizes well-trained GANs as an effective prior for various image processing tasks. Our approach involves using multiple latent codes to generate multiple feature maps at an intermediate layer of the generator and then combining them with adaptive channel importance to recover the input image. This technique results in high-quality image reconstructions that outperform existing competitors, making trained GAN models useful for real-world applications such as image colorization, super-resolution, image inpainting, and semantic manipulation. Additionally, we examine the layer-wise representation learned by GAN models and provide insight into the knowledge each layer can represent.",1
"Graph representation learning (GRL) is a powerful technique for learning low-dimensional vector representation of high-dimensional and often sparse graphs. Most studies explore the structure and metadata associated with the graph using random walks and employ an unsupervised or semi-supervised learning schemes. Learning in these methods is context-free, resulting in only a single representation per node. Recently studies have argued on the adequacy of a single representation and proposed context-sensitive approaches, which are capable of extracting multiple node representations for different contexts. This proved to be highly effective in applications such as link prediction and ranking.   However, most of these methods rely on additional textual features that require complex and expensive RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple contexts of a node.   In this study we show that in-order to extract high-quality context-sensitive node representations it is not needed to rely on supplementary node features, nor to employ computationally heavy and complex models. We propose GOAT, a context-sensitive algorithm inspired by gossip communication and a mutual attention mechanism simply over the structure of the graph. We show the efficacy of GOAT using 6 real-world datasets on link prediction and node clustering tasks and compare it against 12 popular and state-of-the-art (SOTA) baselines. GOAT consistently outperforms them and achieves up to 12% and 19% gain over the best performing methods on link prediction and clustering tasks, respectively.",0
"Graph representation learning (GRL) is a technique that learns low-dimensional vector representations of high-dimensional and often sparse graphs. Typically, this involves exploring the structure and metadata of the graph through random walks and unsupervised or semi-supervised learning. However, these methods only result in a single representation per node, which may not be adequate for all contexts. Recent studies have proposed context-sensitive approaches that can extract multiple node representations for different contexts, which have been particularly effective in applications such as link prediction and ranking. However, most of these methods rely on additional textual features or complex models, which can be computationally expensive. In this study, we propose GOAT, a context-sensitive algorithm inspired by gossip communication and a mutual attention mechanism over the graph structure. We show that GOAT outperforms 12 popular and state-of-the-art baselines on link prediction and node clustering tasks, achieving up to 12% and 19% gain, respectively, without relying on supplementary node features or complex models.",1
"In the current monocular depth research, the dominant approach is to employ unsupervised training on large datasets, driven by warped photometric consistency. Such approaches lack robustness and are unable to generalize to challenging domains such as nighttime scenes or adverse weather conditions where assumptions about photometric consistency break down.   We propose DeFeat-Net (Depth & Feature network), an approach to simultaneously learn a cross-domain dense feature representation, alongside a robust depth-estimation framework based on warped feature consistency. The resulting feature representation is learned in an unsupervised manner with no explicit ground-truth correspondences required.   We show that within a single domain, our technique is comparable to both the current state of the art in monocular depth estimation and supervised feature representation learning. However, by simultaneously learning features, depth and motion, our technique is able to generalize to challenging domains, allowing DeFeat-Net to outperform the current state-of-the-art with around 10% reduction in all error measures on more challenging sequences such as nighttime driving.",0
"Currently, the prevalent method in monocular depth research is to use unsupervised training on large datasets, which relies on warped photometric consistency. However, this approach is not robust and cannot be applied to difficult environments like nighttime scenes or bad weather. Our proposed approach, called DeFeat-Net (Depth & Feature network), addresses this issue by simultaneously learning a dense feature representation that can be used across domains, and a reliable depth-estimation framework based on warped feature consistency. This unsupervised technique does not require explicit ground-truth correspondences. Our results demonstrate that within a single domain, our method is comparable to the state-of-the-art in monocular depth estimation and supervised feature representation learning. However, by learning features, depth, and motion simultaneously, our approach can generalize to challenging domains. As a result, DeFeat-Net outperforms the current state-of-the-art with a 10% reduction in all error measures on more challenging sequences like nighttime driving.",1
"Network embedding is an effective method to learn low-dimensional representations of nodes, which can be applied to various real-life applications such as visualization, node classification, and link prediction. Although significant progress has been made on this problem in recent years, several important challenges remain, such as how to properly capture temporal information in evolving networks. In practice, most networks are continually evolving. Some networks only add new edges or nodes such as authorship networks, while others support removal of nodes or edges such as internet data routing. If patterns exist in the changes of the network structure, we can better understand the relationships between nodes and the evolution of the network, which can be further leveraged to learn node representations with more meaningful information. In this paper, we propose the Embedding via Historical Neighborhoods Aggregation (EHNA) algorithm. More specifically, we first propose a temporal random walk that can identify relevant nodes in historical neighborhoods which have impact on edge formations. Then we apply a deep learning model which uses a custom attention mechanism to induce node embeddings that directly capture temporal information in the underlying feature representation. We perform extensive experiments on a range of real-world datasets, and the results demonstrate the effectiveness of our new approach in the network reconstruction task and the link prediction task.",0
"The process of network embedding is a useful technique for acquiring low-dimensional node representations that can be utilized in a variety of practical applications, including node classification, visualization, and link prediction. While much progress has been achieved in this field recently, several significant challenges still exist, particularly with regards to capturing temporal information in networks that evolve over time. Many networks are in a constant state of flux, with some adding new nodes and edges while others permit the removal of such elements. By discerning patterns in network structural changes, we can better comprehend node relationships and network evolution, which can subsequently aid in the acquisition of more meaningful node representations. Our research introduces the Embedding via Historical Neighborhoods Aggregation (EHNA) algorithm, which utilizes a temporal random walk to identify relevant nodes within historical neighborhoods that influence edge formation. Additionally, we employ a deep learning model that employs a customized attention mechanism to create node embeddings that capture temporal information within feature representation. Through an extensive array of experiments on various real-world datasets, our approach has proven to be highly effective in both network reconstruction and link prediction tasks.",1
"It is known that, without awareness of the process, our brain appears to focus on the general shape of objects rather than superficial statistics of context. On the other hand, learning autonomously allows discovering invariant regularities which help generalization. In this work, we propose a learning framework to improve the shape bias property of self-supervised methods. Our method learns semantic and shape biased representations by integrating domain diversification and jigsaw puzzles. The first module enables the model to create a dynamic environment across arbitrary domains and provides a domain exploration vs. exploitation trade-off, while the second module allows the model to explore this environment autonomously. This universal framework does not require prior knowledge of the domain of interest. Extensive experiments are conducted on several domain generalization datasets, namely, PACS, Office-Home, VLCS, and Digits. We show that our framework outperforms state-of-the-art domain generalization methods by a large margin.",0
"The brain tends to focus on the general shape of objects rather than superficial context statistics, without conscious awareness of the process. In contrast, autonomous learning can lead to the discovery of invariant regularities that aid in generalization. This study proposes a learning framework that enhances the shape bias of self-supervised techniques. The approach integrates jigsaw puzzles and domain diversification to learn semantic and shape-biased representations. The first module generates a dynamic environment across arbitrary domains, balancing domain exploration and exploitation, while the second module enables autonomous exploration of this environment. The framework is universal and does not require prior knowledge of the domain. Extensive experiments were conducted on several domain generalization datasets, including PACS, Office-Home, VLCS, and Digits. The results show that the proposed framework outperforms state-of-the-art domain generalization methods significantly.",1
"Local and global patterns of an object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose to learn point cloud representation by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape without human supervision. Experimental results on various benchmark datasets demonstrate the unsupervisedly learned representation is even better than supervised representation in discriminative power, generalization ability, and robustness. We show that unsupervisedly trained point cloud models can outperform their supervised counterparts on downstream classification tasks. Most notably, by simply increasing the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on both synthetic and real-world 3D object classification datasets. We expect our observations to offer a new perspective on learning better representation from data structures instead of human annotations for point cloud understanding.",0
"The connection between local and global patterns of an object is significant, as the attributes shared among all parts enable reasoning of the entire object from a single part. To develop a powerful representation of a 3D object, it is essential to model these shared attributes and distinguish them from those of other objects. Our hypothesis is based on this idea, and we propose to learn a point cloud representation by bidirectional reasoning between local structures and the global shape, without human supervision. Our experimental results demonstrate that the unsupervisedly learned representation is superior to the supervised representation in terms of discriminative power, generalization ability, and robustness. Moreover, we show that our unsupervised model can outperform its supervised counterparts on downstream classification tasks. By widening the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on synthetic and real-world 3D object classification datasets. Our findings suggest that learning better representations from data structures, rather than human annotations, can enhance point cloud understanding.",1
"In this paper, we propose and end-to-end deep Chinese font generation system. This system can generate new style fonts by interpolation of latent style-related embeding variables that could achieve smooth transition between different style. Our method is simpler and more effective than other methods, which will help to improve the font design efficiency",0
"The present study puts forward a complete Chinese font generation system that operates through deep learning. This system has the capability to produce novel font styles by incorporating latent style-related embedding variables, thereby facilitating seamless transitions between diverse styles. Our approach is both simpler and more efficient than other techniques, leading to enhanced font design productivity.",1
"With the tremendous success of deep learning in visual tasks, the representations extracted from intermediate layers of learned models, that is, deep features, attract much attention of researchers. Previous empirical analysis shows that those features can contain appropriate semantic information. Therefore, with a model trained on a large-scale benchmark data set (e.g., ImageNet), the extracted features can work well on other tasks. In this work, we investigate this phenomenon and demonstrate that deep features can be suboptimal due to the fact that they are learned by minimizing the empirical risk. When the data distribution of the target task is different from that of the benchmark data set, the performance of deep features can degrade. Hence, we propose a hierarchically robust optimization method to learn more generic features. Considering the example-level and concept-level robustness simultaneously, we formulate the problem as a distributionally robust optimization problem with Wasserstein ambiguity set constraints, and an efficient algorithm with the conventional training pipeline is proposed. Experiments on benchmark data sets demonstrate the effectiveness of the robust deep representations.",0
"Researchers are highly interested in deep features, which are representations extracted from intermediate layers of learned models that perform well in visual tasks using deep learning. These features have been found to contain appropriate semantic information and can be used in other tasks when extracted from a model trained on a large-scale benchmark data set such as ImageNet. However, deep features can be suboptimal when the data distribution of the target task differs from that of the benchmark data set. To address this issue, we propose a hierarchically robust optimization method that simultaneously considers example-level and concept-level robustness. This method formulates the problem as a distributionally robust optimization problem with Wasserstein ambiguity set constraints and an efficient algorithm with the conventional training pipeline is proposed. Our experiments on benchmark data sets demonstrate the effectiveness of the robust deep representations.",1
"Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.",0
"In the field of Artificial Intelligence, recent advancements in Computer Vision and Natural Language Processing have introduced new tasks that are paving the way for solving complex problems. One such task is Visual Question Answering (VQA), which requires a system to provide a natural language answer to an open-ended question about an image. This task has drawn significant attention from researchers, resulting in numerous approaches to improve VQA accuracy. These approaches typically involve three main components: independent representation learning for images and questions, feature fusion, and natural language answer generation. However, with so many approaches available, it is unclear how each component affects the model's performance. This paper aims to provide a comprehensive analysis of the impact of each component on VQA models, using a range of experiments that cover both visual and textual elements, as well as fusion and attention mechanisms. The main contribution of this paper is to identify the core components required to train VQA models and maximize their predictive performance.",1
"Deep learning methods capable of handling relational data have proliferated over the last years. In contrast to traditional relational learning methods that leverage first-order logic for representing such data, these deep learning methods aim at re-representing symbolic relational data in Euclidean spaces. They offer better scalability, but can only numerically approximate relational structures and are less flexible in terms of reasoning tasks supported. This paper introduces a novel framework for relational representation learning that combines the best of both worlds. This framework, inspired by the auto-encoding principle, uses first-order logic as a data representation language, and the mapping between the original and latent representation is done by means of logic programs instead of neural networks. We show how learning can be cast as a constraint optimisation problem for which existing solvers can be used. The use of logic as a representation language makes the proposed framework more accurate (as the representation is exact, rather than approximate), more flexible, and more interpretable than deep learning methods. We experimentally show that these latent representations are indeed beneficial in relational learning tasks.",0
"Over the past few years, there has been a surge in deep learning techniques that can handle relational data. These methods differ from traditional relational learning methods that use first-order logic to represent such data, as they aim to re-represent symbolic relational data in Euclidean spaces. Although these deep learning methods offer better scalability, they can only approximate relational structures numerically and are less adaptable for reasoning tasks. This paper presents a new framework for relational representation learning that combines the strengths of both deep learning and traditional methods. Inspired by the auto-encoding principle, this framework employs first-order logic as a data representation language and uses logic programs instead of neural networks for the mapping between the original and latent representation. We demonstrate that learning can be viewed as a constraint optimization problem that can be solved using existing solvers. Using logic as a representation language makes this framework more accurate, flexible, and interpretable than deep learning methods. We conduct experiments that demonstrate the advantages of using these latent representations for relational learning tasks.",1
"A variety of graph neural networks (GNNs) frameworks for representation learning on graphs have been recently developed. These frameworks rely on aggregation and iteration scheme to learn the representation of nodes. However, information between nodes is inevitably lost in the scheme during learning. In order to reduce the loss, we extend the GNNs frameworks by exploring the aggregation and iteration scheme in the methodology of mutual information. We propose a new approach of enlarging the normal neighborhood in the aggregation of GNNs, which aims at maximizing mutual information. Based on a series of experiments conducted on several benchmark datasets, we show that the proposed approach improves the state-of-the-art performance for four types of graph tasks, including supervised and semi-supervised graph classification, graph link prediction and graph edge generation and classification.",0
"Recently, various frameworks of graph neural networks (GNNs) have emerged for representation learning on graphs. These frameworks utilize an aggregation and iteration scheme to learn the node representation. However, this scheme results in the loss of information between nodes during the learning process. To address this issue, we have extended GNNs frameworks by incorporating the aggregation and iteration scheme with mutual information methodology. We have proposed a new approach that enlarges the normal neighborhood in the aggregation process of GNNs to maximize mutual information. Through a series of experiments on multiple benchmark datasets, we have demonstrated that our approach enhances the performance of supervised and semi-supervised graph classification, graph link prediction, and graph edge generation and classification tasks, surpassing the current state-of-the-art.",1
"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",0
"Our work introduces MoCo, a novel approach to unsupervised visual representation learning. We view contrastive learning as a dictionary look-up process and leverage this concept to create a dynamic dictionary using a queue and a moving-averaged encoder. This approach facilitates the creation of a large, consistent dictionary on-the-fly, which enables effective contrastive unsupervised learning. Our results demonstrate that MoCo performs competitively under the linear protocol for ImageNet classification, and more importantly, the learned representations transfer well to downstream tasks. In fact, MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks across datasets like PASCAL VOC and COCO, often with significant margins. These findings suggest that unsupervised and supervised representation learning have converged in many vision tasks.",1
"Low-rank Multi-view Subspace Learning (LMvSL) has shown great potential in cross-view classification in recent years. Despite their empirical success, existing LMvSL based methods are incapable of well handling view discrepancy and discriminancy simultaneously, which thus leads to the performance degradation when there is a large discrepancy among multi-view data. To circumvent this drawback, motivated by the block-diagonal representation learning, we propose Structured Low-rank Matrix Recovery (SLMR), a unique method of effectively removing view discrepancy and improving discriminancy through the recovery of structured low-rank matrix. Furthermore, recent low-rank modeling provides a satisfactory solution to address data contaminated by predefined assumptions of noise distribution, such as Gaussian or Laplacian distribution. However, these models are not practical since complicated noise in practice may violate those assumptions and the distribution is generally unknown in advance. To alleviate such limitation, modal regression is elegantly incorporated into the framework of SLMR (term it MR-SLMR). Different from previous LMvSL based methods, our MR-SLMR can handle any zero-mode noise variable that contains a wide range of noise, such as Gaussian noise, random noise and outliers. The alternating direction method of multipliers (ADMM) framework and half-quadratic theory are used to efficiently optimize MR-SLMR. Experimental results on four public databases demonstrate the superiority of MR-SLMR and its robustness to complicated noise.",0
"In recent years, Low-rank Multi-view Subspace Learning (LMvSL) has shown promise in cross-view classification. However, current LMvSL-based methods struggle to simultaneously handle view discrepancy and discriminancy, leading to performance degradation when there is a significant discrepancy among multi-view data. To overcome this limitation, we propose Structured Low-rank Matrix Recovery (SLMR), which uses block-diagonal representation learning to effectively remove view discrepancy and improve discriminancy through the recovery of a structured low-rank matrix. While low-rank modeling provides a satisfactory solution to data contaminated by predefined assumptions of noise distribution, this is not practical in practice as the distribution is often unknown and complicated. To address this, modal regression is incorporated into the framework of SLMR (called MR-SLMR), allowing it to handle any zero-mode noise variable, including Gaussian noise, random noise, and outliers. We efficiently optimize MR-SLMR using the alternating direction method of multipliers (ADMM) framework and half-quadratic theory. Our experimental results on four public databases demonstrate the superiority and robustness of MR-SLMR to complicated noise compared to previous LMvSL-based methods.",1
"Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs.",0
"Generative modeling through unsupervised representation learning is a common practice in various computer vision applications when labeled data is unavailable. Variational Autoencoders (VAEs) are powerful generative models that learn representations that are useful for generating data. However, VAEs struggle to learn useful representations that can be applied to downstream tasks due to the challenges present in the training objective. Regularization-based methods that aim to improve representation learning come at the cost of poor sample generation. This paper explores the trade-off between representation learning and sample generation for regularized VAEs. To address this issue, a new family of priors called decoupled priors, or dpVAEs, is introduced. These priors decouple the representation space from the generation space, allowing the use of VAE regularizers on the representation space without affecting the generation distribution. This enables the reaping of the benefits of regularizations for representation learning without sacrificing sample generation. dpVAE leverages invertible networks to map a complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be easily adapted to state-of-the-art VAE regularizers without additional hyperparameter tuning. The effectiveness of dpVAEs is demonstrated through experiments on MNIST, SVHN, and CelebA, which show that they fix the sample generation issue for regularized VAEs, both quantitatively and qualitatively.",1
"Despite continuing medical advances, the rate of newborn morbidity and mortality globally remains high, with over 6 million casualties every year. The prediction of pathologies affecting newborns based on their cry is thus of significant clinical interest, as it would facilitate the development of accessible, low-cost diagnostic tools\cut{ based on wearables and smartphones}. However, the inadequacy of clinically annotated datasets of infant cries limits progress on this task. This study explores a neural transfer learning approach to developing accurate and robust models for identifying infants that have suffered from perinatal asphyxia. In particular, we explore the hypothesis that representations learned from adult speech could inform and improve performance of models developed on infant speech. Our experiments show that models based on such representation transfer are resilient to different types and degrees of noise, as well as to signal loss in time and frequency domains.",0
"Despite advances in medical technology, the global rate of newborn morbidity and mortality remains unacceptably high, with over 6 million casualties annually. Predicting potential newborn pathologies based on their crying behavior is of great interest to the medical community, as it could lead to the development of accessible, low-cost diagnostic tools. However, the lack of clinically annotated datasets for infant cries has hindered progress in this area. This study investigates a neural transfer learning approach to accurately identifying infants who have suffered from perinatal asphyxia. Specifically, we test the hypothesis that using representations learned from adult speech can enhance the performance of models developed for infant speech. Our experiments demonstrate that models based on this type of representation transfer are able to withstand various types and levels of noise, as well as signal loss in both time and frequency domains.",1
"Robustness is an increasingly important property of machine learning models as they become more and more prevalent. We propose a defense against adversarial examples based on a k-nearest neighbor (kNN) on the intermediate activation of neural networks. Our scheme surpasses state-of-the-art defenses on MNIST and CIFAR-10 against l2-perturbation by a significant margin. With our models, the mean perturbation norm required to fool our MNIST model is 3.07 and 2.30 on CIFAR-10. Additionally, we propose a simple certifiable lower bound on the l2-norm of the adversarial perturbation using a more specific version of our scheme, a 1-NN on representations learned by a Lipschitz network. Our model provides a nontrivial average lower bound of the perturbation norm, comparable to other schemes on MNIST with similar clean accuracy.",0
"As machine learning models become more prevalent, robustness is a crucial property that is gaining importance. Our approach to defending against adversarial examples is based on a k-nearest neighbor (kNN) method on the intermediate activation of neural networks. Our method outperforms current defenses on MNIST and CIFAR-10 against l2-perturbation by a significant margin. For our MNIST model, the mean perturbation norm needed to deceive it is 3.07, and for CIFAR-10, it is 2.30. Furthermore, we suggest a straightforward certifiable lower bound on the l2-norm of the adversarial perturbation using a more specialized version of our technique, a 1-NN on representations learned by a Lipschitz network. Our approach yields a meaningful average lower bound of the perturbation norm that is comparable to other methods on MNIST with similar clean accuracy.",1
"Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID. Code for our proposed method is available at: https://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification.",0
"The objective of Person re-identification (reID) is to match images of individuals to locate those with the same identity. This task is difficult due to the various human poses and capture viewpoints, incomplete visibility of the bodies due to occlusion, and other factors that cause semantic misalignment of the images to be matched. This paper presents a framework that addresses this challenge by enabling the reID network to learn feature representations that are semantically aligned through meticulous supervision designs. The framework involves a Semantics Aligning Network (SAN) that includes a base network as an encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for generating a densely semantics aligned full texture image. The SAN is jointly trained with supervisions for person re-identification and aligned texture generation. Additionally, the decoder incorporates Triplet ReID constraints over the feature maps as perceptual losses, in addition to the reconstruction loss. The decoder is not used during inference, making our approach computationally efficient. Ablation studies confirm the effectiveness of our design. Our method outperforms the state-of-the-art on benchmark datasets such as CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID. The code for our proposed method is accessible at: https://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification.",1
"Person re-identification (re-ID) aims to recognize instances of the same person contained in multiple images taken across different cameras. Existing methods for re-ID tend to rely heavily on the assumption that both query and gallery images of the same person have the same clothing. Unfortunately, this assumption may not hold for datasets captured over long periods of time (e.g., weeks, months or years). To tackle the re-ID problem in the context of clothing changes, we propose a novel representation learning model which is able to generate a body shape feature representation without being affected by clothing color or patterns. We call our model the Color Agnostic Shape Extraction Network (CASE-Net). CASE-Net learns a representation of identity that depends only on body shape via adversarial learning and feature disentanglement. Due to the lack of large-scale re-ID datasets which contain clothing changes for the same person, we propose two synthetic datasets for evaluation. We create a rendered dataset SMPL-reID with different clothes patterns and a synthesized dataset Div-Market with different clothing color to simulate two types of clothing changes. The quantitative and qualitative results across 5 datasets (SMPL-reID, Div-Market, two benchmark re-ID datasets, a cross-modality re-ID dataset) confirm the robustness and superiority of our approach against several state-of-the-art approaches",0
"Re-identification of persons refers to the process of recognizing the same individual in various images taken by different cameras. Current re-ID techniques rely heavily on the assumption that images of the same person in both the query and gallery have the same clothing. However, this may not be the case for datasets captured over extended periods. To address the re-ID problem when clothing changes occur, we introduce a novel representation learning model, named the Color Agnostic Shape Extraction Network (CASE-Net). CASE-Net generates a body shape feature representation, which is unaffected by clothing color or patterns, to learn a representation of identity through adversarial learning and feature disentanglement. As large-scale re-ID datasets with clothing changes for the same person are not readily available, we propose two synthetic datasets for evaluation, namely SMPL-reID with different clothes patterns and Div-Market with different clothing colors. Our approach is superior and robust compared to several state-of-the-art approaches, as confirmed by the quantitative and qualitative results across five datasets, including SMPL-reID, Div-Market, two benchmark re-ID datasets, and a cross-modality re-ID dataset.",1
"We present a probabilistic forecasting framework based on convolutional neural network for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework outperforms other state-of-the-art methods in both accuracy and efficiency.",0
"Our study introduces a framework for probabilistic forecasting utilizing convolutional neural network for multiple related time series. This framework can estimate probability density in parametric and non-parametric settings. To capture the temporal dependencies of the series, we build stacked residual blocks with dilated causal convolutional nets. By incorporating representation learning, our methodology can recognize complex patterns including seasonality and holiday effects across and within series. This approach leads to more precise forecasts, particularly when historical data is scarce or unavailable. Our methodology is evaluated on various real-world datasets, including JD.com, China's largest online retailer, and is proven to outperform other state-of-the-art methods in terms of both accuracy and efficiency.",1
"The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page",0
"To achieve higher-level cognition, it is important to be able to break down complex multi-object scenes into meaningful abstractions such as objects. However, previous unsupervised approaches for learning object-oriented scene representation have been limited in scalability, which is a major obstacle to modeling real-world scenes. These approaches have typically relied on either spatial-attention or scene-mixture approaches, which are effective at either foreground object representation or background segment decomposition, but not both. To address these limitations, we propose a generative latent variable model called SPACE, which combines the strengths of both spatial-attention and scene-mixture approaches to provide a unified probabilistic modeling framework. This approach can factorize object representations for foreground objects and decompose background segments of complex morphology, making it applicable to scenes with a large number of objects without performance degradation. Our experiments on Atari and 3D-Rooms demonstrate that SPACE consistently outperforms other models such as SPAIR, IODINE, and GENESIS, and our project website provides further details on these results.",1
"Video anomaly detection is of critical practical importance to a variety of real applications because it allows human attention to be focused on events that are likely to be of interest, in spite of an otherwise overwhelming volume of video. We show that applying self-trained deep ordinal regression to video anomaly detection overcomes two key limitations of existing methods, namely, 1) being highly dependent on manually labeled normal training data; and 2) sub-optimal feature learning. By formulating a surrogate two-class ordinal regression task we devise an end-to-end trainable video anomaly detection approach that enables joint representation learning and anomaly scoring without manually labeled normal/abnormal data. Experiments on eight real-world video scenes show that our proposed method outperforms state-of-the-art methods that require no labeled training data by a substantial margin, and enables easy and accurate localization of the identified anomalies. Furthermore, we demonstrate that our method offers effective human-in-the-loop anomaly detection which can be critical in applications where anomalies are rare and the false-negative cost is high.",0
"The detection of anomalies in videos is crucial for various practical applications as it allows humans to focus on events that are likely to be significant, despite the overwhelming amount of video data. Our study demonstrates how self-trained deep ordinal regression can overcome two main limitations of existing methods: the dependence on manually labeled normal training data and sub-optimal feature learning. We propose an end-to-end trainable video anomaly detection approach that combines joint representation learning and anomaly scoring without the need for manually labeled normal/abnormal data. Our experiments on eight real-world video scenes reveal that our proposed method surpasses state-of-the-art methods that require no labeled training data by a significant margin and enables precise localization of the identified anomalies. Additionally, our approach offers effective human-in-the-loop anomaly detection, which is essential in applications where anomalies are rare and the cost of false negatives is high.",1
"The vast majority of visual animals actively control their eyes, heads, and/or bodies to direct their gaze toward different parts of their environment. In contrast, recent applications of reinforcement learning in robotic manipulation employ cameras as passive sensors. These are carefully placed to view a scene from a fixed pose. Active perception allows animals to gather the most relevant information about the world and focus their computational resources where needed. It also enables them to view objects from different distances and viewpoints, providing a rich visual experience from which to learn abstract representations of the environment. Inspired by the primate visual-motor system, we present a framework that leverages the benefits of active perception to accomplish manipulation tasks. Our agent uses viewpoint changes to localize objects, to learn state representations in a self-supervised manner, and to perform goal-directed actions. We apply our model to a simulated grasping task with a 6-DoF action space. Compared to its passive, fixed-camera counterpart, the active model achieves 8% better performance in targeted grasping. Compared to vanilla deep Q-learning algorithms, our model is at least four times more sample-efficient, highlighting the benefits of both active perception and representation learning.",0
"Most animals with vision intentionally move their eyes, heads, or bodies to look at different parts of their surroundings. However, in recent robotic manipulation using reinforcement learning, cameras are used as passive sensors that are placed in a fixed position to capture a scene. Active perception allows animals to gather the most important information about their environment, focus their attention where it's needed, and view objects from different angles and distances, resulting in a richer visual experience that can help them learn more about their environment. Our research is inspired by the visual-motor system of primates, and we propose a framework that utilizes active perception to accomplish manipulation tasks. Our model uses viewpoint changes to locate objects, learn representations of the environment in a self-supervised manner, and perform actions that are goal-oriented. We test our model in a simulated grasping task with a 6-DoF action space and find that it performs 8% better than a passive model with a fixed camera. Additionally, our model is at least four times more sample-efficient than vanilla deep Q-learning algorithms, demonstrating the benefits of active perception and representation learning.",1
"Recently, a number of competitive methods have tackled unsupervised representation learning by maximising the mutual information between the representations produced from augmentations. The resulting representations are then invariant to stochastic augmentation strategies, and can be used for downstream tasks such as clustering or classification. Yet data augmentations preserve many properties of an image and so there is potential for a suboptimal choice of representation that relies on matching easy-to-find features in the data. We demonstrate that greedy or local methods of maximising mutual information (such as stochastic gradient optimisation) discover local optima of the mutual information criterion; the resulting representations are also less-ideally suited to complex downstream tasks. Earlier work has not specifically identified or addressed this issue. We introduce deep hierarchical object grouping (DHOG) that computes a number of distinct discrete representations of images in a hierarchical order, eventually generating representations that better optimise the mutual information objective. We also find that these representations align better with the downstream task of grouping into underlying object classes. We tested DHOG on unsupervised clustering, which is a natural downstream test as the target representation is a discrete labelling of the data. We achieved new state-of-the-art results on the three main benchmarks without any prefiltering or Sobel-edge detection that proved necessary for many previous methods to work. We obtain accuracy improvements of: 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.",0
"Unsupervised representation learning has been tackled by various competitive methods that aim to maximize the mutual information between representations generated from augmentations. The resulting representations are invariant to stochastic augmentation strategies and can be utilized for downstream tasks like clustering or classification. However, there is a possibility of a suboptimal representation choice that relies on matching easily identifiable features in the data as data augmentations retain many image properties. Local methods of maximizing mutual information, such as stochastic gradient optimization, result in local optima of the mutual information criterion and less-ideal representations for complex downstream tasks. Previous work did not address this issue specifically. Our proposed method, deep hierarchical object grouping (DHOG), generates several distinct discrete representations of images in a hierarchical order, which eventually produce representations that better optimize the mutual information objective. These representations align better with the downstream task of grouping images into underlying object classes. We tested DHOG on unsupervised clustering and achieved new state-of-the-art results on the three major benchmarks without the need for prefiltering or Sobel-edge detection, which were necessary for previous methods to work. DHOG improved accuracy by 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.",1
"High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{https://github.com/HRNet}}.",0
"For vision problems that require accurate positioning, such as human pose estimation, semantic segmentation, and object detection, high-resolution representations are crucial. The current state-of-the-art frameworks use a subnetwork that connects high-to-low resolution convolutions in series, like ResNet and VGGNet, to encode the input image as a low-resolution representation before recovering the high-resolution representation. In contrast, our proposed High-Resolution Network (HRNet) maintains high-resolution representations throughout the process. This is achieved by connecting the high-to-low resolution convolution streams in parallel and repeatedly exchanging information across resolutions. The resulting representation is more semantically rich and spatially precise, which makes HRNet a stronger backbone for computer vision problems. We demonstrate the superiority of HRNet in various applications, including human pose estimation, semantic segmentation, and object detection. All the codes are available at {\url{https://github.com/HRNet}}.",1
"Recent advances in deep reinforcement learning require a large amount of training data and generally result in representations that are often over specialized to the target task. In this work, we present a methodology to study the underlying potential causes for this specialization. We use the recently proposed projection weighted Canonical Correlation Analysis (PWCCA) to measure the similarity of visual representations learned in the same environment by performing different tasks.   We then leverage our proposed methodology to examine the task dependence of visual representations learned on related but distinct embodied navigation tasks. Surprisingly, we find that slight differences in task have no measurable effect on the visual representation for both SqueezeNet and ResNet architectures. We then empirically demonstrate that visual representations learned on one task can be effectively transferred to a different task.",0
"The need for vast amounts of training data and the tendency towards specialized representations are challenges faced by modern deep reinforcement learning techniques. In this study, we introduce a methodology aimed at investigating the underlying factors that contribute to such specialization. We employ the projection weighted Canonical Correlation Analysis (PWCCA) to determine the similarity of visual representations learned from different tasks in the same environment. Our methodology enables us to scrutinize the task-specific nature of visual representations created for related but distinct navigation tasks. To our surprise, our analysis reveals that even slight differences in the task have no significant impact on the visual representation of SqueezeNet and ResNet architectures. Furthermore, we demonstrate through empirical evidence that visual representations learned for one task can be effectively utilized for a completely different task.",1
"Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.",0
"The prediction of correct poses for small individuals is challenging for bottom-up human pose estimation methods due to scale variation issues. This paper proposes a solution called HigherHRNet, a novel method that uses high-resolution feature pyramids to learn scale-aware representations. The approach ensures accurate localization of keypoints, particularly for small individuals, by employing multi-resolution supervision for training and multi-resolution aggregation for inference. The feature pyramid in HigherHRNet comprises feature map outputs from HRNet and upsampled higher-resolution outputs via a transposed convolution. HigherHRNet achieves 2.5% AP for medium persons on COCO test-dev, outperforming the previous best bottom-up method. Additionally, it sets a new state-of-the-art record on COCO test-dev (70.5% AP) without using refinement or post-processing techniques. Even on CrowdPose test, HigherHRNet surpasses all top-down methods with a 67.6% AP, indicating its robustness in crowded scenes. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.",1
