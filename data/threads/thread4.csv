"Approximating optimal policies in reinforcement learning (RL) is often necessary in many real-world scenarios, which is termed as policy optimization. By viewing the reinforcement learning from the perspective of variational inference (VI), the policy network is trained to obtain the approximate posterior of actions given the optimality criteria. However, in practice, the policy optimization may lead to suboptimal policy estimates due to the amortization gap and insufficient exploration. In this work, inspired by the previous use of Hamiltonian Monte Carlo (HMC) in VI, we propose to integrate policy optimization with HMC. As such we choose evolving actions from the base policy according to HMC, which has two benefits: i) HMC can improve the policy distribution to better approximate the posterior and hence reduces the amortization gap; ii) HMC can also guide the exploration more to the regions with higher action values, enhancing the exploration efficiency. Instead of directly applying HMC into RL, we propose a new leapfrog operator to simulate the Hamiltonian dynamics. With comprehensive empirical experiments on continuous control baselines, including MuJoCo and PyBullet Roboschool, we show that the proposed approach is a data-efficient, and an easy-to-implement improvement over previous policy optimization methods. Besides, the proposed approach can also outperform previous methods on DeepMind Control Suite which has image-based high-dimensional observation space.",0
"Policy optimization is often necessary for approximating optimal policies in reinforcement learning (RL), particularly in real-world situations. This involves training the policy network to approximate the posterior of actions given the optimality criteria, viewed from the perspective of variational inference (VI). However, suboptimal policy estimates may result from policy optimization due to the amortization gap and insufficient exploration. To address this, we propose integrating policy optimization with Hamiltonian Monte Carlo (HMC), which has two key benefits. Firstly, HMC can improve the policy distribution to better approximate the posterior, thereby reducing the amortization gap. Secondly, HMC can guide exploration more effectively towards regions with higher action values, enhancing efficiency. Rather than directly applying HMC into RL, we propose a new leapfrog operator to simulate Hamiltonian dynamics. Empirical experiments on continuous control baselines, including MuJoCo and PyBullet Roboschool, demonstrate that our proposed approach is a data-efficient and easy-to-implement improvement over previous policy optimization methods. Additionally, we show that our approach outperforms previous methods on DeepMind Control Suite, which has a high-dimensional observation space based on images.",1
"We prove that a single-layer neural network trained with the Q-learning algorithm converges in distribution to a random ordinary differential equation as the size of the model and the number of training steps become large. Analysis of the limit differential equation shows that it has a unique stationary solution which is the solution of the Bellman equation, thus giving the optimal control for the problem. In addition, we study the convergence of the limit differential equation to the stationary solution. As a by-product of our analysis, we obtain the limiting behavior of single-layer neural networks when trained on i.i.d. data with stochastic gradient descent under the widely-used Xavier initialization.",0
"Our research demonstrates that a random ordinary differential equation is reached by a single-layer neural network, which is trained using the Q-learning algorithm, as the model size and number of training steps increase. Examining the differential equation reveals that it has a singular stationary solution, which is also the solution to the Bellman equation, offering the optimal control to the problem. Furthermore, we analyze how the limit differential equation converges to the stationary solution. As a result of our study, we also obtain insight into the behavior of single-layer neural networks trained with stochastic gradient descent on i.i.d. data and Xavier initialization.",1
"This paper addresses distributional offline continuous-time reinforcement learning (DOCTR-L) with stochastic policies for high-dimensional optimal control. A soft distributional version of the classical Hamilton-Jacobi-Bellman (HJB) equation is given by a semilinear partial differential equation (PDE). This `soft HJB equation' can be learned from offline data without assuming that the latter correspond to a previous optimal or near-optimal policy. A data-driven solution of the soft HJB equation uses methods of Neural PDEs and Physics-Informed Neural Networks developed in the field of Scientific Machine Learning (SciML). The suggested approach, dubbed `SciPhy RL', thus reduces DOCTR-L to solving neural PDEs from data. Our algorithm called Deep DOCTR-L converts offline high-dimensional data into an optimal policy in one step by reducing it to supervised learning, instead of relying on value iteration or policy iteration methods. The method enables a computable approach to the quality control of obtained policies in terms of both their expected returns and uncertainties about their values.",0
"The main focus of this paper is on addressing high-dimensional optimal control through distributional offline continuous-time reinforcement learning (DOCTR-L) with stochastic policies. To achieve this, a soft distributional version of the classical Hamilton-Jacobi-Bellman (HJB) equation is proposed, which is given by a semilinear partial differential equation (PDE). Unlike previous approaches, this soft HJB equation can be learned from offline data without making any assumptions about a previous optimal or near-optimal policy. The proposed solution, known as `SciPhy RL', employs methods of Neural PDEs and Physics-Informed Neural Networks developed in the field of Scientific Machine Learning (SciML) to solve the soft HJB equation from data. By reducing DOCTR-L to solving neural PDEs from data, the suggested approach offers a computable method for obtaining optimal policies in one step, without relying on value iteration or policy iteration methods. The algorithm, called Deep DOCTR-L, also enables a computable approach to quality control of obtained policies in terms of both their expected returns and uncertainties about their values.",1
"Functional Electrical Stimulation (FES) can restore motion to a paralysed person's muscles. Yet, control stimulating many muscles to restore the practical function of entire limbs is an unsolved problem. Current neurostimulation engineering still relies on 20th Century control approaches and correspondingly shows only modest results that require daily tinkering to operate at all. Here, we present our state of the art Deep Reinforcement Learning (RL) developed for real time adaptive neurostimulation of paralysed legs for FES cycling. Core to our approach is the integration of a personalised neuromechanical component into our reinforcement learning framework that allows us to train the model efficiently without demanding extended training sessions with the patient and working out of the box. Our neuromechanical component includes merges musculoskeletal models of muscle and or tendon function and a multistate model of muscle fatigue, to render the neurostimulation responsive to a paraplegic's cyclist instantaneous muscle capacity. Our RL approach outperforms PID and Fuzzy Logic controllers in accuracy and performance. Crucially, our system learned to stimulate a cyclist's legs from ramping up speed at the start to maintaining a high cadence in steady state racing as the muscles fatigue. A part of our RL neurostimulation system has been successfully deployed at the Cybathlon 2020 bionic Olympics in the FES discipline with our paraplegic cyclist winning the Silver medal among 9 competing teams.",0
"The issue of restoring practical function to entire limbs by stimulating multiple muscles through Functional Electrical Stimulation (FES) remains unresolved, as current neurostimulation engineering relies on outdated control approaches and yields only modest results, requiring constant adjustments. However, our team has developed a state-of-the-art Deep Reinforcement Learning (RL) system for real-time adaptive neurostimulation of paraplegic legs during FES cycling. Our approach integrates a personalized neuromechanical component into the RL framework, which includes musculoskeletal models of muscle and tendon function and a multistate model of muscle fatigue. This allows our system to respond to a cyclist's instantaneous muscle capacity and outperforms traditional PID and Fuzzy Logic controllers in accuracy and performance. Our RL system has successfully stimulated a paraplegic cyclist's legs from ramping up speed to maintaining a high cadence in steady-state racing, winning the Silver medal among 9 teams at the Cybathlon 2020 bionic Olympics in the FES discipline. Our system operates efficiently without requiring extended training sessions with the patient, making it a promising solution for restoring limb functionality through FES.",1
"Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT imaging. Especially, The Low Dose CT imaging is one of possible options to protect organs of patients when conducting CT imaging. Therefore Low Dose CT imaging can be an alternative instead of Standard dose CT imaging. However Low Dose CT imaging has a fundamental issue with noises within results compared to Standard Dose CT imaging. Currently, there are lots of attempts to erase the noises. Most of methods with artificial intelligence have many parameters and unexplained layers or a kind of black-box methods. Therefore, our research has purposes related to these issues. Our approach has less parameters than usual methods by having Iterative learn-able bilateral filtering approach with Deep reinforcement learning. And we applied The Iterative learn-able filtering approach with deep reinforcement learning to sinograms and reconstructed volume domains. The method and the results of the method can be much more explainable than The other black box AI approaches. And we applied the method to Helical Cone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested this method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA dataset. The results and the performances of our approach overtake the results of the other previous methods.",0
"CBCT is a known method for conducting CT imaging, and Low Dose CT imaging is an option to protect patient organs during the process. Although Low Dose CT imaging can be an alternative to Standard Dose CT imaging, it has a fundamental issue with noise in the results compared to the latter. Many attempts have been made to eliminate the noise, most of which involve black-box methods with many parameters and unexplained layers. Our research aims to address these issues by using an Iterative Learn-able Bilateral Filtering approach with Deep Reinforcement Learning, which has fewer parameters than usual methods. We applied this approach to sinograms and reconstructed volume domains and tested it on 2 abdominal scans from the Mayo Clinic TCIA dataset. The method is more explainable than other black-box AI approaches, and the results and performance of our approach surpass those of previous methods. Additionally, we applied this method to Helical Cone Beam Computed Tomography, which is the recent trend in CBCT.",1
"The difficulty of optimal control problems has classically been characterized in terms of system properties such as minimum eigenvalues of controllability/observability gramians. We revisit these characterizations in the context of the increasing popularity of data-driven techniques like reinforcement learning (RL), and in control settings where input observations are high-dimensional images and transition dynamics are unknown. Specifically, we ask: to what extent are quantifiable control and perceptual difficulty metrics of a task predictive of the performance and sample complexity of data-driven controllers? We modulate two different types of partial observability in a cartpole ""stick-balancing"" problem -- (i) the height of one visible fixation point on the cartpole, which can be used to tune fundamental limits of performance achievable by any controller, and by (ii) the level of perception noise in the fixation point position inferred from depth or RGB images of the cartpole. In these settings, we empirically study two popular families of controllers: RL and system identification-based $H_\infty$ control, using visually estimated system state. Our results show that the fundamental limits of robust control have corresponding implications for the sample-efficiency and performance of learned perception-based controllers. Visit our project website https://jxu.ai/rl-vs-control-web for more information.",0
"In traditional optimal control problems, the challenge has been measured based on system properties such as controllability and observability gramians' minimum eigenvalues. However, with the rise of data-driven techniques like reinforcement learning (RL), and control scenarios where input observations are high-dimensional images and transition dynamics are unknown, we need to re-evaluate these characterizations. To determine whether measurable control and perceptual difficulty metrics are predictive of data-driven controller performance and sample complexity, we examine a cartpole ""stick-balancing"" problem with two different types of partial observability. We modify the height of one visible fixation point and the level of perception noise in the fixation point position inferred from depth or RGB images of the cartpole. We empirically study two popular controller families: RL and system identification-based $H_\infty$ control, using visually estimated system state. Our findings reveal that the fundamental limits of robust control have a direct impact on the performance and sample-efficiency of learned perception-based controllers. For more information, please visit our project website at https://jxu.ai/rl-vs-control-web.",1
"Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character's dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility.",0
"In order to accurately estimate 3D human motion from a single video, it is necessary to model both kinematics (body motion without physical forces) and dynamics (motion with physical forces). SimPoE is presented as an example, which combines image-based kinematic inference and physics-based dynamics modeling to achieve 3D human pose estimation. SimPoE learns a policy that takes the current-frame pose estimate and next image frame as input to control a physically-simulated character and output the next-frame pose estimate. Using a learnable kinematic pose refinement unit, the policy iteratively refines its kinematic pose estimate of the next frame using 2D keypoints. Based on the refined kinematic pose, the policy learns to compute dynamics-based control, such as joint torques, to advance the current-frame pose estimate to the next-frame pose estimate. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Additionally, a meta-control mechanism is proposed to dynamically adjust the character's dynamics parameters based on the character state for more accurate pose estimation. Experiments on large-scale motion datasets demonstrate that this approach achieves state of the art pose accuracy while ensuring physical plausibility.",1
"Model-free or learning-based control, in particular, reinforcement learning (RL), is expected to be applied for complex robotic tasks. Traditional RL requires a policy to be optimized is state-dependent, that means, the policy is a kind of feedback (FB) controllers. Due to the necessity of correct state observation in such a FB controller, it is sensitive to sensing failures. To alleviate this drawback of the FB controllers, feedback error learning integrates one of them with a feedforward (FF) controller. RL can be improved by dealing with the FB/FF policies, but to the best of our knowledge, a methodology for learning them in a unified manner has not been developed. In this paper, we propose a new optimization problem for optimizing both the FB/FF policies simultaneously. Inspired by control as inference, the optimization problem considers minimization/maximization of divergences between trajectory, predicted by the composed policy and a stochastic dynamics model, and optimal/non-optimal trajectories. By approximating the stochastic dynamics model using variational method, we naturally derive a regularization between the FB/FF policies. In numerical simulations and a robot experiment, we verified that the proposed method can stably optimize the composed policy even with the different learning law from the traditional RL. In addition, we demonstrated that the FF policy is robust to the sensing failures and can hold the optimal motion. Attached video is also uploaded on youtube: https://youtu.be/zLL4uXIRmrE",0
"The use of model-free or learning-based control, specifically reinforcement learning (RL), is anticipated to be implemented for intricate robotic tasks. The conventional RL relies on a state-dependent policy optimization, which essentially functions as feedback (FB) controllers. However, because of the requirement for accurate observation of the state in FB controllers, they are susceptible to sensing failures. Feedback error learning has been incorporated to mitigate this limitation by combining a FB controller with a feedforward (FF) controller. Although the FB/FF policies can enhance RL, a unified approach to learning them has yet to be established. In this study, we introduce a novel optimization problem that simultaneously optimizes both the FB/FF policies. Drawing inspiration from control as inference, our optimization problem minimizes/maximizes the divergences between the predicted trajectory generated by the combined policy and a stochastic dynamics model, and the optimal/non-optimal trajectories. By approximating the stochastic dynamics model using variational methods, we can naturally regulate the FB/FF policies. In numerical simulations and a robot experiment, we demonstrate that our approach can stably optimize the combined policy, even with a different learning methodology from traditional RL. We also show that the FF policy can withstand sensing failures and maintain optimal motion. A video demonstrating our work is available on YouTube at https://youtu.be/zLL4uXIRmrE.",1
"We consider a new form of reinforcement learning (RL) that is based on opportunities to directly learn the optimal control policy and a general Markov decision process (MDP) framework devised to support these opportunities. Derivations of general classes of our control-based RL methods are presented, together with forms of exploration and exploitation in learning and applying the optimal control policy over time. Our general MDP framework extends the classical Bellman operator and optimality criteria by generalizing the definition and scope of a policy for any given state. We establish the convergence and optimality-both in general and within various control paradigms (e.g., piecewise linear control policies)-of our control-based methods through this general MDP framework, including convergence of $Q$-learning within the context of our MDP framework. Our empirical results demonstrate and quantify the significant benefits of our approach.",0
"A new type of reinforcement learning (RL) is introduced in this paper, which involves the opportunity to learn the optimal control policy directly. A general Markov decision process (MDP) framework is developed to support this type of RL. The paper presents various control-based RL methods and explores the forms of exploration and exploitation in learning and applying the optimal control policy. The general MDP framework expands on the classical Bellman operator and optimality criteria by broadening the definition and scope of a policy for any given state. The paper establishes convergence and optimality of the control-based methods within various control paradigms, including piecewise linear control policies, through this general MDP framework. The convergence of Q-learning is also discussed within the context of the MDP framework. Empirical results demonstrate the benefits of this approach.",1
"In imitation learning from observation IfO, a learning agent seeks to imitate a demonstrating agent using only observations of the demonstrated behavior without access to the control signals generated by the demonstrator. Recent methods based on adversarial imitation learning have led to state-of-the-art performance on IfO problems, but they typically suffer from high sample complexity due to a reliance on data-inefficient, model-free reinforcement learning algorithms. This issue makes them impractical to deploy in real-world settings, where gathering samples can incur high costs in terms of time, energy, and risk. In this work, we hypothesize that we can incorporate ideas from model-based reinforcement learning with adversarial methods for IfO in order to increase the data efficiency of these methods without sacrificing performance. Specifically, we consider time-varying linear Gaussian policies, and propose a method that integrates the linear-quadratic regulator with path integral policy improvement into an existing adversarial IfO framework. The result is a more data-efficient IfO algorithm with better performance, which we show empirically in four simulation domains: using far fewer interactions with the environment, the proposed method exhibits similar or better performance than the existing technique.",0
"The process of imitation learning from observation (IfO) involves a learning agent mimicking a demonstrating agent's behavior using only observations without access to the demonstrator's control signals. Adversarial imitation learning methods have shown promising results but suffer from high sample complexity, making them impractical for real-world situations where sample gathering is time-consuming, costly, or risky. To overcome this limitation, this study proposes to combine ideas from model-based reinforcement learning with adversarial methods to enhance data efficiency without sacrificing performance. The method involves incorporating time-varying linear Gaussian policies and integrating linear-quadratic regulator with path integral policy improvement into an existing adversarial IfO framework. The proposed method exhibits better performance with fewer interactions with the environment in four simulation domains, demonstrating its enhanced data efficiency.",1
"We introduce a new semi-supervised, time series anomaly detection algorithm that uses deep reinforcement learning (DRL) and active learning to efficiently learn and adapt to anomalies in real-world time series data. Our model - called RLAD - makes no assumption about the underlying mechanism that produces the observation sequence and continuously adapts the detection model based on experience with anomalous patterns. In addition, it requires no manual tuning of parameters and outperforms all state-of-art methods we compare with, both unsupervised and semi-supervised, across several figures of merit. More specifically, we outperform the best unsupervised approach by a factor of 1.58 on the F1 score, with only 1% of labels and up to around 4.4x on another real-world dataset with only 0.1% of labels. We compare RLAD with seven deep-learning based algorithms across two common anomaly detection datasets with up to around 3M data points and between 0.28% to 2.65% anomalies.We outperform all of them across several important performance metrics.",0
"Our new algorithm, RLAD, utilizes both deep reinforcement learning (DRL) and active learning to effectively detect anomalies in real-world time series data. Unlike other methods, RLAD does not rely on any assumptions about the mechanism behind the observation sequence and continuously adapts its detection model based on experience with anomalous patterns. Moreover, it requires no manual parameter tuning and outperforms all existing state-of-the-art methods, both unsupervised and semi-supervised, based on various figures of merit. For instance, we achieved 1.58 times better results on the F1 score than the best unsupervised approach, with only 1% of labels, and up to around 4.4 times better results on another real-world dataset with only 0.1% of labels. We compared RLAD with seven deep-learning based algorithms using two common anomaly detection datasets consisting of up to 3M data points and between 0.28% to 2.65% anomalies, and found that RLAD outperformed all of them on several crucial performance metrics.",1
"Simulation environments are good for learning different driving tasks like lane changing, parking or handling intersections etc. in an abstract manner. However, these simulation environments often restrict themselves to operate under conservative interactions behavior amongst different vehicles. But, as we know that the real driving tasks often involves very high risk scenarios where other drivers often don't behave in the expected sense. There can be many reasons for this behavior like being tired or inexperienced. The simulation environments doesn't take this information into account while training the navigation agent. Therefore, in this study we especially focus on systematically creating these risk prone scenarios with heavy traffic and unexpected random behavior for creating better model-free learning agents. We generate multiple autonomous driving scenarios by creating new custom Markov Decision Process (MDP) environment iterations in highway-env simulation package. The behavior policy is learnt by agents trained with the help from deep reinforcement learning models. Our behavior policy is deliberated to handle collisions and risky randomized driver behavior. We train model free learning agents with supplement information of risk prone driving scenarios and compare their performance with baseline agents. Finally, we casually measure the impact of adding these perturbations in the training process to precisely account for the performance improvement attained from utilizing the learnings from these scenarios.",0
"Simulation environments are effective for acquiring a variety of driving skills, such as changing lanes, parking, and navigating intersections, in an abstract manner. However, these environments tend to limit themselves to conservative interactions between various vehicles. Real-world driving tasks, on the other hand, frequently involve high-risk scenarios where other drivers may not behave as expected due to factors such as fatigue or inexperience. Unfortunately, simulation environments do not consider such factors when training navigation agents. Thus, this study aims to systematically create risk-prone scenarios with heavy traffic and unexpected random behavior to develop better model-free learning agents. We accomplish this by generating multiple autonomous driving scenarios using custom Markov Decision Process (MDP) environment iterations in the highway-env simulation package. We train model-free learning agents using deep reinforcement learning models and supplement them with information on risk-prone driving scenarios. Our behavior policy is designed to handle collisions and risky randomized driver behavior. We compare the performance of these agents with baseline agents and casually measure the impact of adding these perturbations in the training process to precisely account for the performance improvement attained from utilizing the learnings from these scenarios.",1
"In this work, we study the problem of energy-efficient computation offloading enabled by edge computing. In the considered scenario, multiple users simultaneously compete for limited radio and edge computing resources to get offloaded tasks processed under a delay constraint, with the possibility of exploiting low power sleep modes at all network nodes. The radio resource allocation takes into account inter- and intra-cell interference, and the duty cycles of the radio and computing equipment have to be jointly optimized to minimize the overall energy consumption. To address this issue, we formulate the underlying problem as a dynamic long-term optimization. Then, based on Lyapunov stochastic optimization tools, we decouple the formulated problem into a CPU scheduling problem and a radio resource allocation problem to be solved in a per-slot basis. Whereas the first one can be optimally and efficiently solved using a fast iterative algorithm, the second one is solved using distributed multi-agent reinforcement learning due to its non-convexity and NP-hardness. The resulting framework achieves up to 96.5% performance of the optimal strategy based on exhaustive search, while drastically reducing complexity. The proposed solution also allows to increase the network's energy efficiency compared to a benchmark heuristic approach.",0
"The aim of our research is to investigate the issue of energy-efficient computation offloading through edge computing. We examine a scenario where multiple users are vying for limited radio and edge computing resources to complete their offloaded tasks within a specified time frame. Additionally, we explore the possibility of utilizing low power sleep modes at all network nodes. Our radio resource allocation accounts for both inter- and intra-cell interference, and we aim to minimize the overall energy consumption by jointly optimizing the duty cycles of the radio and computing equipment. To solve this problem, we formulate it as a dynamic long-term optimization challenge. We then use Lyapunov stochastic optimization tools to break down the problem into a CPU scheduling problem and a radio resource allocation problem to be solved each slot. The CPU scheduling issue can be solved optimally and efficiently using a fast iterative algorithm, while the radio resource allocation problem requires distributed multi-agent reinforcement learning due to its non-convexity and NP-hardness. By using our proposed solution, we achieve up to 96.5% of the optimal strategy based on exhaustive search while significantly reducing complexity. Additionally, our approach improves energy efficiency in the network compared to a benchmark heuristic approach.",1
"We present a deep reinforcement learning-based artificial intelligence agent that could provide optimized development plans given a basic description of the reservoir and rock/fluid properties with minimal computational cost. This artificial intelligence agent, comprising of a convolutional neural network, provides a mapping from a given state of the reservoir model, constraints, and economic condition to the optimal decision (drill/do not drill and well location) to be taken in the next stage of the defined sequential field development planning process. The state of the reservoir model is defined using parameters that appear in the governing equations of the two-phase flow. A feedback loop training process referred to as deep reinforcement learning is used to train an artificial intelligence agent with such a capability. The training entails millions of flow simulations with varying reservoir model descriptions (structural, rock and fluid properties), operational constraints, and economic conditions. The parameters that define the reservoir model, operational constraints, and economic conditions are randomly sampled from a defined range of applicability. Several algorithmic treatments are introduced to enhance the training of the artificial intelligence agent. After appropriate training, the artificial intelligence agent provides an optimized field development plan instantly for new scenarios within the defined range of applicability. This approach has advantages over traditional optimization algorithms (e.g., particle swarm optimization, genetic algorithm) that are generally used to find a solution for a specific field development scenario and typically not generalizable to different scenarios.",0
"Our study introduces an artificial intelligence agent based on deep reinforcement learning, which can generate development plans for reservoirs with minimal computational cost. The agent, consisting of a convolutional neural network, maps the current state of the reservoir model, constraints, and economic conditions to the optimal decision for the next stage of the sequential field development planning process, including whether to drill and where to place the well. The state of the reservoir model is represented by parameters in the governing equations of two-phase flow. The agent is trained using a feedback loop process that simulates millions of flows with varying reservoir models, operational constraints, and economic conditions, all randomly sampled from a defined range. We introduced various algorithmic approaches to improve the training of the artificial intelligence agent. After successful training, the agent can produce an optimized field development plan instantly for new scenarios within the defined range, granting it advantages over traditional optimization algorithms that are generally not generalizable to different scenarios.",1
"Reinforcement learning (RL) is attracting attention as an effective way to solve sequential optimization problems that involve high dimensional state/action space and stochastic uncertainties. Many such problems involve constraints expressed by inequality constraints. This study focuses on using RL to solve constrained optimal control problems. Most RL application studies have dealt with inequality constraints by adding soft penalty terms for violating the constraints to the reward function. However, while training neural networks to learn the value (or Q) function, one can run into computational issues caused by the sharp change in the function value at the constraint boundary due to the large penalty imposed. This difficulty during training can lead to convergence problems and ultimately lead to poor closed-loop performance. To address this issue, this study proposes a dynamic penalty (DP) approach where the penalty factor is gradually and systematically increased during training as the iteration episodes proceed. We first examine the ability of a neural network to represent a value function when uniform, linear, or DP functions are added to prevent constraint violation. The agent trained by a Deep Q Network (DQN) algorithm with the DP function approach was compared with agents with other constant penalty functions in a simple vehicle control problem. Results show that the proposed approach can improve the neural network approximation accuracy and provide faster convergence when close to a solution.",0
"Reinforcement learning (RL) has gained significant attention as a viable solution for resolving sequential optimization problems that involve stochastic uncertainties and high-dimensional state/action space. In many cases, such problems have inequality constraints. This study aims to leverage RL to solve controlled optimal problems with constraints. In previous studies, RL has addressed inequality constraints by integrating soft penalty terms into the reward function. However, this approach has limitations, as sharp changes in function value at constraint boundaries due to the significant penalty imposed can cause computational issues while training neural networks to learn the value function, impacting convergence and performance. To overcome this limitation, our study proposes a dynamic penalty (DP) approach, where the penalty factor increases systematically and gradually during training as iteration episodes proceed. We tested a neural network's ability to represent a value function using uniform, linear, or DP functions to prevent constraint violation. We then compared agents trained by the Deep Q Network (DQN) algorithm utilizing the DP function approach with those using other constant penalty functions in a simple vehicle control problem. Our results demonstrate that the proposed DP approach can improve neural network approximation accuracy and facilitate faster convergence when close to a solution.",1
"In this work we propose and analyze a new framework to learn feedback control policies that exhibit provable guarantees on the closed-loop performance and robustness to bounded (adversarial) perturbations. These policies are learned from expert demonstrations without any prior knowledge of the task, its cost function, and system dynamics. In contrast to the existing algorithms in imitation learning and inverse reinforcement learning, we use a Lipschitz-constrained loss minimization scheme to learn control policies with certified robustness. We establish robust stability of the closed-loop system under the learned control policy and derive an upper bound on its regret, which bounds the sub-optimality of the closed-loop performance with respect to the expert policy. We also derive a robustness bound for the deterioration of the closed-loop performance under bounded (adversarial) perturbations on the state measurements. Ultimately, our results suggest the existence of an underlying tradeoff between nominal closed-loop performance and adversarial robustness, and that improvements in nominal closed-loop performance can only be made at the expense of robustness to adversarial perturbations. Numerical results validate our analysis and demonstrate the effectiveness of our robust feedback policy learning framework.",0
"Our work introduces a novel framework for learning feedback control policies that offer guaranteed closed-loop performance and resistance to bounded (adversarial) disturbances. These policies are learned from expert demonstrations without any prior knowledge of the task, cost function, or system dynamics. Unlike existing algorithms in imitation learning and inverse reinforcement learning, we employ a Lipschitz-constrained loss minimization approach to develop control policies that are certified to be robust. Our analysis proves the robust stability of the closed-loop system under the learned control policy and establishes an upper bound on its sub-optimality compared to the expert policy. We also derive a robustness bound for the deterioration of the closed-loop performance under bounded (adversarial) perturbations on state measurements. Our findings reveal a tradeoff between nominal closed-loop performance and adversarial robustness, indicating that improvements in nominal performance come at the expense of robustness. Numerical results confirm our analysis and demonstrate the effectiveness of our robust feedback policy learning framework.",1
"Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior -- i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function -- is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ""what if"" outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making -- where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.",0
"It is crucial to develop understandable representations of actual decision-making processes based on observable behaviors. These behaviors include the actions taken by an expert who is maximizing an unknown reward function. This is necessary for evaluating and analyzing policies in various institutions. Our proposed approach in this paper involves learning to explain expert decisions by modeling their reward function using preferences towards hypothetical outcomes. We achieve this by integrating counterfactual reasoning into batch inverse reinforcement learning. This method allows us to define reward functions and explain expert behavior in a systematic way that adheres to the constraints of real-world decision-making, where active experimentation is often impossible, such as in healthcare. Additionally, counterfactuals can handle the off-policy nature of policy evaluation in the batch setting and can adapt to situations where expert policies depend on observation histories rather than just current states. We present illustrative experiments in both real and simulated medical environments to demonstrate the effectiveness of our approach in recovering accurate and easy-to-understand descriptions of behavior.",1
"Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal control. Recently, the finite-time analysis of Greedy-GQ has been developed under linear function approximation and Markovian sampling, and the algorithm is shown to achieve an $\epsilon$-stationary point with a sample complexity in the order of $\mathcal{O}(\epsilon^{-3})$. Such a high sample complexity is due to the large variance induced by the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algorithm applies the SVRG-based variance reduction scheme to reduce the stochastic variance of the two time-scale updates. We study the finite-time convergence of VR-Greedy-GQ under linear function approximation and Markovian sampling and show that the algorithm achieves a much smaller bias and variance error than the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an improved sample complexity that is in the order of $\mathcal{O}(\epsilon^{-2})$. We further compare the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL experiments to corroborate our theoretical findings.",0
"The value-based reinforcement learning algorithm, Greedy-GQ, is utilized for optimal control, but its high sample complexity is attributed to the large variance caused by Markovian samples. To overcome this issue, we propose a variance-reduced version of Greedy-GQ, called VR-Greedy-GQ, which implements the SVRG-based variance reduction scheme for off-policy optimal control. Our study shows that VR-Greedy-GQ achieves significantly lower bias and variance error than Greedy-GQ by reducing the stochastic variance of the two time-scale updates. Additionally, we demonstrate that VR-Greedy-GQ has an improved sample complexity of $\mathcal{O}(\epsilon^{-2})$ and compare its performance with that of Greedy-GQ in various RL experiments to support our theoretical results.",1
"Deep Reinforcement Learning (DRL) has recently achieved significant advances in various domains. However, explaining the policy of RL agents still remains an open problem due to several factors, one being the complexity of explaining neural networks decisions. Recently, a group of works have used decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been demonstrated to achieve both good performance and share the benefit of having explainable policies. In this work, we further improve the results for tree-based explainable RL in both performance and explainability. Our proposal, Cascading Decision Trees (CDTs) apply representation learning on the decision path to allow richer expressivity. Empirical results show that in both situations, where CDTs are used as policy function approximators or as imitation learners to explain black-box policies, CDTs can achieve better performances with more succinct and explainable models than SDTs. As a second contribution our study reveals limitations of explaining black-box policies via imitation learning with tree-based explainable models, due to its inherent instability.",0
"Although Deep Reinforcement Learning (DRL) has made significant strides in various areas, there remains an unresolved issue of explaining the policies of RL agents. This is due, in part, to the complexity of interpreting neural network decisions. Some researchers have attempted to address this problem by using decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been found to be effective in achieving good performance and providing explainable policies. In this study, we propose Cascading Decision Trees (CDTs) to further improve tree-based explainable RL in both performance and explainability by applying representation learning on the decision path. Our empirical findings demonstrate that CDTs outperform SDTs in both policy function approximation and imitation learning to explain black-box policies, while also producing more concise and interpretable models. Furthermore, our study highlights the limitations of using tree-based explainable models to explain black-box policies via imitation learning, due to its inherent instability.",1
"We present a novel two-layer hierarchical reinforcement learning approach equipped with a Goals Relational Graph (GRG) for tackling the partially observable goal-driven task, such as goal-driven visual navigation. Our GRG captures the underlying relations of all goals in the goal space through a Dirichlet-categorical process that facilitates: 1) the high-level network raising a sub-goal towards achieving a designated final goal; 2) the low-level network towards an optimal policy; and 3) the overall system generalizing unseen environments and goals. We evaluate our approach with two settings of partially observable goal-driven tasks -- a grid-world domain and a robotic object search task. Our experimental results show that our approach exhibits superior generalization performance on both unseen environments and new goals.",0
"Our innovative approach to hierarchical reinforcement learning features a Goals Relational Graph (GRG) and is designed to address partially observable goal-oriented tasks, such as visual navigation. The GRG is capable of capturing the underlying relationships among all goals in the goal space using a Dirichlet-categorical process. This enables our high-level network to elevate a sub-goal towards achieving a designated final goal, our low-level network to develop an optimal policy, and our overall system to generalize to unseen environments and goals. We assessed our approach using two partially observable goal-driven settings: a grid-world domain and a robotic object search task. Our experimental findings demonstrate that our approach outperforms existing methods in terms of generalization performance for unseen environments and new goals.",1
"By planning through a learned dynamics model, model-based reinforcement learning (MBRL) offers the prospect of good performance with little environment interaction. However, it is common in practice for the learned model to be inaccurate, impairing planning and leading to poor performance. This paper aims to improve planning with an importance sampling framework that accounts and corrects for discrepancy between the true and learned dynamics. This framework also motivates an alternative objective for fitting the dynamics model: to minimize the variance of value estimation during planning. We derive and implement this objective, which encourages better prediction on trajectories with larger returns. We observe empirically that our approach improves the performance of current MBRL algorithms on two stochastic control problems, and provide a theoretical basis for our method.",0
"Model-based reinforcement learning (MBRL) allows for effective planning without extensive interaction with the environment, but inaccuracies in the learned dynamics model can hinder its performance. This paper proposes an importance sampling framework that addresses the discrepancy between the true and learned dynamics, and introduces a new objective for fitting the dynamics model that minimizes the variance of value estimation during planning. By implementing this objective, the model can better predict trajectories with larger returns. Empirical observations demonstrate that this approach enhances the performance of current MBRL algorithms on two stochastic control problems, and the method is supported by theoretical evidence.",1
"Effective planning in model-based reinforcement learning (MBRL) and model-predictive control (MPC) relies on the accuracy of the learned dynamics model. In many instances of MBRL and MPC, this model is assumed to be stationary and is periodically re-trained from scratch on state transition experience collected from the beginning of environment interactions. This implies that the time required to train the dynamics model - and the pause required between plan executions - grows linearly with the size of the collected experience. We argue that this is too slow for lifelong robot learning and propose HyperCRL, a method that continually learns the encountered dynamics in a sequence of tasks using task-conditional hypernetworks. Our method has three main attributes: first, it includes dynamics learning sessions that do not revisit training data from previous tasks, so it only needs to store the most recent fixed-size portion of the state transition experience; second, it uses fixed-capacity hypernetworks to represent non-stationary and task-aware dynamics; third, it outperforms existing continual learning alternatives that rely on fixed-capacity networks, and does competitively with baselines that remember an ever increasing coreset of past experience. We show that HyperCRL is effective in continual model-based reinforcement learning in robot locomotion and manipulation scenarios, such as tasks involving pushing and door opening. Our project website with videos is at this link https://rvl.cs.toronto.edu/blog/2020/hypercrl",0
"Model-based reinforcement learning (MBRL) and model-predictive control (MPC) require precise planning that relies on an accurate learned dynamics model. Typically, these models are assumed to be static and retrained periodically from the beginning of environment interactions, which leads to longer training times and pauses between plan executions as the size of the collected experience grows. This approach is not suitable for lifelong robot learning. To address this issue, we propose HyperCRL, a method that uses task-conditional hypernetworks to continually learn encountered dynamics in a sequence of tasks. HyperCRL has three key features: it includes dynamics learning sessions that do not revisit previous training data, uses fixed-capacity hypernetworks that represent non-stationary and task-aware dynamics, and outperforms other continual learning alternatives. Our experiments show that HyperCRL is effective in robot locomotion and manipulation tasks and is available on our project website with accompanying videos.",1
"Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability. Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability. We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House. Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40%-60% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.",0
"The growth of wearable technologies has made it possible to monitor complex human context, which allows for the development of human-in-the-loop IoT systems that can adapt to the human and environment state on their own. However, the challenge of designing personalized IoT applications arises from human variability. This variability can come from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change their behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To address these challenges, we propose FaiR-IoT, a general framework for adaptive and fairness-aware human-in-the-loop IoT applications that uses reinforcement learning. FaiR-IoT includes three levels of reinforcement learning agents that interact to continuously learn human preferences, maximize system performance and fairness, and account for intra-, inter- and multi-human variability. We tested FaiR-IoT on two applications, Human-in-the-Loop Automotive Advanced Driver Assistance Systems and Human-in-the-Loop Smart House, and found that it provides a personalized experience while enhancing system performance by 40%-60% compared to non-personalized systems and enhancing the fairness of multi-human systems by 1.5 orders of magnitude.",1
"The inputs and preferences of human users are important considerations in situations where these users interact with autonomous cyber or cyber-physical systems. In these scenarios, one is often interested in aligning behaviors of the system with the preferences of one or more human users. Cumulative prospect theory (CPT) is a paradigm that has been empirically shown to model a tendency of humans to view gains and losses differently. In this paper, we consider a setting where an autonomous agent has to learn behaviors in an unknown environment. In traditional reinforcement learning, these behaviors are learned through repeated interactions with the environment by optimizing an expected utility. In order to endow the agent with the ability to closely mimic the behavior of human users, we optimize a CPT-based cost. We introduce the notion of the CPT-value of an action taken in a state, and establish the convergence of an iterative dynamic programming-based approach to estimate this quantity. We develop two algorithms to enable agents to learn policies to optimize the CPT-vale, and evaluate these algorithms in environments where a target state has to be reached while avoiding obstacles. We demonstrate that behaviors of the agent learned using these algorithms are better aligned with that of a human user who might be placed in the same environment, and is significantly improved over a baseline that optimizes an expected utility.",0
"In situations where human users interact with autonomous cyber or cyber-physical systems, it is important to consider their inputs and preferences. The goal is often to align the system's behaviors with those of the human user(s). Cumulative prospect theory (CPT) is a proven paradigm that models how humans view gains and losses differently. This paper presents a scenario where an autonomous agent needs to learn behaviors in an unknown environment. In traditional reinforcement learning, the agent learns behaviors by repeatedly interacting with the environment to optimize expected utility. To enable the agent to closely mimic human behavior, a CPT-based cost is optimized. The concept of CPT-value of an action taken in a state is introduced, and an iterative dynamic programming-based approach is established to estimate this. Two algorithms are developed to enable agents to learn policies to optimize the CPT-value, and these algorithms are evaluated in environments where a target state must be reached while avoiding obstacles. The results demonstrate that the behaviors learned using these algorithms are more aligned with those of a human user, significantly improving over a baseline that optimizes expected utility.",1
"Multi-agent reinforcement learning involves multiple agents interacting with each other and a shared environment to complete tasks. When rewards provided by the environment are sparse, agents may not receive immediate feedback on the quality of actions that they take, thereby affecting learning of policies. In this paper, we propose a method called Shaping Advice in deep Multi-agent reinforcement learning (SAM) to augment the reward signal from the environment with an additional reward termed shaping advice. The shaping advice is given by a difference of potential functions at consecutive time-steps. Each potential function is a function of observations and actions of the agents. The shaping advice needs to be specified only once at the start of training, and can be easily provided by non-experts. We show through theoretical analyses and experimental validation that shaping advice provided by SAM does not distract agents from completing tasks specified by the environment reward. Theoretically, we prove that convergence of policy gradients and value functions when using SAM implies convergence of these quantities in the absence of SAM. Experimentally, we evaluate SAM on three tasks in the multi-agent Particle World environment that have sparse rewards. We observe that using SAM results in agents learning policies to complete tasks faster, and obtain higher rewards than: i) using sparse rewards alone; ii) a state-of-the-art reward redistribution method.",0
"The process of multi-agent reinforcement learning involves multiple agents collaborating within a shared environment to accomplish tasks. However, in situations where the environment provides minimal rewards, agents may struggle to receive immediate feedback on their actions, which can negatively impact their policy learning. To address this issue, we introduce a new approach called Shaping Advice in deep Multi-agent reinforcement learning (SAM). SAM enhances the reward signal from the environment by adding an extra reward known as shaping advice, which is obtained from the difference of potential functions between consecutive time-steps. These potential functions are based on the agents' actions and observations. SAM requires only one-time specification of the shaping advice, which can be effortlessly provided by non-experts. We demonstrate through theoretical analysis and experimentation that SAM's shaping advice does not hinder agents from completing tasks set by the environment reward. Our theoretical analysis shows that the convergence of policy gradients and value functions using SAM implies convergence of these quantities even without SAM. In our experiments, we evaluate SAM on three tasks within the multi-agent Particle World environment, which have sparse rewards. Our results indicate that SAM enables agents to learn policies faster and achieve higher rewards compared to using sparse rewards alone or a state-of-the-art reward redistribution method.",1
"Machine learning models are often trained on data from one distribution and deployed on others. So it becomes important to design models that are robust to distribution shifts. Most of the existing work focuses on optimizing for either adversarial shifts or interventional shifts. Adversarial methods lack expressivity in representing plausible shifts as they consider shifts to joint distributions in the data. Interventional methods allow more expressivity but provide robustness to unbounded shifts, resulting in overly conservative models. In this work, we combine the complementary strengths of the two approaches and propose a new formulation, RISe, for designing robust models against a set of distribution shifts that are at the intersection of adversarial and interventional shifts. We employ the distributionally robust optimization framework to optimize the resulting objective in both supervised and reinforcement learning settings. Extensive experimentation with synthetic and real world datasets from healthcare demonstrate the efficacy of the proposed approach.",0
"Training machine learning models on one set of data and deploying them on others requires designing models that can withstand shifts in distribution. Current approaches focus on optimizing for either adversarial or interventional shifts, but both have limitations. Adversarial methods lack the ability to represent plausible shifts, while interventional methods are too conservative and can only handle bounded shifts. In this study, we propose a new approach called RISe that combines the strengths of both methods to design robust models against shifts at the intersection of adversarial and interventional. We use the distributionally robust optimization framework to optimize the objective in both supervised and reinforcement learning settings. Our experiments with real-world healthcare datasets demonstrate the effectiveness of our approach.",1
"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",0
"The goal of Multi-Task Learning (MTL) is to enhance the generalization performance of related tasks by utilizing valuable information from multiple tasks. This paper provides a comprehensive survey of MTL, covering algorithmic modeling, applications, and theoretical analyses. The algorithmic modeling section defines MTL and categorizes various MTL algorithms into five types: feature learning, low-rank, task clustering, task relation learning, and decomposition approaches. The characteristics of each approach are also discussed. To further improve the learning task's performance, MTL can be combined with other paradigms such as semi-supervised, active, unsupervised, reinforcement, multi-view, and graphical models. The paper also reviews online, parallel, and distributed MTL models as well as dimensionality reduction and feature hashing to address the computation and storage challenges of large task numbers or high-dimensional data. Many real-world applications benefit from MTL, and representative works are reviewed. Finally, theoretical analyses and future directions for MTL are presented.",1
"General game testing relies on the use of human play testers, play test scripting, and prior knowledge of areas of interest to produce relevant test data. Using deep reinforcement learning (DRL), we introduce a self-learning mechanism to the game testing framework. With DRL, the framework is capable of exploring and/or exploiting the game mechanics based on a user-defined, reinforcing reward signal. As a result, test coverage is increased and unintended game play mechanics, exploits and bugs are discovered in a multitude of game types. In this paper, we show that DRL can be used to increase test coverage, find exploits, test map difficulty, and to detect common problems that arise in the testing of first-person shooter (FPS) games.",0
"The conventional method of game testing involves human play testers, play test scripting, and existing knowledge of relevant areas to gather suitable test data. Our game testing framework incorporates deep reinforcement learning (DRL) to introduce a self-learning mechanism. By using DRL, the framework can navigate and manipulate game mechanics using a reinforcing reward signal defined by the user. This approach results in a wider test coverage and the discovery of unintended game play mechanics, exploits, and bugs in various game genres. Our study demonstrates that DRL can enhance test coverage, identify exploits, assess map difficulty, and identify common issues that arise in testing first-person shooter (FPS) games.",1
"Assigning resources in business processes execution is a repetitive task that can be effectively automated. However, different automation methods may give varying results that may not be optimal. Proper resource allocation is crucial as it may lead to significant cost reductions or increased effectiveness that results in increased revenues.   In this work, we first propose a novel representation that allows modeling of a multi-process environment with different process-based rewards. These processes can share resources that differ in their eligibility. Then, we use double deep reinforcement learning to look for optimal resource allocation policy. We compare those results with two popular strategies that are widely used in the industry. Learning optimal policy through reinforcement learning requires frequent interactions with the environment, so we also designed and developed a simulation engine that can mimic real-world processes.   The results obtained are promising. Deep reinforcement learning based resource allocation achieved significantly better results compared to two commonly used techniques.",0
"Automating the task of assigning resources in business processes execution can yield desirable results, but different automation methods may not produce optimal outcomes. Resource allocation is pivotal in terms of reducing costs and enhancing effectiveness, leading to increased revenues. This study proposes a new representation for modeling a multi-process environment with various process-based rewards, where resources can have different eligibility. The study then uses double deep reinforcement learning to determine the best resource allocation policy. To compare the results, two commonly used industry strategies are also examined. The reinforcement learning approach necessitates frequent interactions with the environment; therefore, a simulation engine was developed to mimic real-world processes. The findings indicate that deep reinforcement learning-based resource allocation produced significantly better outcomes than the two popular strategies.",1
"Reinforcement learning has achieved remarkable performance in a wide range of tasks these days. Nevertheless, some unsolved problems limit its applications in real-world control. One of them is model misspecification, a situation where an agent is trained and deployed in environments with different transition dynamics. We propose an novel framework that utilize history trajectory and Partial Observable Markov Decision Process Modeling to deal with this dilemma. Additionally, we put forward an efficient adversarial attack method to assist robust training. Our experiments in four gym domains validate the effectiveness of our framework.",0
"In recent times, reinforcement learning has displayed impressive results in various tasks. However, its practical application in real-world control is restricted by a few unresolved issues. One such challenge is model misspecification, which arises when an agent is trained and deployed in environments with varying transition dynamics. To address this problem, we suggest a new approach that leverages history trajectory and Partial Observable Markov Decision Process (POMDP) modeling. Moreover, we introduce an effective adversarial attack technique to facilitate strong training. Our framework's efficacy is confirmed through experiments conducted in four gym domains.",1
"The NeurIPS 2020 Procgen Competition was designed as a centralized benchmark with clearly defined tasks for measuring Sample Efficiency and Generalization in Reinforcement Learning. Generalization remains one of the most fundamental challenges in deep reinforcement learning, and yet we do not have enough benchmarks to measure the progress of the community on Generalization in Reinforcement Learning. We present the design of a centralized benchmark for Reinforcement Learning which can help measure Sample Efficiency and Generalization in Reinforcement Learning by doing end to end evaluation of the training and rollout phases of thousands of user submitted code bases in a scalable way. We designed the benchmark on top of the already existing Procgen Benchmark by defining clear tasks and standardizing the end to end evaluation setups. The design aims to maximize the flexibility available for researchers who wish to design future iterations of such benchmarks, and yet imposes necessary practical constraints to allow for a system like this to scale. This paper presents the competition setup and the details and analysis of the top solutions identified through this setup in context of 2020 iteration of the competition at NeurIPS.",0
"The NeurIPS 2020 Procgen Competition was created to provide a centralized benchmark for measuring Sample Efficiency and Generalization in Reinforcement Learning using defined tasks. Despite Generalization being a major hurdle in deep reinforcement learning, there are not enough benchmarks to track community progress. Our centralized benchmark for Reinforcement Learning evaluates the training and rollout phases of numerous user submitted code bases in a scalable manner, allowing for end-to-end evaluation. We built this benchmark on top of the Procgen Benchmark, which we standardized and defined clear tasks for. Our design maximizes flexibility for future iterations of such benchmarks while imposing practical constraints to ensure scalability. This paper presents the competition setup and details the top solutions identified in the 2020 NeurIPS iteration of the competition.",1
"In this paper, we explore a multi-agent reinforcement learning approach to address the design problem of communication and control strategies for multi-agent cooperative transport. Typical end-to-end deep neural network policies may be insufficient for covering communication and control; these methods cannot decide the timing of communication and can only work with fixed-rate communications. Therefore, our framework exploits event-triggered architecture, namely, a feedback controller that computes the communication input and a triggering mechanism that determines when the input has to be updated again. Such event-triggered control policies are efficiently optimized using a multi-agent deep deterministic policy gradient. We confirmed that our approach could balance the transport performance and communication savings through numerical simulations.",0
"The objective of this paper is to investigate the use of a multi-agent reinforcement learning method to tackle the problem of devising communication and control strategies for cooperative transport among multiple agents. Conventional neural network policies that operate end-to-end may not be sufficient to handle communication and control as they cannot determine the timing of communication and can only function with fixed-rate communications. Therefore, our proposed framework utilizes an event-triggered architecture, which consists of a feedback controller that computes the communication input and a triggering mechanism that decides when the input needs updating. These event-triggered control policies are effectively optimized using a multi-agent deep deterministic policy gradient. Our numerical simulations show that our approach can achieve a balance between transport performance and communication savings.",1
"Point cloud registration is a common step in many 3D computer vision tasks such as object pose estimation, where a 3D model is aligned to an observation. Classical registration methods generalize well to novel domains but fail when given a noisy observation or a bad initialization. Learning-based methods, in contrast, are more robust but lack in generalization capacity. We propose to consider iterative point cloud registration as a reinforcement learning task and, to this end, present a novel registration agent (ReAgent). We employ imitation learning to initialize its discrete registration policy based on a steady expert policy. Integration with policy optimization, based on our proposed alignment reward, further improves the agent's registration performance. We compare our approach to classical and learning-based registration methods on both ModelNet40 (synthetic) and ScanObjectNN (real data) and show that our ReAgent achieves state-of-the-art accuracy. The lightweight architecture of the agent, moreover, enables reduced inference time as compared to related approaches. In addition, we apply our method to the object pose estimation task on real data (LINEMOD), outperforming state-of-the-art pose refinement approaches.",0
"Many 3D computer vision tasks, including object pose estimation, require point cloud registration. While classical registration methods work well for new domains, they struggle with noisy observations or poor initialization. In contrast, learning-based methods are more robust but lack generalization ability. To address this, we propose treating iterative point cloud registration as a reinforcement learning task using our new registration agent, ReAgent. We use imitation learning to initialize its discrete registration policy based on an expert policy and optimize it with our alignment reward. We compare our approach to classical and learning-based methods on synthetic and real data and show that ReAgent achieves state-of-the-art accuracy with a lightweight architecture that reduces inference time. We also apply our method to real data for object pose estimation and outperform state-of-the-art approaches.",1
"Modern data sources are typically of large scale and multi-modal natures, and acquired on irregular domains, which poses serious challenges to traditional deep learning models. These issues are partially mitigated by either extending existing deep learning algorithms to irregular domains through graphs, or by employing tensor methods to alleviate the computational bottlenecks imposed by the Curse of Dimensionality. To simultaneously resolve both these issues, we introduce a novel Multi-Graph Tensor Network (MGTN) framework, which leverages on the desirable properties of graphs, tensors and neural networks in a physically meaningful and compact manner. This equips MGTNs with the ability to exploit local information in irregular data sources at a drastically reduced parameter complexity, and over a range of learning paradigms such as regression, classification and reinforcement learning. The benefits of the MGTN framework, especially its ability to avoid overfitting through the inherent low-rank regularization properties of tensor networks, are demonstrated through its superior performance against competing models in the individual tensor, graph, and neural network domains.",0
"Deep learning models face significant challenges when dealing with large-scale and multi-modal data sources obtained from irregular domains. To address these issues, existing deep learning algorithms can be extended to irregular domains using graphs, or tensor methods can be employed to overcome computational bottlenecks caused by the Curse of Dimensionality. To tackle both issues simultaneously, a new framework called Multi-Graph Tensor Network (MGTN) has been introduced. MGTN combines the desirable properties of graphs, tensors, and neural networks to analyze irregular data sources more efficiently and with lower parameter complexity. MGTN can be used in various learning paradigms, including regression, classification, and reinforcement learning, to exploit local information in data sources while avoiding overfitting through low-rank regularization properties. The MGTN framework outperforms other models in tensor, graph, and neural network domains.",1
"An increasing amount of studies have investigated the decision-making process of VQA models. Many of these studies focus on the reason behind the correct answer chosen by a model. Yet, the reason why the distracting answer chose by a model has rarely been studied. To this end, we introduce a novel task called \textit{textual Distractors Generation for VQA} (DG-VQA) that explaining the decision boundaries of existing VQA models. The goal of DG-VQA is to generate the most confusing set of textual distractors in multi-choice VQA tasks which expose the vulnerability of existing models (i.e. to generate distractors that lure existing models to fail). We show that DG-VQA can be formulated as a Markov Decision Process, and present a reinforcement learning solution to come up with distractors in an unsupervised manner. The solution addresses the lack of large annotated corpus issues in previous distractor generation methods. Our proposed model receives reward signals from fully-trained multi-choice VQA models and updates its parameters via policy gradient. The empirical results show that the generated textual distractors can successfully attack several popular VQA models with an average $20\%$ accuracy drop from $64\%$. Furthermore, we conduct adversarial training to improve the robustness of VQA models by incorporating the generated distractors. Empirical results validate the effectiveness of adversarial training by showing a performance improvement of $27\%$ for the multi-choice VQA task.",0
"Numerous studies have investigated the decision-making process of VQA models, with many focusing on the reason behind the correct answer chosen by a model. However, the reason for the distracting answer chosen by a model has been rarely studied. To address this gap, we present a new task called ""textual Distractors Generation for VQA"" (DG-VQA) which aims to expose the decision boundaries of existing VQA models. DG-VQA generates the most confusing set of textual distractors in multi-choice VQA tasks to lure existing models to fail. We propose a reinforcement learning solution to formulate DG-VQA as a Markov Decision Process, which addresses the lack of large annotated corpus issues in previous distractor generation methods. Our model receives reward signals from fully-trained multi-choice VQA models and updates its parameters via policy gradient. Our empirical results show that the generated textual distractors can successfully attack several popular VQA models, resulting in an average 20% accuracy drop from 64%. Additionally, we conduct adversarial training to improve the robustness of VQA models by incorporating the generated distractors. Empirical results validate the effectiveness of adversarial training by showing a performance improvement of 27% for the multi-choice VQA task.",1
"Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitation Learning framework, validating that the agents can benefit from each other without external supervision.",0
"The use of imitation learning as a means to enhance the efficiency of reinforcement learning is a common approach that involves leveraging expert demonstrations. However, in many real-life scenarios, accessing such demonstrations can be costly or impossible. To address this challenge, this paper introduces a new learning framework, known as Co-Imitation Learning (CoIL), that enables agents to make use of their own past successful experiences instead of relying on expert demonstrations. CoIL works by training two agents to alternate between exploring the environment and learning from the other agent's experience. While this approach may yield valuable or misleading experiences, CoIL proposes to estimate the potential utility of each experience by evaluating the expected gain of the value function. The agents can then selectively imitate each other by emphasizing the more useful experiences while filtering out noisy ones. The experimental results demonstrate the superiority of the CoIL framework in various tasks, validating the effectiveness of agents learning from each other without external supervision.",1
"Driving in a dynamic, multi-agent, and complex urban environment is a difficult task requiring a complex decision-making policy. The learning of such a policy requires a state representation that can encode the entire environment. Mid-level representations that encode a vehicle's environment as images have become a popular choice. Still, they are quite high-dimensional, limiting their use in data-hungry approaches such as reinforcement learning. In this article, we propose to learn a low-dimensional and rich latent representation of the environment by leveraging the knowledge of relevant semantic factors. To do this, we train an encoder-decoder deep neural network to predict multiple application-relevant factors such as the trajectories of other agents and the ego car. We also propose a hazard signal in addition to the learned latent representation as input to a down-stream policy. We demonstrate that using the multi-head encoder-decoder neural network results in a more informative representation than a standard single-head model. In particular, the proposed representation learning and the hazard signal help reinforcement learning to learn faster, with increased performance and less data than baseline methods.",0
"Driving in a complex urban environment with multiple agents requires a decision-making policy that is difficult to learn. To encode the entire environment, mid-level representations that use images have become popular. However, these representations are high-dimensional, limiting their use in data-hungry approaches like reinforcement learning. To address this, we propose a low-dimensional and rich latent representation of the environment, leveraging relevant semantic factors. We train an encoder-decoder neural network to predict multiple application-relevant factors like trajectories. We also propose a hazard signal as input to a policy. Our approach results in a more informative representation than a standard single-head model, helping reinforcement learning to learn faster with less data.",1
"Deep regression trackers are among the fastest tracking algorithms available, and therefore suitable for real-time robotic applications. However, their accuracy is inadequate in many domains due to distribution shift and overfitting. In this paper we overcome such limitations by presenting the first methodology for domain adaption of such a class of trackers. To reduce the labeling effort we propose a weakly-supervised adaptation strategy, in which reinforcement learning is used to express weak supervision as a scalar application-dependent and temporally-delayed feedback. At the same time, knowledge distillation is employed to guarantee learning stability and to compress and transfer knowledge from more powerful but slower trackers. Extensive experiments on five different robotic vision domains demonstrate the relevance of our methodology. Real-time speed is achieved on embedded devices and on machines without GPUs, while accuracy reaches significant results.",0
"One of the quickest tracking algorithms available for real-time robotic applications are deep regression trackers. However, their accuracy can be insufficient due to overfitting and distribution shift in certain domains. To address this issue, we present the first methodology for domain adaptation of this tracker class. Our weakly-supervised adaptation strategy reduces labeling effort by using reinforcement learning to provide scalar feedback that is application-dependent and temporally-delayed. Additionally, knowledge distillation is employed to ensure learning stability and transfer knowledge from more powerful but slower trackers. Our experiments in five different robotic vision domains demonstrate the relevance of our methodology, achieving real-time speed on embedded devices and machines without GPUs while also achieving significant accuracy.",1
"The visuomotor system of any animal is critical for its survival, and the development of a complex one within humans is large factor in our success as a species on Earth. This system is an essential part of our ability to adapt to our environment. We use this system continuously throughout the day, when picking something up, or walking around while avoiding bumping into objects. Equipping robots with such capabilities will help produce more intelligent locomotion with the ability to more easily understand their surroundings and to move safely. In particular, such capabilities are desirable for traversing the lunar surface, as it is full of hazardous obstacles, such as rocks. These obstacles need to be identified and avoided in real time. This paper seeks to demonstrate the development of a visuomotor system within a robot for navigation and obstacle avoidance, with complex rock shaped objects representing hazards. Our approach uses deep reinforcement learning with only image data. In this paper, we compare the results from several neural network architectures and a preprocessing methodology which includes producing a segmented image and downsampling.",0
"The survival of any animal heavily relies on its visuomotor system, and the complexity of this system in humans has greatly contributed to our success as a species. This system plays a crucial role in our ability to adapt to our surroundings and is consistently used throughout the day; for instance, when we pick up objects or navigate our surroundings without colliding with obstacles. Enhancing robots with similar capabilities would allow for more intelligent and safer movement by enabling them to understand their surroundings better. This is particularly significant for traversing the moon's surface, which is hazardous due to its rocky terrain. Identifying and avoiding obstacles in real-time is imperative. This paper aims to showcase a robot's visuomotor system development for navigation and obstacle avoidance, with the hazards represented by complex rock-shaped objects. Our methodology utilizes deep reinforcement learning, relying solely on image data. We compare the outcomes of various neural network architectures and a preprocessing method, which involves segmenting images and downsampling them.",1
"We propose a selective learning method using meta-learning and deep reinforcement learning for medical image interpretation in the setting of limited labeling resources. Our method, MedSelect, consists of a trainable deep learning selector that uses image embeddings obtained from contrastive pretraining for determining which images to label, and a non-parametric selector that uses cosine similarity to classify unseen images. We demonstrate that MedSelect learns an effective selection strategy outperforming baseline selection strategies across seen and unseen medical conditions for chest X-ray interpretation. We also perform an analysis of the selections performed by MedSelect comparing the distribution of latent embeddings and clinical features, and find significant differences compared to the strongest performing baseline. We believe that our method may be broadly applicable across medical imaging settings where labels are expensive to acquire.",0
"In the context of limited labeling resources for medical image interpretation, we propose a method called MedSelect that utilizes meta-learning and deep reinforcement learning to facilitate selective learning. The MedSelect approach involves a trainable deep learning selector which leverages image embeddings obtained from contrastive pretraining to determine which images to label, as well as a non-parametric selector that uses cosine similarity to classify unseen images. Our results demonstrate that MedSelect outperforms baseline selection strategies for chest X-ray interpretation across both seen and unseen medical conditions. To further evaluate our approach, we conducted an analysis of the selections made by MedSelect and found significant differences in the distribution of latent embeddings and clinical features compared to the strongest performing baseline. We believe that MedSelect can be applied in a variety of medical imaging contexts where obtaining labels is costly.",1
"A fundamental problem in computer animation is that of realizing purposeful and realistic human movement given a sufficiently-rich set of motion capture clips. We learn data-driven generative models of human movement using autoregressive conditional variational autoencoders, or Motion VAEs. The latent variables of the learned autoencoder define the action space for the movement and thereby govern its evolution over time. Planning or control algorithms can then use this action space to generate desired motions. In particular, we use deep reinforcement learning to learn controllers that achieve goal-directed movements. We demonstrate the effectiveness of the approach on multiple tasks. We further evaluate system-design choices and describe the current limitations of Motion VAEs.",0
"One of the main challenges in computer animation is creating natural and purposeful human movements using an extensive collection of motion capture clips. To tackle this issue, we employ autoregressive conditional variational autoencoders, known as Motion VAEs, to generate data-driven models of human motion. These models utilize latent variables to define the motion's action space and to control its evolution over time, allowing planning and control algorithms to generate desired movements. Our method employs deep reinforcement learning to develop controllers capable of achieving goal-directed movements, which we demonstrate on various tasks. Additionally, we analyze and discuss design choices and the current limitations of Motion VAEs.",1
"Learning from demonstrations has made great progress over the past few years. However, it is generally data hungry and task specific. In other words, it requires a large amount of data to train a decent model on a particular task, and the model often fails to generalize to new tasks that have a different distribution. In practice, demonstrations from new tasks will be continuously observed and the data might be unlabeled or only partially labeled. Therefore, it is desirable for the trained model to adapt to new tasks that have limited data samples available. In this work, we build an adaptable imitation learning model based on the integration of Meta-learning and Adversarial Inverse Reinforcement Learning (Meta-AIRL). We exploit the adversarial learning and inverse reinforcement learning mechanisms to learn policies and reward functions simultaneously from available training tasks and then adapt them to new tasks with the meta-learning framework. Simulation results show that the adapted policy trained with Meta-AIRL can effectively learn from limited number of demonstrations, and quickly reach the performance comparable to that of the experts on unseen tasks.",0
"Over recent years, significant progress has been made in the field of learning from demonstrations. However, this approach typically requires a large amount of task-specific data and struggles to generalize to new tasks with different distributions. In practical scenarios, new task demonstrations may be continuously observed with limited or no labeling. Thus, it is crucial for the trained model to adapt to new tasks with limited data. To address this issue, we present an adaptable imitation learning model, Meta-AIRL, which integrates Meta-learning and Adversarial Inverse Reinforcement Learning. Meta-AIRL utilizes adversarial learning and inverse reinforcement learning mechanisms to simultaneously learn policies and reward functions from available training tasks and adapt them to new tasks with the meta-learning framework. Our simulation results demonstrate that the adapted policy trained with Meta-AIRL can effectively learn from a limited number of demonstrations and quickly achieve performance comparable to that of experts on unseen tasks.",1
"We revisit offline reinforcement learning on episodic time-homogeneous tabular Markov Decision Processes with $S$ states, $A$ actions and planning horizon $H$. Given the collected $N$ episodes data with minimum cumulative reaching probability $d_m$, we obtain the first set of nearly $H$-free sample complexity bounds for evaluation and planning using the empirical MDPs: 1.For the offline evaluation, we obtain an $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}} \right)$ error rate, which matches the lower bound and does not have additional dependency on $\poly\left(S,A\right)$ in higher-order term, that is different from previous works~\citep{yin2020near,yin2020asymptotically}. 2.For the offline policy optimization, we obtain an $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}} + \frac{S}{Nd_m}\right)$ error rate, improving upon the best known result by \cite{cui2020plug}, which has additional $H$ and $S$ factors in the main term. Furthermore, this bound approaches the $\Omega\left(\sqrt{\frac{1}{Nd_m}}\right)$ lower bound up to logarithmic factors and a high-order term. To the best of our knowledge, these are the first set of nearly horizon-free bounds in offline reinforcement learning.",0
"We are examining offline reinforcement learning on tabular Markov Decision Processes that are time-homogeneous and episodic in nature, with $S$ states, $A$ actions, and a planning horizon of $H$. Using $N$ episodes of collected data with a minimum cumulative reaching probability of $d_m$, we have derived the first set of sample complexity bounds for evaluation and planning using empirical MDPs. Our results are as follows: 1) For offline evaluation, we have obtained an error rate of $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}} \right)$ which matches the lower bound and does not have any additional dependency on $\poly\left(S,A\right)$ in higher-order term, unlike previous works~\citep{yin2020near,yin2020asymptotically}. 2) For offline policy optimization, we have obtained an error rate of $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}} + \frac{S}{Nd_m}\right)$, which improves upon the best known result by \cite{cui2020plug} and does not have any additional $H$ and $S$ factors in the main term. Moreover, this bound approaches the lower bound of $\Omega\left(\sqrt{\frac{1}{Nd_m}}\right)$ up to logarithmic factors and a high-order term. These are the first set of nearly horizon-free bounds in offline reinforcement learning, to the best of our knowledge.",1
"Meta-learning is a branch of machine learning which aims to quickly adapt models, such as neural networks, to perform new tasks by learning an underlying structure across related tasks. In essence, models are being trained to learn new tasks effectively rather than master a single task. Meta-learning is appealing for process control applications because the perturbations to a process required to train an AI controller can be costly and unsafe. Additionally, the dynamics and control objectives are similar across many different processes, so it is feasible to create a generalizable controller through meta-learning capable of quickly adapting to different systems. In this work, we construct a deep reinforcement learning (DRL) based controller and meta-train the controller using a latent context variable through a separate embedding neural network. We test our meta-algorithm on its ability to adapt to new process dynamics as well as different control objectives on the same process. In both cases, our meta-learning algorithm adapts very quickly to new tasks, outperforming a regular DRL controller trained from scratch. Meta-learning appears to be a promising approach for constructing more intelligent and sample-efficient controllers.",0
"Meta-learning refers to a type of machine learning that focuses on enabling models, like neural networks, to swiftly adapt to novel tasks by identifying an underlying structure that connects related tasks. Rather than mastering one specific task, models are trained to learn new tasks effectively. This approach is advantageous for process control applications since training an AI controller can be both hazardous and expensive. Furthermore, the dynamics and control objectives are often comparable across various processes, making it possible to create a generalizable controller through meta-learning that can quickly adapt to different systems. In this research, we develop a deep reinforcement learning (DRL) based controller and meta-train it using a latent context variable via a separate embedding neural network. Our meta-algorithm is tested on its ability to adjust to new process dynamics and different control objectives on the same process, demonstrating that it adapts promptly to new tasks in both scenarios, outperforming a regular DRL controller trained from scratch. Meta-learning is a promising method for generating more intelligent and sample-efficient controllers.",1
"Conventional anti-jamming method mostly rely on frequency hopping to hide or escape from jammer. These approaches are not efficient in terms of bandwidth usage and can also result in a high probability of jamming. Different from existing works, in this paper, a novel anti-jamming strategy is proposed based on the idea of deceiving the jammer into attacking a victim channel while maintaining the communications of legitimate users in safe channels. Since the jammer's channel information is not known to the users, an optimal channel selection scheme and a sub optimal power allocation are proposed using reinforcement learning (RL). The performance of the proposed anti-jamming technique is evaluated by deriving the statistical lower bound of the total received power (TRP). Analytical results show that, for a given access point, over 50 % of the highest achievable TRP, i.e. in the absence of jammers, is achieved for the case of a single user and three frequency channels. Moreover, this value increases with the number of users and available channels. The obtained results are compared with two existing RL based anti-jamming techniques, and random channel allocation strategy without any jamming attacks. Simulation results show that the proposed anti-jamming method outperforms the compared RL based anti-jamming methods and random search method, and yields near optimal achievable TRP.",0
"Traditional methods of preventing jamming typically rely on frequency hopping to evade or conceal from jammers. However, these methods are not effective in terms of bandwidth utilization and can result in a high chance of jamming. This paper presents a new anti-jamming strategy that is based on misleading jammers into attacking a victim channel while preserving the communication of legitimate users in secure channels. Since the users are unaware of the jammer's channel information, this paper proposes an optimal channel selection scheme and a suboptimal power allocation using reinforcement learning. The proposed anti-jamming technique's performance is evaluated by determining the statistical lower limit of the total received power (TRP). Analytical results show that, for a single user and three frequency channels, over 50% of the highest possible TRP is achievable in the absence of jammers, and this value increases with the number of users and available channels. The results are compared with two existing RL-based anti-jamming techniques and a random channel allocation strategy without jamming attacks. Simulation results show that the proposed anti-jamming method performs better than the compared RL-based anti-jamming methods and random search method and produces almost optimal achievable TRP.",1
"This paper considers batch Reinforcement Learning (RL) with general value function approximation. Our study investigates the minimal assumptions to reliably estimate/minimize Bellman error, and characterizes the generalization performance by (local) Rademacher complexities of general function classes, which makes initial steps in bridging the gap between statistical learning theory and batch RL. Concretely, we view the Bellman error as a surrogate loss for the optimality gap, and prove the followings: (1) In double sampling regime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the Rademacher complexity of the function class. (2) In the single sampling regime, sample-efficient risk minimization is not possible without further assumptions, regardless of algorithms. However, with completeness assumptions, the excess risk of FQI and a minimax style algorithm can be again bounded by the Rademacher complexity of the corresponding function classes. (3) Fast statistical rates can be achieved by using tools of local Rademacher complexity. Our analysis covers a wide range of function classes, including finite classes, linear spaces, kernel spaces, sparse linear features, etc.",0
"In this paper, we explore batch Reinforcement Learning (RL) and its general value function approximation. Our research focuses on identifying the minimal requirements for accurately estimating/minimizing Bellman error, as well as assessing the generalization performance by evaluating the (local) Rademacher complexities of general function classes. Our goal is to connect statistical learning theory to batch RL. Specifically, we view the Bellman error as a substitute loss for the optimality gap, and our findings demonstrate the following: (1) In the double sampling regime, the Empirical Risk Minimizer (ERM) excess risk is limited by the function class's Rademacher complexity. (2) In the single sampling regime, risk minimization that is sample-efficient is unachievable without additional assumptions, regardless of the algorithm. However, with completeness assumptions, the Fitted Q-Iteration (FQI) and a minimax style algorithm's excess risk can be bounded by the corresponding function class's Rademacher complexity. (3) The tools of local Rademacher complexity can deliver rapid statistical rates. Our analysis encompasses a broad range of function classes, including finite classes, linear spaces, kernel spaces, sparse linear features, and more.",1
"A collective flashing ratchet transports Brownian particles using a spatially periodic, asymmetric, and time-dependent on-off switchable potential. The net current of the particles in this system can be substantially increased by feedback control based on the particle positions. Several feedback policies for maximizing the current have been proposed, but optimal policies have not been found for a moderate number of particles. Here, we use deep reinforcement learning (RL) to find optimal policies, with results showing that policies built with a suitable neural network architecture outperform the previous policies. Moreover, even in a time-delayed feedback situation where the on-off switching of the potential is delayed, we demonstrate that the policies provided by deep RL provide higher currents than the previous strategies.",0
"Using a spatially periodic, asymmetric, and time-dependent on-off switchable potential, a collective flashing ratchet is utilized to transport Brownian particles. The current of the particles in this system can be greatly enhanced through feedback control based on their positions. Various feedback policies have been suggested to maximize the current, but none have been deemed optimal for a moderate number of particles. In this study, we employ deep reinforcement learning (RL) to uncover optimal policies and discover that policies developed with a suitable neural network architecture outperform previous policies. Furthermore, our findings demonstrate that even in a scenario where there is a delay in the on-off switching of the potential, the policies generated by deep RL result in higher currents compared to previous strategies.",1
"In the wake of the highly electrified future ahead of us, the role of energy storage is crucial wherever distributed generation is abundant, such as in microgrid settings. Given the variety of storage options that are becoming more and more economical, determining which type of storage technology to invest in, along with the appropriate timing and capacity becomes a critical research question. It is inevitable that these problems will continue to become increasingly relevant in the future and require strategic planning and holistic and modern frameworks in order to be solved. Reinforcement Learning algorithms have already proven to be successful in problems where sequential decision-making is inherent. In the operations planning area, these algorithms are already used but mostly in short-term problems with well-defined constraints. On the contrary, we expand and tailor these techniques to long-term planning by utilizing model-free algorithms combined with simulation-based models. A model and expansion plan have been developed to optimally determine microgrid designs as they evolve to dynamically react to changing conditions and to exploit energy storage capabilities. We show that it is possible to derive better engineering solutions that would point to the types of energy storage units which could be at the core of future microgrid applications. Another key finding is that the optimal storage capacity threshold for a system depends heavily on the price movements of the available storage units. By utilizing the proposed approaches, it is possible to model inherent problem uncertainties and optimize the whole streamline of sequential investment decision-making.",0
"Energy storage is essential in areas with distributed generation, such as microgrids, as we move towards a highly electrified future. With various cost-effective storage options available, it is important to determine the appropriate technology, timing, and capacity for investment. These challenges will continue to be significant and require strategic planning and modern frameworks to solve. Reinforcement Learning algorithms have been successful in sequential decision-making problems, and we have adapted these techniques to long-term planning using simulation-based models and model-free algorithms. Our approach optimizes microgrid designs for dynamic response and energy storage utilization, leading to better engineering solutions. We also found that optimal storage capacity depends on storage unit prices. By modeling uncertainties and optimizing investment decisions, our approach streamlines the decision-making process.",1
"Deep reinforcement learning (DRL) provides a promising way for learning navigation in complex autonomous driving scenarios. However, identifying the subtle cues that can indicate drastically different outcomes remains an open problem with designing autonomous systems that operate in human environments. In this work, we show that explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework can help address this difficulty. We encode prior knowledge on the latent states of other drivers through a framework that combines the reinforcement learner with a supervised learner. In addition, we model the influence passing between different vehicles through graph neural networks (GNNs). The proposed framework significantly improves performance in the context of navigating T-intersections compared with state-of-the-art baseline approaches.",0
"Autonomous driving in complex scenarios can benefit from deep reinforcement learning (DRL). However, identifying the subtle cues that can indicate different outcomes remains a challenge in designing autonomous systems in human environments. This study proposes to address this difficulty by explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework. The proposed approach combines a reinforcement learner with a supervised learner to encode prior knowledge on the latent states of other drivers. The influence passing between different vehicles is modeled through graph neural networks (GNNs). The results show that the proposed framework significantly improves performance in navigating T-intersections compared to state-of-the-art baseline approaches.",1
"We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for $K$ episodes, and has to learn to approximate the optimal policy as closely as possible. In this work we show that the minimax regret for this setting is $\widetilde O(B_\star \sqrt{|S| |A| K})$ where $B_\star$ is a bound on the expected cost of the optimal policy from any state, $S$ is the state space, and $A$ is the action space. This matches the lower bound of Rosenberg et al. (2020) up to logarithmic factors, and improves their regret bound by a factor of $\sqrt{|S|}$. Our algorithm runs in polynomial-time per episode, and is based on a novel reduction to reinforcement learning in finite-horizon MDPs. To that end, we provide an algorithm for the finite-horizon setting whose leading term in the regret depends only logarithmically on the horizon, yielding the same regret guarantees for SSP.",0
"The Stochastic Shortest Path (SSP) problem is the focus of our study, where an agent aims to minimize the overall expected cost in reaching a goal state. In the learning approach, the agent lacks prior knowledge of the model's dynamics and costs. Thus, the agent engages with the model repeatedly for $K$ episodes to approximate the optimal policy as closely as possible. Our work proves that the minimax regret for this scenario is $\widetilde O(B_\star \sqrt{|S| |A| K})$, where $B_\star$ is the expected cost of the optimal policy from any state, $S$ denotes the state space, and $A$ represents the action space. This is consistent with Rosenberg et al.'s (2020) lower bound, except for the logarithmic factors, and it outperforms their regret bound by a factor of $\sqrt{|S|}$. Our algorithm operates in polynomial-time per episode and relies on a unique conversion to reinforcement learning in finite-horizon MDPs. As a result, we offer an algorithm for the finite-horizon setting, whose regret's primary component is logarithmic in the horizon, thereby providing the same regret guarantees for SSP.",1
"The paper considers a distributed version of deep reinforcement learning (DRL) for multi-agent decision-making process in the paradigm of federated learning. Since the deep neural network models in federated learning are trained locally and aggregated iteratively through a central server, frequent information exchange incurs a large amount of communication overheads. Besides, due to the heterogeneity of agents, Markov state transition trajectories from different agents are usually unsynchronized within the same time interval, which will further influence the convergence bound of the aggregated deep neural network models. Therefore, it is of vital importance to reasonably evaluate the effectiveness of different optimization methods. Accordingly, this paper proposes a utility function to consider the balance between reducing communication overheads and improving convergence performance. Meanwhile, this paper develops two new optimization methods on top of variation-aware periodic averaging methods: 1) the decay-based method which gradually decreases the weight of the model's local gradients within the progress of local updating, and 2) the consensus-based method which introduces the consensus algorithm into federated learning for the exchange of the model's local gradients. This paper also provides novel convergence guarantees for both developed methods and demonstrates their effectiveness and efficiency through theoretical analysis and numerical simulation results.",0
"In this paper, the focus is on distributed deep reinforcement learning (DRL) for multi-agent decision-making using federated learning. Federated learning involves training deep neural network models locally and aggregating them iteratively through a central server. However, frequent information exchange can lead to significant communication overheads. Moreover, the heterogeneity of agents can result in unsynchronized Markov state transition trajectories within the same time interval, which can affect the convergence of the aggregated models. Therefore, this paper proposes a utility function that considers both reducing communication overheads and improving convergence performance when evaluating different optimization methods. The paper introduces two new optimization methods based on variation-aware periodic averaging methods: the decay-based method, which gradually reduces the weight of the model's local gradients during local updating, and the consensus-based method, which uses the consensus algorithm for exchanging the model's local gradients. The paper also provides convergence guarantees for both methods and demonstrates their effectiveness and efficiency using theoretical analysis and numerical simulation results.",1
"In recent years, deep learning-based visual object trackers have achieved state-of-the-art performance on several visual object tracking benchmarks. However, most tracking benchmarks are focused on ground level videos, whereas aerial tracking presents a new set of challenges. In this paper, we compare ten trackers based on deep learning techniques on four aerial datasets. We choose top performing trackers utilizing different approaches, specifically tracking by detection, discriminative correlation filters, Siamese networks and reinforcement learning. In our experiments, we use a subset of OTB2015 dataset with aerial style videos; the UAV123 dataset without synthetic sequences; the UAV20L dataset, which contains 20 long sequences; and DTB70 dataset as our benchmark datasets. We compare the advantages and disadvantages of different trackers in different tracking situations encountered in aerial data. Our findings indicate that the trackers perform significantly worse in aerial datasets compared to standard ground level videos. We attribute this effect to smaller target size, camera motion, significant camera rotation with respect to the target, out of view movement, and clutter in the form of occlusions or similar looking distractors near tracked object.",0
"Several visual object tracking benchmarks have seen state-of-the-art performance by deep learning-based trackers. However, these benchmarks have primarily centered around ground level videos, and the challenges that come with aerial tracking are relatively new. The paper compares ten trackers that use deep learning techniques on four aerial datasets, utilizing different approaches such as tracking by detection, discriminative correlation filters, Siamese networks, and reinforcement learning. These experiments are conducted using a subset of the OTB2015 dataset, UAV123 dataset, UAV20L dataset, and DTB70 dataset as benchmark datasets. The paper assesses different trackers' advantages and disadvantages in different tracking situations encountered in aerial data. The results indicate that the trackers perform significantly worse in aerial datasets due to smaller target size, camera motion, significant camera rotation with respect to the target, out of view movement, and clutter in the form of occlusions or similar looking distractors near the tracked object.",1
"A fundamental question in the theory of reinforcement learning is: suppose the optimal $Q$-function lies in the linear span of a given $d$ dimensional feature mapping, is sample-efficient reinforcement learning (RL) possible? The recent and remarkable result of Weisz et al. (2020) resolved this question in the negative, providing an exponential (in $d$) sample size lower bound, which holds even if the agent has access to a generative model of the environment. One may hope that this information theoretic barrier for RL can be circumvented by further supposing an even more favorable assumption: there exists a \emph{constant suboptimality gap} between the optimal $Q$-value of the best action and that of the second-best action (for all states). The hope is that having a large suboptimality gap would permit easier identification of optimal actions themselves, thus making the problem tractable; indeed, provided the agent has access to a generative model, sample-efficient RL is in fact possible with the addition of this more favorable assumption.   This work focuses on this question in the standard online reinforcement learning setting, where our main result resolves this question in the negative: our hardness result shows that an exponential sample complexity lower bound still holds even if a constant suboptimality gap is assumed in addition to having a linearly realizable optimal $Q$-function. Perhaps surprisingly, this implies an exponential separation between the online RL setting and the generative model setting. Complementing our negative hardness result, we give two positive results showing that provably sample-efficient RL is possible either under an additional low-variance assumption or under a novel hypercontractivity assumption (both implicitly place stronger conditions on the underlying dynamics model).",0
"The theory of reinforcement learning poses a significant question regarding the possibility of sample-efficient RL when the optimal $Q$-function can be found within the linear span of a given $d$ dimensional feature mapping. However, the recent study by Weisz et al. (2020) showed that such RL is not possible due to an exponential sample size lower bound, even if the agent has access to a generative model of the environment. To overcome this barrier, one may assume the existence of a constant suboptimality gap between the optimal $Q$-value of the best action and that of the second-best action for all states. This assumption could potentially make the problem tractable by facilitating the identification of optimal actions. Sample-efficient RL is possible with this assumption and access to a generative model. This research focuses on this question in the standard online RL setting and reveals that even if a constant suboptimality gap is assumed, an exponential sample complexity lower bound still exists with a linearly realizable optimal $Q$-function. This result implies an exponential separation between the online RL setting and the generative model setting. Two positive results are also presented, indicating that provably sample-efficient RL is possible under either a low-variance assumption or a novel hypercontractivity assumption, both of which impose stronger conditions on the underlying dynamics model.",1
"In the standard Markov decision process formalism, users specify tasks by writing down a reward function. However, in many scenarios, the user is unable to describe the task in words or numbers, but can readily provide examples of what the world would look like if the task were solved. Motivated by this observation, we derive a control algorithm from first principles that aims to visit states that have a high probability of leading to successful outcomes, given only examples of successful outcome states. Prior work has approached similar problem settings in a two-stage process, first learning an auxiliary reward function and then optimizing this reward function using another reinforcement learning algorithm. In contrast, we derive a method based on recursive classification that eschews auxiliary reward functions and instead directly learns a value function from transitions and successful outcomes. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.",0
"When using the Markov decision process, tasks are typically defined by a reward function. However, in some situations, individuals may struggle to articulate a task in words or numbers, but can provide examples of successful outcomes. To address this issue, we have created a control algorithm that prioritizes states that are likely to lead to successful outcomes based on these examples alone. In contrast to previous approaches, our method uses recursive classification and does not require an auxiliary reward function, resulting in fewer hyperparameters and lines of code. Our method adheres to a novel data-driven Bellman equation that substitutes examples for the typical reward function term, and our experiments have demonstrated that our approach outperforms previous methods that utilize explicit reward functions.",1
"DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep neural networks. DQNs require a large buffer and batch processing for an experience replay and rely on a backpropagation based iterative optimization, making them difficult to be implemented on resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement learning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based on-device learning approach that does not rely on the backpropagation method but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a combination of L2 regularization and spectral normalization for the on-device reinforcement learning so that output values of the neural network can be fit into a certain range and the reinforcement learning becomes stable. The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate that the proposed algorithm and its FPGA implementation complete a CartPole-v0 task 29.77x and 89.40x faster than a conventional DQN-based approach when the number of hidden-layer nodes is 64.",0
"A method called DQN (Deep Q-Network) uses deep neural networks to perform Q-learning for reinforcement learning. However, implementing this method on resource-limited edge devices is challenging due to the need for large buffers, batch processing, and backpropagation based iterative optimization. To address this, we propose a lightweight on-device reinforcement learning approach that utilizes a neural-network based training algorithm called OS-ELM (Online Sequential Extreme Learning Machine) and a combination of L2 regularization and spectral normalization to stabilize the reinforcement learning process. Our approach is designed for low-cost FPGA devices, specifically the PYNQ-Z1 board. Evaluation results using OpenAI Gym show that our approach completes a CartPole-v0 task significantly faster than a conventional DQN-based approach (29.77x and 89.40x faster when the number of hidden-layer nodes is 64).",1
"Autonomous navigation in crowded, complex urban environments requires interacting with other agents on the road. A common solution to this problem is to use a prediction model to guess the likely future actions of other agents. While this is reasonable, it leads to overly conservative plans because it does not explicitly model the mutual influence of the actions of interacting agents. This paper builds a reinforcement learning-based method named MIDAS where an ego-agent learns to affect the control actions of other cars in urban driving scenarios. MIDAS uses an attention-mechanism to handle an arbitrary number of other agents and includes a ""driver-type"" parameter to learn a single policy that works across different planning objectives. We build a simulation environment that enables diverse interaction experiments with a large number of agents and methods for quantitatively studying the safety, efficiency, and interaction among vehicles. MIDAS is validated using extensive experiments and we show that it (i) can work across different road geometries, (ii) results in an adaptive ego policy that can be tuned easily to satisfy performance criteria such as aggressive or cautious driving, (iii) is robust to changes in the driving policies of external agents, and (iv) is more efficient and safer than existing approaches to interaction-aware decision-making.",0
"Interacting with other agents on the road is essential for autonomous navigation in crowded, complex urban environments. A commonly used method is to predict the future actions of other agents, but this can lead to overly cautious plans as it does not consider the mutual influence of interacting agents. This paper presents MIDAS, a reinforcement learning-based method that allows an ego-agent to influence the control actions of other cars in urban driving scenarios. MIDAS uses an attention-mechanism to handle any number of other agents and includes a ""driver-type"" parameter to learn a single policy that works for different planning objectives. A simulation environment is built to study the safety, efficiency, and interaction among vehicles. Extensive experiments show that MIDAS can work in different road geometries, and results in an adaptive ego policy that can be easily tuned for performance criteria. Additionally, MIDAS is robust to changes in the driving policies of external agents and is more efficient and safer than existing approaches to interaction-aware decision-making.",1
"Distributional Reinforcement Learning (RL) maintains the entire probability distribution of the reward-to-go, i.e. the return, providing more learning signals that account for the uncertainty associated with policy performance, which may be beneficial for trading off exploration and exploitation and policy learning in general. Previous works in distributional RL focused mainly on computing the state-action-return distributions, here we model the state-return distributions. This enables us to translate successful conventional RL algorithms that are based on state values into distributional RL. We formulate the distributional Bellman operation as an inference-based auto-encoding process that minimises Wasserstein metrics between target/model return distributions. The proposed algorithm, BDPG (Bayesian Distributional Policy Gradients), uses adversarial training in joint-contrastive learning to estimate a variational posterior from the returns. Moreover, we can now interpret the return prediction uncertainty as an information gain, which allows to obtain a new curiosity measure that helps BDPG steer exploration actively and efficiently. We demonstrate in a suite of Atari 2600 games and MuJoCo tasks, including well known hard-exploration challenges, how BDPG learns generally faster and with higher asymptotic performance than reference distributional RL algorithms.",0
"The use of Distributional Reinforcement Learning (RL) involves maintaining the complete probability distribution of the reward-to-go, also known as the return. This approach provides more learning opportunities that consider the uncertainty linked to policy performance, which can aid in balancing exploration and exploitation and enhancing policy learning in general. Previous studies on distributional RL have primarily focused on computing the state-action-return distributions; however, we now model the state-return distributions, making it possible to apply conventional RL algorithms that rely on state values to distributional RL. Our proposed algorithm, BDPG (Bayesian Distributional Policy Gradients), formulates the distributional Bellman operation as an inference-based auto-encoding process that minimizes Wasserstein metrics between target/model return distributions. We use adversarial training in joint-contrastive learning to estimate a variational posterior from the returns. Furthermore, we can now view the return prediction uncertainty as an information gain, resulting in a new curiosity measure that helps BDPG steer exploration actively and efficiently. Our experiments, which include challenging Atari 2600 games and MuJoCo tasks, demonstrate that BDPG learns more quickly and with higher asymptotic performance than reference distributional RL algorithms.",1
"We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop-Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.",0
"We introduce Drop-Bottleneck, a novel information bottleneck (IB) technique that discretely drops irrelevant input features to the target variable. This approach offers a straightforward and manageable compression objective while also providing a deterministic compressed representation of the input variable that is beneficial for inference tasks requiring consistent representation. Furthermore, Drop-Bottleneck can jointly learn a feature extractor and select relevant features for the target task, a capability that most neural network-based IB methods lack. We apply Drop-Bottleneck to reinforcement learning tasks using an exploration method and achieve state-of-the-art performance in numerous noisy and reward sparse maze navigation tasks in VizDoom and DMLab. Compared to Variational Information Bottleneck (VIB), Drop-Bottleneck shows superior performance in various aspects, including adversarial robustness and dimensionality reduction, establishing it as a new IB framework.",1
"The options framework for hierarchical reinforcement learning has increased its popularity in recent years and has made improvements in tackling the scalability problem in reinforcement learning. Yet, most of these recent successes are linked with a proper options initialization or discovery. When an expert is available, the options discovery problem can be addressed by learning an options-type hierarchical policy directly from expert demonstrations. This problem is referred to as hierarchical imitation learning and can be handled as an inference problem in a Hidden Markov Model, which is done via an Expectation-Maximization type algorithm. In this work, we propose a novel online algorithm to perform hierarchical imitation learning in the options framework. Further, we discuss the benefits of such an algorithm and compare it with its batch version in classical reinforcement learning benchmarks. We show that this approach works well in both discrete and continuous environments and, under certain conditions, it outperforms the batch version.",0
"In recent years, the options framework for hierarchical reinforcement learning has gained popularity and has made strides in addressing the scalability issue in reinforcement learning. However, the success of this framework largely depends on a proper initialization or discovery of options. When an expert is available, the options discovery problem can be solved by learning a hierarchical policy directly from expert demonstrations, known as hierarchical imitation learning. This can be achieved using an Expectation-Maximization algorithm in a Hidden Markov Model. Our work introduces a new online algorithm for hierarchical imitation learning in the options framework and compares it with the batch version in standard reinforcement learning benchmarks. We demonstrate that this approach is effective in both discrete and continuous environments and, in certain circumstances, outperforms the batch version. We also discuss the advantages of our algorithm.",1
"Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.   Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.",0
"Batch reinforcement learning (RL) algorithms operate offline and endeavor to learn an optimal policy from a fixed dataset without active data collection. There are two primary types of methods used depending on the composition of the batch dataset: imitation learning, which is ideal for expert datasets, and vanilla offline RL, which usually requires uniform coverage datasets. However, datasets typically fall between these two categories, and their exact composition is often unknown beforehand. To address this issue, we propose a new offline RL framework that smoothly transitions between the two extremes, effectively unifying imitation learning and vanilla offline RL. This framework is based on a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. We also examine whether it is possible to design an algorithm that achieves a minimax optimal rate while also adapting to unknown data composition. To this end, we introduce a lower confidence bound (LCB) algorithm that is developed based on pessimism in the face of uncertainty in offline RL. We investigate the finite-sample properties of LCB and the information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis indicates that LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL in all three settings. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, smoothly transitioning from imitation learning to offline RL. Additionally, we demonstrate that LCB is nearly adaptively optimal in MDPs.",1
"Recently, autonomous driving has made substantial progress in addressing the most common traffic scenarios like intersection navigation and lane changing. However, most of these successes have been limited to scenarios with well-defined traffic rules and require minimal negotiation with other vehicles. In this paper, we introduce a previously unconsidered, yet everyday, high-conflict driving scenario requiring negotiations between agents of equal rights and priorities. There exists no centralized control structure and we do not allow communications. Therefore, it is unknown if other drivers are willing to cooperate, and if so to what extent. We train policies to robustly negotiate with opposing vehicles of an unobservable degree of cooperativeness using multi-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft Actor-Critic (DASAC), a maximum-entropy off-policy MARL algorithm allowing for centralized training with decentralized execution. We show that using DASAC we are able to successfully negotiate and traverse the scenario considered over 99% of the time. Our agents are robust to an unknown timing of opponent decisions, an unobservable degree of cooperativeness of the opposing vehicle, and previously unencountered policies. Furthermore, they learn to exhibit human-like behaviors such as defensive driving, anticipating solution options and interpreting the behavior of other agents.",0
"The advancement of self-driving technology has made significant strides in dealing with common traffic situations such as changing lanes and navigating intersections. However, these accomplishments have been limited to scenarios with clear traffic rules and minimal interaction with other vehicles. This paper introduces a high-stress driving situation that involves negotiations between equally prioritized agents, which has not previously been explored. There is no centralized control system, and communication is prohibited, making it unclear if other drivers are willing to cooperate. The study trains policies to effectively negotiate with opposing vehicles using multi-agent reinforcement learning (MARL) through the Discrete Asymmetric Soft Actor-Critic (DASAC) algorithm. The algorithm allows for centralized training with decentralized execution and is successful in navigating the scenario over 99% of the time. The agents exhibit human-like behaviors, such as defensive driving, anticipating potential solutions, and interpreting the actions of other agents. The agents are also robust to unknown factors, such as the timing of opponent decisions and the degrees of cooperativeness of opposing vehicles.",1
"We consider the problem of knowledge transfer when an agent is facing a series of Reinforcement Learning (RL) tasks. We introduce a novel metric between Markov Decision Processes (MDPs) and establish that close MDPs have close optimal value functions. Formally, the optimal value functions are Lipschitz continuous with respect to the tasks space. These theoretical results lead us to a value-transfer method for Lifelong RL, which we use to build a PAC-MDP algorithm with improved convergence rate. Further, we show the method to experience no negative transfer with high probability. We illustrate the benefits of the method in Lifelong RL experiments.",0
"The focus of our study is on knowledge transfer in Reinforcement Learning (RL) tasks that agents face in a sequence. We introduce a new measure of similarity between Markov Decision Processes (MDPs) and demonstrate that MDPs that are closely related also have similar optimal value functions. We support this claim by showing that the optimal value functions are Lipschitz continuous in relation to the task space. These findings have led us to develop a value-transfer technique for Lifelong RL, which we have used to create a PAC-MDP algorithm with increased convergence rate. Furthermore, we have demonstrated that this method has a high probability of not experiencing any negative transfer. To showcase the effectiveness of our approach, we have conducted experiments in Lifelong RL.",1
"A key challenge in applying reinforcement learning to safety-critical domains is understanding how to balance exploration (needed to attain good performance on the task) with safety (needed to avoid catastrophic failure). Although a growing line of work in reinforcement learning has investigated this area of ""safe exploration,"" most existing techniques either 1) do not guarantee safety during the actual exploration process; and/or 2) limit the problem to a priori known and/or deterministic transition dynamics with strong smoothness assumptions. Addressing this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm for provably safe exploration in MDPs with unknown, stochastic dynamics. Our method exploits analogies between state-action pairs to safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE also guides exploration towards the most task-relevant states, which empirically results in significant improvements in terms of sample efficiency, when compared to existing methods.",0
"Balancing exploration and safety is a major challenge when implementing reinforcement learning in safety-critical domains. While some research has been conducted in the area of ""safe exploration,"" most current techniques do not guarantee safety during the exploration process or limit the problem to known and/or deterministic transition dynamics with strong smoothness assumptions. To address this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm that ensures safe exploration in MDPs with unknown, stochastic dynamics. Our approach utilizes analogies between state-action pairs to learn a near-optimal policy in a PAC-MDP sense, while also directing exploration towards the most relevant states. Empirical results show significant improvements in sample efficiency compared to existing methods.",1
"Current value-based multi-agent reinforcement learning methods optimize individual Q values to guide individuals' behaviours via centralized training with decentralized execution (CTDE). However, such expected, i.e., risk-neutral, Q value is not sufficient even with CTDE due to the randomness of rewards and the uncertainty in environments, which causes the failure of these methods to train coordinating agents in complex environments. To address these issues, we propose RMIX, a novel cooperative MARL method with the Conditional Value at Risk (CVaR) measure over the learned distributions of individuals' Q values. Specifically, we first learn the return distributions of individuals to analytically calculate CVaR for decentralized execution. Then, to handle the temporal nature of the stochastic outcomes during executions, we propose a dynamic risk level predictor for risk level tuning. Finally, we optimize the CVaR policies with CVaR values used to estimate the target in TD error during centralized training and the CVaR values are used as auxiliary local rewards to update the local distribution via Quantile Regression loss. Empirically, we show that our method significantly outperforms state-of-the-art methods on challenging StarCraft II tasks, demonstrating enhanced coordination and improved sample efficiency.",0
"At present, value-based multi-agent reinforcement learning techniques focus on optimizing individual Q values. This is achieved through centralized training with decentralized execution (CTDE), which guides individual behaviors. However, this approach falls short due to the randomness of rewards and uncertainty in environments, making it difficult to train coordinating agents in complex environments. To overcome this, our proposed RMIX method utilizes the Conditional Value at Risk (CVaR) measure to optimize cooperative MARL. We first learn the return distributions of individuals to calculate CVaR for decentralized execution. We then introduce a dynamic risk level predictor to handle the stochastic outcomes during execution. Finally, we optimize the CVaR policies with CVaR values to estimate the target in TD error during centralized training. Additionally, we use CVaR values as auxiliary local rewards to update the local distribution via Quantile Regression loss. Our empirical results demonstrate the superiority of our method over state-of-the-art methods on challenging StarCraft II tasks, showcasing enhanced coordination and improved sample efficiency.",1
"Policy optimization methods remain a powerful workhorse in empirical Reinforcement Learning (RL), with a focus on neural policies that can easily reason over complex and continuous state and/or action spaces. Theoretical understanding of strategic exploration in policy-based methods with non-linear function approximation, however, is largely missing. In this paper, we address this question by designing ENIAC, an actor-critic method that allows non-linear function approximation in the critic. We show that under certain assumptions, e.g., a bounded eluder dimension $d$ for the critic class, the learner finds a near-optimal policy in $O(\poly(d))$ exploration rounds. The method is robust to model misspecification and strictly extends existing works on linear function approximation. We also develop some computational optimizations of our approach with slightly worse statistical guarantees and an empirical adaptation building on existing deep RL tools. We empirically evaluate this adaptation and show that it outperforms prior heuristics inspired by linear methods, establishing the value via correctly reasoning about the agent's uncertainty under non-linear function approximation.",0
"Empirical Reinforcement Learning (RL) heavily relies on policy optimization methods, particularly neural policies that can handle complex and continuous state and action spaces. However, there is a lack of theoretical understanding of strategic exploration in policy-based methods that use non-linear function approximation. In this study, we introduce ENIAC, an actor-critic method that allows non-linear function approximation in the critic, to address this issue. We demonstrate that our method can find a near-optimal policy in $O(\poly(d))$ exploration rounds under certain assumptions, such as a bounded eluder dimension $d$ for the critic class. Our approach is robust to model misspecification and offers an improvement over existing works that use linear function approximation. We also present computational optimizations with slightly worse statistical guarantees and an empirical adaptation using existing deep RL tools. Our empirical evaluation shows that our adaptation outperforms previous heuristics based on linear methods and correctly accounts for the agent's uncertainty when using non-linear function approximation.",1
"To improve the system performance towards the Shannon limit, advanced radio resource management mechanisms play a fundamental role. In particular, scheduling should receive much attention, because it allocates radio resources among different users in terms of their channel conditions and QoS requirements. The difficulties of scheduling algorithms are the tradeoffs need to be made among multiple objectives, such as throughput, fairness and packet drop rate. We propose a smart scheduling scheme based on deep reinforcement learning (DRL). We not only verify the performance gain achieved, but also provide implementation-friend designs, i.e., a scalable neural network design for the agent and a virtual environment training framework. With the scalable neural network design, the DRL agent can easily handle the cases when the number of active users is time-varying without the need to redesign and retrain the DRL agent. Training the DRL agent in a virtual environment offline first and using it as the initial version in the practical usage helps to prevent the system from suffering from performance and robustness degradation due to the time-consuming training. Through both simulations and field tests, we show that the DRL-based smart scheduling outperforms the conventional scheduling method and can be adopted in practical systems.",0
"Advanced radio resource management mechanisms are crucial for improving system performance towards the Shannon limit. Among these mechanisms, scheduling is particularly important as it distributes radio resources to users based on their channel conditions and QoS requirements. However, scheduling algorithms face challenges in balancing multiple objectives such as throughput, fairness, and packet drop rate. To address this, we propose a smart scheduling scheme that utilizes deep reinforcement learning (DRL). Our approach not only demonstrates performance gains, but also offers implementation-friendly designs such as a scalable neural network for the agent and a virtual environment training framework. The scalable neural network design allows the DRL agent to handle varying numbers of active users without requiring retraining. Additionally, training the DRL agent in a virtual environment prior to practical usage prevents performance and robustness degradation. Our simulations and field tests confirm that our DRL-based smart scheduling outperforms conventional scheduling methods and is suitable for practical systems.",1
"Deep learning has shown great promise for CT image reconstruction, in particular to enable low dose imaging and integrated diagnostics. These merits, however, stand at great odds with the low availability of diverse image data which are needed to train these neural networks. We propose to overcome this bottleneck via a deep reinforcement learning (DRL) approach that is integrated with a style-transfer (ST) methodology, where the DRL generates the anatomical shapes and the ST synthesizes the texture detail. We show that our method bears high promise for generating novel and anatomically accurate high resolution CT images at large and diverse quantities. Our approach is specifically designed to work with even small image datasets which is desirable given the often low amount of image data many researchers have available to them.",0
"The use of deep learning has demonstrated significant potential for the reconstruction of CT images, particularly in facilitating low dose imaging and integrated diagnostics. However, the limited availability of diverse image data required for the training of these neural networks creates a significant obstacle. To address this issue, we propose a combined deep reinforcement learning (DRL) and style-transfer (ST) approach, where the DRL generates anatomical shapes while the ST synthesizes texture detail. Our approach shows promise in producing high-quality, anatomically accurate, and diverse CT images at high resolutions, even with limited image datasets. This is particularly valuable for researchers who often have limited access to image data.",1
"Projecting high-dimensional environment observations into lower-dimensional structured representations can considerably improve data-efficiency for reinforcement learning in domains with limited data such as robotics. Can a single generally useful representation be found? In order to answer this question, it is important to understand how the representation will be used by the agent and what properties such a 'good' representation should have. In this paper we systematically evaluate a number of common learnt and hand-engineered representations in the context of three robotics tasks: lifting, stacking and pushing of 3D blocks. The representations are evaluated in two use-cases: as input to the agent, or as a source of auxiliary tasks. Furthermore, the value of each representation is evaluated in terms of three properties: dimensionality, observability and disentanglement. We can significantly improve performance in both use-cases and demonstrate that some representations can perform commensurate to simulator states as agent inputs. Finally, our results challenge common intuitions by demonstrating that: 1) dimensionality strongly matters for task generation, but is negligible for inputs, 2) observability of task-relevant aspects mostly affects the input representation use-case, and 3) disentanglement leads to better auxiliary tasks, but has only limited benefits for input representations. This work serves as a step towards a more systematic understanding of what makes a 'good' representation for control in robotics, enabling practitioners to make more informed choices for developing new learned or hand-engineered representations.",0
"Lower-dimensional structured representations can enhance data-efficiency in reinforcement learning for domains such as robotics with limited data by projecting high-dimensional environment observations. However, it remains unclear whether a universally beneficial representation exists and what properties it should possess. To address this question, we evaluate various common hand-engineered and learned representations in three robotics tasks involving stacking, lifting, and pushing 3D blocks, assessing their value in two use-cases: as input to the agent and as a source of auxiliary tasks. We also measure the representations' dimensionality, observability, and disentanglement. Our findings reveal that some representations can perform on par with simulator states as agent inputs, and that dimensionality plays a crucial role in task generation but is relatively unimportant for inputs. Additionally, observability of task-relevant aspects primarily affects the input representation use-case, while disentanglement leads to higher-quality auxiliary tasks but has limited benefits for input representations. This work takes a step towards a more methodical understanding of what constitutes a 'good' representation for control in robotics, empowering practitioners to make better-informed decisions when developing new hand-engineered or learned representations.",1
"Visual navigation for autonomous agents is a core task in the fields of computer vision and robotics. Learning-based methods, such as deep reinforcement learning, have the potential to outperform the classical solutions developed for this task; however, they come at a significantly increased computational load. Through this work, we design a novel approach that focuses on performing better or comparable to the existing learning-based solutions but under a clear time/computational budget. To this end, we propose a method to encode vital scene semantics such as traversable paths, unexplored areas, and observed scene objects -- alongside raw visual streams such as RGB, depth, and semantic segmentation masks -- into a semantically informed, top-down egocentric map representation. Further, to enable the effective use of this information, we introduce a novel 2-D map attention mechanism, based on the successful multi-layer Transformer networks. We conduct experiments on 3-D reconstructed indoor PointGoal visual navigation and demonstrate the effectiveness of our approach. We show that by using our novel attention schema and auxiliary rewards to better utilize scene semantics, we outperform multiple baselines trained with only raw inputs or implicit semantic information while operating with an 80% decrease in the agent's experience.",0
"The task of visual navigation for autonomous agents is essential in computer vision and robotics. While deep reinforcement learning offers potential advantages over traditional methods, it requires a significant increase in computational load. This study proposes a new approach that aims to perform similarly or better than existing learning-based solutions while staying within a specific computational budget. The approach involves encoding important scene semantics, such as navigable paths, unexplored areas, and observed objects, alongside raw visual streams like RGB, depth, and semantic segmentation masks into a semantically informed, top-down egocentric map representation. Additionally, a novel 2-D map attention mechanism based on multi-layer Transformer networks is introduced to effectively utilize this information. The effectiveness of this approach is demonstrated through experiments on PointGoal visual navigation in 3-D reconstructed indoor environments. Results show that the proposed attention schema and auxiliary rewards outperform multiple baselines that were trained using only raw inputs or implicit semantic information while utilizing 80% less agent experience.",1
"Data augmentation (DA) plays a critical role in training deep neural networks for improving the generalization of models. Recent work has shown that automatic DA policy, such as AutoAugment (AA), significantly improves model performance. However, most automatic DA methods search for DA policies at the image-level without considering that the optimal policies for different regions in an image may be diverse. In this paper, we propose a patch-level automatic DA algorithm called Patch AutoAugment (PAA). PAA divides an image into a grid of patches and searches for the optimal DA policy of each patch. Specifically, PAA allows each patch DA operation to be controlled by an agent and models it as a Multi-Agent Reinforcement Learning (MARL) problem. At each step, PAA samples the most effective operation for each patch based on its content and the semantics of the whole image. The agents cooperate as a team and share a unified team reward for achieving the joint optimal DA policy of the whole image. The experiment shows that PAA consistently improves the target network performance on many benchmark datasets of image classification and fine-grained image recognition. PAA also achieves remarkable computational efficiency, i.e 2.3x faster than FastAA and 56.1x faster than AA on ImageNet.",0
"Training deep neural networks relies heavily on data augmentation (DA) to improve model generalization. Recent research has highlighted the benefits of automatic DA policies, such as AutoAugment (AA), in enhancing model performance. However, most automatic DA methods focus solely on image-level policies, ignoring the potential diversity of optimal policies across different regions of an image. In this study, we introduce Patch AutoAugment (PAA), a patch-level automatic DA algorithm. PAA divides images into patches and searches for the optimal DA policy for each patch. PAA employs a Multi-Agent Reinforcement Learning (MARL) approach, allowing each patch to be controlled by an agent. At each step, PAA selects the most effective operation for each patch based on its content and the image's semantics. The agents collaborate to achieve a unified team reward for the optimal DA policy of the entire image. Our experiments demonstrate that PAA consistently improves the performance of target networks across various image classification and fine-grained image recognition benchmark datasets. Furthermore, PAA exhibits notable computational efficiency, outperforming FastAA by 2.3x and AA by 56.1x on ImageNet.",1
"Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between multiple models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zero-cost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4$\times$ quicker than the best previous result. Our code is made public at: https://github.com/mohsaied/zero-cost-nas.",0
"The use of Neural Architecture Search (NAS) has become the norm for creating neural network models, but the process is usually computationally demanding as multiple models must be evaluated to select the best one. To mitigate this, a proxy task is commonly employed instead of full training. This study examines traditional reduced-training proxies to determine their ability to maintain model ranking during search, compared to final trained accuracy. Additionally, the study presents a set of zero-cost proxies based on recent pruning literature, which utilize only one minibatch of training data to determine a model's score. These zero-cost proxies use significantly less computation and can outperform traditional proxies, as evidenced by the Spearman's rank correlation coefficient on NAS-Bench-201. The study also demonstrates the effectiveness of the zero-cost proxies in improving search efficiency across various NAS datasets and search methodologies. The code for this study is publicly available on GitHub.",1
"Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent's decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. While several methods have attempted to interpret vision-based RL, most come without detailed explanation for the agent's behavior. In this paper, we propose a self-supervised interpretable framework, which can discover interpretable features to enable easy understanding of RL agents even for non-experts. Specifically, a self-supervised interpretable network (SSINet) is employed to produce fine-grained attention masks for highlighting task-relevant information, which constitutes most evidence for the agent's decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown, which is a challenging self-driving car simulator environment. The results show that our method renders empirical evidences about how the agent makes decisions and why the agent performs well or badly, especially when transferred to novel scenes. Overall, our method provides valuable insight into the internal decision-making process of vision-based RL. In addition, our method does not use any external labelled data, and thus demonstrates the possibility to learn high-quality mask through a self-supervised manner, which may shed light on new paradigms for label-free vision learning such as self-supervised segmentation and detection.",0
"Numerous breakthroughs have been achieved in complex control tasks through the application of deep reinforcement learning (RL). However, the decision-making process of the agent is often not transparent, which limits the usability of RL in safety-critical situations. While several approaches have been taken to interpret vision-based RL, they typically do not thoroughly explain the behavior of the agent. This study proposes a self-supervised interpretable framework that can identify interpretable features, enabling even non-experts to understand RL agents. The framework employs a self-supervised interpretable network (SSINet) to create fine-grained attention masks, highlighting task-relevant information that serves as evidence for the agent's decisions. The study evaluates this method on various Atari 2600 games and the Duckietown self-driving car simulator environment, demonstrating that the method provides insight into the internal decision-making process of vision-based RL. Additionally, this self-supervised approach does not require external labeled data, demonstrating the potential for label-free vision learning paradigms such as self-supervised segmentation and detection.",1
"Purpose: Image classification may be the fundamental task in imaging artificial intelligence. We have recently shown that reinforcement learning can achieve high accuracy for lesion localization and segmentation even with minuscule training sets. Here, we introduce reinforcement learning for image classification. In particular, we apply the approach to normal vs. tumor-containing 2D MRI brain images.   Materials and Methods: We applied multi-step image classification to allow for combined Deep Q learning and TD(0) Q learning. We trained on a set of 30 images (15 normal and 15 tumor-containing). We tested on a separate set of 30 images (15 normal and 15 tumor-containing). For comparison, we also trained and tested a supervised deep-learning classification network on the same set of training and testing images.   Results: Whereas the supervised approach quickly overfit the training data and as expected performed poorly on the testing set (57% accuracy, just over random guessing), the reinforcement learning approach achieved an accuracy of 100%.   Conclusion: We have shown a proof-of-principle application of reinforcement learning to the classification of brain tumors. We achieved perfect testing set accuracy with a training set of merely 30 images.",0
"The main objective of imaging artificial intelligence is image classification, which is crucial. It has been demonstrated that reinforcement learning can achieve high accuracy for lesion localization and segmentation with small training sets. This study presents an approach that utilizes reinforcement learning for image classification, specifically for distinguishing between normal and tumor-containing 2D MRI brain images. The method employed multi-step image classification, incorporating Deep Q learning and TD(0) Q learning. Training was conducted on a set of 30 images (15 normal and 15 tumor-containing) and testing was done on an independent set of 30 images. A supervised deep-learning classification network was also trained and tested on the same images for comparison purposes. The results indicated that the reinforcement learning approach achieved 100% accuracy on the testing set, whereas the supervised approach overfit the training data and performed poorly (57% accuracy). This study demonstrates the potential of using reinforcement learning to classify brain tumors, as perfect accuracy was achieved with only 30 training images.",1
"Purpose: Lesion segmentation in medical imaging is key to evaluating treatment response. We have recently shown that reinforcement learning can be applied to radiological images for lesion localization. Furthermore, we demonstrated that reinforcement learning addresses important limitations of supervised deep learning; namely, it can eliminate the requirement for large amounts of annotated training data and can provide valuable intuition lacking in supervised approaches. However, we did not address the fundamental task of lesion/structure-of-interest segmentation. Here we introduce a method combining unsupervised deep learning clustering with reinforcement learning to segment brain lesions on MRI.   Materials and Methods: We initially clustered images using unsupervised deep learning clustering to generate candidate lesion masks for each MRI image. The user then selected the best mask for each of 10 training images. We then trained a reinforcement learning algorithm to select the masks. We tested the corresponding trained deep Q network on a separate testing set of 10 images. For comparison, we also trained and tested a U-net supervised deep learning network on the same set of training/testing images.   Results: Whereas the supervised approach quickly overfit the training data and predictably performed poorly on the testing set (16% average Dice score), the unsupervised deep clustering and reinforcement learning achieved an average Dice score of 83%.   Conclusion: We have demonstrated a proof-of-principle application of unsupervised deep clustering and reinforcement learning to segment brain tumors. The approach represents human-allied AI that requires minimal input from the radiologist without the need for hand-traced annotation.",0
"The purpose of this study is to segment brain lesions in medical imaging to evaluate treatment response. Previous research has shown that reinforcement learning can be used for lesion localization in radiological images, which addresses limitations in supervised deep learning. However, the fundamental task of lesion/structure-of-interest segmentation was not addressed. This study introduces a method that combines unsupervised deep learning clustering with reinforcement learning to segment brain lesions on MRI. The materials and methods involved clustering images using unsupervised deep learning to generate candidate lesion masks, selecting the best mask for each training image, and training a reinforcement learning algorithm to select the masks. The trained deep Q network achieved an average Dice score of 83%, while the supervised approach overfit and performed poorly on the testing set. This study demonstrates a proof-of-principle application of unsupervised deep clustering and reinforcement learning for segmenting brain tumors, which requires minimal input from the radiologist without the need for hand-traced annotation.",1
"Purpose Supervised deep learning in radiology suffers from notorious inherent limitations: 1) It requires large, hand-annotated data sets, 2) It is non-generalizable, and 3) It lacks explainability and intuition. We have recently proposed Reinforcement Learning to address all threes. However, we applied it to images with radiologist eye tracking points, which limits the state-action space. Here we generalize the Deep-Q Learning to a gridworld-based environment, so that only the images and image masks are required.   Materials and Methods We trained a Deep Q network on 30 two-dimensional image slices from the BraTS brain tumor database. Each image contained one lesion. We then tested the trained Deep Q network on a separate set of 30 testing set images. For comparison, we also trained and tested a keypoint detection supervised deep learning network for the same set of training / testing images.   Results Whereas the supervised approach quickly overfit the training data, and predicably performed poorly on the testing set (11\% accuracy), the Deep-Q learning approach showed progressive improved generalizability to the testing set over training time, reaching 70\% accuracy.   Conclusion We have shown a proof-of-principle application of reinforcement learning to radiological images, here using 2D contrast-enhanced MRI brain images with the goal of localizing brain tumors. This represents a generalization of recent work to a gridworld setting, naturally suitable for analyzing medical images.",0
"Supervised deep learning in radiology has limitations that include the need for large, hand-annotated data sets, non-generalizability, and lack of explainability and intuition. To address these limitations, Reinforcement Learning was proposed, but it was only applied to images with radiologist eye tracking points, which limited the state-action space. To generalize the Deep-Q Learning, a gridworld-based environment was utilized with only images and image masks required. A Deep Q network was trained on 30 two-dimensional image slices from the BraTS brain tumor database, with each image containing one lesion. The trained Deep Q network was tested on a separate set of 30 testing set images, and a keypoint detection supervised deep learning network was also trained and tested for comparison. The Deep-Q learning approach showed progressive improved generalizability to the testing set over training time, reaching 70% accuracy, while the supervised approach quickly overfit the training data and performed poorly on the testing set with only 11% accuracy. This study demonstrates the proof-of-principle application of reinforcement learning to radiological images, particularly in localizing brain tumors using 2D contrast-enhanced MRI brain images. The utilization of a gridworld setting is naturally suitable for analyzing medical images.",1
"Reinforcement Learning (RL) algorithms such as DQN owe their success to Markov Decision Processes, and the fact that maximizing the sum of rewards allows using backward induction and reduce to the Bellman optimality equation. However, many real-world problems require optimization of an objective that is non-linear in cumulative rewards for which dynamic programming cannot be applied directly. For example, in a resource allocation problem, one of the objectives is to maximize long-term fairness among the users. We notice that when the function of the sum of rewards is considered, the problem loses its Markov nature. This paper addresses and formalizes the problem of optimizing a non-linear function of the long term average of rewards. We propose model-based and model-free algorithms to learn the policy, where the model-based policy is shown to achieve a regret of $\Tilde{O}\left(KDSA\sqrt{\frac{A}{T}}\right)$ for $K$ users. Further, using the fairness in cellular base-station scheduling, and queueing system scheduling as examples, the proposed algorithm is shown to significantly outperform the conventional RL approaches.",0
"The success of Reinforcement Learning (RL) algorithms like DQN is attributed to Markov Decision Processes, which allow backward induction and the use of the Bellman optimality equation to maximize the sum of rewards. However, real-world problems often require optimization of non-linear cumulative rewards, which cannot be directly solved through dynamic programming. For instance, optimizing long-term fairness among users in resource allocation problems involves a non-Markovian nature when the sum of rewards function is considered. In this paper, we formalize the optimization of non-linear long-term average rewards and propose model-based and model-free algorithms for policy learning. The model-based policy achieves a regret of $\Tilde{O}\left(KDSA\sqrt{\frac{A}{T}}\right)$ for $K$ users. Using fairness in cellular base-station scheduling and queueing system scheduling as examples, we demonstrate that the proposed algorithm significantly outperforms conventional RL approaches.",1
"We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.",0
"Our proposal introduces a new learning approach, named Self-Imitation via Reduction (SIR), which aims to solve complex reinforcement learning challenges with compositional structures. SIR relies on two main concepts: task reduction and self-imitation. The former involves actively breaking down a difficult task into smaller, easier ones that the RL agent can solve. Once the original task is accomplished, the agent creates a self-generated trajectory to follow. By continuously gathering and replicating these trajectories, the agent can gradually expand its knowledge and skills in the entire task space. Our experiments demonstrate that SIR significantly enhances learning speed and performance on various challenging continuous-control problems with sparse rewards and compositional structures. We provide access to code and videos on our website: https://sites.google.com/view/sir-compositional.",1
"In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness against worst-case situations. The robust RL framework addresses this challenge via a worst-case optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty and efficiently explores both the agent and adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally, we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments.",0
"During real-world tasks, reinforcement learning (RL) agents often encounter unforeseen situations that were not present during their training. To ensure dependable performance, RL agents must demonstrate resilience against worst-case scenarios. The robust RL framework addresses this issue by engaging in a worst-case optimization involving an agent and an adversary. However, previous robust RL algorithms have had problems with sample inefficiency, the lack of robustness guarantees, or scalability issues for larger problems. Our solution is the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm, which is designed to solve this problem while achieving near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty and efficiently explores both the agent and adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks through the use of neural network ensemble models and neural network policies. In our experiments, we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in various adversarial environments.",1
"Biomedical knowledge graphs permit an integrative computational approach to reasoning about biological systems. The nature of biological data leads to a graph structure that differs from those typically encountered in benchmarking datasets. To understand the implications this may have on the performance of reasoning algorithms, we conduct an empirical study based on the real-world task of drug repurposing. We formulate this task as a link prediction problem where both compounds and diseases correspond to entities in a knowledge graph. To overcome apparent weaknesses of existing algorithms, we propose a new method, PoLo, that combines policy-guided walks based on reinforcement learning with logical rules. These rules are integrated into the algorithm by using a novel reward function. We apply our method to Hetionet, which integrates biomedical information from 29 prominent bioinformatics databases. Our experiments show that our approach outperforms several state-of-the-art methods for link prediction while providing interpretability.",0
"The use of biomedical knowledge graphs allows for a comprehensive computational approach to understanding biological systems. However, the structure of biological data is distinct from that of typical benchmarking datasets. To evaluate how this affects the performance of reasoning algorithms, we undertook an empirical study examining drug repurposing. This involved a link prediction problem where compounds and diseases were entities in a knowledge graph. To address the limitations of current algorithms, we developed a novel method called PoLo. This incorporates policy-guided walks using reinforcement learning and logical rules, which are integrated through a new reward function. We applied PoLo to Hetionet, which combines biomedical information from 29 bioinformatics databases. Our results demonstrate that our approach surpasses a number of state-of-the-art methods for link prediction while also being interpretable.",1
"Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.",0
"It is uncommon for reinforcement learning methods trained on a small number of environments to develop policies that apply to new environments. To enhance generalization, we integrate the sequential structure inherent in reinforcement learning into the representation learning process. This method differs from recent approaches, which rarely use this structure explicitly. Specifically, we introduce a policy similarity metric (PSM) that measures the similarity of behavior between states based on a theoretical foundation. The PSM assigns high similarity to states in which the optimal policies in those states, as well as future states, are similar. Additionally, we propose a contrastive representation learning procedure that can embed any state similarity metric, which we utilize with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs boost generalization across various benchmarks, including LQR with spurious correlations, a pixel-based jumping task, and Distracting DM Control Suite.",1
"In recent years, meta-learning, in which a model is trained on a family of tasks (i.e. a task distribution), has emerged as an approach to training neural networks to perform tasks that were previously assumed to require structured representations, making strides toward closing the gap between humans and machines. However, we argue that evaluating meta-learning remains a challenge, and can miss whether meta-learning actually uses the structure embedded within the tasks. These meta-learners might therefore still be significantly different from humans learners. To demonstrate this difference, we first define a new meta-reinforcement learning task in which a structured task distribution is generated using a compositional grammar. We then introduce a novel approach to constructing a ""null task distribution"" with the same statistical complexity as this structured task distribution but without the explicit rule-based structure used to generate the structured task. We train a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compare it with human performance across the two task distributions. We find a double dissociation in which humans do better in the structured task distribution whereas agents do better in the null task distribution -- despite comparable statistical complexity. This work highlights that multiple strategies can achieve reasonable meta-test performance, and that careful construction of control task distributions is a valuable way to understand which strategies meta-learners acquire, and how they might differ from humans.",0
"Over the past few years, meta-learning has emerged as a promising method for training neural networks to perform tasks that previously required structured representations. This involves training a model on a family of tasks, or a task distribution, which has brought machines closer to humans in terms of performance. However, evaluating the effectiveness of meta-learning remains challenging, as it may not reveal whether the structure embedded within the tasks is actually being used. As a result, meta-learners may still differ significantly from human learners. To demonstrate this difference, we created a new meta-reinforcement learning task that employed a compositional grammar to generate a structured task distribution. We also developed a ""null task distribution"" with the same statistical complexity as the structured task distribution, but without the explicit rule-based structure. We then trained a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compared its performance with that of humans across the two task distributions. Our findings revealed that humans performed better on the structured task distribution, while the agents performed better on the null task distribution, despite the comparable statistical complexity. Our work highlights the importance of constructing control task distributions to better understand the strategies that meta-learners acquire and how they differ from human learners.",1
"Mixture models are an expressive hypothesis class that can approximate a rich set of policies. However, using mixture policies in the Maximum Entropy (MaxEnt) framework is not straightforward. The entropy of a mixture model is not equal to the sum of its components, nor does it have a closed-form expression in most cases. Using such policies in MaxEnt algorithms, therefore, requires constructing a tractable approximation of the mixture entropy. In this paper, we derive a simple, low-variance mixture-entropy estimator. We show that it is closely related to the sum of marginal entropies. Equipped with our entropy estimator, we derive an algorithmic variant of Soft Actor-Critic (SAC) to the mixture policy case and evaluate it on a series of continuous control tasks.",0
"Mixture models have a wide range of policies that they can approximate, making them a powerful hypothesis class. However, when using them in the Maximum Entropy framework, it is not a straightforward process. The entropy of a mixture model cannot be calculated by adding up its components, and it is often impossible to express it in a closed-form. To utilize mixture policies in MaxEnt algorithms, it is necessary to create a feasible approximation of the mixture entropy. This paper presents a low-variance mixture-entropy estimator that is easy to use. The estimator is closely related to the sum of marginal entropies. By using our entropy estimator, we developed a variant of the Soft Actor-Critic (SAC) algorithm that can handle mixture policies. We evaluated this algorithm on a series of continuous control tasks.",1
A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient Wasserstein natural gradient (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including a divergence penalty in the objective to establish a trust region. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines.,0
"The proposal presents a fresh optimization method that can be employed in reinforcement learning (RL) policy gradient methods and evolution strategies. The technique employs a computationally efficient Wasserstein natural gradient (WNG) descent that utilizes the geometry induced by a Wasserstein penalty to expedite optimization. This technique is consistent with the recent trend in RL, which involves incorporating a divergence penalty in the objective to establish a trust region. Experimental results on difficult tasks show that this method enhances both computational efficiency and performance compared to advanced baselines.",1
"Goal-conditioned hierarchical reinforcement learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is often large. Searching in a large goal space poses difficulties for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a $k$-step adjacent region of the current state using an adjacency constraint. We theoretically prove that the proposed adjacency constraint preserves the optimal hierarchical policy in deterministic MDPs, and show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks show that incorporating the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.",0
"Goal-conditioned hierarchical reinforcement learning (HRL) has the potential to enhance reinforcement learning (RL) techniques. However, it often struggles with low training efficiency due to the large action space in the high-level, or goal space. This makes generating high-level subgoals and learning low-level policies difficult. To address this issue, we propose limiting the high-level action space to a $k$-step adjacent region of the current state using an adjacency constraint. Theoretical proof shows that the proposed constraint preserves the optimal hierarchical policy in deterministic MDPs. We also present a practical implementation of the constraint by training an adjacency network that distinguishes between adjacent and non-adjacent subgoals. Our experimental results on discrete and continuous control tasks demonstrate that the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.",1
"We introduce Adaptive Procedural Task Generation (APT-Gen), an approach to progressively generate a sequence of tasks as curricula to facilitate reinforcement learning in hard-exploration problems. At the heart of our approach, a task generator learns to create tasks from a parameterized task space via a black-box procedural generation module. To enable curriculum learning in the absence of a direct indicator of learning progress, we propose to train the task generator by balancing the agent's performance in the generated tasks and the similarity to the target tasks. Through adversarial training, the task similarity is adaptively estimated by a task discriminator defined on the agent's experiences, allowing the generated tasks to approximate target tasks of unknown parameterization or outside of the predefined task space. Our experiments on the grid world and robotic manipulation task domains show that APT-Gen achieves substantially better performance than various existing baselines by generating suitable tasks of rich variations.",0
"APT-Gen is a method for creating a series of tasks to aid reinforcement learning in complex problems. This approach involves a task generator that learns to create tasks from a set of parameters using a procedural generation module. To teach the generator to create effective curricula, we train it to balance the agent's performance in the generated tasks and the similarity to the target tasks, even when progress is not directly observable. Using adversarial training, a task discriminator estimates similarity based on the agent's experiences, allowing the generated tasks to approximate target tasks beyond the predefined task space. Our experiments on grid worlds and robotic manipulation demonstrate that APT-Gen outperforms existing methods by creating diverse, suitable tasks.",1
"In this paper, we investigate the sample complexity of policy evaluation in infinite-horizon offline reinforcement learning (also known as the off-policy evaluation problem) with linear function approximation. We identify a hard regime $d\gamma^{2}>1$, where $d$ is the dimension of the feature vector and $\gamma$ is the discount rate. In this regime, for any $q\in[\gamma^{2},1]$, we can construct a hard instance such that the smallest eigenvalue of its feature covariance matrix is $q/d$ and it requires $\Omega\left(\frac{d}{\gamma^{2}\left(q-\gamma^{2}\right)\varepsilon^{2}}\exp\left(\Theta\left(d\gamma^{2}\right)\right)\right)$ samples to approximate the value function up to an additive error $\varepsilon$. Note that the lower bound of the sample complexity is exponential in $d$. If $q=\gamma^{2}$, even infinite data cannot suffice. Under the low distribution shift assumption, we show that there is an algorithm that needs at most $O\left(\max\left\{ \frac{\left\Vert \theta^{\pi}\right\Vert _{2}^{4}}{\varepsilon^{4}}\log\frac{d}{\delta},\frac{1}{\varepsilon^{2}}\left(d+\log\frac{1}{\delta}\right)\right\} \right)$ samples ($\theta^{\pi}$ is the parameter of the policy in linear function approximation) and guarantees approximation to the value function up to an additive error of $\varepsilon$ with probability at least $1-\delta$.",0
"The aim of this study is to examine the complexity of sample collection required for policy evaluation in infinite-horizon offline reinforcement learning with linear function approximation, also known as the off-policy evaluation problem. We have discovered a challenging regime where the feature vector's dimension is represented by ""d"" and the discount rate by """", and ""d2>1."" In this scenario, we can construct a complicated instance for any q[2,1] such that the feature covariance matrix's smallest eigenvalue is q/d. To approximate the value function within an additive error of , we require a minimum of (d/2(q2)2) samples, which is an exponential lower bound in ""d"". Even infinite data cannot suffice when q=2. However, assuming a low distribution shift, we have demonstrated an algorithm that can approximate the value function up to an additive error of  with a probability of at least 1-. This algorithm needs only a maximum of O(max{2^4/^4 log(d/), (1/^2)(d+log(1/))}) samples, where  is the policy's parameter in linear function approximation.",1
"Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments. An accompanying video can be found here: https://youtu.be/nxGGHdZOFts",0
"Offline learning is crucial for the practical application of reinforcement learning (RL) in real-world systems. This involves analyzing scenarios where there is data available from a system's operation, but no direct access to the system during policy learning. Recent research has shown that RL policies can be trained using offline data, either with model-free policies learned directly from the data or by planning on top of learned models of the data. While model-free policies tend to perform better, they are less transparent, more difficult to control externally, and less easy to integrate into larger systems. To address these issues, we propose an offline learner that generates a model, which can be used to control the system through planning. This approach allows for easily controllable policies directly from data without ever interacting with the system. We demonstrate the effectiveness of our Model-Based Offline Planning (MBOP) algorithm on a series of robotics-inspired tasks and show its ability to respect environmental constraints. Our algorithm can find near-optimal policies for certain simulated systems with as little as 50 seconds of real-time system interaction and create zero-shot goal-conditioned policies on a series of environments. A video demonstrating our algorithm can be found at https://youtu.be/nxGGHdZOFts.",1
"Since its introduction a decade ago, \emph{relative entropy policy search} (REPS) has demonstrated successful policy learning on a number of simulated and real-world robotic domains, not to mention providing algorithmic components used by many recently proposed reinforcement learning (RL) algorithms. While REPS is commonly known in the community, there exist no guarantees on its performance when using stochastic and gradient-based solvers. In this paper we aim to fill this gap by providing guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. We first consider the setting in which we are given access to exact gradients and demonstrate how near-optimality of the objective translates to near-optimality of the policy. We then consider the practical setting of stochastic gradients, and introduce a technique that uses \emph{generative} access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.",0
"A decade ago, the introduction of \emph{relative entropy policy search} (REPS) led to successful policy learning in various simulated and real-world robotic domains and served as a crucial component in several proposed reinforcement learning (RL) algorithms. Despite its popularity, there are no guarantees on the performance of REPS when using stochastic and gradient-based solvers. This paper aims to fill this void by providing guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. Initially, we consider the setting with exact gradients and show how the near-optimality of the objective corresponds to similar performance of the policy. Subsequently, we explore the practical setting of stochastic gradients and introduce a technique that utilizes generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.",1
"The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.",0
"Autonomous vehicle control increasingly employs neural networks and reinforcement learning, but the resulting control policies' lack of transparency hinders their deployment. This study proposes a reinforcement learning-based approach to autonomous vehicle longitudinal control, incorporating rule-based safety cages to enhance safety and offer weak supervision to the reinforcement learning agent. By guiding the agent towards meaningful states and actions, weak supervision improves training convergence and final policy safety. Additionally, the rule-based supervisory controller is fully interpretable, enabling traditional validation and verification approaches to ensure vehicle safety. Comparing models with and without safety cages and optimal and constrained model parameters, the study demonstrates the consistent improvement of weak supervision in exploration safety, convergence speed, and model performance. The study also shows that safety cages can facilitate learning a safe driving policy when the model parameters are constrained or sub-optimal, where reinforcement learning alone may fail.",1
"In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior and achieve the maximal empowerment, we propose two methods respectively based on the transitional probability model and Gaussian mixture model. We substantiate our claims through rigorous mathematical derivations and experimental analyses.",0
"This article revisits the unsupervised reinforcement learning technique called variational intrinsic control (VIC) that allows agents to identify the largest set of intrinsic options available to them. The original work by Gregor et al. (2016) introduced two VIC algorithms: one that explicitly represents options, and the other that does it implicitly. However, we identify that the intrinsic reward used in the latter may be biased in stochastic environments, leading to suboptimal solutions. To address this issue and achieve maximum empowerment, we propose two solutions based on the transitional probability model and Gaussian mixture model, respectively. We support our arguments with thorough mathematical derivations and experimental analysis.",1
"Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.",0
"The use of state-of-the-art deep reinforcement learning (RL) algorithms is difficult due to their sensitivity to hyperparameters, despite significant progress in solving challenging problems across different fields. This is partly due to the non-stationarity of the RL problem, which may require different hyperparameter settings at various stages of the learning process. Additionally, hyperparameter optimization (HPO) in the RL setting necessitates a large number of environment interactions, making it difficult to apply RL successes to real-world applications. In response to these issues, we propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. Our approach optimizes both hyperparameters and neural architecture while training the agent and shares collected experience across the population, resulting in a substantially increased sample efficiency of the meta-optimization. We demonstrate the efficacy of our approach in a case study using the TD3 algorithm in the MuJoCo benchmark suite, reducing the number of environment interactions required for meta-optimization compared to population-based training by up to an order of magnitude.",1
"Deeply-learned planning methods are often based on learning representations that are optimized for unrelated tasks. For example, they might be trained on reconstructing the environment. These representations are then combined with predictor functions for simulating rollouts to navigate the environment. We find this principle of learning representations unsatisfying and propose to learn them such that they are directly optimized for the task at hand: to be maximally predictable for the predictor function. This results in representations that are by design optimal for the downstream task of planning, where the learned predictor function is used as a forward model.   To this end, we propose a new way of jointly learning this representation along with the prediction function, a system we dub Latent Representation Prediction Network (LARP). The prediction function is used as a forward model for search on a graph in a viewpoint-matching task and the representation learned to maximize predictability is found to outperform a pre-trained representation. Our approach is shown to be more sample-efficient than standard reinforcement learning methods and our learned representation transfers successfully to dissimilar objects.",0
"Planning methods based on deep learning often utilize representations that are optimized for tasks unrelated to the planning task at hand. For instance, they may be trained to reconstruct the environment and then merged with predictor functions to navigate the environment through simulating rollouts. However, we find this approach to learning representations unsatisfactory and propose optimizing them directly for the planning task, making them maximally predictable for the predictor function. This results in representations that are optimized for downstream planning tasks, where the predictor function is deployed as a forward model. Our proposed system, the Latent Representation Prediction Network (LARP), jointly learns this representation and prediction function. In a viewpoint-matching task, we use the prediction function as a forward model for graph search, and the learned representation maximizes predictability and outperforms pre-trained representations. Our method is more sample-efficient than standard reinforcement learning methods, and the learned representation transfers effectively to dissimilar objects.",1
"Offline reinforcement learning restricts the learning process to rely only on logged-data without access to an environment. While this enables real-world applications, it also poses unique challenges. One important challenge is dealing with errors caused by the overestimation of values for state-action pairs not well-covered by the training data. Due to bootstrapping, these errors get amplified during training and can lead to divergence, thereby crippling learning. To overcome this challenge, we introduce Regularized Behavior Value Estimation (R-BVE). Unlike most approaches, which use policy improvement during training, R-BVE estimates the value of the behavior policy during training and only performs policy improvement at deployment time. Further, R-BVE uses a ranking regularisation term that favours actions in the dataset that lead to successful outcomes. We provide ample empirical evidence of R-BVE's effectiveness, including state-of-the-art performance on the RL Unplugged ATARI dataset. We also test R-BVE on new datasets, from bsuite and a challenging DeepMind Lab task, and show that R-BVE outperforms other state-of-the-art discrete control offline RL methods.",0
"The learning process in offline reinforcement learning is limited to logged-data and excludes access to an environment. Although this is useful for real-world applications, it presents unique obstacles. One such challenge is managing errors that arise from state-action pairs with inadequate training data and result in overestimated values. These errors are amplified during training due to bootstrapping and can lead to divergence, hindering the learning process. To tackle this issue, we propose Regularized Behavior Value Estimation (R-BVE). Unlike other approaches, R-BVE estimates the value of the behavior policy during training and only implements policy improvement during deployment. Additionally, R-BVE applies a ranking regularisation term that favors actions leading to successful outcomes in the dataset. We provide substantial empirical evidence demonstrating R-BVE's effectiveness, including top-notch performance on the RL Unplugged ATARI dataset. We also evaluate R-BVE on new datasets, from bsuite and a challenging DeepMind Lab task, and prove that it surpasses other state-of-the-art offline RL methods for discrete control.",1
"Deploying Reinforcement Learning (RL) agents in the real-world require that the agents satisfy safety constraints. Current RL agents explore the environment without considering these constraints, which can lead to damage to the hardware or even other agents in the environment. We propose a new method, LBPO, that uses a Lyapunov-based barrier function to restrict the policy update to a safe set for each training iteration. Our method also allows the user to control the conservativeness of the agent with respect to the constraints in the environment. LBPO significantly outperforms state-of-the-art baselines in terms of the number of constraint violations during training while being competitive in terms of performance. Further, our analysis reveals that baselines like CPO and SDDPG rely mostly on backtracking to ensure safety rather than safe projection, which provides insight into why previous methods might not have effectively limit the number of constraint violations.",0
"To ensure the safety of Reinforcement Learning (RL) agents in the real world, it is necessary for them to comply with specific safety constraints. However, current RL agents do not account for these constraints during their exploration of the environment, which can result in damage to both hardware and other agents. We have developed a new approach called LBPO that utilizes a Lyapunov-based barrier function to restrict policy updates to a safe set during each training iteration. Our method also offers users the ability to regulate the agent's level of conservativeness in relation to the environment's constraints. Compared to existing methods, LBPO significantly reduces the number of constraint violations during training while maintaining competitive performance. Additionally, our analysis shows that previous methods such as CPO and SDDPG rely heavily on backtracking to ensure safety rather than safe projection, which explains why they may not effectively limit the number of constraint violations.",1
"Recent work has shown that sparse representations -- where only a small percentage of units are active -- can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Specifically, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision -- reduced discrimination -- due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We first show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains.",0
"Recent research has demonstrated that sparse representations, in which only a small portion of units are active, can significantly diminish interference. However, these studies relied on complex regularization or meta-learning methods that were only utilized in an offline pre-training phase. In this study, we pursue an alternative approach that achieves sparsity through design, rather than learning. Specifically, we have developed an activation function that deterministically produces sparse representations, making it more conducive to online training. Our approach employs binning, but overcomes the limitations of zero gradients and reduced discrimination by introducing a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and overlap between bins. We demonstrate the robustness of FTA under covariate shift in a synthetic online supervised learning problem and then apply it to deep reinforcement learning in discrete and continuous control environments. Our results indicate that algorithms using FTAs can learn a stable policy faster without requiring target networks in most domains.",1
"Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state-action pairs are drawn from a generative model in each iteration), substantial progress has been made recently towards understanding the sample efficiency of Q-learning. Take a $\gamma$-discounted infinite-horizon MDP with state space $\mathcal{S}$ and action space $\mathcal{A}$: to yield an entrywise $\varepsilon$-accurate estimate of the optimal Q-function, state-of-the-art theory for Q-learning proves that a sample size on the order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$ is sufficient, which, however, fails to match with the existing minimax lower bound. This gives rise to natural questions: what is the sharp sample complexity of Q-learning? Is Q-learning provably sub-optimal? In this work, we settle these questions by (1) demonstrating that the sample complexity of Q-learning is at most on the order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to some log factor) for any $0<\varepsilon <1$, and (2) developing a matching lower bound to confirm the sharpness of our result. Our findings unveil both the effectiveness and limitation of Q-learning: its sample complexity matches that of speedy Q-learning without requiring extra computation and storage, albeit still being considerably higher than the minimax lower bound.",0
"Reinforcement learning relies heavily on Q-learning, which aims to learn the optimal Q-function of a Markov decision process (MDP) in a model-free manner. Recent developments in the synchronous setting, where independent samples are drawn for all state-action pairs from a generative model in each iteration, have substantially improved our understanding of the sample efficiency of Q-learning. For a $\gamma$-discounted infinite-horizon MDP with state space $\mathcal{S}$ and action space $\mathcal{A}$, the state-of-the-art theory for Q-learning suggests that a sample size of roughly $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$ is sufficient to yield an entrywise $\varepsilon$-accurate estimate of the optimal Q-function. However, this falls short of the existing minimax lower bound, raising questions about the true sample complexity of Q-learning and whether it is provably sub-optimal. Our work settles these questions by showing that the sample complexity of Q-learning is at most on the order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to some log factor) for any $0<\varepsilon <1$, and by developing a matching lower bound to confirm the sharpness of our result. These findings shed light on the strengths and limitations of Q-learning, which can achieve the same sample complexity as speedy Q-learning without additional computation and storage, but still falls significantly short of the minimax lower bound.",1
"This paper develops a hierarchical reinforcement learning architecture for multi-mission spaceflight campaign design under uncertainty, including vehicle design, infrastructure deployment planning, and space transportation scheduling. This problem involves a high-dimensional design space and is challenging especially with uncertainty present. To tackle this challenge, the developed framework has a hierarchical structure with reinforcement learning (RL) and network-based mixed-integer linear programming (MILP), where the former optimizes campaign-level decisions (e.g., design of the vehicle used throughout the campaign, destination demand assigned to each mission in the campaign), whereas the latter optimizes the detailed mission-level decisions (e.g., when to launch what from where to where). The framework is applied to a set of human lunar exploration campaign scenarios with uncertain in-situ resource utilization (ISRU) performance as a case study. The main value of this work is its integration of the rapidly growing RL research and the existing MILP-based space logistics methods through a hierarchical framework to handle the otherwise intractable complexity of space mission design under uncertainty. We expect this unique framework to be a critical steppingstone for the emerging research direction of artificial intelligence for space mission design.",0
"In this paper, a hierarchical reinforcement learning structure is presented to address the challenges of multi-mission spaceflight campaign design under uncertainty. This problem involves a complex design space and is particularly challenging when uncertainty is present. To overcome this challenge, the framework employs reinforcement learning (RL) and network-based mixed-integer linear programming (MILP) in a hierarchical structure. RL optimizes campaign-level decisions (such as vehicle design and destination demand allocation), while MILP optimizes mission-level decisions (such as launch timing and location). The framework is applied to human lunar exploration campaign scenarios with uncertain in-situ resource utilization (ISRU) performance as a case study. The main contribution of the work is the integration of RL and MILP-based space logistics methods through a hierarchical framework, providing a solution to the complexity of space mission design under uncertainty. This framework is expected to be a critical milestone for the emerging research direction of artificial intelligence for space mission design.",1
"Reinforcement learning algorithms are widely used in domains where it is desirable to provide a personalized service. In these domains it is common that user data contains sensitive information that needs to be protected from third parties. Motivated by this, we study privacy in the context of finite-horizon Markov Decision Processes (MDPs) by requiring information to be obfuscated on the user side. We formulate this notion of privacy for RL by leveraging the local differential privacy (LDP) framework. We present an optimistic algorithm that simultaneously satisfies LDP requirements, and achieves sublinear regret. We also establish a lower bound for regret minimization in finite-horizon MDPs with LDP guarantees. These results show that while LDP is appealing in practical applications, the setting is inherently more complex. In particular, our results demonstrate that the cost of privacy is multiplicative when compared to non-private settings.",0
"Reinforcement learning algorithms are widely utilized in domains that necessitate personalized services. In such domains, it is not uncommon for user data to comprise sensitive information that requires protection from third parties. In light of this, we examine privacy within the finite-horizon Markov Decision Processes (MDPs) context by mandating the obfuscation of information on the user's end. To accomplish this notion of privacy for RL, we employ the local differential privacy (LDP) framework. Our optimistic algorithm satisfies LDP requirements while concurrently achieving sublinear regret. Additionally, we establish a lower bound for regret minimization in finite-horizon MDPs with LDP guarantees. Our findings indicate that although LDP is attractive in practical applications, the setting is inherently more complex. In particular, our research demonstrates that the cost of privacy is multiplicative when compared to non-private settings.",1
"Safety is essential for reinforcement learning (RL) applied in real-world situations. Chance constraints are suitable to represent the safety requirements in stochastic systems. Previous chance-constrained RL methods usually have a low convergence rate, or only learn a conservative policy. In this paper, we propose a model-based chance constrained actor-critic (CCAC) algorithm which can efficiently learn a safe and non-conservative policy. Different from existing methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems, where the objective function and safe probability is simultaneously optimized with adaptive weights. In order to improve the convergence rate, CCAC utilizes the gradient of dynamic model to accelerate policy optimization. The effectiveness of CCAC is demonstrated by a stochastic car-following task. Experiments indicate that compared with previous RL methods, CCAC improves the performance while guaranteeing safety, with a five times faster convergence rate. It also has 100 times higher online computation efficiency than traditional safety techniques such as stochastic model predictive control.",0
"Ensuring safety is imperative when using reinforcement learning (RL) in real-life scenarios. Stochastic systems may utilize chance constraints to depict safety requirements. However, prior chance-constrained RL techniques usually exhibit a slow convergence rate or only learn a cautious policy. Our study introduces a model-based chance constrained actor-critic (CCAC) algorithm that efficiently learns a safe and non-conservative policy. Unlike current methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems. The objective function and safe probability are simultaneously optimized with adaptive weights. CCAC utilizes the gradient of the dynamic model to enhance policy optimization and improve the convergence rate. Our experiments on a stochastic car-following task demonstrate that CCAC improves performance while ensuring safety, with a five times faster convergence rate than previous RL methods. Additionally, it exhibits 100 times higher online computation efficiency than traditional safety techniques like stochastic model predictive control.",1
"Model-Based Reinforcement Learning (MBRL) is one category of Reinforcement Learning (RL) algorithms which can improve sampling efficiency by modeling and approximating system dynamics. It has been widely adopted in the research of robotics, autonomous driving, etc. Despite its popularity, there still lacks some sophisticated and reusable open-source frameworks to facilitate MBRL research and experiments. To fill this gap, we develop a flexible and modularized framework, Baconian, which allows researchers to easily implement a MBRL testbed by customizing or building upon our provided modules and algorithms. Our framework can free users from re-implementing popular MBRL algorithms from scratch thus greatly save users' efforts on MBRL experiments.",0
"MBRL, a type of RL algorithms, enhances sampling efficiency by approximating system dynamics and is commonly used in areas such as robotics and autonomous driving. However, there is a need for more advanced and reusable open-source frameworks to support MBRL research and experiments. Therefore, we have designed Baconian, a flexible and modularized framework that enables researchers to build a MBRL testbed by customizing or incorporating our modules and algorithms. This framework eliminates the need for users to recreate popular MBRL algorithms from scratch, saving time and effort in MBRL experimentation.",1
"Successor-style representations have many advantages for reinforcement learning: for example, they can help an agent generalize from past experience to new goals, and they have been proposed as explanations of behavioral and neural data from human and animal learners. They also form a natural bridge between model-based and model-free RL methods: like the former they make predictions about future experiences, and like the latter they allow efficient prediction of total discounted rewards. However, successor-style representations are not optimized to generalize across policies: typically, we maintain a limited-length list of policies, and share information among them by representation learning or GPI. Successor-style representations also typically make no provision for gathering information or reasoning about latent variables. To address these limitations, we bring together ideas from predictive state representations, belief space value iteration, successor features, and convex analysis: we develop a new, general successor-style representation, together with a Bellman equation that connects multiple sources of information within this representation, including different latent states, policies, and reward functions. The new representation is highly expressive: for example, it lets us efficiently read off an optimal policy for a new reward function, or a policy that imitates a new demonstration. For this paper, we focus on exact computation of the new representation in small, known environments, since even this restricted setting offers plenty of interesting questions. Our implementation does not scale to large, unknown environments -- nor would we expect it to, since it generalizes POMDP value iteration, which is difficult to scale. However, we believe that future work will allow us to extend our ideas to approximate reasoning in large, unknown environments.",0
"The use of successor-style representations is beneficial for reinforcement learning due to its ability to help agents learn from previous experiences and apply them to new goals. This approach has also been used to explain behavioral and neural data from both human and animal learners. Successor-style representations can bridge the gap between model-based and model-free RL methods by predicting future experiences and efficiently predicting total discounted rewards. However, these representations are not designed to generalize across policies, and typically only consider a limited list of policies. Furthermore, they do not account for latent variables. To overcome these limitations, we combine ideas from various approaches and develop a new successor-style representation that can connect information from different sources, including latent states, policies, and reward functions. This new representation is highly expressive and allows for the efficient computation of optimal policies for new reward functions or imitating new demonstrations. While our implementation is limited to small, known environments, we believe that future work will enable us to apply our ideas to larger, unknown environments.",1
"Adversarial training, a special case of multi-objective optimization, is an increasingly prevalent machine learning technique: some of its most notable applications include GAN-based generative modeling and self-play techniques in reinforcement learning which have been applied to complex games such as Go or Poker. In practice, a \emph{single} pair of networks is typically trained in order to find an approximate equilibrium of a highly nonconcave-nonconvex adversarial problem. However, while a classic result in game theory states such an equilibrium exists in concave-convex games, there is no analogous guarantee if the payoff is nonconcave-nonconvex. Our main contribution is to provide an approximate minimax theorem for a large class of games where the players pick neural networks including WGAN, StarCraft II, and Blotto Game. Our findings rely on the fact that despite being nonconcave-nonconvex with respect to the neural networks parameters, these games are concave-convex with respect to the actual models (e.g., functions or distributions) represented by these neural networks.",0
"Adversarial training is a machine learning technique that is becoming more popular, and it is a type of multi-objective optimization. It has been used in various applications, including GAN-based generative modeling and self-play techniques in reinforcement learning for complex games like Go or Poker. Typically, only one pair of networks is trained to find an approximate equilibrium of a highly nonconcave-nonconvex adversarial problem. However, in nonconcave-nonconvex games, there is no guarantee of an equilibrium, unlike in concave-convex games. Our main contribution is an approximate minimax theorem for a wide range of games, such as WGAN, StarCraft II, and Blotto Game, where players select neural networks. Despite being nonconcave-nonconvex concerning neural network parameters, these games are concave-convex concerning the actual models represented by these neural networks, such as functions or distributions.",1
"Developing an agent in reinforcement learning (RL) that is capable of performing complex control tasks directly from high-dimensional observation such as raw pixels is yet a challenge as efforts are made towards improving sample efficiency and generalization. This paper considers a learning framework for Curiosity Contrastive Forward Dynamics Model (CCFDM) in achieving a more sample-efficient RL based directly on raw pixels. CCFDM incorporates a forward dynamics model (FDM) and performs contrastive learning to train its deep convolutional neural network-based image encoder (IE) to extract conducive spatial and temporal information for achieving a more sample efficiency for RL. In addition, during training, CCFDM provides intrinsic rewards, produced based on FDM prediction error, encourages the curiosity of the RL agent to improve exploration. The diverge and less-repetitive observations provide by both our exploration strategy and data augmentation available in contrastive learning improve not only the sample efficiency but also the generalization. Performance of existing model-free RL methods such as Soft Actor-Critic built on top of CCFDM outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.",0
"The challenge of developing an agent in reinforcement learning (RL) that can perform complex control tasks directly from high-dimensional observation, such as raw pixels, remains. However, there are efforts to improve sample efficiency and generalization, and this paper explores a learning framework for Curiosity Contrastive Forward Dynamics Model (CCFDM) that aims to achieve more sample-efficient RL based directly on raw pixels. CCFDM utilizes a forward dynamics model (FDM) and contrastive learning to train its deep convolutional neural network-based image encoder (IE) to extract useful spatial and temporal information for achieving more sample efficiency for RL. During training, CCFDM provides intrinsic rewards based on FDM prediction error, which encourages the RL agent's curiosity to improve exploration. By utilizing exploration strategies and data augmentation in contrastive learning, CCFDM provides divergent and less-repetitive observations, which improve both sample efficiency and generalization. CCFDM's performance, combined with existing model-free RL methods such as Soft Actor-Critic, outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.",1
"Reinforcement learning (RL) algorithms aim to learn optimal decisions in unknown environments through experience of taking actions and observing the rewards gained. In some cases, the environment is not influenced by the actions of the RL agent, in which case the problem can be modeled as a contextual multi-armed bandit and lightweight \emph{myopic} algorithms can be employed. On the other hand, when the RL agent's actions affect the environment, the problem must be modeled as a Markov decision process and more complex RL algorithms are required which take the future effects of actions into account. Moreover, in many modern RL settings, it is unknown from the outset whether or not the agent's actions will impact the environment and it is often not possible to determine which RL algorithm is most fitting. In this work, we propose to avoid this dilemma entirely and incorporate a choice mechanism into our RL framework. Rather than assuming a specific problem structure, we use a probabilistic structure estimation procedure based on a likelihood-ratio (LR) test to make a more informed selection of learning algorithm. We derive a sufficient condition under which myopic policies are optimal, present an LR test for this condition, and derive a bound on the regret of our framework. We provide examples of real-world scenarios where our framework is needed and provide extensive simulations to validate our approach.",0
"The goal of reinforcement learning (RL) algorithms is to learn the best decisions in unfamiliar environments by taking actions and observing the rewards. When the environment is not affected by the RL agent's actions, the problem can be represented as a contextual multi-armed bandit, and lightweight myopic algorithms can be used. However, when the agent's actions have an impact on the environment, more complex RL algorithms are needed, which consider the future consequences of actions, and the problem must be modeled as a Markov decision process. In contemporary RL settings, it is often unclear whether the agent's actions will affect the environment, and it is difficult to determine which RL algorithm is most suitable. To address this issue, we introduce a choice mechanism in our RL framework that employs a probabilistic structure estimation method based on a likelihood-ratio (LR) test to make a more informed selection of learning algorithm. We present a sufficient condition under which myopic policies are optimal, propose an LR test for this condition, and establish a bound on the regret of our framework. We offer examples of real-life circumstances where our framework is necessary and provide comprehensive simulations to validate our approach.",1
"The recent remarkable progress of deep reinforcement learning (DRL) stands on regularization of policy for stable and efficient learning. A popular method, named proximal policy optimization (PPO), has been introduced for this purpose. PPO clips density ratio of the latest and baseline policies with a threshold, while its minimization target is unclear. As another problem of PPO, the symmetric threshold is given numerically while the density ratio itself is in asymmetric domain, thereby causing unbalanced regularization of the policy. This paper therefore proposes a new variant of PPO by considering a regularization problem of relative Pearson (RPE) divergence, so-called PPO-RPE. This regularization yields the clear minimization target, which constrains the latest policy to the baseline one. Through its analysis, the intuitive threshold-based design consistent with the asymmetry of the threshold and the domain of density ratio can be derived. Through four benchmark tasks, PPO-RPE performed as well as or better than the conventional methods in terms of the task performance by the learned policy.",0
"Deep reinforcement learning (DRL) has made significant advancements recently by regularizing policy to ensure stable and efficient learning. One popular method, called proximal policy optimization (PPO), clips the density ratio of the latest and baseline policies using a threshold, but its minimization target lacks clarity. Additionally, PPO faces an issue where the symmetric threshold is numerically given, while the density ratio exists in an asymmetric domain, leading to unbalanced policy regularization. To address these problems, this study proposes a new version of PPO called PPO-RPE, which regularizes policy using relative Pearson (RPE) divergence. This leads to a clear minimization target that constrains the latest policy to the baseline one. Through analysis, the study derives an intuitive threshold-based design consistent with the asymmetry of the threshold and the domain of density ratio. In four benchmark tasks, PPO-RPE performs just as well or better than conventional methods in terms of task performance by the learned policy.",1
"Reinforcement learning has been shown to be highly successful at many challenging tasks. However, success heavily relies on well-shaped rewards. Intrinsically motivated RL attempts to remove this constraint by defining an intrinsic reward function. Motivated by the self-consciousness concept in psychology, we make a natural assumption that the agent knows what constitutes itself, and propose a new intrinsic objective that encourages the agent to have maximum control on the environment. We mathematically formalize this reward as the mutual information between the agent state and the surrounding state under the current agent policy. With this new intrinsic motivation, we are able to outperform previous methods, including being able to complete the pick-and-place task for the first time without using any task reward. A video showing experimental results is available at https://youtu.be/AUCwc9RThpk.",0
"Numerous difficult tasks have been successfully accomplished through the use of reinforcement learning. However, the degree of success is heavily reliant on having well-designed rewards. Intrinsically motivated RL seeks to alleviate this limitation by creating an internal reward system. By drawing inspiration from self-awareness theories in psychology, we assume that the agent has knowledge of its own makeup and propose a new intrinsic objective that emphasizes the ability to control the environment. We quantify this reward as the mutual information between the agent state and the surrounding state using the current agent policy. This new intrinsic motivation has resulted in superior performance compared to previous methods, including successfully completing the pick-and-place task without relying on any task reward. To view the experimental results, please watch the following video: https://youtu.be/AUCwc9RThpk.",1
"Many modern approaches to offline Reinforcement Learning (RL) utilize behavior regularization, typically augmenting a model-free actor critic algorithm with a penalty measuring divergence of the policy from the offline data. In this work, we propose an alternative approach to encouraging the learned policy to stay close to the data, namely parameterizing the critic as the log-behavior-policy, which generated the offline data, plus a state-action value offset term, which can be learned using a neural network. Behavior regularization then corresponds to an appropriate regularizer on the offset term. We propose using a gradient penalty regularizer for the offset term and demonstrate its equivalence to Fisher divergence regularization, suggesting connections to the score matching and generative energy-based model literature. We thus term our resulting algorithm Fisher-BRC (Behavior Regularized Critic). On standard offline RL benchmarks, Fisher-BRC achieves both improved performance and faster convergence over existing state-of-the-art methods.",0
"Numerous contemporary offline Reinforcement Learning (RL) approaches employ behavior regularization. This usually involves augmenting a model-free actor critic algorithm with a penalty that measures the divergence of the policy from the offline data. Our work proposes an alternative approach to encourage the learned policy to remain close to the data. We accomplish this by parameterizing the critic as the log-behavior-policy that generated the offline data, together with a state-action value offset term that a neural network can learn. The appropriate regularizer on the offset term results in behavior regularization. We suggest using a gradient penalty regularizer and show that it is equivalent to Fisher divergence regularization. This suggests connections to the score matching and generative energy-based model literature. Our resulting algorithm is called Fisher-BRC (Behavior Regularized Critic). On standard offline RL benchmarks, Fisher-BRC outperforms existing state-of-the-art methods in terms of improved performance and faster convergence.",1
"A core operation in reinforcement learning (RL) is finding an action that is optimal with respect to a learned value function. This operation is often challenging when the learned value function takes continuous actions as input. We introduce deep radial-basis value functions (RBVFs): value functions learned using a deep network with a radial-basis function (RBF) output layer. We show that the maximum action-value with respect to a deep RBVF can be approximated easily and accurately. Moreover, deep RBVFs can represent any true value function owing to their support for universal function approximation. We extend the standard DQN algorithm to continuous control by endowing the agent with a deep RBVF. We show that the resultant agent, called RBF-DQN, significantly outperforms value-function-only baselines, and is competitive with state-of-the-art actor-critic algorithms.",0
"The fundamental process in reinforcement learning is to find the best action based on a learned value function. However, this can be difficult when the value function uses continuous actions as input. To address this issue, we introduce deep radial-basis value functions (RBVFs), which are learned through a deep network that includes a radial-basis function (RBF) output layer. Our research demonstrates that finding the maximum action-value with respect to a deep RBVF can be done with ease and accuracy. Additionally, deep RBVFs can represent any true value function due to their support for universal function approximation. We have expanded the standard DQN algorithm for continuous control by equipping the agent with a deep RBVF. Our findings indicate that the resulting agent, RBF-DQN, outperforms value-function-only methods and is on par with state-of-the-art actor-critic algorithms.",1
"This book chapter describes a novel approach to training machine learning systems by means of a hybrid computer setup i.e. a digital computer tightly coupled with an analog computer. As an example a reinforcement learning system is trained to balance an inverted pendulum which is simulated on an analog computer, thus demonstrating a solution to the major challenge of adequately simulating the environment for reinforcement learning.",0
"In this book chapter, a new method for training machine learning systems is explained. The method involves using a hybrid computer setup, which combines a digital computer with an analog computer. The chapter provides an example of how this method can be used to train a reinforcement learning system to balance an inverted pendulum. The simulation of the pendulum is done using the analog computer, which addresses the main issue of accurately simulating the environment for reinforcement learning.",1
"Digital twins are emerging in many industries, typically consisting of simulation models and data associated with a specific physical system. One of the main reasons for developing a digital twin, is to enable the simulation of possible consequences of a given action, without the need to interfere with the physical system itself. Physical systems of interest, and the environments they operate in, do not always behave deterministically. Moreover, information about the system and its environment is typically incomplete or imperfect. Probabilistic representations of systems and environments may therefore be called for, especially to support decisions in application areas where actions may have severe consequences.   In this paper we introduce the probabilistic digital twin (PDT). We will start by discussing how epistemic uncertainty can be treated using measure theory, by modelling epistemic information via $\sigma$-algebras. Based on this, we give a formal definition of how epistemic uncertainty can be updated in a PDT. We then study the problem of optimal sequential decision making. That is, we consider the case where the outcome of each decision may inform the next. Within the PDT framework, we formulate this optimization problem. We discuss how this problem may be solved (at least in theory) via the maximum principle method or the dynamic programming principle. However, due to the curse of dimensionality, these methods are often not tractable in practice. To mend this, we propose a generic approximate solution using deep reinforcement learning together with neural networks defined on sets. We illustrate the method on a practical problem, considering optimal information gathering for the estimation of a failure probability.",0
"Many industries are adopting digital twins, which consist of simulation models and data related to a specific physical system. A primary purpose of digital twins is to enable the simulation of potential outcomes of a given action without interfering with the physical system. However, physical systems and their environments may not always behave deterministically, and information about them may be incomplete or imperfect. Therefore, probabilistic representations of systems and environments may be necessary, particularly in areas where actions could have severe consequences. This paper introduces the probabilistic digital twin (PDT), which addresses epistemic uncertainty using measure theory and $\sigma$-algebras. The paper defines how epistemic uncertainty can be updated in a PDT and explores the problem of optimal sequential decision making within the framework. The paper proposes a generic approximate solution using deep reinforcement learning with neural networks defined on sets, as traditional methods are often impractical due to the curse of dimensionality. The method is illustrated on a practical problem involving optimal information gathering for estimating failure probability.",1
"Negotiation, as an essential and complicated aspect of online shopping, is still challenging for an intelligent agent. To that end, we propose the Price Negotiator, a modular deep neural network that addresses the unsolved problems in recent studies by (1) considering images of the items as a crucial, though neglected, source of information in a negotiation, (2) heuristically finding the most similar items from an external online source to predict the potential value and an acceptable agreement price, (3) predicting a general price-based action at each turn which is fed into the language generator to output the supporting natural language, and (4) adjusting the prices based on the predicted actions. Empirically, we show that our model, that is trained in both supervised and reinforcement learning setting, significantly improves negotiation on the CraigslistBargain dataset, in terms of the agreement price, price consistency, and dialogue quality.",0
"Online shopping negotiations remain a complex and fundamental aspect that poses a challenge for intelligent agents. Our proposed solution, the Price Negotiator, utilizes a modular deep neural network that addresses unresolved issues in recent studies. This is achieved by taking into account crucial yet overlooked information sources such as images of items during negotiations. The Price Negotiator also employs a heuristic approach to identify the most similar items from external online sources. This helps in predicting both the potential value and an acceptable agreement price. Additionally, it predicts a general price-based action at each turn, which is then used to generate supporting natural language. Lastly, the Price Negotiator adjusts prices based on predicted actions. Our model, trained in both supervised and reinforcement learning settings, significantly improves negotiation on the CraigslistBargain dataset in terms of agreement price, price consistency, and dialogue quality.",1
"Reinforcement learning (RL) algorithms are typically limited to learning a single solution of a specified task, even though there often exists diverse solutions to a given task. Compared with learning a single solution, learning a set of diverse solutions is beneficial because diverse solutions enable robust few-shot adaptation and allow the user to select a preferred solution. Although previous studies have showed that diverse behaviors can be modeled with a policy conditioned on latent variables, an approach for modeling an infinite set of diverse solutions with continuous latent variables has not been investigated. In this study, we propose an RL method that can learn infinitely many solutions by training a policy conditioned on a continuous or discrete low-dimensional latent variable. Through continuous control tasks, we demonstrate that our method can learn diverse solutions in a data-efficient manner and that the solutions can be used for few-shot adaptation to solve unseen tasks.",0
"Typically, Reinforcement Learning (RL) algorithms can only learn a single solution to a given task, despite there being multiple solutions available. Learning a diverse set of solutions is advantageous, as it allows for robust few-shot adaptation and the ability to select a preferred solution. While previous studies have shown that policies conditioned on latent variables can model diverse behaviors, there has been no investigation into modeling an infinite set of diverse solutions using continuous latent variables. This study proposes an RL method that can learn an infinite number of solutions by training a policy based on a low-dimensional latent variable, either continuous or discrete. Through performing continuous control tasks, this method demonstrates that it can efficiently learn diverse solutions and apply them to solving new tasks with few-shot adaptation.",1
"In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood-free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.",0
"This paper suggests an extension to Dreamer, a prominent model-based reinforcement learning approach that learns from pixels. Dreamer is effective in training latent state-space models and policy optimization using latent trajectory imagination, but its autoencoding based approach often results in object vanishing, hindering its potential. This study aims to enhance Dreamer's performance by eliminating the decoder and incorporating two components to improve the training performance. The proposed approach achieved the highest scores on 5 challenging simulated robotics tasks where Dreamer experienced object vanishing, outperforming Dreamer and other model-free reinforcement learning methods.",1
"We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of ""batch simulation"": accepting and executing large batches of requests simultaneously. Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput. To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days. We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.",0
"Our method significantly improves the speed of deep reinforcement learning-based training in 3D environments with complex visuals. By utilizing batch simulation, we are able to process large amounts of work simultaneously, leading to an impressive end-to-end training speed of more than 19,000 frames per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. This approach allows for better simulation throughput and a higher number of simulated agents per GPU. To optimize DNN performance and balance training costs with faster simulation, we have also developed a computationally efficient policy DNN and modified training algorithms to maintain sample efficiency when training with large mini-batches. Through our approach, we have successfully trained PointGoal navigation agents in complex 3D environments on a single GPU in just 1.5 days, achieving 97% accuracy compared to agents trained on a previous state-of-the-art system using a 64-GPU cluster over three days. Our batch 3D renderer and simulator are available as open-source reference implementations to aid in the incorporation of these ideas into RL systems.",1
"Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.",0
"The ill-posed nature of the inverse reinforcement learning problem can be solved by using Bayesian inference over the reward. However, current methods are limited in their ability to scale beyond small tabular settings due to the need for an inner-loop MDP solver. Additionally, non-Bayesian methods that do scale often require extensive interaction with the environment, making them unsuitable for high stakes or costly applications like healthcare. To address these issues, we present Approximate Variational Reward Imitation Learning (AVRIL), a method that jointly learns an approximate posterior distribution over the reward and an appropriate policy in an offline manner through a variational approach to the latent reward. Our approach scales to arbitrarily complicated state spaces and demonstrates Bayesian reward inference in environments beyond the scope of current methods. We also show competitive task performance when compared to focused offline imitation learning algorithms, using real medical data and classic control simulations.",1
"With the vast amount of data collected on football and the growth of computing abilities, many games involving decision choices can be optimized. The underlying rule is the maximization of an expected utility of outcomes and the law of large numbers. The data available allows us to compute with high accuracy the probabilities of outcomes of decisions and the well defined points system in the game allows us to have the necessary terminal utilities. With some well established theory we can then optimize choices at a single play level.",0
"Due to the vast amount of football data collected and advancing computing abilities, numerous games with decision-making elements can now be optimized. The fundamental principle is to maximize the expected utility of outcomes, alongside the law of large numbers. By utilizing available data, we can accurately calculate the probabilities of various decision outcomes, while the well-defined points system in the game provides necessary terminal utilities. With established theories, we can optimize single-play decisions.",1
"Compressed sensing can decrease scanning transmission electron microscopy electron dose and scan time with minimal information loss. Traditionally, sparse scans used in compressed sensing sample a static set of probing locations. However, dynamic scans that adapt to specimens are expected to be able to match or surpass the performance of static scans as static scans are a subset of possible dynamic scans. Thus, we present a prototype for a contiguous sparse scan system that piecewise adapts scan paths to specimens as they are scanned. Sampling directions for scan segments are chosen by a recurrent neural network based on previously observed scan segments. The recurrent neural network is trained by reinforcement learning to cooperate with a feedforward convolutional neural network that completes the sparse scans. This paper presents our learning policy, experiments, and example partial scans, and discusses future research directions. Source code, pretrained models, and training data is openly accessible at https://github.com/Jeffrey-Ede/adaptive-scans",0
"The utilization of compressed sensing can decrease the electron dose and scan time in scanning transmission electron microscopy while retaining the necessary information. In the past, compressed sensing has utilized static probing locations in sparse scans. However, dynamic scans that adjust to the specimen being scanned have the potential to outperform static scans as they are a subset of possible dynamic scans. As a result, we have introduced a prototype for a contiguous sparse scan system that can adjust scan paths in response to the specimen being scanned. The recurrent neural network selects sampling directions for scan segments based on previous observations, and it is trained through reinforcement learning to collaborate with a feedforward convolutional neural network to complete the sparse scans. This paper presents our learning policy, experiments, and sample partial scans, as well as discusses future research directions. The source code, pretrained models, and training data are freely available at https://github.com/Jeffrey-Ede/adaptive-scans.",1
"In this work we discuss the incorporation of quadratic neurons into policy networks in the context of model-free actor-critic reinforcement learning. Quadratic neurons admit an explicit quadratic function approximation in contrast to conventional approaches where the the non-linearity is induced by the activation functions. We perform empiric experiments on several MuJoCo continuous control tasks and find that when quadratic neurons are added to MLP policy networks those outperform the baseline MLP whilst admitting a smaller number of parameters. The top returned reward is in average increased by $5.8\%$ while being about $21\%$ more sample efficient. Moreover, it can maintain its advantage against added action and observation noise.",0
"This study examines the integration of quadratic neurons into policy networks for model-free actor-critic reinforcement learning. Unlike traditional methods that use activation functions to induce non-linearity, quadratic neurons utilize an explicit quadratic function approximation. We conduct empirical experiments on various MuJoCo continuous control tasks and discover that incorporating quadratic neurons into MLP policy networks results in superior performance compared to the baseline MLP with fewer parameters. On average, the top reward is boosted by 5.8%, and the network is 21% more sample-efficient. Additionally, it remains advantageous even when action and observation noise is introduced.",1
"Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation. Additional details and code can be obtained from the project page at https://sites.google.com/view/mbrl-game",0
"The potential of Model-based reinforcement learning (MBRL) to incorporate off-policy data and its sample efficiency has garnered significant interest. However, developing stable and efficient algorithms using rich function approximators has proven to be a challenge. To ease algorithm design and expose practical challenges in MBRL, we propose a new framework. The framework views MBRL as a game between two players: a policy player maximizing rewards under the learned model and a model player trying to fit the real-world data collected by the policy player. We construct a Stackelberg game between the two players for algorithm development, which can be solved with approximate bi-level optimization. This leads to two families of algorithms for MBRL based on which player is chosen as the leader. Our framework unifies and generalizes previous MBRL algorithms and is consistent with important heuristics from prior works. Our proposed algorithms are highly sample efficient and scale well to high-dimensional tasks like dexterous hand manipulation. More information and code can be found on our project page at https://sites.google.com/view/mbrl-game.",1
"Reinforcement learning algorithms, just like any other Machine learning algorithm pose a serious threat from adversaries. The adversaries can manipulate the learning algorithm resulting in non-optimal policies. In this paper, we analyze the Multi-task Federated Reinforcement Learning algorithms, where multiple collaborative agents in various environments are trying to maximize the sum of discounted return, in the presence of adversarial agents. We argue that the common attack methods are not guaranteed to carry out a successful attack on Multi-task Federated Reinforcement Learning and propose an adaptive attack method with better attack performance. Furthermore, we modify the conventional federated reinforcement learning algorithm to address the issue of adversaries that works equally well with and without the adversaries. Experimentation on different small to mid-size reinforcement learning problems show that the proposed attack method outperforms other general attack methods and the proposed modification to federated reinforcement learning algorithm was able to achieve near-optimal policies in the presence of adversarial agents.",0
"Adversaries can pose a serious threat to reinforcement learning algorithms, similar to other machine learning algorithms, by manipulating the learning algorithm and resulting in non-optimal policies. This paper analyzes Multi-task Federated Reinforcement Learning algorithms, where multiple agents collaborate in different environments to maximize the sum of discounted returns, while facing adversaries. The authors suggest that common attack methods may not be effective against Multi-task Federated Reinforcement Learning and propose an adaptive attack method with improved performance. Additionally, the conventional federated reinforcement learning algorithm is modified to address the issue of adversaries and works effectively with or without them. The proposed attack method outperforms other attack methods, and the modified federated reinforcement learning algorithm achieves near-optimal policies in the presence of adversaries, as demonstrated through experimentation on small to mid-size reinforcement learning problems.",1
"Adversarial Examples (AEs) generated by perturbing original training examples are useful in improving the robustness of Deep Learning (DL) based models. Most prior works, generate AEs that are either unconscionable due to lexical errors or semantically or functionally deviant from original examples. In this paper, we present ReinforceBug, a reinforcement learning framework, that learns a policy that is transferable on unseen datasets and generates utility-preserving and transferable (on other models) AEs. Our results show that our method is on average 10% more successful as compared to the state-of-the-art attack TextFooler. Moreover, the target models have on average 73.64% confidence in the wrong prediction, the generated AEs preserve the functional equivalence and semantic similarity (83.38% ) to their original counterparts, and are transferable on other models with an average success rate of 46%.",0
"Generating Adversarial Examples (AEs) by altering original training samples can improve the resilience of Deep Learning (DL) models. However, previous approaches have resulted in AEs that are inappropriate due to lexical errors or significant deviations from the original examples. This paper introduces ReinforceBug, a reinforcement learning framework that generates utility-preserving and transferable AEs that can be applied to unseen datasets and models. Our approach outperforms the state-of-the-art TextFooler attack by an average of 10% and produces AEs that maintain functional equivalence and semantic similarity (83.38%) to the original examples. Additionally, our AEs can be transferred to other models with a success rate of 46% on average, and the target models have an average confidence of 73.64% in the incorrect prediction.",1
"We analyze the hidden activations of neural network policies of deep reinforcement learning (RL) agents and show, empirically, that it's possible to know a priori if a state representation will lend itself to fast learning. RL agents in high-dimensional states have two main learning burdens: (1) to learn an action-selection policy and (2) to learn to discern between useful and non-useful information in a given state. By learning a latent representation of these high-dimensional states with an auxiliary model, the latter burden is effectively removed, thereby leading to accelerated training progress. We examine this phenomenon across tasks in the PyBullet Kuka environment, where an agent must learn to control a robotic gripper to pick up an object. Our analysis reveals how neural network policies learn to organize their internal representation of the state space throughout training. The results from this analysis provide three main insights into how deep RL agents learn. First, a well-organized internal representation within the policy network is a prerequisite to learning good action-selection. Second, a poor initial representation can cause an unrecoverable collapse within a policy network. Third, a good initial representation allows an agent's policy network to organize its internal representation even before any training begins.",0
"Through empirical analysis, we demonstrate that it is possible to determine in advance whether a state representation will facilitate rapid learning in deep reinforcement learning (RL) agents by scrutinizing the concealed activations of neural network policies. For RL agents that operate in high-dimensional states, two primary learning challenges exist: (1) developing an action-selection policy and (2) distinguishing between useful and non-useful information in a given state. However, via an auxiliary model that acquires a latent representation of these high-dimensional states, the latter challenge can be effectively eliminated, thereby expediting the training process. We evaluated this phenomenon in the PyBullet Kuka environment, where an agent must learn to manipulate a robotic gripper to pick up an object, and examined the way in which neural network policies learn to organize their internal representation of the state space over the course of training. Our analysis yields three key insights into the learning process of deep RL agents: First, a well-organized internal representation within the policy network is a prerequisite to learning good action-selection. Second, a poor initial representation can cause an unrecoverable collapse within a policy network. Third, a good initial representation allows an agent's policy network to organize its internal representation even before any training begins.",1
"We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower-level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods.",0
"Our proposed method, HIDIO, utilizes hierarchical reinforcement learning to solve sparse-reward tasks by learning task-agnostic options in a self-supervised manner. Unlike current hierarchical RL approaches, HIDIO does not rely on pre-defined lower-level policies or formulate goal-reaching low-level tasks. Instead, it encourages lower-level option learning that is independent of the task at hand, requiring minimal assumptions or knowledge about the task structure. Options are learned through intrinsic entropy minimization objective conditioned on the option sub-trajectories, resulting in diverse and task-agnostic options. In experiments on robotic manipulation and navigation tasks with sparse rewards, HIDIO outperforms regular RL baselines and two state-of-the-art hierarchical RL methods in terms of success rates and sample efficiency.",1
"Meta-reinforcement learning typically requires orders of magnitude more samples than single task reinforcement learning methods. This is because meta-training needs to deal with more diverse distributions and train extra components such as context encoders. To address this, we propose a novel self-supervised learning task, which we named Trajectory Contrastive Learning (TCL), to improve meta-training. TCL adopts contrastive learning and trains a context encoder to predict whether two transition windows are sampled from the same trajectory. TCL leverages the natural hierarchical structure of context-based meta-RL and makes minimal assumptions, allowing it to be generally applicable to context-based meta-RL algorithms. It accelerates the training of context encoders and improves meta-training overall. Experiments show that TCL performs better or comparably than a strong meta-RL baseline in most of the environments on both meta-RL MuJoCo (5 of 6) and Meta-World benchmarks (44 out of 50).",0
"Compared to single task reinforcement learning methods, meta-reinforcement learning requires significantly more samples due to the need to handle diverse distributions and train additional components such as context encoders. To address this issue, we propose a new self-supervised learning task, called Trajectory Contrastive Learning (TCL), which adopts contrastive learning and trains a context encoder to predict if two transition windows belong to the same trajectory. TCL is applicable to various context-based meta-RL algorithms and takes advantage of their natural hierarchical structure while making minimal assumptions. It accelerates context encoder training and improves meta-training overall. Experimental results demonstrate that TCL performs equally or better than a strong meta-RL baseline in most environments on both Meta-RL MuJoCo and Meta-World benchmarks.",1
"Industrial Internet of Things (IIoT) revolutionizes the future manufacturing facilities by integrating the Internet of Things technologies into industrial settings. With the deployment of massive IIoT devices, it is difficult for the wireless network to support the ubiquitous connections with diverse quality-of-service (QoS) requirements. Although machine learning is regarded as a powerful data-driven tool to optimize wireless network, how to apply machine learning to deal with the massive IIoT problems with unique characteristics remains unsolved. In this paper, we first summarize the QoS requirements of the typical massive non-critical and critical IIoT use cases. We then identify unique characteristics in the massive IIoT scenario, and the corresponding machine learning solutions with its limitations and potential research directions. We further present the existing machine learning solutions for individual layer and cross-layer problems in massive IIoT. Last but not the least, we present a case study of massive access problem based on deep neural network and deep reinforcement learning techniques, respectively, to validate the effectiveness of machine learning in massive IIoT scenario.",0
"The future of manufacturing facilities is being revolutionized by the Industrial Internet of Things (IIoT), which incorporates Internet of Things technologies into industrial settings. However, the deployment of numerous IIoT devices poses a challenge for wireless networks to support diverse quality-of-service (QoS) requirements. While machine learning is a powerful tool for optimizing wireless networks, its application to unique IIoT problems is still unresolved. This paper summarizes the QoS requirements of typical non-critical and critical IIoT use cases, identifies unique characteristics of massive IIoT scenarios, and proposes machine learning solutions with limitations and potential research directions. The paper also presents existing machine learning solutions for individual and cross-layer problems in massive IIoT, followed by a case study of a massive access problem using deep neural network and deep reinforcement learning techniques to validate the effectiveness of machine learning in massive IIoT scenarios.",1
"Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.",0
"The effectiveness of reinforcement learning (RL) in various applications is dependent on the agent's ability to adapt to changes in the reward function or dynamics. We present a theoretical proof that the standard maximum entropy RL can withstand some disturbances in the dynamics and reward function. Although this capability has been observed previously, our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. Other robust RL algorithms require additional components and hyperparameters, whereas MaxEnt RL is robust without any modifications. While MaxEnt RL may not be the most optimal robust RL method, it offers simplicity and formal guarantees.",1
"How do we formalize the challenge of credit assignment in reinforcement learning? Common intuition would draw attention to reward sparsity as a key contributor to difficult credit assignment and traditional heuristics would look to temporal recency for the solution, calling upon the classic eligibility trace. We posit that it is not the sparsity of the reward itself that causes difficulty in credit assignment, but rather the \emph{information sparsity}. We propose to use information theory to define this notion, which we then use to characterize when credit assignment is an obstacle to efficient learning. With this perspective, we outline several information-theoretic mechanisms for measuring credit under a fixed behavior policy, highlighting the potential of information theory as a key tool towards provably-efficient credit assignment.",0
"The challenge of credit assignment in reinforcement learning can be formalized by considering the sparsity of information rather than the sparsity of rewards. Although traditional heuristics suggest using temporal recency to solve the problem, we propose using information theory to define and measure credit assignment difficulty. By characterizing when credit assignment impedes efficient learning, we can develop information-theoretic mechanisms to measure credit under a fixed behavior policy. With this approach, we can leverage information theory as a powerful tool for achieving provably-efficient credit assignment.",1
"This paper presents a method for learning logical task specifications and cost functions from demonstrations. Linear temporal logic (LTL) formulas are widely used to express complex objectives and constraints for autonomous systems. Yet, such specifications may be challenging to construct by hand. Instead, we consider demonstrated task executions, whose temporal logic structure and transition costs need to be inferred by an autonomous agent. We employ a spectral learning approach to extract a weighted finite automaton (WFA), approximating the unknown logic structure of the task. Thereafter, we define a product between the WFA for high-level task guidance and a Labeled Markov decision process (L-MDP) for low-level control and optimize a cost function that matches the demonstrator's behavior. We demonstrate that our method is capable of generalizing the execution of the inferred task specification to new environment configurations.",0
"In this paper, a method is presented for acquiring logical task specifications and cost functions through demonstrations. Autonomous systems often require complex objectives and constraints, which are typically expressed using Linear Temporal Logic (LTL) formulas. However, creating such specifications manually can be difficult. Instead, the author proposes using demonstrated task executions to infer the temporal logic structure and transition costs using an autonomous agent. To achieve this, a spectral learning approach is utilized to extract a Weighted Finite Automaton (WFA) that approximates the task's unknown logic structure. A product is defined between the WFA for high-level task guidance and a Labeled Markov Decision Process (L-MDP) for low-level control, and a cost function that matches the demonstrator's behavior is optimized. The results demonstrate that the proposed method can generalize the execution of the inferred task specification to new environment configurations.",1
"We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems where the goal is to find a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of stochastic policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update step is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an $\epsilon$-first-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used in the update during the test time.",0
"Our focus is on using Model-Agnostic Meta-Learning (MAML) techniques in Reinforcement Learning (RL) scenarios, where the aim is to identify a policy using data from multiple tasks represented by Markov Decision Processes (MDPs) that can be updated by a single step of stochastic policy gradient for the MDP at hand. As it is difficult to compute exact gradients without access to a large number of potential trajectories, the use of stochastic gradients in the MAML update step is imperative for RL problems. To this end, we propose a variant of the MAML method called Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and investigate its convergence properties. We establish the iteration and sample complexity of SG-MRL for discovering an $\epsilon$-first-order stationary point, which is the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms, to the best of our knowledge. Additionally, we explore how our findings apply to cases where more than one step of stochastic policy gradient technique is employed during the test phase.",1
"Reinforcement learning (RL) in episodic, factored Markov decision processes (FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the factorization structure of FMDP. The regret of FMDP-BF is shown to be exponentially smaller than that of optimal algorithms designed for non-factored MDPs, and improves on the best previous result for FMDPs~\citep{osband2014near} by a factored of $\sqrt{H|\mathcal{S}_i|}$, where $|\mathcal{S}_i|$ is the cardinality of the factored state subspace and $H$ is the planning horizon. To show the optimality of our bounds, we also provide a lower bound for FMDP, which indicates that our algorithm is near-optimal w.r.t. timestep $T$, horizon $H$ and factored state-action subspace cardinality. Finally, as an application, we study a new formulation of constrained RL, known as RL with knapsack constraints (RLwK), and provides the first sample-efficient algorithm based on FMDP-BF.",0
"The focus of our study is on reinforcement learning (RL) in factored Markov decision processes (FMDPs) that are episodic in nature. We introduce an algorithm called FMDP-BF that takes advantage of the factorization structure of FMDP. Our analysis reveals that the regret of FMDP-BF is exponentially smaller than that of optimal algorithms created for non-factored MDPs. Additionally, FMDP-BF outperforms the previous best result for FMDPs~\citep{osband2014near} by a factor of $\sqrt{H|\mathcal{S}_i|}$, where $|\mathcal{S}_i|$ represents the cardinality of the factored state subspace, and $H$ refers to the planning horizon. To demonstrate the optimality of our bounds, we provide a lower bound for FMDP, which establishes that our algorithm is almost optimal concerning the timestep $T$, horizon $H$, and factored state-action subspace cardinality. Finally, we present an application of our work by studying a new formulation of constrained RL, referred to as RL with knapsack constraints (RLwK), and present the first sample-efficient algorithm based on FMDP-BF.",1
"Off-policy evaluation (OPE) is the task of estimating the expected reward of a given policy based on offline data previously collected under different policies. Therefore, OPE is a key step in applying reinforcement learning to real-world domains such as medical treatment, where interactive data collection is expensive or even unsafe. As the observed data tends to be noisy and limited, it is essential to provide rigorous uncertainty quantification, not just a point estimation, when applying OPE to make high stakes decisions. This work considers the problem of constructing non-asymptotic confidence intervals in infinite-horizon off-policy evaluation, which remains a challenging open question. We develop a practical algorithm through a primal-dual optimization-based approach, which leverages the kernel Bellman loss (KBL) of Feng et al.(2019) and a new martingale concentration inequality of KBL applicable to time-dependent data with unknown mixing conditions. Our algorithm makes minimum assumptions on the data and the function class of the Q-function, and works for the behavior-agnostic settings where the data is collected under a mix of arbitrary unknown behavior policies. We present empirical results that clearly demonstrate the advantages of our approach over existing methods.",0
"The task of estimating the expected reward of a given policy based on previously collected offline data under different policies is known as off-policy evaluation (OPE). OPE is essential in real-world applications, such as medical treatment, where interactive data collection can be expensive or dangerous. Due to the limited and noisy observed data, it is crucial to provide rigorous uncertainty quantification instead of just a point estimation when using OPE to make significant decisions. However, constructing non-asymptotic confidence intervals in infinite-horizon off-policy evaluation remains a challenging open question. In this study, we propose a practical algorithm that uses a primal-dual optimization-based approach, which leverages a new martingale concentration inequality and the kernel Bellman loss of Feng et al. (2019) to solve this problem. Our algorithm requires minimal assumptions on the data and the function class of the Q-function, and it works for behavior-agnostic settings where the data is collected under a mix of arbitrary unknown behavior policies. We present empirical results that demonstrate the superiority of our approach over existing methods.",1
"Advances in reinforcement learning (RL) have resulted in recent breakthroughs in the application of artificial intelligence (AI) across many different domains. An emerging landscape of development environments is making powerful RL techniques more accessible for a growing community of researchers. However, most existing frameworks do not directly address the problem of learning in complex operating environments, such as dense urban settings or defense-related scenarios, that incorporate distributed, heterogeneous teams of agents. To help enable AI research for this important class of applications, we introduce the AI Arena: a scalable framework with flexible abstractions for distributed multi-agent reinforcement learning. The AI Arena extends the OpenAI Gym interface to allow greater flexibility in learning control policies across multiple agents with heterogeneous learning strategies and localized views of the environment. To illustrate the utility of our framework, we present experimental results that demonstrate performance gains due to a distributed multi-agent learning approach over commonly-used RL techniques in several different learning environments.",0
"Recent advances in reinforcement learning (RL) have led to significant breakthroughs in the use of artificial intelligence (AI) in various fields. An increasing number of development environments are now available, making powerful RL techniques more accessible to researchers. However, the problem of learning in complex operating environments, such as dense urban settings or defense-related scenarios, which involve distributed, heterogeneous teams of agents, is not directly addressed by most existing frameworks. To address this challenge, we introduce the AI Arena, a scalable framework with flexible abstractions for distributed multi-agent reinforcement learning. The AI Arena extends the OpenAI Gym interface, providing greater flexibility in learning control policies across multiple agents with different learning strategies and localized views of the environment. We present experimental results to demonstrate the effectiveness of our approach in several different learning environments, showing that distributed multi-agent learning outperforms commonly used RL techniques.",1
"Many healthcare decisions involve navigating through a multitude of treatment options in a sequential and iterative manner to find an optimal treatment pathway with the goal of an optimal patient outcome. Such optimization problems may be amenable to reinforcement learning. A reinforcement learning agent could be trained to provide treatment recommendations for physicians, acting as a decision support tool. However, a number of difficulties arise when using RL beyond benchmark environments, such as specifying the reward function, choosing an appropriate state representation and evaluating the learned policy.",0
"When making healthcare decisions, a person must often sift through several treatment options in a step-by-step process to determine the best course of action, all with the ultimate goal of achieving the best possible outcome for the patient. Reinforcement learning can be useful in tackling these optimization problems. By training a reinforcement learning agent to offer treatment recommendations to healthcare providers, it can act as a decision-making tool. However, using RL in real-world situations presents challenges, such as defining the reward function, selecting the appropriate state representation, and assessing the effectiveness of the learned policy.",1
"Reinforcement learning (RL) has demonstrated its ability to solve high dimensional tasks by leveraging non-linear function approximators. However, these successes are mostly achieved by 'black-box' policies in simulated domains. When deploying RL to the real world, several concerns regarding the use of a 'black-box' policy might be raised. In order to make the learned policies more transparent, we propose in this paper a policy iteration scheme that retains a complex function approximator for its internal value predictions but constrains the policy to have a concise, hierarchical, and human-readable structure, based on a mixture of interpretable experts. Each expert selects a primitive action according to a distance to a prototypical state. A key design decision to keep such experts interpretable is to select the prototypical states from trajectory data. The main technical contribution of the paper is to address the challenges introduced by this non-differentiable prototypical state selection procedure. Experimentally, we show that our proposed algorithm can learn compelling policies on continuous action deep RL benchmarks, matching the performance of neural network based policies, but returning policies that are more amenable to human inspection than neural network or linear-in-feature policies.",0
"The use of non-linear function approximators in reinforcement learning (RL) has proven effective in solving high dimensional tasks, particularly in simulated scenarios. However, there are concerns about the use of 'black-box' policies when applying RL to real-world situations. To address this issue, we propose a policy iteration approach that maintains a complex function approximator for internal value predictions but constrains the policy to be concise, hierarchical, and human-readable. Our approach is based on a mixture of interpretable experts, with each expert selecting a primitive action based on its distance to a prototypical state. We select the prototypical states from trajectory data to ensure interpretability. Our paper's main technical contribution is to address the challenges arising from the non-differentiable prototypical state selection procedure. We demonstrate that our algorithm can learn effective policies on continuous action deep RL benchmarks, matching the performance of neural network-based policies but producing policies that are more readily interpretable by humans than neural network or linear-in-feature policies.",1
"Trust region methods are a popular tool in reinforcement learning as they yield robust policy updates in continuous and discrete action spaces. However, enforcing such trust regions in deep reinforcement learning is difficult. Hence, many approaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are based on approximations. Due to those approximations, they violate the constraints or fail to find the optimal solution within the trust region. Moreover, they are difficult to implement, often lack sufficient exploration, and have been shown to depend on seemingly unrelated implementation choices. In this work, we propose differentiable neural network layers to enforce trust regions for deep Gaussian policies via closed-form projections. Unlike existing methods, those layers formalize trust regions for each state individually and can complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. We empirically demonstrate that those projection layers achieve similar or better results than existing methods while being almost agnostic to specific implementation choices. The code is available at https://git.io/Jthb0.",0
"Although trust region methods are commonly used in reinforcement learning to ensure reliable policy updates in both continuous and discrete action spaces, their implementation in deep reinforcement learning is challenging. As a result, many approaches, including Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), rely on approximations that often violate constraints or fail to produce optimal solutions within the trust region. Moreover, these methods are complex to implement, lack adequate exploration, and are sensitive to seemingly unrelated implementation choices. To address these limitations, we propose differentiable neural network layers that enforce trust regions for deep Gaussian policies through closed-form projections. These layers define trust regions for each state individually and can be used to complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. Our empirical results show that these projection layers achieve comparable or superior outcomes to existing methods, while being almost indifferent to specific implementation choices. The code is accessible at https://git.io/Jthb0.",1
"In this paper, we are tackling the proposal-free referring expression grounding task, aiming at localizing the target object according to a query sentence, without relying on off-the-shelf object proposals. Existing proposal-free methods employ a query-image matching branch to select the highest-score point in the image feature map as the target box center, with its width and height predicted by another branch. Such methods, however, fail to utilize the contextual relation between the target and reference objects, and lack interpretability on its reasoning procedure. To solve these problems, we propose an iterative shrinking mechanism to localize the target, where the shrinking direction is decided by a reinforcement learning agent, with all contents within the current image patch comprehensively considered. Beside, the sequential shrinking process enables to demonstrate the reasoning about how to iteratively find the target. Experiments show that the proposed method boosts the accuracy by 4.32% against the previous state-of-the-art (SOTA) method on the RefCOCOg dataset, where query sentences are long and complex, with many targets referred by other reference objects.",0
"The main focus of this paper is to address the task of referring expression grounding without relying on pre-existing object proposals. The objective is to locate the target object based on a given query sentence. Existing methods for this task use a query-image matching approach to identify the highest-scoring point on the feature map as the center of the target box, which is then predicted by another branch. However, this approach does not take into account the contextual relationship between the target and reference objects and lacks interpretability on the reasoning process. To overcome these issues, we introduce an iterative shrinking mechanism that uses reinforcement learning to determine the shrinking direction. This mechanism considers all the contents within the current image patch comprehensively. Additionally, the sequential shrinking process allows for the demonstration of the reasoning behind the iterative search for the target object. Our experiments demonstrate that our proposed method outperforms the previous state-of-the-art method by 4.32% on the RefCOCOg dataset, which consists of long and complex query sentences with multiple target objects referred to by other reference objects.",1
"This work proposes a scheme that allows learning complex multi-agent behaviors in a sample efficient manner, applied to 2v2 soccer. The problem is formulated as a Markov game, and solved using deep reinforcement learning. We propose a basic multi-agent extension of TD3 for learning the policy of each player, in a decentralized manner. To ease learning, the task of 2v2 soccer is divided in three stages: 1v0, 1v1 and 2v2. The process of learning in multi-agent stages (1v1 and 2v2) uses agents trained on a previous stage as fixed opponents. In addition, we propose using experience sharing, a method that shares experience from a fixed opponent, trained in a previous stage, for training the agent currently learning, and a form of frame-skipping, to raise performance significantly. Our results show that high quality soccer play can be obtained with our approach in just under 40M interactions. A summarized video of the resulting game play can be found in https://youtu.be/f25l1j1U9RM.",0
"In this study, a new technique is introduced for efficient learning of complex multi-agent behaviors in the context of 2v2 soccer. The approach involves formulating the problem as a Markov game and utilizing deep reinforcement learning to solve it. To enable decentralized learning, a basic multi-agent extension of TD3 is proposed for training each player's policy. To facilitate learning, the 2v2 soccer task is segmented into three stages: 1v0, 1v1, and 2v2. The multi-agent stages (1v1 and 2v2) utilize agents trained in previous stages as fixed opponents, and experience sharing and frame-skipping are employed to improve performance. The results demonstrate that high-quality soccer play can be achieved with this method after approximately 40M interactions. A condensed video of the game play can be viewed at https://youtu.be/f25l1j1U9RM.",1
"Reinforcement learning in cooperative multi-agent settings has recently advanced significantly in its scope, with applications in cooperative estimation for advertising, dynamic treatment regimes, distributed control, and federated learning. In this paper, we discuss the problem of cooperative multi-agent RL with function approximation, where a group of agents communicates with each other to jointly solve an episodic MDP. We demonstrate that via careful message-passing and cooperative value iteration, it is possible to achieve near-optimal no-regret learning even with a fixed constant communication budget. Next, we demonstrate that even in heterogeneous cooperative settings, it is possible to achieve Pareto-optimal no-regret learning with limited communication. Our work generalizes several ideas from the multi-agent contextual and multi-armed bandit literature to MDPs and reinforcement learning.",0
"Recently, reinforcement learning has made significant strides in cooperative multi-agent scenarios, finding applications in areas such as advertising, dynamic treatment regimes, distributed control, and federated learning. This paper focuses on cooperative multi-agent RL with function approximation, where agents work together to solve an episodic MDP through communication. Through cooperative value iteration and message-passing, we show that near-optimal no-regret learning is achievable, even with a fixed communication budget. Furthermore, we demonstrate that limited communication can still lead to Pareto-optimal no-regret learning in heterogeneous cooperative settings. Our work builds on ideas from multi-agent contextual and multi-armed bandit literature, extending them to MDPs and reinforcement learning.",1
"In offline reinforcement learning (RL), we seek to utilize offline data to evaluate (or learn) policies in scenarios where the data are collected from a distribution that substantially differs from that of the target policy to be evaluated. Recent theoretical advances have shown that such sample-efficient offline RL is indeed possible provided certain strong representational conditions hold, else there are lower bounds exhibiting exponential error amplification (in the problem horizon) unless the data collection distribution has only a mild distribution shift relative to the target policy. This work studies these issues from an empirical perspective to gauge how stable offline RL methods are. In particular, our methodology explores these ideas when using features from pre-trained neural networks, in the hope that these representations are powerful enough to permit sample efficient offline RL. Through extensive experiments on a range of tasks, we see that substantial error amplification does occur even when using such pre-trained representations (trained on the same task itself); we find offline RL is stable only under extremely mild distribution shift. The implications of these results, both from a theoretical and an empirical perspective, are that successful offline RL (where we seek to go beyond the low distribution shift regime) requires substantially stronger conditions beyond those which suffice for successful supervised learning.",0
"The goal of offline reinforcement learning is to utilize existing data to evaluate or learn policies, even when the data was collected from a distribution that differs greatly from the target policy. Recent theoretical advancements have shown that successful offline RL is achievable, but only if specific representational conditions are met. If not, there are lower bounds that exhibit exponential error amplification, unless the data collection distribution has only a slight shift compared to the target policy. This study explores these issues empirically, specifically when using features from pre-trained neural networks, which are expected to be powerful enough for sample-efficient offline RL. The results of extensive experiments on various tasks show that significant error amplification occurs, even when using pre-trained representations from the same task. Offline RL stability is only possible with exceedingly mild distribution shift. These findings suggest that successful offline RL requires much stronger conditions than those for successful supervised learning, both theoretically and empirically.",1
"We study episodic reinforcement learning under unknown adversarial corruptions in both the rewards and the transition probabilities of the underlying system. We propose new algorithms which, compared to the existing results in (Lykouris et al., 2020), achieve strictly better regret bounds in terms of total corruptions for the tabular setting. To be specific, firstly, our regret bounds depend on more precise numerical values of total rewards corruptions and transition corruptions, instead of only on the total number of corrupted episodes. Secondly, our regret bounds are the first of their kind in the reinforcement learning setting to have the number of corruptions show up additively with respect to $\min\{\sqrt{T}, \text{PolicyGapComplexity}\}$ rather than multiplicatively. Our results follow from a general algorithmic framework that combines corruption-robust policy elimination meta-algorithms, and plug-in reward-free exploration sub-algorithms. Replacing the meta-algorithm or sub-algorithm may extend the framework to address other corrupted settings with potentially more structure.",0
"We explore the topic of episodic reinforcement learning in the face of unknown adversarial corruptions affecting both the rewards and transition probabilities of the underlying system. Our proposed algorithms improve upon the existing findings of Lykouris et al. (2020) by achieving better regret bounds in terms of total corruptions for the tabular setting. Specifically, our regret bounds are more precise as they depend on numerical values of total rewards corruptions and transition corruptions, rather than just the total number of corrupted episodes. Additionally, our approach is the first of its kind in the reinforcement learning setting to have the number of corruptions additively show up with respect to $\min\{\sqrt{T}, \text{PolicyGapComplexity}\}$, instead of multiplicatively. We achieve these results through a general algorithmic framework that employs corruption-robust policy elimination meta-algorithms, along with plug-in reward-free exploration sub-algorithms. By swapping out these algorithms, our framework can potentially address other corrupted settings with varying levels of structure.",1
"The emerging field of Reinforcement Learning (RL) has led to impressive results in varied domains like strategy games, robotics, etc. This handout aims to give a simple introduction to RL from control perspective and discuss three possible approaches to solve an RL problem: Policy Gradient, Policy Iteration, and Model-building. Dynamical systems might have discrete action-space like cartpole where two possible actions are +1 and -1 or continuous action space like linear Gaussian systems. Our discussion covers both cases.",0
"Reinforcement Learning (RL) is a burgeoning field that has produced impressive outcomes in domains such as strategy games and robotics. This guide provides a basic introduction to RL from a control perspective and examines three potential approaches to addressing an RL problem: Policy Gradient, Policy Iteration, and Model-building. Dynamical systems can possess either a discrete action-space, such as the cartpole with two potential actions of +1 and -1, or a continuous action space, like linear Gaussian systems. Our discussion encompasses both scenarios.",1
"Despite the rich theoretical foundation of model-based deep reinforcement learning (RL) agents, their effectiveness in real-world robotics-applications is less studied and understood. In this paper, we, therefore, investigate how such agents generalize to real-world autonomous-vehicle control-tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional LiDAR sensors, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination, substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the observation-model choice. Finally, we provide extensive empirical evidence for the effectiveness of model-based agents provided with long enough memory horizons in sim2real tasks.",0
"Although model-based deep reinforcement learning (RL) agents have a strong theoretical foundation, their effectiveness in real-world robotics applications remains poorly understood. Therefore, this study examines how these agents perform in real-world autonomous-vehicle control tasks, which are challenging for model-free deep RL algorithms. Specifically, we design a series of time-lap tasks for an F1TENTH racing robot, fitted with high-dimensional LiDAR sensors, on progressively more complex test tracks. In this continuous-control environment, we demonstrate that model-based agents that can learn in imagination outperform model-free agents in terms of performance, sample efficiency, successful task completion, and generalization. Additionally, we find that the generalization ability of model-based agents depends heavily on the observation model chosen. Finally, we offer extensive empirical evidence supporting the effectiveness of model-based agents with long memory horizons in sim2real tasks.",1
"Transferring knowledge among various environments is important to efficiently learn multiple tasks online. Most existing methods directly use the previously learned models or previously learned optimal policies to learn new tasks. However, these methods may be inefficient when the underlying models or optimal policies are substantially different across tasks. In this paper, we propose Template Learning (TempLe), the first PAC-MDP method for multi-task reinforcement learning that could be applied to tasks with varying state/action space. TempLe generates transition dynamics templates, abstractions of the transition dynamics across tasks, to gain sample efficiency by extracting similarities between tasks even when their underlying models or optimal policies have limited commonalities. We present two algorithms for an ""online"" and a ""finite-model"" setting respectively. We prove that our proposed TempLe algorithms achieve much lower sample complexity than single-task learners or state-of-the-art multi-task methods. We show via systematically designed experiments that our TempLe method universally outperforms the state-of-the-art multi-task methods (PAC-MDP or not) in various settings and regimes.",0
"Effective acquisition of knowledge across diverse environments is crucial for proficiently learning multiple tasks online. Prior approaches often rely on the use of previously learned models or optimal policies to learn new tasks, but this can be ineffective if the underlying models or optimal policies differ significantly. To address this, we introduce Template Learning (TempLe), which is the first PAC-MDP method for multi-task reinforcement learning that can be applied to tasks with varying state/action space. TempLe utilizes transition dynamics templates, which are abstractions of the transition dynamics across tasks, to improve sample efficiency by identifying similarities between tasks, even when their underlying models or optimal policies have limited commonalities. We present two algorithms for the ""online"" and ""finite-model"" settings, respectively, and prove that our TempLe algorithms require far fewer samples than single-task learners or other state-of-the-art multi-task methods. Through carefully designed experiments, we demonstrate that TempLe outperforms other state-of-the-art multi-task methods in various settings and regimes.",1
"Obstacle avoidance is a fundamental and challenging problem for autonomous navigation of mobile robots. In this paper, we consider the problem of obstacle avoidance in simple 3D environments where the robot has to solely rely on a single monocular camera. In particular, we are interested in solving this problem without relying on localization, mapping, or planning techniques. Most of the existing work consider obstacle avoidance as two separate problems, namely obstacle detection, and control. Inspired by the recent advantages of deep reinforcement learning in Atari games and understanding highly complex situations in Go, we tackle the obstacle avoidance problem as a data-driven end-to-end deep learning approach. Our approach takes raw images as input and generates control commands as output. We show that discrete action spaces are outperforming continuous control commands in terms of expected average reward in maze-like environments. Furthermore, we show how to accelerate the learning and increase the robustness of the policy by incorporating predicted depth maps by a generative adversarial network.",0
"The autonomous navigation of mobile robots is a challenging issue, and obstacle avoidance is a fundamental part of it. This paper aims to solve the obstacle avoidance problem in 3D environments where a single monocular camera is the only available tool. The goal is to address the problem without relying on mapping, planning, or localization techniques. Previous research has separated obstacle avoidance into two distinct issues, obstacle detection, and control. However, inspired by deep reinforcement learning's recent success in complex tasks, we adopt an end-to-end deep learning approach to tackle the obstacle avoidance problem. Our approach is based on raw image input and control command output. We found that, in maze-like environments, discrete action spaces outperform continuous control commands in terms of expected average reward. Additionally, we incorporate predicted depth maps generated by a generative adversarial network to accelerate learning and improve the policy's robustness.",1
"Although the notion of task similarity is potentially interesting in a wide range of areas such as curriculum learning or automated planning, it has mostly been tied to transfer learning. Transfer is based on the idea of reusing the knowledge acquired in the learning of a set of source tasks to a new learning process in a target task, assuming that the target and source tasks are close enough. In recent years, transfer learning has succeeded in making Reinforcement Learning (RL) algorithms more efficient (e.g., by reducing the number of samples needed to achieve the (near-)optimal performance). Transfer in RL is based on the core concept of similarity: whenever the tasks are similar, the transferred knowledge can be reused to solve the target task and significantly improve the learning performance. Therefore, the selection of good metrics to measure these similarities is a critical aspect when building transfer RL algorithms, especially when this knowledge is transferred from simulation to the real world. In the literature, there are many metrics to measure the similarity between MDPs, hence, many definitions of similarity or its complement distance have been considered. In this paper, we propose a categorization of these metrics and analyze the definitions of similarity proposed so far, taking into account such categorization. We also follow this taxonomy to survey the existing literature, as well as suggesting future directions for the construction of new metrics.",0
"The concept of task similarity has potential applications in various fields, including curriculum learning and automated planning. However, it has mainly been associated with transfer learning, which involves utilizing knowledge gained from a set of source tasks in a new learning process for a target task. This approach assumes that the tasks are sufficiently similar. Transfer learning has proven successful in enhancing Reinforcement Learning (RL) algorithms by reducing the number of samples required for optimal performance. Similarity is the core concept behind transfer in RL, and selecting appropriate metrics to measure similarity is crucial, especially when transferring knowledge from simulation to the real world. The literature offers numerous metrics to measure the similarity between Markov decision processes (MDPs), and various definitions of similarity and distance have been proposed. In this paper, we present a classification of these metrics and analyze their definitions, considering this categorization. We also use this taxonomy to survey existing literature and suggest future avenues for developing new metrics.",1
"Over the past few years, the use of swarms of Unmanned Aerial Vehicles (UAVs) in monitoring and remote area surveillance applications has become widespread thanks to the price reduction and the increased capabilities of drones. The drones in the swarm need to cooperatively explore an unknown area, in order to identify and monitor interesting targets, while minimizing their movements. In this work, we propose a distributed Reinforcement Learning (RL) approach that scales to larger swarms without modifications. The proposed framework relies on the possibility for the UAVs to exchange some information through a communication channel, in order to achieve context-awareness and implicitly coordinate the swarm's actions. Our experiments show that the proposed method can yield effective strategies, which are robust to communication channel impairments, and that can easily deal with non-uniform distributions of targets and obstacles. Moreover, when agents are trained in a specific scenario, they can adapt to a new one with minimal additional training. We also show that our approach achieves better performance compared to a computationally intensive look-ahead heuristic.",0
"In recent years, the use of swarms of Unmanned Aerial Vehicles (UAVs) for monitoring and remote area surveillance has become widespread due to the decreased cost and enhanced capabilities of drones. The swarm of drones must work together to explore an unknown area and detect interesting targets while minimizing their movements. We propose a distributed Reinforcement Learning (RL) method that can scale up to larger swarms without any modifications. This approach utilizes a communication channel to exchange information among the UAVs to achieve context-awareness and coordinate the swarm's actions. Our experiments indicate that this method can generate effective strategies that are resilient to communication channel disruptions and can handle non-uniform target and obstacle distributions with ease. Additionally, our approach can adapt to new scenarios with minimal training, and we demonstrate that it outperforms a computationally intensive look-ahead heuristic.",1
"Model-based planning holds great promise for improving both sample efficiency and generalization in reinforcement learning (RL). We show that energy-based models (EBMs) are a promising class of models to use for model-based planning. EBMs naturally support inference of intermediate states given start and goal state distributions. We provide an online algorithm to train EBMs while interacting with the environment, and show that EBMs allow for significantly better online learning than corresponding feed-forward networks. We further show that EBMs support maximum entropy state inference and are able to generate diverse state space plans. We show that inference purely in state space - without planning actions - allows for better generalization to previously unseen obstacles in the environment and prevents the planner from exploiting the dynamics model by applying uncharacteristic action sequences. Finally, we show that online EBM training naturally leads to intentionally planned state exploration which performs significantly better than random exploration.",0
"The use of model-based planning in reinforcement learning has the potential to enhance sample efficiency and generalization. Our study demonstrates that energy-based models are a promising model class for model-based planning. These models enable intermediate state inference based on initial and target state distributions. We present an online algorithm to train energy-based models in conjunction with environmental interaction, and our results indicate that they surpass feed-forward networks for online learning. Moreover, energy-based models support maximum entropy state inference and enable the creation of varied state space plans. We find that state space inference, independent of action planning, enhances generalization by avoiding atypical action sequences and unanticipated obstacles. Additionally, our research suggests that intentional state exploration achieved through online EBM training outperforms random exploration.",1
"We study the sample complexity of teaching, termed as ""teaching dimension"" (TDim) in the literature, for the teaching-by-reinforcement paradigm, where the teacher guides the student through rewards. This is distinct from the teaching-by-demonstration paradigm motivated by robotics applications, where the teacher teaches by providing demonstrations of state/action trajectories. The teaching-by-reinforcement paradigm applies to a wider range of real-world settings where a demonstration is inconvenient, but has not been studied systematically. In this paper, we focus on a specific family of reinforcement learning algorithms, Q-learning, and characterize the TDim under different teachers with varying control power over the environment, and present matching optimal teaching algorithms. Our TDim results provide the minimum number of samples needed for reinforcement learning, and we discuss their connections to standard PAC-style RL sample complexity and teaching-by-demonstration sample complexity results. Our teaching algorithms have the potential to speed up RL agent learning in applications where a helpful teacher is available.",0
"The ""teaching dimension"" (TDim) is a measure of sample complexity in teaching, specifically in the teaching-by-reinforcement paradigm where rewards guide the student. This is different from teaching-by-demonstration, which involves providing demonstrations of state/action trajectories, and is often used in robotics applications. While teaching-by-reinforcement is more versatile, it has not been systematically studied. This paper focuses on Q-learning, a family of reinforcement learning algorithms, and examines TDim under different teachers with varying control over the environment. Optimal teaching algorithms are presented. The TDim results establish the minimum number of samples required for reinforcement learning and their relationship with standard PAC-style RL sample complexity and teaching-by-demonstration sample complexity results is discussed. The teaching algorithms developed can enhance the speed of RL agent learning when there is a helpful teacher available.",1
"In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress. But even when a model is imperfect, it may still contain information that is useful for planning. In this paper, we investigate the idea of using an imperfect model selectively. The agent should plan in parts of the state space where the model would be helpful but refrain from using the model where it would be harmful. An effective selective planning mechanism requires estimating predictive uncertainty, which arises out of aleatoric uncertainty, parameter uncertainty, and model inadequacy, among other sources. Prior work has focused on parameter uncertainty for selective planning. In this work, we emphasize the importance of model inadequacy. We show that heteroscedastic regression can signal predictive uncertainty arising from model inadequacy that is complementary to that which is detected by methods designed for parameter uncertainty, indicating that considering both parameter uncertainty and model inadequacy may be a more promising direction for effective selective planning than either in isolation.",0
"The potential negative impact of planning with an imperfect model in model-based reinforcement learning is explored in this paper. Nonetheless, since an imperfect model may still contain useful information for planning, the authors investigate the concept of selective use of such models. The idea is for the agent to plan in areas of the state space where the model would be beneficial but avoid using it in areas where it would be detrimental. To achieve this, the authors propose an effective selective planning mechanism that involves estimating predictive uncertainty. Predictive uncertainty results from various sources, including aleatoric uncertainty, parameter uncertainty, and model inadequacy. While prior research has focused on parameter uncertainty for selective planning, the authors highlight the importance of model inadequacy. They demonstrate that heteroscedastic regression can detect predictive uncertainty arising from model inadequacy, which complements that detected by methods designed for parameter uncertainty. Therefore, considering both parameter uncertainty and model inadequacy may be a more promising direction for effective selective planning than either in isolation.",1
"Inverse reinforcement learning (IRL) is the task of finding a reward function that generates a desired optimal policy for a given Markov Decision Process (MDP). This paper develops an information-theoretic lower bound for the sample complexity of the finite state, finite action IRL problem. A geometric construction of $\beta$-strict separable IRL problems using spherical codes is considered. Properties of the ensemble size as well as the Kullback-Leibler divergence between the generated trajectories are derived. The resulting ensemble is then used along with Fano's inequality to derive a sample complexity lower bound of $O(n \log n)$, where $n$ is the number of states in the MDP.",0
"The objective of inverse reinforcement learning (IRL) is to determine a reward function that produces an optimal policy for a given Markov Decision Process (MDP). This study establishes an information-theoretic estimate for the sample complexity of the IRL problem in the context of a finite state, finite action scenario. It explores the use of spherical codes to construct $\beta$-strict separable IRL problems and establishes the properties of ensemble size and Kullback-Leibler divergence between generated trajectories. The resulting ensemble is employed, along with Fano's inequality, to establish a sample complexity lower bound of $O(n \log n)$, where $n$ represents the number of states in the MDP.",1
"Some reinforcement learning methods suffer from high sample complexity causing them to not be practical in real-world situations. $Q$-function reuse, a transfer learning method, is one way to reduce the sample complexity of learning, potentially improving usefulness of existing algorithms. Prior work has shown the empirical effectiveness of $Q$-function reuse for various environments when applied to model-free algorithms. To the best of our knowledge, there has been no theoretical work showing the regret of $Q$-function reuse when applied to the tabular, model-free setting. We aim to bridge the gap between theoretical and empirical work in $Q$-function reuse by providing some theoretical insights on the effectiveness of $Q$-function reuse when applied to the $Q$-learning with UCB-Hoeffding algorithm. Our main contribution is showing that in a specific case if $Q$-function reuse is applied to the $Q$-learning with UCB-Hoeffding algorithm it has a regret that is independent of the state or action space. We also provide empirical results supporting our theoretical findings.",0
"Reinforcement learning methods can be impractical in real-world scenarios due to their high sample complexity. To address this issue, one approach is to use transfer learning through $Q$-function reuse in order to reduce sample complexity and improve the effectiveness of existing algorithms. While previous research has demonstrated the empirical efficacy of $Q$-function reuse in model-free algorithms across various environments, there has been no theoretical analysis of its impact in the tabular, model-free setting. This study aims to bridge this gap by investigating the effectiveness of $Q$-function reuse in the $Q$-learning with UCB-Hoeffding algorithm, providing theoretical insights that demonstrate that in a specific case, it has a regret that is independent of the state or action space. Empirical results were also obtained to support our findings.",1
"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.",0
"We suggest a straightforward approach to augment data that can be utilized in standard model-free reinforcement learning techniques. This enables effective learning without the necessity of auxiliary losses or pre-training directly from pixels. The method employs common input perturbations from computer vision tasks to regularize the value function. Although Soft Actor-Critic (SAC) and other model-free methods struggle to train deep networks from image pixels, our augmentation method significantly improves SAC's performance. As a result, SAC surpasses model-based methods such as Dreamer, PlaNet, and SLAC, as well as recently proposed contrastive learning methods (CURL), achieving state-of-the-art performance on the DeepMind control suite. Our approach requires only minor adjustments and can be combined with any model-free reinforcement learning algorithm. The implementation is available at https://sites.google.com/view/data-regularized-q.",1
"Hindsight Experience Replay (HER) is a multi-goal reinforcement learning algorithm for sparse reward functions. The algorithm treats every failure as a success for an alternative (virtual) goal that has been achieved in the episode. Virtual goals are randomly selected, irrespective of which are most instructive for the agent. In this paper, we present two improvements over the existing HER algorithm. First, we prioritize virtual goals from which the agent will learn more valuable information. We call this property the instructiveness of the virtual goal and define it by a heuristic measure, which expresses how well the agent will be able to generalize from that virtual goal to actual goals. Secondly, we reduce existing bias in HER by the removal of misleading samples. To test our algorithms, we built two challenging environments with sparse reward functions. Our empirical results in both environments show vast improvement in the final success rate and sample efficiency when compared to the original HER algorithm. A video showing experimental results is available at https://youtu.be/3cZwfK8Nfps .",0
"The multi-goal reinforcement learning algorithm, Hindsight Experience Replay (HER), is designed to address sparse reward functions. This algorithm considers any failure as a success for a virtual goal achieved during the episode, regardless of its instructive value for the agent. In this study, we propose two enhancements to the existing HER algorithm. First, we prioritize virtual goals based on their instructiveness, measured by a heuristic that quantifies the generalization potential of the virtual goals. Second, we reduce bias in HER by removing misleading samples. To evaluate our improvements, we developed two challenging environments with sparse reward functions. Our empirical results demonstrate significant progress in the final success rate and sample efficiency compared to the original HER algorithm. A video presenting experimental results is available at https://youtu.be/3cZwfK8Nfps.",1
"3D object detection is an important task, especially in the autonomous driving application domain. However, it is challenging to support the real-time performance with the limited computation and memory resources on edge-computing devices in self-driving cars. To achieve this, we propose a compiler-aware unified framework incorporating network enhancement and pruning search with the reinforcement learning techniques, to enable real-time inference of 3D object detection on the resource-limited edge-computing devices. Specifically, a generator Recurrent Neural Network (RNN) is employed to provide the unified scheme for both network enhancement and pruning search automatically, without human expertise and assistance. And the evaluated performance of the unified schemes can be fed back to train the generator RNN. The experimental results demonstrate that the proposed framework firstly achieves real-time 3D object detection on mobile devices (Samsung Galaxy S20 phone) with competitive detection performance.",0
"Detecting 3D objects is crucial for autonomous driving, but it is difficult to do in real-time on edge-computing devices in self-driving cars due to limited resources. Our proposed solution is a compiler-aware framework that combines network enhancement and pruning search using reinforcement learning techniques. This unified approach enables real-time 3D object detection on resource-limited edge-computing devices. A generator Recurrent Neural Network (RNN) automates network enhancement and pruning search without human assistance. The unified schemes' performance is fed back to train the generator RNN. Our experimental results show that our framework achieves real-time 3D object detection on mobile devices (Samsung Galaxy S20 phone) with competitive detection performance.",1
"This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a human-interpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth's performance in a set of experiments that includes the Atari game Montezuma's Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.",0
"The purpose of this paper is to introduce DeepSynth, a technique that enhances the training of deep Reinforcement Learning (RL) agents in situations where the reward is scarce and non-Markovian, but where achieving high-level goals requires an unknown sequence of objectives. We developed an innovative algorithm to create compact automata that identifies this sequential structure automatically. By collecting trace data from the environment, we synthesize a human-readable automaton. We then enrich the environment's state space with the automaton to guide the generation of a control policy by deep RL. Our technique handles high-dimensional, low-level features and unknown, sparse, non-Markovian rewards. We tested the approach, including the Atari game Montezuma's Revenge, and achieved a significant reduction in iterations needed for policy synthesis and improved scalability compared to existing methods.",1
"Deep reinforcement learning (DRL) has great potential for acquiring the optimal action in complex environments such as games and robot control. However, it is difficult to analyze the decision-making of the agent, i.e., the reasons it selects the action acquired by learning. In this work, we propose Mask-Attention A3C (Mask A3C), which introduces an attention mechanism into Asynchronous Advantage Actor-Critic (A3C), which is an actor-critic-based DRL method, and can analyze the decision-making of an agent in DRL. A3C consists of a feature extractor that extracts features from an image, a policy branch that outputs the policy, and a value branch that outputs the state value. In this method, we focus on the policy and value branches and introduce an attention mechanism into them. The attention mechanism applies a mask processing to the feature maps of each branch using mask-attention that expresses the judgment reason for the policy and state value with a heat map. We visualized mask-attention maps for games on the Atari 2600 and found we could easily analyze the reasons behind an agent's decision-making in various game tasks. Furthermore, experimental results showed that the agent could achieve a higher performance by introducing the attention mechanism.",0
"Although deep reinforcement learning (DRL) is promising for optimizing actions in complicated environments such as robot control and games, it is challenging to comprehend an agent's decision-making process. This paper suggests the use of Mask-Attention A3C (Mask A3C), a method that combines an attention mechanism with Asynchronous Advantage Actor-Critic (A3C), an actor-critic-centered DRL approach, to analyze the decision-making of an agent in DRL. A3C contains a feature extractor, a policy branch, and a value branch. This study concentrates on the policy and value branches and adds an attention mechanism to them. The attention mechanism applies a mask processing to the feature maps of each branch, utilizing mask-attention to represent the reasoning behind the policy and state value with a heat map. We applied this method to games on the Atari 2600 and obtained mask-attention maps that allowed us to analyze the agent's decision-making. Additionally, the experimental findings indicated that the agent could achieve better performance with the attention mechanism.",1
"In the standard data analysis framework, data is first collected (once for all), and then data analysis is carried out. With the advancement of digital technology, decisionmakers constantly analyze past data and generate new data through the decisions they make. In this paper, we model this as a Markov decision process and show that the dynamic interaction between data generation and data analysis leads to a new type of bias -- reinforcement bias -- that exacerbates the endogeneity problem in standard data analysis.   We propose a class of instrument variable (IV)-based reinforcement learning (RL) algorithms to correct for the bias and establish their asymptotic properties by incorporating them into a two-timescale stochastic approximation framework. A key contribution of the paper is the development of new techniques that allow for the analysis of the algorithms in general settings where noises feature time-dependency.   We use the techniques to derive sharper results on finite-time trajectory stability bounds: with a polynomial rate, the entire future trajectory of the iterates from the algorithm fall within a ball that is centered at the true parameter and is shrinking at a (different) polynomial rate. We also use the technique to provide formulas for inferences that are rarely done for RL algorithms. These formulas highlight how the strength of the IV and the degree of the noise's time dependency affect the inference.",0
"Traditionally, data is collected before being analyzed. However, as technology has evolved, decision makers now constantly analyze past data and generate new data through their decisions. In this paper, we introduce a Markov decision process to model this dynamic interaction between data generation and analysis. We identify a new type of bias, reinforcement bias, that exacerbates the endogeneity problem in standard data analysis. To correct for this bias, we propose a class of instrument variable-based reinforcement learning algorithms and establish their asymptotic properties through a two-timescale stochastic approximation framework. This paper also presents new techniques for analyzing the algorithms in general settings, with time-dependent noises. Using these techniques, we derive sharper results on finite-time trajectory stability bounds, with a polynomial rate. Additionally, we provide formulas for inferences that are rarely done for RL algorithms, highlighting how the strength of the IV and the degree of noise's time dependency affect the inference.",1
"In order to model risk aversion in reinforcement learning, an emerging line of research adapts familiar algorithms to optimize coherent risk functionals, a class that includes conditional value-at-risk (CVaR). Because optimizing the coherent risk is difficult in Markov decision processes, recent work tends to focus on the Markov coherent risk (MCR), a time-consistent surrogate. While, policy gradient (PG) updates have been derived for this objective, it remains unclear (i) whether PG finds a global optimum for MCR; (ii) how to estimate the gradient in a tractable manner. In this paper, we demonstrate that, in general, MCR objectives (unlike the expected return) are not gradient dominated and that stationary points are not, in general, guaranteed to be globally optimal. Moreover, we present a tight upper bound on the suboptimality of the learned policy, characterizing its dependence on the nonlinearity of the objective and the degree of risk aversion. Addressing (ii), we propose a practical implementation of PG that uses state distribution reweighting to overcome previous limitations. Through experiments, we demonstrate that when the optimality gap is small, PG can learn risk-sensitive policies. However, we find that instances with large suboptimality gaps are abundant and easy to construct, outlining an important challenge for future research.",0
"A new approach to representing risk aversion in reinforcement learning involves modifying existing algorithms to optimize coherent risk functionals, which include conditional value-at-risk (CVaR). Due to the difficulty of optimizing coherent risk in Markov decision processes, recent research has focused on the Markov coherent risk (MCR) as a time-consistent surrogate. Although policy gradient (PG) updates have been developed for this objective, it is uncertain if PG can find a global optimum for MCR or how to estimate the gradient efficiently. Our study demonstrates that MCR objectives are not dominated by gradients and that stationary points are not always globally optimal. We also provide an upper bound on the suboptimality of the learned policy, based on the objective's nonlinearity and level of risk aversion. To address gradient estimation, we suggest a practical implementation of PG that employs state distribution reweighting. Our experiments reveal that PG can learn risk-sensitive policies when the optimality gap is small, but large suboptimality gaps are common and pose a significant challenge for future research.",1
"We study episodic reinforcement learning in non-stationary linear (a.k.a. low-rank) Markov Decision Processes (MDPs), i.e, both the reward and transition kernel are linear with respect to a given feature map and are allowed to evolve either slowly or abruptly over time. For this problem setting, we propose OPT-WLSVI an optimistic model-free algorithm based on weighted least squares value iteration which uses exponential weights to smoothly forget data that are far in the past. We show that our algorithm, when competing against the best policy at each time, achieves a regret that is upper bounded by $\widetilde{\mathcal{O}}(d^{5/4}H^2 \Delta^{1/4} K^{3/4})$ where $d$ is the dimension of the feature space, $H$ is the planning horizon, $K$ is the number of episodes and $\Delta$ is a suitable measure of non-stationarity of the MDP. Moreover, we point out technical gaps in the study of forgetting strategies in non-stationary linear bandits setting made by previous works and we propose a fix to their regret analysis.",0
"Our focus is on studying episodic reinforcement learning in non-stationary linear Markov Decision Processes (MDPs) where both the reward and transition kernel are linear with respect to a given feature map and can evolve slowly or abruptly over time. To address this problem, we propose OPT-WLSVI, an optimistic model-free algorithm that utilizes exponential weights to smoothly forget past data. When pitted against the best policy at each time, our algorithm achieves a regret that is upper bounded by $\widetilde{\mathcal{O}}(d^{5/4}H^2 \Delta^{1/4} K^{3/4})$, where $d$ is the feature space dimension, $H$ is the planning horizon, $K$ is the number of episodes, and $\Delta$ is a suitable measure of non-stationarity of the MDP. Additionally, we highlight technical gaps in prior research on forgetting strategies in non-stationary linear bandit settings and propose a solution to improve their regret analysis.",1
"In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation.",0
"The use of exploratory control inputs in reinforcement learning (RL) algorithms helps to acquire knowledge for decision making and control, even when the true dynamics of the controlled object are unknown. However, the exploration process can sometimes lead to undesired situations by violating constraints related to the state of the controlled object. To address this issue, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces. Our method utilizes a linear nominal model of the controlled object and selects whether to use exploratory inputs based on the state and its predicted value. Additionally, the method adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We show that our exploration process adjustment method guarantees the satisfaction of constraints with a pre-specified probability, specifically a joint chance constraint at every time. Finally, we demonstrate the validity and effectiveness of our method through numerical simulation.",1
"We study the inverse reinforcement learning (IRL) problem under a transition dynamics mismatch between the expert and the learner. Specifically, we consider the Maximum Causal Entropy (MCE) IRL learner model and provide a tight upper bound on the learner's performance degradation based on the $\ell_1$-distance between the two transition dynamics of the expert and the learner. Then, by leveraging insights from the Robust RL literature, we propose a robust MCE IRL algorithm, which is a principled approach to help with this mismatch. Finally, we empirically demonstrate the stable performance of our algorithm compared to the standard MCE IRL algorithm under transition mismatches in finite MDP problems.",0
"The problem we investigate involves inverse reinforcement learning (IRL) when the expert and the learner have different transition dynamics. Specifically, we examine the Maximum Causal Entropy (MCE) IRL learner model and establish an upper bound on performance degradation based on the $\ell_1$-distance between the two transition dynamics. We then introduce a robust MCE IRL algorithm, informed by the Robust RL literature, to address this issue in a principled manner. Finally, we demonstrate through empirical analysis that our algorithm performs more stably than the standard MCE IRL algorithm in finite MDP problems with transition mismatches.",1
"Robust model fitting is a core algorithm in a large number of computer vision applications. Solving this problem efficiently for datasets highly contaminated with outliers is, however, still challenging due to the underlying computational complexity. Recent literature has focused on learning-based algorithms. However, most approaches are supervised which require a large amount of labelled training data. In this paper, we introduce a novel unsupervised learning framework that learns to directly solve robust model fitting. Unlike other methods, our work is agnostic to the underlying input features, and can be easily generalized to a wide variety of LP-type problems with quasi-convex residuals. We empirically show that our method outperforms existing unsupervised learning approaches, and achieves competitive results compared to traditional methods on several important computer vision problems.",0
"Many computer vision applications rely on robust model fitting, which is a challenging task when dealing with datasets heavily contaminated with outliers. While recent literature has focused on learning-based algorithms, most of these approaches are supervised and require a significant amount of labelled training data. In this paper, we propose an innovative unsupervised learning framework that can directly solve the problem of robust model fitting. What sets our method apart is that it is not dependent on input features and can be applied to a wide range of LP-type problems with quasi-convex residuals. Our empirical results demonstrate that our approach outperforms existing unsupervised learning methods and achieves comparable results to traditional methods on various important computer vision problems.",1
"With the freight delivery demands and shipping costs increasing rapidly, intelligent control of fleets to enable efficient and cost-conscious solutions becomes an important problem. In this paper, we propose DeepFreight, a model-free deep-reinforcement-learning-based algorithm for multi-transfer freight delivery, which includes two closely-collaborative components: truck-dispatch and package-matching. Specifically, a deep multi-agent reinforcement learning framework called QMIX is leveraged to learn a dispatch policy, with which we can obtain the multi-step joint dispatch decisions for the fleet with respect to the delivery requests. Then an efficient multi-transfer matching algorithm is executed to assign the delivery requests to the trucks. Also, DeepFreight is integrated with a Mixed-Integer Linear Programming optimizer for further optimization. The evaluation results show that the proposed system is highly scalable and ensures a 100% delivery success while maintaining low delivery time and fuel consumption.",0
"As freight delivery demands and shipping costs rise rapidly, it is essential to implement intelligent fleet control to achieve efficient and cost-effective solutions. This paper introduces DeepFreight, an algorithm based on deep-reinforcement-learning that requires no model. It addresses multi-transfer freight delivery and features two closely-collaborative components: truck-dispatch and package-matching. The dispatch policy is learned using a deep multi-agent reinforcement learning framework called QMIX, which enables multi-step joint dispatch decisions for the fleet concerning the delivery requests. Following this, an effective multi-transfer matching algorithm is applied to assign delivery requests to trucks. DeepFreight is also optimized using a Mixed-Integer Linear Programming optimizer. The results of the evaluation demonstrate that the proposed system is scalable and ensures a 100% delivery success rate while maintaining low delivery time and fuel consumption.",1
"Reinforcement learning (RL) is a foundation of learning in biological systems and provides a framework to address numerous challenges with real-world artificial intelligence applications. Efficient implementations of RL techniques could allow for agents deployed in edge-use cases to gain novel abilities, such as improved navigation, understanding complex situations and critical decision making. Towards this goal, we describe a flexible architecture to carry out reinforcement learning on neuromorphic platforms. This architecture was implemented using an Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking dynamics. Our study proposes a usable energy efficient solution for real-world RL applications and demonstrates applicability of the neuromorphic platforms for RL problems.",0
"Reinforcement learning (RL) is a fundamental principle for learning in biological systems and offers a framework for addressing numerous challenges with real-world artificial intelligence (AI) applications. The efficient implementation of RL techniques could result in agents deployed in edge-use cases acquiring new abilities, such as improved navigation, the ability to comprehend complex situations, and make critical decisions. To achieve this objective, we present a versatile architecture for carrying out reinforcement learning on neuromorphic platforms. Our team implemented this architecture using an Intel neuromorphic processor and demonstrated its ability to solve a variety of tasks using spiking dynamics. Our research proposes an energy-efficient and practical solution for real-world RL applications and showcases the potential of neuromorphic platforms for RL problems.",1
"Scheduling computational tasks represented by directed acyclic graphs (DAGs) is challenging because of its complexity. Conventional scheduling algorithms rely heavily on simple heuristics such as shortest job first (SJF) and critical path (CP), and are often lacking in scheduling quality. In this paper, we present a novel learning-based approach to scheduling DAG tasks. The algorithm employs a reinforcement learning agent to iteratively add directed edges to the DAG, one at a time, to enforce ordering (i.e., priorities of execution and resource allocation) of ""tricky"" job nodes. By doing so, the original DAG scheduling problem is dramatically reduced to a much simpler proxy problem, on which heuristic scheduling algorithms such as SJF and CP can be efficiently improved. Our approach can be easily applied to any existing heuristic scheduling algorithms. On the benchmark dataset of TPC-H, we show that our learning based approach can significantly improve over popular heuristic algorithms and consistently achieves the best performance among several methods under a variety of settings.",0
"The complexity of scheduling computational tasks that are represented by directed acyclic graphs (DAGs) poses a challenge. Common scheduling algorithms, such as shortest job first (SJF) and critical path (CP), rely on simple heuristics, which often result in poor scheduling quality. To address this, we introduce a novel approach to scheduling DAG tasks that utilizes reinforcement learning to iteratively add directed edges to the DAG, enforcing priority order of execution and resource allocation for ""tricky"" job nodes. This reduces the original DAG scheduling problem to a simpler proxy problem, improving the efficiency of heuristic scheduling algorithms like SJF and CP. Our approach can be easily integrated with any existing heuristic scheduling algorithms. In testing against the TPC-H benchmark dataset, our learning-based approach consistently outperforms popular heuristic algorithms and achieves the best performance across various settings.",1
"Reinforcement Learning (RL) has been able to solve hard problems such as playing Atari games or solving the game of Go, with a unified approach. Yet modern deep RL approaches are still not widely used in real-world applications. One reason could be the lack of guarantees on the performance of the intermediate executed policies, compared to an existing (already working) baseline policy. In this paper, we propose an online model-free algorithm that solves conservative exploration in the policy optimization problem. We show that the regret of the proposed approach is bounded by $\tilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous parameter spaces.",0
"Despite its success in solving challenging tasks like playing Atari games and solving the game of Go using a unified approach, modern deep reinforcement learning (RL) methods are not yet commonly applied in real-world scenarios. This could be attributed to the absence of assurances regarding the performance of intermediate policies compared to an already established baseline policy. To address this issue, our paper suggests an online model-free algorithm that resolves conservative exploration in the policy optimization problem. We demonstrate that our proposed approach guarantees regret bounded by $\tilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous parameter spaces.",1
"The standard feedback model of reinforcement learning requires revealing the reward of every visited state-action pair. However, in practice, it is often the case that such frequent feedback is not available. In this work, we take a first step towards relaxing this assumption and require a weaker form of feedback, which we refer to as \emph{trajectory feedback}. Instead of observing the reward obtained after every action, we assume we only receive a score that represents the quality of the whole trajectory observed by the agent, namely, the sum of all rewards obtained over this trajectory. We extend reinforcement learning algorithms to this setting, based on least-squares estimation of the unknown reward, for both the known and unknown transition model cases, and study the performance of these algorithms by analyzing their regret. For cases where the transition model is unknown, we offer a hybrid optimistic-Thompson Sampling approach that results in a tractable algorithm.",0
"The feedback model for reinforcement learning typically requires knowledge of the reward for each state-action pair. However, this is often not feasible in practical scenarios. This study aims to relax this assumption by introducing a less demanding form of feedback known as ""trajectory feedback"". Instead of receiving feedback after each action, the agent only receives a score that represents the quality of the entire trajectory, i.e., the cumulative reward obtained throughout the trajectory. We extend existing reinforcement learning algorithms to adapt to this setting using least-squares estimation to determine the unknown reward. We also analyze the performance of these algorithms and their regret. For scenarios where the transition model is unknown, we propose a hybrid optimistic-Thompson Sampling method that results in a feasible algorithm.",1
"Deep reinforcement learning (RL) has made groundbreaking advancements in robotics, data center management and other applications. Unfortunately, system-level bottlenecks in RL workloads are poorly understood; we observe fundamental structural differences in RL workloads that make them inherently less GPU-bound than supervised learning (SL). To explain where training time is spent in RL workloads, we propose RL-Scope, a cross-stack profiler that scopes low-level CPU/GPU resource usage to high-level algorithmic operations, and provides accurate insights by correcting for profiling overhead. Using RL-Scope, we survey RL workloads across its major dimensions including ML backend, RL algorithm, and simulator. For ML backends, we explain a $2.3\times$ difference in runtime between equivalent PyTorch and TensorFlow algorithm implementations, and identify a bottleneck rooted in overly abstracted algorithm implementations. For RL algorithms and simulators, we show that on-policy algorithms are at least $3.5\times$ more simulation-bound than off-policy algorithms. Finally, we profile a scale-up workload and demonstrate that GPU utilization metrics reported by commonly used tools dramatically inflate GPU usage, whereas RL-Scope reports true GPU-bound time. RL-Scope is an open-source tool available at https://github.com/UofT-EcoSystem/rlscope .",0
"The use of deep reinforcement learning (RL) has led to significant advancements in various fields, including robotics and data center management. However, there is a lack of understanding regarding the system-level bottlenecks in RL workloads. Unlike supervised learning (SL), RL workloads are not inherently GPU-bound due to fundamental structural differences. To address this issue, we have developed RL-Scope, a cross-stack profiler that accurately identifies where training time is spent in RL workloads by examining low-level CPU/GPU resource usage. This tool considers major dimensions of RL workloads, including ML backend, RL algorithm, and simulator. Our analysis using RL-Scope has revealed that PyTorch and TensorFlow algorithm implementations have a $2.3\times$ difference in runtime, and the bottleneck in some RL algorithms is rooted in overly abstracted algorithm implementations. Additionally, on-policy algorithms are at least $3.5\times$ more simulation-bound than off-policy algorithms. Finally, we demonstrate that commonly used tools inflate GPU usage metrics, but RL-Scope reports true GPU-bound time. RL-Scope is an open-source tool available at https://github.com/UofT-EcoSystem/rlscope.",1
"Reinforcement Learning (RL) is a key technique to address sequential decision-making problems and is crucial to realize advanced artificial intelligence. Recent years have witnessed remarkable progress in RL by virtue of the fast development of deep neural networks. Along with the promising prospects of RL in numerous domains, such as robotics and game-playing, transfer learning has arisen as an important technique to tackle various challenges faced by RL, by transferring knowledge from external expertise to accelerate the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible RL backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the RL perspective and explore their potential challenges as well as open questions that await future research progress.",0
"Reinforcement Learning (RL) is an essential method for addressing problems involving sequential decision-making and plays a crucial role in the development of advanced artificial intelligence. The use of deep neural networks in RL has led to significant improvements in recent years. Transfer learning has emerged as an important technique for overcoming various challenges in RL by incorporating external expertise to expedite the learning process. This survey investigates the recent progress of transfer learning approaches in deep reinforcement learning. We present a framework for categorizing the state-of-the-art transfer learning approaches and analyze their goals, methodologies, compatible RL backbones, and practical applications. We also explore the relationship between transfer learning and other relevant topics in RL, and discuss potential challenges and open questions for future research.",1
"We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into the bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's placement also subjects to the constraints of collision avoidance and physical stability. We formulate this online 3D-BPP as a constrained Markov decision process. To solve the problem, we propose an effective and easy-to-implement constrained deep reinforcement learning (DRL) method under the actor-critic framework. In particular, we introduce a feasibility predictor to predict the feasibility mask for the placement actions and use it to modulate the action probabilities output by the actor during training. Such supervisions and transformations to DRL facilitate the agent to learn feasible policies efficiently. Our method can also be generalized e.g., with the ability to handle lookahead or items with different orientations. We have conducted extensive evaluation showing that the learned policy significantly outperforms the state-of-the-art methods. A user study suggests that our method attains a human-level performance.",0
"We present a 3D Bin Packing Problem (3D-BPP) that is both challenging and practical. In this variant, the agent has limited information about the items to be packed, and they must be packed immediately upon arrival without buffering or readjusting. The placement of items is also subject to constraints such as collision avoidance and physical stability. To address this problem, we propose a constrained Markov decision process and a constrained deep reinforcement learning (DRL) method within an actor-critic framework. Our approach includes a feasibility predictor that predicts the feasibility mask for placement actions and modulates the action probabilities during training. Our method can be generalized to handle lookahead and items with different orientations. We extensively evaluate our approach and show that it outperforms the state-of-the-art methods. A user study indicates that our method achieves human-level performance.",1
"The Traveling Salesman Problem (TSP) is the most popular and most studied combinatorial problem, starting with von Neumann in 1951. It has driven the discovery of several optimization techniques such as cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing. The last five years have seen the emergence of promising techniques where (graph) neural networks have been capable to learn new combinatorial algorithms. The main question is whether deep learning can learn better heuristics from data, i.e. replacing human-engineered heuristics? This is appealing because developing algorithms to tackle efficiently NP-hard problems may require years of research, and many industry problems are combinatorial by nature. In this work, we propose to adapt the recent successful Transformer architecture originally developed for natural language processing to the combinatorial TSP. Training is done by reinforcement learning, hence without TSP training solutions, and decoding uses beam search. We report improved performances over recent learned heuristics with an optimal gap of 0.004% for TSP50 and 0.39% for TSP100.",0
"The Traveling Salesman Problem (TSP) has been extensively researched since von Neumann first introduced it in 1951, making it the most commonly studied combinatorial problem. The discovery of optimization techniques such as cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing were all driven by efforts to solve the TSP. Recently, neural networks, particularly graph neural networks, have shown potential in learning new combinatorial algorithms. This raises the question of whether deep learning can replace human-engineered heuristics and improve algorithm efficiency for NP-hard problems. The Transformer architecture, originally developed for natural language processing, has been adapted for the TSP in this study using reinforcement learning for training and beam search for decoding. Results show improved performance, with an optimal gap of 0.004% for TSP50 and 0.39% for TSP100, surpassing recent learned heuristics. This is significant as many industry problems are combinatorial, and developing efficient algorithms for them often requires extensive research.",1
"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called the realworldrl-suite which we propose an as an open-source benchmark.",0
"Reinforcement learning (RL) has demonstrated its effectiveness in artificial domains and is starting to make headway in real-world situations. However, many of the progressions made in RL research are challenging to utilize in practical systems because they rely on assumptions that are seldom met. In this study, we have identified and formalized several individual challenges that embody the difficulties that must be overcome for RL to become a widely used tool in real-world systems. We have defined each challenge in the context of a Markov Decision Process, analyzed its impact on cutting-edge learning algorithms, and presented existing efforts to tackle it. Our proposed approach that addresses these challenges could be quickly deployed in various real-world problems. We have implemented our proposed challenges in a continuous control environment suite called the realworldrl-suite, which we suggest as an open-source benchmark.",1
"Recent advances in off-policy deep reinforcement learning (RL) have led to impressive success in complex tasks from visual observations. Experience replay improves sample-efficiency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efficient Reinforcement Learning (SEER), a simple modification of existing off-policy RL methods, to address these computational and memory requirements. To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experiments, we show that SEER does not degrade the performance of RL agents while significantly saving computation and memory across a diverse set of DeepMind Control environments and Atari games. Finally, we show that SEER is useful for computation-efficient transfer learning in RL because lower layers of CNNs extract generalizable features, which can be used for different tasks and domains.",0
"Impressive successes have been achieved in complex tasks from visual observations with recent advances in off-policy deep reinforcement learning (RL). However, these techniques require high memory and computational bandwidth. Experience replay and convolutional neural networks (CNNs) have improved sample efficiency and effectively processed high-dimensional inputs, respectively. To address these computational and memory requirements, this paper presents Stored Embeddings for Efficient Reinforcement Learning (SEER). SEER is a simple modification of existing off-policy RL methods that freezes the lower layers of CNN encoders early in training to reduce the computational overhead of gradient updates. Additionally, SEER reduces memory requirements by storing low-dimensional latent vectors for experience replay instead of high-dimensional images, allowing for an adaptive increase in replay buffer capacity. Our experiments show that SEER maintains RL agent performance while significantly saving computation and memory across various DeepMind Control environments and Atari games. Furthermore, SEER is useful for computation-efficient transfer learning in RL as lower layers of CNNs extract generalizable features that can be applied to different tasks and domains.",1
"Various methods for solving the inverse reinforcement learning (IRL) problem have been developed independently in machine learning and economics. In particular, the method of Maximum Causal Entropy IRL is based on the perspective of entropy maximization, while related advances in the field of economics instead assume the existence of unobserved action shocks to explain expert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability method, Nested Pseudo-Likelihood Algorithm). In this work, we make previously unknown connections between these related methods from both fields. We achieve this by showing that they all belong to a class of optimization problems, characterized by a common form of the objective, the associated policy and the objective gradient. We demonstrate key computational and algorithmic differences which arise between the methods due to an approximation of the optimal soft value function, and describe how this leads to more efficient algorithms. Using insights which emerge from our study of this class of optimization problems, we identify various problem scenarios and investigate each method's suitability for these problems.",0
"The inverse reinforcement learning (IRL) problem has been tackled differently in machine learning and economics. The Maximum Causal Entropy IRL method utilizes entropy maximization, while economics-based approaches assume unseen action shocks to account for expert behavior (such as the Nested Fixed Point Algorithm, Conditional Choice Probability method, and Nested Pseudo-Likelihood Algorithm). This study establishes connections among these methods by highlighting their commonalities in terms of objective, policy, and objective gradient. We also examine the computational and algorithmic variations that occur due to an approximation of the optimal soft value function, leading to more efficient algorithms. Our analysis of this optimization problem class enables us to identify different problem scenarios and assess the suitability of each method for those situations.",1
"We present a neural optimization model trained with reinforcement learning to solve the coordinate ordering problem for sets of star glyphs. Given a set of star glyphs associated to multiple class labels, we propose to use shape context descriptors to measure the perceptual distance between pairs of glyphs, and use the derived silhouette coefficient to measure the perception of class separability within the entire set. To find the optimal coordinate order for the given set, we train a neural network using reinforcement learning to reward orderings with high silhouette coefficients. The network consists of an encoder and a decoder with an attention mechanism. The encoder employs a recurrent neural network (RNN) to encode input shape and class information, while the decoder together with the attention mechanism employs another RNN to output a sequence with the new coordinate order. In addition, we introduce a neural network to efficiently estimate the similarity between shape context descriptors, which allows to speed up the computation of silhouette coefficients and thus the training of the axis ordering network. Two user studies demonstrate that the orders provided by our method are preferred by users for perceiving class separation. We tested our model on different settings to show its robustness and generalization abilities and demonstrate that it allows to order input sets with unseen data size, data dimension, or number of classes. We also demonstrate that our model can be adapted to coordinate ordering of other types of plots such as RadViz by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.",0
"A reinforcement learning model using neural optimization is presented to address the coordinate ordering issue for sets of star glyphs. The approach involves utilizing shape context descriptors to assess the perceptual distance between pairs of glyphs and determine the class separability of the entire set using the silhouette coefficient. To achieve the optimal coordinate order, a neural network is trained through reinforcement learning to reward high silhouette coefficients. The network comprises an encoder and a decoder with an attention mechanism, which utilizes recurrent neural networks (RNNs) to encode input shape and class information and output a new coordinate order sequence. Additionally, a neural network is introduced to estimate the similarity between shape context descriptors, thereby speeding up the computation of silhouette coefficients and network training. Two user studies reveal that the orders provided by this method are preferable for perceiving class separation. The model's robustness and generalization abilities are demonstrated through various settings, including input sets with unseen data size, data dimension, or number of classes. Furthermore, this model can be adapted to coordinate ordering of other plot types, such as RadViz, by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.",1
"MuZero, a model-based reinforcement learning algorithm that uses a value equivalent dynamics model, achieved state-of-the-art performance in Chess, Shogi and the game of Go. In contrast to standard forward dynamics models that predict a full next state, value equivalent models are trained to predict a future value, thereby emphasizing value relevant information in the representations. While value equivalent models have shown strong empirical success, there is no research yet that visualizes and investigates what types of representations these models actually learn. Therefore, in this paper we visualize the latent representation of MuZero agents. We find that action trajectories may diverge between observation embeddings and internal state transition dynamics, which could lead to instability during planning. Based on this insight, we propose two regularization techniques to stabilize MuZero's performance. Additionally, we provide an open-source implementation of MuZero along with an interactive visualizer of learned representations, which may aid further investigation of value equivalent algorithms.",0
"MuZero, an algorithm that utilizes a value equivalent dynamics model for reinforcement learning, demonstrated exceptional performance in Chess, Shogi, and Go. Unlike standard forward dynamics models that predict the complete next state, value equivalent models are trained to predict a future value, prioritizing value-relevant information in the representations. Despite their successful track record, there has been no research conducted on the types of representations that these models learn. Therefore, this study visualizes the latent representation of MuZero agents and identifies that action trajectories can differ between observation embeddings and internal state transition dynamics, potentially causing instability during planning. Based on this observation, two regularization techniques are proposed to enhance MuZero's performance. Additionally, an open-source implementation of MuZero and an interactive visualizer of learned representations are provided, which may enable further investigation of value equivalent algorithms.",1
"Deep reinforcement learning (DRL) algorithms have been demonstrated to be effective in a wide range of challenging decision making and control tasks. However, these methods typically suffer from severe action oscillations in particular in discrete action setting, which means that agents select different actions within consecutive steps even though states only slightly differ. This issue is often neglected since the policy is usually evaluated by its cumulative rewards only. Action oscillation strongly affects the user experience and can even cause serious potential security menace especially in real-world domains with the main concern of safety, such as autonomous driving. To this end, we introduce Policy Inertia Controller (PIC) which serves as a generic plug-in framework to off-the-shelf DRL algorithms, to enables adaptive trade-off between the optimality and smoothness of the learned policy in a formal way. We propose Nested Policy Iteration as a general training algorithm for PIC-augmented policy which ensures monotonically non-decreasing updates under some mild conditions. Further, we derive a practical DRL algorithm, namely Nested Soft Actor-Critic. Experiments on a collection of autonomous driving tasks and several Atari games suggest that our approach demonstrates substantial oscillation reduction in comparison to a range of commonly adopted baselines with almost no performance degradation.",0
"A variety of complex decision making and control tasks have been successfully tackled using Deep Reinforcement Learning (DRL) algorithms. However, these methods often suffer from severe oscillations in action selection, particularly when dealing with discrete actions. This means that agents may choose different actions in consecutive steps, even when the states are only slightly different. This issue is often overlooked, as the policy is evaluated based solely on cumulative rewards. However, action oscillation has a significant impact on user experience and can pose serious safety risks in real-world scenarios such as autonomous driving. To address this problem, we propose a Policy Inertia Controller (PIC) as a generic plug-in framework for existing DRL algorithms. This allows for an adaptive trade-off between the optimality and smoothness of the learned policy in a formal manner. We utilize Nested Policy Iteration as a general training algorithm for the PIC-augmented policy, which guarantees monotonically non-decreasing updates under certain conditions. Furthermore, we develop a practical DRL algorithm called Nested Soft Actor-Critic. Our experiments demonstrate that our approach significantly reduces oscillation compared to commonly used baselines, with minimal performance degradation. We test our approach on a range of autonomous driving tasks and Atari games.",1
"Value function is the central notion of Reinforcement Learning (RL). Value estimation, especially with function approximation, can be challenging since it involves the stochasticity of environmental dynamics and reward signals that can be sparse and delayed in some cases. A typical model-free RL algorithm usually estimates the values of a policy by Temporal Difference (TD) or Monte Carlo (MC) algorithms directly from rewards, without explicitly taking dynamics into consideration. In this paper, we propose Value Decomposition with Future Prediction (VDFP), providing an explicit two-step understanding of the value estimation process: 1) first foresee the latent future, 2) and then evaluate it. We analytically decompose the value function into a latent future dynamics part and a policy-independent trajectory return part, inducing a way to model latent dynamics and returns separately in value estimation. Further, we derive a practical deep RL algorithm, consisting of a convolutional model to learn compact trajectory representation from past experiences, a conditional variational auto-encoder to predict the latent future dynamics and a convex return model that evaluates trajectory representation. In experiments, we empirically demonstrate the effectiveness of our approach for both off-policy and on-policy RL in several OpenAI Gym continuous control tasks as well as a few challenging variants with delayed reward.",0
"Reinforcement Learning (RL) revolves around the concept of Value function, which can be difficult to estimate, especially when function approximation is involved, due to the stochasticity of environmental dynamics and sparse and delayed reward signals. Model-free RL methods usually estimate the policy values directly from rewards using Temporal Difference (TD) or Monte Carlo (MC) algorithms, without considering dynamics. This paper proposes Value Decomposition with Future Prediction (VDFP), which involves a two-step process: first predicting the latent future and then evaluating it. The value function is analytically decomposed into a latent future dynamics part and a policy-independent trajectory return part, allowing for separate modeling of dynamics and returns in value estimation. The authors derive a practical deep RL algorithm that uses a convolutional model to learn trajectory representation from past experiences, a conditional variational auto-encoder to predict latent future dynamics, and a convex return model to evaluate trajectory representation. The effectiveness of the approach is demonstrated through experiments on OpenAI Gym continuous control tasks, as well as challenging variants with delayed reward, for both off-policy and on-policy RL.",1
"At the working heart of policy iteration algorithms commonly used and studied in the discounted setting of reinforcement learning, the policy evaluation step estimates the value of states with samples from a Markov reward process induced by following a Markov policy in a Markov decision process. We propose a simple and efficient estimator called loop estimator that exploits the regenerative structure of Markov reward processes without explicitly estimating a full model. Our method enjoys a space complexity of $O(1)$ when estimating the value of a single positive recurrent state $s$ unlike TD with $O(S)$ or model-based methods with $O\left(S^2\right)$. Moreover, the regenerative structure enables us to show, without relying on the generative model approach, that the estimator has an instance-dependent convergence rate of $\widetilde{O}\left(\sqrt{\tau_s/T}\right)$ over steps $T$ on a single sample path, where $\tau_s$ is the maximal expected hitting time to state $s$. In preliminary numerical experiments, the loop estimator outperforms model-free methods, such as TD(k), and is competitive with the model-based estimator.",0
"The policy evaluation step is crucial in policy iteration algorithms used in the discounted setting of reinforcement learning. It involves estimating the value of states by sampling from a Markov reward process that results from following a Markov policy in a Markov decision process. We propose a more efficient estimator, the loop estimator, which takes advantage of the regenerative structure of Markov reward processes. Unlike TD and model-based methods, our estimator has a space complexity of $O(1)$ when estimating the value of a single positive recurrent state $s$. Additionally, our estimator has an instance-dependent convergence rate of $\widetilde{O}\left(\sqrt{\tau_s/T}\right)$ on a single sample path, where $\tau_s$ is the maximal expected hitting time to state $s$. In initial experiments, the loop estimator outperformed TD(k) and was comparable to the model-based estimator.",1
"Automated neural network design has received ever-increasing attention with the evolution of deep convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems that neural architecture search (NAS) confronts is that a large number of candidate neural architectures are required to train, using, for instance, reinforcement learning and evolutionary optimisation algorithms, at a vast computation cost. Even recent differentiable neural architecture search (DNAS) samples a small number of candidate neural architectures based on the probability distribution of learned architecture parameters to select the final neural architecture. To address this computational complexity issue, we introduce a novel \emph{architecture parameterisation} based on scaled sigmoid function, and propose a general \emph{Differentiable Neural Architecture Learning} (DNAL) method to optimize the neural architecture without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization. We address the non-convex optimization problem of neural architecture by the continuous scaled sigmoid method with convergence guarantees. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of neural architecture search cost. The optimal networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity.",0
"In recent years, there has been a growing interest in automated neural network design, particularly with the development of deep convolutional neural networks (CNNs) and their use on embedded and mobile platforms. However, the process of neural architecture search (NAS) presents a significant challenge due to the need to train a large number of candidate neural architectures using computationally expensive methods such as reinforcement learning and evolutionary optimization algorithms. Even differentiable neural architecture search (DNAS) only considers a small number of architectures based on the probability distribution of learned architecture parameters. To address this issue, we propose a novel approach called Differentiable Neural Architecture Learning (DNAL) that uses a scaled sigmoid function for architecture parameterization. Our method optimizes the neural architecture without evaluating candidate networks, using a channel-wise module layer that is trained from scratch. We decouple the weight and architecture optimization and use the continuous scaled sigmoid method to address the non-convex optimization problem of neural architecture, with guaranteed convergence. Our experiments show that DNAL outperforms state-of-the-art methods in terms of neural architecture search cost and produces optimal networks that surpass those produced by other methods in accuracy, model size, and computational complexity, as demonstrated on the benchmark CIFAR-10 and ImageNet-1K datasets.",1
"We present a novel off-policy loss function for learning a transition model in model-based reinforcement learning. Notably, our loss is derived from the off-policy policy evaluation objective with an emphasis on correcting distribution shift. Compared to previous model-based techniques, our approach allows for greater robustness under model misspecification or distribution shift induced by learning/evaluating policies that are distinct from the data-generating policy. We provide a theoretical analysis and show empirical improvements over existing model-based off-policy evaluation methods. We provide further analysis showing our loss can be used for off-policy optimization (OPO) and demonstrate its integration with more recent improvements in OPO.",0
"Our study introduces a unique loss function for acquiring a transition model in the realm of model-based reinforcement learning. Our approach stands out as it is based on the off-policy policy evaluation objective, with a particular emphasis on correcting distribution shift. In contrast to prior model-based methods, our technique offers more robustness in the face of model misspecification or distribution shift that is brought about by the learning and evaluation of policies that differ from the data-generating policy. We also provide a theoretical assessment and exhibit how our method outperforms existing model-based off-policy evaluation techniques. We even demonstrate how our loss function can be used for off-policy optimization (OPO) and how it can be integrated with the latest OPO improvements.",1
"Learning to autonomously navigate the web is a difficult sequential decision making task. The state and action spaces are large and combinatorial in nature, and websites are dynamic environments consisting of several pages. One of the bottlenecks of training web navigation agents is providing a learnable curriculum of training environments that can cover the large variety of real-world websites. Therefore, we propose using Adversarial Environment Generation (AEG) to generate challenging web environments in which to train reinforcement learning (RL) agents. We provide a new benchmarking environment, gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites. To train the adversary, we propose a new technique for maximizing regret using the difference in the scores obtained by a pair of navigator agents. Our results show that our approach significantly outperforms prior methods for minimax regret AEG. The regret objective trains the adversary to design a curriculum of environments that are ""just-the-right-challenge"" for the navigator agents; our results show that over time, the adversary learns to generate increasingly complex web navigation tasks. The navigator agents trained with our technique learn to complete challenging, high-dimensional web navigation tasks, such as form filling, booking a flight etc. We show that the navigator agent trained with our proposed Flexible b-PAIRED technique significantly outperforms competitive automatic curriculum generation baselines -- including a state-of-the-art RL web navigation approach -- on a set of challenging unseen test environments, and achieves more than 80% success rate on some tasks.",0
"The task of learning to navigate the web independently is a complex process that involves sequential decision making. The spaces for states and actions are both vast and combinational, and websites are dynamic environments that consist of multiple pages. A major challenge in training web navigation agents is creating a curriculum of training environments that can cover the diverse range of real-world websites. To address this, we propose the use of Adversarial Environment Generation (AEG) to generate challenging web environments that can be used to train reinforcement learning (RL) agents. We introduce a new benchmarking environment called gMiniWoB, which allows an RL adversary to use compositional primitives to generate arbitrarily complex websites. To train the adversary, we propose a new technique for maximizing regret using the difference in scores obtained by a pair of navigator agents. Our results demonstrate that our approach significantly outperforms previous methods for minimax regret AEG. By using the regret objective, the adversary learns to design a curriculum of environments that are optimally challenging for the navigator agents. Our technique enables the navigator agents to learn to complete challenging web navigation tasks, such as form filling and booking a flight. We demonstrate that the navigator agent trained with our Flexible b-PAIRED technique outperforms state-of-the-art RL web navigation approaches and achieves over 80% success rate on some tasks in a set of challenging unseen test environments.",1
"Sample efficiency remains a fundamental issue of reinforcement learning. Model-based algorithms try to make better use of data by simulating the environment with a model. We propose a new neural network architecture for world models based on a vector quantized-variational autoencoder (VQ-VAE) to encode observations and a convolutional LSTM to predict the next embedding indices. A model-free PPO agent is trained purely on simulated experience from the world model. We adopt the setup introduced by Kaiser et al. (2020), which only allows 100K interactions with the real environment. We apply our method on 36 Atari environments and show that we reach comparable performance to their SimPLe algorithm, while our model is significantly smaller.",0
"Reinforcement learning continues to face a crucial problem with regard to sample efficiency. In order to optimize data usage, model-based algorithms simulate the environment using a model. Our proposal introduces a novel neural network architecture for world models, utilizing a vector quantized-variational autoencoder (VQ-VAE) to encode observations and a convolutional LSTM to predict the next embedding indices. A model-free PPO agent is then trained on simulated experience from this world model. We adopt the approach introduced by Kaiser et al. (2020), which restricts interactions with the real environment to 100K. Our method is applied to 36 Atari environments and is shown to achieve comparable performance to their SimPLe algorithm, while utilizing a significantly smaller model.",1
"Power networks, responsible for transporting electricity across large geographical regions, are complex infrastructures on which modern life critically depend. Variations in demand and production profiles, with increasing renewable energy integration, as well as the high voltage network technology, constitute a real challenge for human operators when optimizing electricity transportation while avoiding blackouts. Motivated to investigate the potential of Artificial Intelligence methods in enabling adaptability in power network operation, we have designed a L2RPN challenge to encourage the development of reinforcement learning solutions to key problems present in the next-generation power networks. The NeurIPS 2020 competition was well received by the international community attracting over 300 participants worldwide. The main contribution of this challenge is our proposed comprehensive Grid2Op framework, and associated benchmark, which plays realistic sequential network operations scenarios. The framework is open-sourced and easily re-usable to define new environments with its companion GridAlive ecosystem. It relies on existing non-linear physical simulators and let us create a series of perturbations and challenges that are representative of two important problems: a) the uncertainty resulting from the increased use of unpredictable renewable energy sources, and b) the robustness required with contingent line disconnections. In this paper, we provide details about the competition highlights. We present the benchmark suite and analyse the winning solutions of the challenge, observing one super-human performance demonstration by the best agent. We propose our organizational insights for a successful competition and conclude on open research avenues. We expect our work will foster research to create more sustainable solutions for power network operations.",0
"Modern life critically depends on complex infrastructures called power networks, which are responsible for transporting electricity across large geographical regions. These networks face challenges due to variations in demand and production profiles, increasing renewable energy integration, and high voltage network technology, which can result in blackouts. To investigate the potential of Artificial Intelligence methods in enabling adaptability in power network operation, we created a L2RPN challenge that encourages the development of reinforcement learning solutions to key problems present in the next-generation power networks. The NeurIPS 2020 competition attracted over 300 participants worldwide, and our proposed comprehensive Grid2Op framework and associated benchmark played realistic sequential network operations scenarios. The framework is open-sourced and easily re-usable to define new environments with its companion GridAlive ecosystem. In this paper, we provide details about the competition highlights, present the benchmark suite, and analyze the winning solutions of the challenge, including one super-human performance demonstration by the best agent. Our work aims to foster research to create more sustainable solutions for power network operations.",1
"Modern day computing increasingly relies on specialization to satiate growing performance and efficiency requirements. A core challenge in designing such specialized hardware architectures is how to perform mapping space search, i.e., search for an optimal mapping from algorithm to hardware. Prior work shows that choosing an inefficient mapping can lead to multiplicative-factor efficiency overheads. Additionally, the search space is not only large but also non-convex and non-smooth, precluding advanced search techniques. As a result, previous works are forced to implement mapping space search using expert choices or sub-optimal search heuristics.   This work proposes Mind Mappings, a novel gradient-based search method for algorithm-accelerator mapping space search. The key idea is to derive a smooth, differentiable approximation to the otherwise non-smooth, non-convex search space. With a smooth, differentiable approximation, we can leverage efficient gradient-based search algorithms to find high-quality mappings. We extensively compare Mind Mappings to black-box optimization schemes used in prior work. When tasked to find mappings for two important workloads (CNN and MTTKRP), the proposed search finds mappings that achieve an average $1.40\times$, $1.76\times$, and $1.29\times$ (when run for a fixed number of steps) and $3.16\times$, $4.19\times$, and $2.90\times$ (when run for a fixed amount of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns mappings with only $5.32\times$ higher EDP than a possibly unachievable theoretical lower-bound, indicating proximity to the global optima.",0
"To keep up with increasing performance and efficiency demands, modern computing relies heavily on specialization. However, designing specialized hardware architectures presents a challenge in determining the optimal mapping from algorithm to hardware. Previous research has shown that choosing an inefficient mapping can lead to significant efficiency overheads. Unfortunately, the search space for optimal mapping is large, non-convex, and non-smooth, which makes advanced search techniques impractical. Consequently, prior works have had to rely on expert choices or sub-optimal search heuristics. This paper proposes a new method called Mind Mappings, which utilizes a smooth, differentiable approximation to the non-smooth, non-convex search space. By doing so, efficient gradient-based search algorithms can be used to find high-quality mappings. The proposed method was compared to black-box optimization schemes used in previous work. Results showed that Mind Mappings achieved significantly better energy-delay product (EDP) than Simulated Annealing, Genetic Algorithms, and Reinforcement Learning when tasked with finding mappings for two important workloads (CNN and MTTKRP). Additionally, Mind Mappings returned mappings that were only $5.32\times$ higher in EDP than the theoretical lower-bound, indicating proximity to the global optima.",1
"In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.",0
"The aim of offline reinforcement learning (RL) is to develop a policy that yields high rewards based purely on a dataset of past interactions with the environment. The potential to train RL policies offline can greatly increase the scope of RL, its efficiency in data usage, and the speed of experimentation. However, prior research in offline RL has largely focused on model-free RL methods. Our research introduces MOReL, an algorithmic framework for model-based offline RL. This framework consists of two phases: (a) training a pessimistic Markov Decision Process (P-MDP) using the offline dataset; and (b) developing a near-optimal policy within this P-MDP. The P-MDP that is learned has the property that, irrespective of the policy, the performance in the real environment is bounded by the performance in the P-MDP. This facilitates its use as a reliable surrogate for policy evaluation and learning, and can overcome the challenges of model-based RL such as model exploitation. We prove that MOReL is minimax optimal (up to log factors) for offline RL. We demonstrate through experiments that MOReL matches or surpasses the current state-of-the-art results in widely studied offline RL benchmarks. Furthermore, the modular design of MOReL allows future improvements in its components (such as generative modeling, uncertainty estimation, planning, etc.) to directly translate into advances for offline RL.",1
"Large-scale finite element simulations of complex physical systems governed by partial differential equations crucially depend on adaptive mesh refinement (AMR) to allocate computational budget to regions where higher resolution is required. Existing scalable AMR methods make heuristic refinement decisions based on instantaneous error estimation and thus do not aim for long-term optimality over an entire simulation. We propose a novel formulation of AMR as a Markov decision process and apply deep reinforcement learning (RL) to train refinement policies directly from simulation. AMR poses a new problem for RL in that both the state dimension and available action set changes at every step, which we solve by proposing new policy architectures with differing generality and inductive bias. The model sizes of these policy architectures are independent of the mesh size and hence scale to arbitrarily large and complex simulations. We demonstrate in comprehensive experiments on static function estimation and the advection of different fields that RL policies can be competitive with a widely-used error estimator and generalize to larger, more complex, and unseen test problems.",0
"For large-scale finite element simulations of complicated physical systems governed by partial differential equations, adaptive mesh refinement (AMR) plays a crucial role in assigning computational resources to areas that require higher resolution. However, current scalable AMR methods rely on heuristic refinement decisions based on instantaneous error estimation and lack the objective of achieving long-term optimality throughout the simulation. Our solution is to introduce a novel formulation of AMR as a Markov decision process and train refinement policies directly from simulation using deep reinforcement learning (RL). Nevertheless, AMR creates a new challenge for RL because both the state dimension and available action set change at every step. To tackle this challenge, we propose new policy architectures with different levels of generality and inductive bias. These policy architectures have model sizes that are independent of the mesh size, allowing them to scale to arbitrarily large and complex simulations. Our experiments on static function estimation and the advection of different fields demonstrate that RL policies can compete with widely-used error estimators and generalize to larger, more complex, and unseen test problems.",1
"We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm for reinforcement learning in tabular and possibly stage-dependent, episodic Markov decision process. UCBMQ is based on Q-learning where we add a momentum term and rely on the principle of optimism in face of uncertainty to deal with exploration. Our new technical ingredient of UCBMQ is the use of momentum to correct the bias that Q-learning suffers while, at the same time, limiting the impact it has on the second-order term of the regret. For UCBMQ , we are able to guarantee a regret of at most $O(\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the length of an episode, $S$ the number of states, $A$ the number of actions, $T$ the number of episodes and ignoring terms in poly$log(SAHT)$. Notably, UCBMQ is the first algorithm that simultaneously matches the lower bound of $\Omega(\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with respect to the horizon $T$) that scales only linearly with the number of states $S$.",0
"Our new algorithm, UCBMQ (Upper Confidence Bound Momentum Q-learning), is designed for reinforcement learning in tabular, and possibly stage-dependent, episodic Markov decision processes. Building on Q-learning, we have introduced a momentum term and leveraged the principle of optimism to handle exploration. UCBMQ's unique feature is the use of momentum to address the bias in Q-learning while minimizing its impact on the second-order term of regret. We can ensure that UCBMQ's regret is at most $O(\sqrt{H^3SAT}+ H^4 S A )$, disregarding terms in poly$log(SAHT)$. Remarkably, UCBMQ is the first algorithm to meet the lower bound of $\Omega(\sqrt{H^3SAT})$ for sufficiently large $T$ and has a second-order term that scales linearly with the number of states $S$.",1
"Many physical systems have underlying safety considerations that require that the policy employed ensures the satisfaction of a set of constraints. The analytical formulation usually takes the form of a Constrained Markov Decision Process (CMDP). We focus on the case where the CMDP is unknown, and RL algorithms obtain samples to discover the model and compute an optimal constrained policy. Our goal is to characterize the relationship between safety constraints and the number of samples needed to ensure a desired level of accuracy -- both objective maximization and constraint satisfaction -- in a PAC sense. We explore two classes of RL algorithms, namely, (i) a generative model based approach, wherein samples are taken initially to estimate a model, and (ii) an online approach, wherein the model is updated as samples are obtained. Our main finding is that compared to the best known bounds of the unconstrained regime, the sample complexity of constrained RL algorithms are increased by a factor that is logarithmic in the number of constraints, which suggests that the approach may be easily utilized in real systems.",0
"To ensure safe operation, many physical systems require policies that satisfy a set of constraints. Analytical formulations often involve Constrained Markov Decision Processes (CMDP), but when CMDP is unknown, RL algorithms can obtain samples to discover the model and compute an optimal constrained policy. Our aim is to determine the relationship between safety constraints and the number of samples needed for accurate objective maximization and constraint satisfaction, using a PAC approach. We examine two classes of RL algorithms: generative model-based (initially estimating a model with samples) and online (updating the model with samples). Our key finding is that compared to unconstrained regimes, the sample complexity of constrained RL algorithms increases logarithmically with the number of constraints, making it a feasible approach for real systems.",1
"Training reinforcement learning agents at solving a given task is highly dependent on identifying optimal sets of hyperparameters and selecting suitable environment input / output configurations. This tedious process could be eased with a straightforward toolbox allowing its user to quickly compare different training parameter sets. We present rl_reach, a self-contained, open-source and easy-to-use software package designed to run reproducible reinforcement learning experiments for customisable robotic reaching tasks. rl_reach packs together training environments, agents, hyperparameter optimisation tools and policy evaluation scripts, allowing its users to quickly investigate and identify optimal training configurations. rl_reach is publicly available at this URL: https://github.com/PierreExeter/rl_reach.",0
"The success of training reinforcement learning agents to solve a particular task relies heavily on identifying the best hyperparameters and environment input/output configurations. This can be a tiresome process, but a convenient toolbox that enables quick comparison of various training parameter sets can simplify the task. We introduce rl_reach, a user-friendly, self-sufficient, and open-source software package that facilitates reproducible reinforcement learning experiments for customized robotic reaching tasks. rl_reach consolidates training environments, agents, hyperparameter optimization tools, and policy evaluation scripts, enabling users to swiftly investigate and determine optimal training configurations. rl_reach is available to the public at this URL: https://github.com/PierreExeter/rl_reach.",1
"Event-based cameras are dynamic vision sensors that can provide asynchronous measurements of changes in per-pixel brightness at a microsecond level. This makes them significantly faster than conventional frame-based cameras, and an appealing choice for high-speed navigation. While an interesting sensor modality, this asynchronous data poses a challenge for common machine learning techniques. In this paper, we present an event variational autoencoder for unsupervised representation learning from asynchronous event camera data. We show that it is feasible to learn compact representations from spatiotemporal event data to encode the context. Furthermore, we show that such pretrained representations can be beneficial for navigation, allowing for usage in reinforcement learning instead of end-to-end reward driven perception. We validate this framework of learning visuomotor policies by applying it to an obstacle avoidance scenario in simulation. We show that representations learnt from event data enable training fast control policies that can adapt to different control capacities, and demonstrate a higher degree of robustness than end-to-end learning from event images.",0
"Dynamic vision sensors known as event-based cameras can detect changes in per-pixel brightness at a microsecond level, providing asynchronous measurements that make them faster than conventional frame-based cameras. They are an attractive option for high-speed navigation. However, this asynchronous data presents a challenge for traditional machine learning methods. This paper introduces an event variational autoencoder for unsupervised representation learning using asynchronous event camera data. The study demonstrates that it is possible to learn compressed representations from spatiotemporal event data that encode context. Furthermore, these pretrained representations can be useful for navigation and can be employed in reinforcement learning instead of reward-driven perception. The framework is evaluated by applying it to an obstacle avoidance scenario in simulation. The results indicate that representations learned from event data enable the training of fast control policies that can adapt to different control capacities and are more robust than end-to-end learning from event images.",1
"We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. We find in our experiments that it substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks.",0
"The problem of communication in multi-agent reinforcement learning (MARL) is examined in this study. Traditionally, data from a replay buffer is used to learn off-policy. Nevertheless, this can be complicated by the fact that past messages may not accurately reflect each agent's current communication policy. To address this issue, a 'communication correction' is proposed. This method corrects for the non-stationarity of observed communication that results from multi-agent learning by relabelling received messages to reflect the communicator's current policy. Additionally, an ordered relabelling scheme is introduced for cases where agents are both senders and receivers. This correction is efficient and can be integrated into various off-policy algorithms. Results show that this approach enhances the performance of communicating MARL systems in cooperative and competitive tasks.",1
"Visual navigation is often cast as a reinforcement learning (RL) problem. Current methods typically result in a suboptimal policy that learns general obstacle avoidance and search behaviours. For example, in the target-object navigation setting, the policies learnt by traditional methods often fail to complete the task, even when the target is clearly within reach from a human perspective. In order to address this issue, we propose to learn to imagine a latent representation of the successful (sub-)goal state. To do so, we have developed a module which we call Foresight Imagination (ForeSIT). ForeSIT is trained to imagine the recurrent latent representation of a future state that leads to success, e.g. either a sub-goal state that is important to reach before the target, or the goal state itself. By conditioning the policy on the generated imagination during training, our agent learns how to use this imagination to achieve its goal robustly. Our agent is able to imagine what the (sub-)goal state may look like (in the latent space) and can learn to navigate towards that state. We develop an efficient learning algorithm to train ForeSIT in an on-policy manner and integrate it into our RL objective. The integration is not trivial due to the constantly evolving state representation shared between both the imagination and the policy. We, empirically, observe that our method outperforms the state-of-the-art methods by a large margin in the commonly accepted benchmark AI2THOR environment. Our method can be readily integrated or added to other model-free RL navigation frameworks.",0
"The problem of navigating visually is often approached as a reinforcement learning (RL) problem, but current methods only result in general obstacle avoidance and search abilities, rather than achieving specific goals. Traditional methods fail even when the target is close by, so we propose a new module called Foresight Imagination (ForeSIT) to learn to imagine a successful (sub-)goal state. ForeSIT is trained to imagine a future state that leads to success, and by conditioning the policy on this imagination, our agent can navigate robustly towards its goals. We have developed an efficient learning algorithm to train ForeSIT and integrate it into our RL objective, although this integration is not easy due to the constantly evolving state representation shared between imagination and policy. Our method significantly outperforms state-of-the-art methods in the AI2THOR environment and can be easily integrated into other model-free RL navigation frameworks.",1
"Researchers and practitioners in the field of reinforcement learning (RL) frequently leverage parallel computation, which has led to a plethora of new algorithms and systems in the last few years. In this paper, we re-examine the challenges posed by distributed RL and try to view it through the lens of an old idea: distributed dataflow. We show that viewing RL as a dataflow problem leads to highly composable and performant implementations. We propose RLlib flow, a hybrid actor-dataflow programming model for distributed RL, and validate its practicality by porting the full suite of algorithms in RLlib, a widely-adopted distributed RL library.",0
"Parallel computation is a common tool used by researchers and practitioners in reinforcement learning (RL). This has resulted in a range of new algorithms and systems in recent years. This paper takes a fresh look at the challenges posed by distributed RL by examining it from the perspective of distributed dataflow. The authors demonstrate that viewing RL as a dataflow issue results in highly effective and adaptable implementations. They introduce RLlib flow, a hybrid actor-dataflow programming model for distributed RL, and confirm its usefulness by transferring the complete suite of algorithms in RLlib, a popular distributed RL library.",1
"How do you incentivize self-interested agents to $\textit{explore}$ when they prefer to $\textit{exploit}$ ? We consider complex exploration problems, where each agent faces the same (but unknown) MDP. In contrast with traditional formulations of reinforcement learning, agents control the choice of policies, whereas an algorithm can only issue recommendations. However, the algorithm controls the flow of information, and can incentivize the agents to explore via information asymmetry. We design an algorithm which explores all reachable states in the MDP. We achieve provable guarantees similar to those for incentivizing exploration in static, stateless exploration problems studied previously.",0
"The challenge is to encourage self-interested agents to engage in exploration when their preference is to exploit. Our focus is on intricate exploration problems where every agent confronts the same MDP whose specifics are unknown. Unlike conventional reinforcement learning, agents determine policy selection while an algorithm only provides suggestions. Yet, the algorithm regulates the information flow and can motivate exploration by introducing information asymmetry. Our algorithm is designed to explore all MDP states that can be reached. We are able to deliver verifiable assurances that are comparable to those obtained for encouraging exploration in static, stateless exploration problems studied previously.",1
"For real-time semantic video segmentation, most recent works utilised a dynamic framework with a key scheduler to make online key/non-key decisions. Some works used a fixed key scheduling policy, while others proposed adaptive key scheduling methods based on heuristic strategies, both of which may lead to suboptimal global performance. To overcome this limitation, we model the online key decision process in dynamic video segmentation as a deep reinforcement learning problem and learn an efficient and effective scheduling policy from expert information about decision history and from the process of maximising global return. Moreover, we study the application of dynamic video segmentation on face videos, a field that has not been investigated before. By evaluating on the 300VW dataset, we show that the performance of our reinforcement key scheduler outperforms that of various baselines in terms of both effective key selections and running speed. Further results on the Cityscapes dataset demonstrate that our proposed method can also generalise to other scenarios. To the best of our knowledge, this is the first work to use reinforcement learning for online key-frame decision in dynamic video segmentation, and also the first work on its application on face videos.",0
"Recent studies on real-time semantic video segmentation have employed a dynamic framework with a key scheduler to make online key/non-key determinations. While some studies have used a fixed key scheduling policy, others have proposed adaptive key scheduling methods based on heuristic strategies, which may not result in optimal global performance. To address this issue, we have approached the online key decision process in dynamic video segmentation as a deep reinforcement learning problem, and have developed an effective scheduling policy by analyzing decision history and maximizing global return. Additionally, we have explored the use of dynamic video segmentation on face videos, a previously unexplored area. Our evaluation of the 300VW dataset demonstrates that our reinforcement key scheduler offers superior performance compared to various baseline methods in terms of both effective key selections and running speed. Furthermore, we have demonstrated that our proposed method can generalize to other scenarios, as evidenced by results on the Cityscapes dataset. This is the first study to apply reinforcement learning to online key-frame decision in dynamic video segmentation, and also the first to focus on its application on face videos.",1
"The objective of most Reinforcement Learning painting agents is to minimize the loss between a target image and the paint canvas. Human painter artistry emphasizes important features of the target image rather than simply reproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL painting models, although its final output is generally a work of finesse, produces a stroke sequence that is vastly different from that which a human would produce since the model does not have knowledge about the abstract features in the target image. In order to increase the human-like planning of the model without the use of expensive human data, we introduce a new loss function for use with the model's reward function: Content Masked Loss. In the context of robot painting, Content Masked Loss employs an object detection model to extract features which are used to assign higher weight to regions of the canvas that a human would find important for recognizing content. The results, based on 332 human evaluators, show that the digital paintings produced by our Content Masked model show detectable subject matter earlier in the stroke sequence than existing methods without compromising on the quality of the final painting. Our code is available at https://github.com/pschaldenbrand/ContentMaskedLoss.",0
"Most Reinforcement Learning painting agents aim to minimize the loss between a paint canvas and a target image. However, human painters focus on essential features of the image rather than just duplicating it. While using adversarial or L2 losses in RL painting models results in a refined final output, the stroke sequence produced is considerably different from that of a human because the model lacks knowledge of the abstract features in the target image. To enhance the model's human-like planning without the need for expensive human data, we propose a new loss function, Content Masked Loss, to be used in conjunction with the model's reward function. In the context of robot painting, this loss function utilizes an object detection model to extract features that assign more weight to the areas of the canvas that a human would consider critical for recognizing content. Based on evaluations by 332 human evaluators, our Content Masked model produces digital paintings that reveal detectable subject matter earlier in the stroke sequence than existing methods, without sacrificing the final painting's quality. Our code can be found at https://github.com/pschaldenbrand/ContentMaskedLoss.",1
"Meta-reinforcement learning (meta-RL) aims to learn from multiple training tasks the ability to adapt efficiently to unseen test tasks. Despite the success, existing meta-RL algorithms are known to be sensitive to the task distribution shift. When the test task distribution is different from the training task distribution, the performance may degrade significantly. To address this issue, this paper proposes Model-based Adversarial Meta-Reinforcement Learning (AdMRL), where we aim to minimize the worst-case sub-optimality gap -- the difference between the optimal return and the return that the algorithm achieves after adaptation -- across all tasks in a family of tasks, with a model-based approach. We propose a minimax objective and optimize it by alternating between learning the dynamics model on a fixed task and finding the adversarial task for the current model -- the task for which the policy induced by the model is maximally suboptimal. Assuming the family of tasks is parameterized, we derive a formula for the gradient of the suboptimality with respect to the task parameters via the implicit function theorem, and show how the gradient estimator can be efficiently implemented by the conjugate gradient method and a novel use of the REINFORCE estimator. We evaluate our approach on several continuous control benchmarks and demonstrate its efficacy in the worst-case performance over all tasks, the generalization power to out-of-distribution tasks, and in training and test time sample efficiency, over existing state-of-the-art meta-RL algorithms.",0
"The goal of meta-reinforcement learning (meta-RL) is to acquire the ability to adapt to new test tasks efficiently by learning from multiple training tasks. However, current meta-RL methods are sensitive to changes in the task distribution, resulting in significant performance degradation when the test task distribution differs from the training task distribution. To solve this issue, a new approach called Model-based Adversarial Meta-Reinforcement Learning (AdMRL) is proposed in this paper. The AdMRL aims to minimize the difference between the optimal return and the actual return achieved by the algorithm after adaptation for all tasks in a family of tasks using a model-based approach. The proposed approach uses a minimax objective and alternates between learning the dynamics model on a fixed task and finding the adversarial task for the current model. To efficiently implement the gradient estimator for the suboptimality with respect to the task parameters, the conjugate gradient method and a novel use of the REINFORCE estimator are used. The proposed approach is evaluated on several continuous control benchmarks and shown to outperform existing state-of-the-art meta-RL methods in worst-case performance over all tasks, generalization power to out-of-distribution tasks, and training and test time sample efficiency.",1
"Action advising is a budget-constrained knowledge exchange mechanism between teacher-student peers that can help tackle exploration and sample inefficiency problems in deep reinforcement learning (RL). Most recently, student-initiated techniques that utilise state novelty and uncertainty estimations have obtained promising results. However, the approaches built on these estimations have some potential weaknesses. First, they assume that the convergence of the student's RL model implies less need for advice. This can be misleading in scenarios with teacher absence early on where the student is likely to learn suboptimally by itself; yet also ignore the teacher's assistance later. Secondly, the delays between encountering states and having them to take effect in the RL model updates in presence of the experience replay dynamics cause a feedback lag in what the student actually needs advice for. We propose a student-initiated algorithm that alleviates these by employing Random Network Distillation (RND) to measure the novelty of a piece of advice. Furthermore, we perform RND updates only for the advised states to ensure that the student's own learning does not impair its ability to leverage the teacher. Experiments in GridWorld and MinAtar show that our approach performs on par with the state-of-the-art and demonstrates significant advantages in the scenarios where the existing methods are prone to fail.",0
"Budget-limited knowledge exchange through action advising among teacher-student peers can address the problems of exploration and sample inefficiency in deep reinforcement learning (RL). Recent techniques initiated by students using state novelty and uncertainty estimations have shown promise, but they have some potential weaknesses. Firstly, they assume that the convergence of the student's RL model means less need for advice, which can be misleading in situations where the teacher is absent early on, leading the student to learn suboptimally on its own, while also neglecting later assistance from the teacher. Secondly, in the presence of experience replay dynamics, there is a feedback lag between encountering states and their effect on the RL model updates, causing delays in the student's need for advice. To address these issues, we propose a student-initiated algorithm that uses Random Network Distillation (RND) to measure the novelty of advice. Additionally, we only perform RND updates for advised states to ensure that the student's learning does not hinder its ability to leverage the teacher. Our experiments in GridWorld and MinAtar demonstrate that our approach is comparable to state-of-the-art methods and offers significant advantages in scenarios where existing methods are likely to fail.",1
"Off-policy multi-step reinforcement learning algorithms consist of conservative and non-conservative algorithms: the former actively cut traces, whereas the latter do not. Recently, Munos et al. (2016) proved the convergence of conservative algorithms to an optimal Q-function. In contrast, non-conservative algorithms are thought to be unsafe and have a limited or no theoretical guarantee. Nonetheless, recent studies have shown that non-conservative algorithms empirically outperform conservative ones. Motivated by the empirical results and the lack of theory, we carry out theoretical analyses of Peng's Q($\lambda$), a representative example of non-conservative algorithms. We prove that it also converges to an optimal policy provided that the behavior policy slowly tracks a greedy policy in a way similar to conservative policy iteration. Such a result has been conjectured to be true but has not been proven. We also experiment with Peng's Q($\lambda$) in complex continuous control tasks, confirming that Peng's Q($\lambda$) often outperforms conservative algorithms despite its simplicity. These results indicate that Peng's Q($\lambda$), which was thought to be unsafe, is a theoretically-sound and practically effective algorithm.",0
"Multi-step reinforcement learning algorithms that are off-policy can be classified as either conservative or non-conservative. Conservative algorithms actively cut traces, while non-conservative algorithms do not. Recent research has shown that conservative algorithms converge to an optimal Q-function, but there is limited or no theoretical guarantee for non-conservative algorithms, which are considered unsafe. Despite this, empirical studies have demonstrated that non-conservative algorithms outperform conservative ones. To address this lack of theoretical understanding, we conducted a theoretical analysis of Peng's Q($\lambda$), which is a representative example of a non-conservative algorithm. Our results show that Peng's Q($\lambda$) also converges to an optimal policy as long as the behavior policy slowly tracks a greedy policy, similar to conservative policy iteration. This confirms a previously unproven conjecture. Furthermore, we tested Peng's Q($\lambda$) in complex continuous control tasks and found that it often outperforms conservative algorithms, despite its simplicity. Our findings indicate that Peng's Q($\lambda$), previously thought to be unsafe, is a theoretically-sound and practical algorithm.",1
"In offline reinforcement learning, a policy learns to maximize cumulative rewards with a fixed collection of data. Towards conservative strategy, current methods choose to regularize the behavior policy or learn a lower bound of the value function. However, exorbitant conservation tends to impair the policy's generalization ability and degrade its performance, especially for the mixed datasets. In this paper, we propose the method of reducing conservativeness oriented reinforcement learning. On the one hand, the policy is trained to pay more attention to the minority samples in the static dataset to address the data imbalance problem. On the other hand, we give a tighter lower bound of value function than previous methods to discover potential optimal actions. Consequently, our proposed method is able to tackle the skewed distribution of the provided dataset and derive a value function closer to the expected value function. Experimental results demonstrate that our proposed method outperforms the state-of-the-art methods in D4RL offline reinforcement learning evaluation tasks and our own designed mixed datasets.",0
"The process of offline reinforcement learning involves a policy that strives to maximize rewards using a fixed set of data. To ensure a more cautious approach, present techniques typically regulate the behavior policy or establish a lower limit for the value function. However, excessive conservatism can negatively impact the policy's overall performance and ability to generalize, particularly when dealing with mixed datasets. This research introduces a less conservative approach to reinforcement learning. Firstly, we teach the policy to concentrate more on minority samples within the static dataset to tackle the issue of imbalanced data. Secondly, we offer a more precise lower limit for the value function than previous methods to identify potential optimal actions. Consequently, our proposed approach can address the skewed distribution of the provided dataset and achieve a value function closer to the anticipated value function. Experimental results indicate that our technique outperforms current methods in both D4RL offline reinforcement learning evaluations and our own mixed datasets.",1
"A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of ""superhuman"" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).",0
"Reinforcement learning faces a significant challenge in the form of intelligent exploration, especially in situations where rewards are sparse or misleading. Montezuma's Revenge and Pitfall are two Atari games that serve as benchmarks for such difficult exploration domains, and current RL algorithms struggle to perform well on them, even those that use intrinsic motivation. To address this issue, a new algorithm called Go-Explore has been introduced, which relies on three key principles: remembering previously visited states, first returning to promising states and then exploring, and solving simulated environments through any means available and then robustifying via imitation learning. These principles have led to a substantial improvement in performance on hard-exploration problems, with Go-Explore achieving a mean score of over 43k points on Montezuma's Revenge, nearly four times the previous state of the art. Additionally, Go-Explore can utilize human-provided domain knowledge and has exceeded even expert human performance on both Montezuma's Revenge and Pitfall. This algorithm has the potential to open up new avenues of research and progress on previously unsolvable hard-exploration problems in various domains, particularly those that utilize simulators during training, such as robotics.",1
"Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyperparameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.",0
"The Model-based Reinforcement Learning (MBRL) framework is a promising approach for efficient control learning. However, MBRL algorithms can be complex due to the separate dynamics modeling and planning algorithm, leading to numerous hyperparameters and architectural choices. Human expertise is often necessary to apply MBRL to new problems and domains. To address this issue, we propose using automatic hyperparameter optimization (HPO). Our study shows that automated HPO significantly improves performance compared to human experts. We also demonstrate that dynamically tuning MBRL hyperparameters during training improves performance compared to using static hyperparameters. Furthermore, our experiments shed light on the impact of hyperparameters such as plan horizon and learning rate on training stability and rewards.",1
"In reinforcement learning, robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the percentile criterion, which minimizes the probability of a catastrophic failure. Unfortunately, such policies are typically overly conservative as the percentile criterion is non-convex, difficult to optimize, and ignores the mean performance. To overcome these shortcomings, we study the soft-robust criterion, which uses risk measures to balance the mean and percentile criterion better. In this paper, we establish the soft-robust criterion's fundamental properties, show that it is NP-hard to optimize, and propose and analyze two algorithms to approximately optimize it. Our theoretical analyses and empirical evaluations demonstrate that our algorithms compute much less conservative solutions than the existing approximate methods for optimizing the percentile-criterion.",0
"The process of reinforcement learning involves creating strong policies for making high-stakes decisions with limited data. Typically, these policies are developed by optimizing the percentile criterion to reduce the likelihood of a disastrous outcome. Unfortunately, this approach can lead to overly cautious policies, as the percentile criterion is complex and does not consider the average performance. To address these issues, we explore the soft-robust criterion, which incorporates risk measures to more effectively balance the mean and percentile criteria. Our research delves into the core features of the soft-robust criterion, demonstrates that it is challenging to optimize, and proposes two algorithms for approximating the optimal solution. Our analysis shows that our algorithms produce less conservative results than existing methods for optimizing the percentile-criterion, both in theory and in practice.",1
"We propose a graphical model framework for goal-conditioned RL, with an EM algorithm that operates on the lower bound of the RL objective. The E-step provides a natural interpretation of how 'learning in hindsight' techniques, such as HER, to handle extremely sparse goal-conditioned rewards. The M-step reduces policy optimization to supervised learning updates, which greatly stabilizes end-to-end training on high-dimensional inputs such as images. We show that the combined algorithm, hEM significantly outperforms model-free baselines on a wide range of goal-conditioned benchmarks with sparse rewards.",0
"Our proposal is a graphical model framework for goal-conditioned RL that employs an EM algorithm to operate on the RL objective's lower bound. The E-step offers a clear understanding of how 'learning in hindsight' techniques like HER can handle extremely sparse goal-conditioned rewards. The M-step simplifies policy optimization into supervised learning updates, which enhances the stability of end-to-end training on high-dimensional inputs like images. Our findings reveal that the hEM algorithm significantly outperforms model-free baselines on diverse goal-conditioned benchmarks with sparse rewards.",1
"Safety in reinforcement learning (RL) is a key property in both training and execution in many domains such as autonomous driving or finance. In this paper, we formalize it with a constrained RL formulation in the distributional RL setting. Our general model accepts various definitions of safety(e.g., bounds on expected performance, CVaR, variance, or probability of reaching bad states). To ensure safety during learning, we extend a safe policy optimization method to solve our problem. The distributional RL perspective leads to a more efficient algorithm while additionally catering for natural safe constraints. We empirically validate our propositions on artificial and real domains against appropriate state-of-the-art safe RL algorithms.",0
"In numerous fields, such as finance and autonomous driving, safety is a significant aspect of both training and execution in reinforcement learning (RL). This study presents a formalization of safety through a constrained RL framework in the distributional RL domain. Our comprehensive model allows for various safety definitions, such as limits on expected performance, probability of reaching unfavorable states, CVaR, or variance. To ensure safety during learning, we augment a secure policy optimization technique to address our problem. The distributional RL approach results in a more efficient algorithm while accommodating natural safe restrictions. We substantiate our claims by empirically testing them on genuine and artificial domains against suitable state-of-the-art safe RL algorithms.",1
"In federated learning (FL), fair and accurate measurement of the contribution of each federated participant is of great significance. The level of contribution not only provides a rational metric for distributing financial benefits among federated participants, but also helps to discover malicious participants that try to poison the FL framework. Previous methods for contribution measurement were based on enumeration over possible combination of federated participants. Their computation costs increase drastically with the number of participants or feature dimensions, making them inapplicable in practical situations. In this paper an efficient method is proposed to evaluate the contributions of federated participants. This paper focuses on the horizontal FL framework, where client servers calculate parameter gradients over their local data, and upload the gradients to the central server. Before aggregating the client gradients, the central server train a data value estimator of the gradients using reinforcement learning techniques. As shown by experimental results, the proposed method consistently outperforms the conventional leave-one-out method in terms of valuation authenticity as well as time complexity.",0
"The precise and unbiased measurement of each participant's contribution is crucial in federated learning (FL). It not only serves as a fair basis for distributing financial benefits, but also enables the detection of any malicious activity aimed at sabotaging the FL framework. The existing approaches for measuring contribution rely on an exhaustive enumeration of all possible combinations of participants, which becomes impractical with an increase in the number of participants or feature dimensions. Therefore, this paper presents a more efficient method for evaluating the contributions of federated participants. Specifically, it focuses on the horizontal FL framework, where client servers calculate parameter gradients over their local data and upload them to the central server for aggregation. To achieve this, the central server trains a data value estimator of the gradients using reinforcement learning techniques. The experimental results show that this method consistently outperforms the conventional leave-one-out method both in terms of valuation authenticity and time complexity.",1
"Heterogeneous ensembles that can aggregate an unrestricted number and variety of base predictors can effectively address challenging prediction problems. In particular, accurate ensembles that are also parsimonious, i.e., consist of as few base predictors as possible, can help reveal potentially useful knowledge about the target problem domain. Although ensemble selection offers a potential approach to achieving these goals, the currently available algorithms are limited in their abilities. In this paper, we present several algorithms that incorporate ensemble diversity into a reinforcement learning (RL)-based ensemble selection framework to build accurate and parsimonious ensembles. These algorithms, as well as several baselines, are rigorously evaluated on datasets from diverse domains in terms of the predictive performance and parsimony of their ensembles. This evaluation demonstrates that our diversity-incorporated RL-based algorithms perform better than the others for constructing simultaneously accurate and parsimonious ensembles. These algorithms can eventually aid the interpretation or reverse engineering of predictive models assimilated into effective ensembles. To enable such a translation, an implementation of these algorithms, as well the experimental setup they are evaluated in, has been made available at https://github.com/GauravPandeyLab/lens-learning-ensembles-using-reinforcement-learning.",0
"Difficult prediction problems can be effectively addressed by heterogeneous ensembles that can aggregate a diverse range of base predictors. Ensembles that are both accurate and parsimonious, with a minimal number of base predictors, can provide valuable insights into the problem domain. However, current ensemble selection algorithms have limitations. This paper introduces a reinforcement learning-based ensemble selection framework that incorporates ensemble diversity to construct accurate and parsimonious ensembles. The proposed algorithms are evaluated against several baselines on diverse datasets to demonstrate their superior predictive performance and parsimony. These algorithms have the potential to enhance the interpretation and reverse engineering of predictive models in effective ensembles. The implementation and experimental setup are available at https://github.com/GauravPandeyLab/lens-learning-ensembles-using-reinforcement-learning.",1
"Heterogeneous ensembles built from the predictions of a wide variety and large number of diverse base predictors represent a potent approach to building predictive models for problems where the ideal base/individual predictor may not be obvious. Ensemble selection is an especially promising approach here, not only for improving prediction performance, but also because of its ability to select a collectively predictive subset, often a relatively small one, of the base predictors. In this paper, we present a set of algorithms that explicitly incorporate ensemble diversity, a known factor influencing predictive performance of ensembles, into a reinforcement learning framework for ensemble selection. We rigorously tested these approaches on several challenging problems and associated data sets, yielding that several of them produced more accurate ensembles than those that don't explicitly consider diversity. More importantly, these diversity-incorporating ensembles were much smaller in size, i.e., more parsimonious, than the latter types of ensembles. This can eventually aid the interpretation or reverse engineering of predictive models assimilated into the resultant ensemble(s).",0
"Building predictive models for complex problems can be challenging when the best individual predictors are not obvious. To address this issue, heterogeneous ensembles made up of diverse base predictors can be a powerful solution. Ensemble selection is a promising approach that not only improves prediction performance, but also helps to identify a small subset of base predictors that work well together. This paper presents a reinforcement learning framework for ensemble selection that incorporates diversity as a factor in predicting performance. Our experimental results show that ensembles with explicit diversity consideration are more accurate and parsimonious than those without. This can facilitate the interpretation and reverse engineering of the predictive models used in the ensembles.",1
"We address the problem of computing reliable policies in reinforcement learning problems with limited data. In particular, we compute policies that achieve good returns with high confidence when deployed. This objective, known as the \emph{percentile criterion}, can be optimized using Robust MDPs~(RMDPs). RMDPs generalize MDPs to allow for uncertain transition probabilities chosen adversarially from given ambiguity sets. We show that the RMDP solution's sub-optimality depends on the spans of the ambiguity sets along the value function. We then propose new algorithms that minimize the span of ambiguity sets defined by weighted $L_1$ and $L_\infty$ norms. Our primary focus is on Bayesian guarantees, but we also describe how our methods apply to frequentist guarantees and derive new concentration inequalities for weighted $L_1$ and $L_\infty$ norms. Experimental results indicate that our optimized ambiguity sets improve significantly on prior construction methods.",0
"Our aim is to tackle the challenge of generating dependable policies in reinforcement learning scenarios where data is limited. Specifically, we strive to produce policies that yield satisfactory returns with a high level of certainty when put into effect, which is referred to as the ""percentile criterion."" To accomplish this, we utilize Robust MDPs (RMDPs), which are an extension of MDPs that permit uncertain transition probabilities to be chosen adversarially from prescribed ambiguity sets. We observe that the sub-optimality of the RMDP solution is determined by the range of the ambiguity sets along the value function. We then put forward fresh algorithms that minimize the range of ambiguity sets defined by weighted $L_1$ and $L_\infty$ norms. While our main emphasis is on Bayesian guarantees, we also detail how our techniques are applicable to frequentist guarantees and present novel concentration inequalities for weighted $L_1$ and $L_\infty$ norms. Our experimental results demonstrate that our optimized ambiguity sets represent a significant improvement over previous construction methods.",1
"""Nonstationarity"" is a fundamental problem in cooperative multi-agent reinforcement learning (MARL). It results from every agent's policy changing during learning, while being part of the environment from the perspective of other agents. This causes information to inherently oscillate between agents during learning, greatly slowing convergence. We use the MAILP model of information transfer during multi-agent learning to show that increasing centralization during learning arbitrarily mitigates the slowing of convergence due to nonstationarity. The most centralized case of learning is parameter sharing, an uncommonly used MARL method, specific to environments with homogeneous agents. It bootstraps single-agent reinforcement learning (RL) methods and learns an identical policy for each agent. We experimentally replicate our theoretical result of increased learning centralization leading to better performance. We further apply parameter sharing to 8 more modern single-agent deep RL methods for the first time, achieving up to 44 times more average reward in 16% as many episodes compared to previous parameter sharing experiments. We finally give a formal proof of a set of methods that allow parameter sharing to serve in environments with heterogeneous agents.",0
"Cooperative multi-agent reinforcement learning (MARL) is hindered by the problem of ""nonstationarity,"" which occurs when each agent's policy changes during learning, impacting other agents' perspectives and causing information to fluctuate. This reduces convergence speed, necessitating increased centralization during learning. In our study, we employ the MAILP model to demonstrate that centralization improves convergence, with the most centralized approach being parameter sharing. This method is uncommon in MARL and restricted to homogeneous agent environments, but it learns an identical policy for each agent through single-agent reinforcement learning (RL) bootstrapping. Our experimental results confirm that increased centralization leads to better performance, and we apply parameter sharing to eight modern single-agent deep RL methods for the first time, achieving up to 44 times more average reward in only 16% of the episodes compared to previous experiments. Finally, we offer a formal proof of methods that enable parameter sharing to function in heterogeneous agent environments.",1
"We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.",0
"Our proposed algorithm for reinforcement learning is based on a regularized linear-programming formulation of optimal control in MDPs. It shares similarities with Peters et al.'s classic Relative Entropy Policy Search (REPS) algorithm, but introduces a Q-function to allow for efficient exact model-free implementation. Our algorithm, known as QREPS, offers a convex loss function for policy evaluation as a theoretically sound alternative to the commonly used squared Bellman error. We have developed a practical saddle-point optimization method for minimizing this loss function and conducted an error-propagation analysis to relate the quality of individual updates to the performance of the output policy. To demonstrate the effectiveness of QREPS, we have tested it on a range of benchmark problems.",1
"Learning from Observations (LfO) is a practical reinforcement learning scenario from which many applications can benefit through the reuse of incomplete resources. Compared to conventional imitation learning (IL), LfO is more challenging because of the lack of expert action guidance. In both conventional IL and LfO, distribution matching is at the heart of their foundation. Traditional distribution matching approaches are sample-costly which depend on on-policy transitions for policy learning. Towards sample-efficiency, some off-policy solutions have been proposed, which, however, either lack comprehensive theoretical justifications or depend on the guidance of expert actions. In this work, we propose a sample-efficient LfO approach that enables off-policy optimization in a principled manner. To further accelerate the learning procedure, we regulate the policy update with an inverse action model, which assists distribution matching from the perspective of mode-covering. Extensive empirical results on challenging locomotion tasks indicate that our approach is comparable with state-of-the-art in terms of both sample-efficiency and asymptotic performance.",0
"The practical application of Learning from Observations (LfO) can benefit many areas by utilizing incomplete resources. However, compared to conventional imitation learning (IL), LfO presents more challenges due to the lack of expert action guidance. Both IL and LfO rely on distribution matching, but traditional methods are sample-costly and depend on on-policy transitions for policy learning. While some off-policy solutions have been proposed, they either lack theoretical justifications or require expert action guidance. This study proposes a principled, sample-efficient LfO approach that incorporates an inverse action model to assist with distribution matching and mode-covering. Empirical results on difficult locomotion tasks show that our approach is comparable to state-of-the-art approaches in terms of sample-efficiency and asymptotic performance.",1
"Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution.",0
"The main focus of deep reinforcement learning is learning behavior, often at the expense of considering the importance of an agent's form in determining its function. The question then arises of how to find a suitable morphology for achieving tasks within a specific environment. Current methods that co-adapt morphology and behavior rely on task rewards to optimize morphology, but this approach can be costly and lead to task-specific morphologies that lack generalization. To address these issues, we introduce a new approach called Task-Agnostic Morphology Evolution (TAME). TAME evolves morphologies without any task or reward specifications by applying randomly sampled action primitives to a population of agents. Using an information-theoretic objective, TAME efficiently ranks agents based on their ability to reach diverse states in the environment and the causality of their actions. We demonstrate empirically that TAME can evolve morphologies that perform as well as those learned through task-supervised algorithms in various 2D, 3D, and manipulation environments. Our code and videos can be accessed at https://sites.google.com/view/task-agnostic-evolution.",1
"While auxiliary tasks play a key role in shaping the representations learnt by reinforcement learning agents, much is still unknown about the mechanisms through which this is achieved. This work develops our understanding of the relationship between auxiliary tasks, environment structure, and representations by analysing the dynamics of temporal difference algorithms. Through this approach, we establish a connection between the spectral decomposition of the transition operator and the representations induced by a variety of auxiliary tasks. We then leverage insights from these theoretical results to inform the selection of auxiliary tasks for deep reinforcement learning agents in sparse-reward environments.",0
"The impact of auxiliary tasks on the learned representations of reinforcement learning agents is significant, but the methods by which this occurs are not yet fully understood. This study aims to enhance our comprehension of the correlation between environment structure, auxiliary tasks, and representations by examining the dynamics of temporal difference algorithms. By using this method, we establish a link between the transition operator's spectral decomposition and the representations induced by various auxiliary tasks. We apply our theoretical findings to guide the selection of auxiliary tasks for deep reinforcement learning agents in sparse-reward environments.",1
"Current work in explainable reinforcement learning generally produces policies in the form of a decision tree over the state space. Such policies can be used for formal safety verification, agent behavior prediction, and manual inspection of important features. However, existing approaches fit a decision tree after training or use a custom learning procedure which is not compatible with new learning techniques, such as those which use neural networks. To address this limitation, we propose a novel Markov Decision Process (MDP) type for learning decision tree policies: Iterative Bounding MDPs (IBMDPs). An IBMDP is constructed around a base MDP so each IBMDP policy is guaranteed to correspond to a decision tree policy for the base MDP when using a method-agnostic masking procedure. Because of this decision tree equivalence, any function approximator can be used during training, including a neural network, while yielding a decision tree policy for the base MDP. We present the required masking procedure as well as a modified value update step which allows IBMDPs to be solved using existing algorithms. We apply this procedure to produce IBMDP variants of recent reinforcement learning methods. We empirically show the benefits of our approach by solving IBMDPs to produce decision tree policies for the base MDPs.",0
"Explainable reinforcement learning currently generates policies in the form of a decision tree over the state space, which can be utilized for formal safety verification, agent behavior prediction, and manual inspection of essential features. However, the current decision tree approaches fit a decision tree after training or use a custom learning procedure that is incompatible with new learning techniques, such as neural networks. To overcome this limitation, we propose a novel Markov Decision Process (MDP) type for learning decision tree policies, called Iterative Bounding MDPs (IBMDPs). An IBMDP is built around a base MDP, ensuring that each IBMDP policy corresponds to a decision tree policy for the base MDP, using a method-agnostic masking procedure. Thus, any function approximator, including a neural network, can be used during training while yielding a decision tree policy for the base MDP. We present the required masking procedure and a modified value update step that enable IBMDPs to be solved using existing algorithms. We apply this process to produce IBMDP variants of recent reinforcement learning methods and demonstrate the benefits of our approach by solving IBMDPs to generate decision tree policies for the base MDPs.",1
"A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed from the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.",0
"Recent research indicates that embodied gameplay is crucial for the development of neural flexibility, creative problem solving, decision making, and socialization, not only among humans but also various animal species like turtles and ravens. However, little is known about the impact of embodied gameplay on artificial agents. Even though some agents have been proficient in abstract games, these environments are too far from reality to provide any insight into the advantages of embodied play. Hide-and-seek games are universal and offer a rich ground for studying the impact of embodied gameplay on representation learning, including perspective taking, secret keeping, and false belief understanding. In this study, we demonstrate that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Our agents' representations are enhanced by intentionality and memory, developed through interaction and play, and closely resemble biologically motivated learning strategies. These findings provide a model for exploring how vision develops through interaction, offer an experimental framework for assessing what artificial agents learn, and underscore the value of moving away from large, static datasets to experiential, interactive representation learning.",1
"Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two section, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. The survey also draws connection to several related RL fields, like hierarchical RL and transfer. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.",0
"Artificial intelligence faces a significant challenge in sequential decision making, which is often formalized as Markov Decision Process (MDP) optimization. Reinforcement learning (RL) and planning are the two primary approaches to this problem. This article provides an extensive survey of the integration of these two fields, which is commonly referred to as model-based reinforcement learning. The first step of this approach involves dynamics model learning, which includes addressing challenges such as stochasticity, uncertainty, partial observability, and temporal abstraction. The second step involves categorizing planning-learning integration, considering factors like where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning into the learning and acting loop. Additionally, the article discusses implicit model-based RL as a comprehensive alternative to model learning and planning. The benefits of model-based RL, such as enhanced data efficiency, targeted exploration, and improved stability, are also covered. The survey also explores connections to related RL fields such as hierarchical RL and transfer. In summary, this survey provides a comprehensive conceptual overview of planning-learning combinations for MDP optimization.",1
"Despite its experimental success, Model-based Reinforcement Learning still lacks a complete theoretical understanding. To this end, we analyze the error in the cumulative reward using a contraction approach. We consider both stochastic and deterministic state transitions for continuous (non-discrete) state and action spaces. This approach doesn't require strong assumptions and can recover the typical quadratic error to the horizon. We prove that branched rollouts can reduce this error and are essential for deterministic transitions to have a Bellman contraction. Our analysis of policy mismatch error also applies to Imitation Learning. In this case, we show that GAN-type learning has an advantage over Behavioral Cloning when its discriminator is well-trained.",0
"Although Model-based Reinforcement Learning has shown success in experiments, it still lacks a complete theoretical understanding. In order to address this, we utilize a contraction approach to analyze the error in cumulative reward. Our analysis considers both stochastic and deterministic state transitions, and covers continuous (non-discrete) state and action spaces without relying on strong assumptions. By using branched rollouts, we demonstrate that the error can be reduced, and that Bellman contraction is essential for deterministic transitions. Furthermore, our analysis of policy mismatch error is applicable to Imitation Learning, and we find that GAN-type learning has an advantage over Behavioral Cloning when the discriminator is well-trained.",1
"Reinforcement learning algorithms typically assume rewards to be sampled from light-tailed distributions, such as Gaussian or bounded. However, a wide variety of real-world systems generate rewards that follow heavy-tailed distributions. We consider such scenarios in the setting of undiscounted reinforcement learning. By constructing a lower bound, we show that the difficulty of learning heavy-tailed rewards asymptotically dominates the difficulty of learning transition probabilities. Leveraging techniques from robust mean estimation, we propose Heavy-UCRL2 and Heavy-Q-Learning, and show that they achieve near-optimal regret bounds in this setting. Our algorithms also naturally generalize to deep reinforcement learning applications; we instantiate Heavy-DQN as an example of this. We demonstrate that all of our algorithms outperform baselines on both synthetic MDPs and standard RL benchmarks.",0
"Reward distributions in the real world often follow heavy-tailed distributions, which is different from the light-tailed distributions assumed by reinforcement learning algorithms like Gaussian or bounded. This poses a challenge for undiscounted reinforcement learning, where the difficulty of learning heavy-tailed rewards is shown to asymptotically dominate the difficulty of learning transition probabilities. To address this challenge, we propose Heavy-UCRL2 and Heavy-Q-Learning using robust mean estimation techniques. These algorithms achieve near-optimal regret bounds and can be applied to deep reinforcement learning, such as Heavy-DQN. Our algorithms outperform baselines on both synthetic MDPs and standard RL benchmarks.",1
"We propose a novel hierarchical reinforcement learning framework for control with continuous state and action spaces. In our framework, the user specifies subgoal regions which are subsets of states; then, we (i) learn options that serve as transitions between these subgoal regions, and (ii) construct a high-level plan in the resulting abstract decision process (ADP). A key challenge is that the ADP may not be Markov, which we address by proposing two algorithms for planning in the ADP. Our first algorithm is conservative, allowing us to prove theoretical guarantees on its performance, which help inform the design of subgoal regions. Our second algorithm is a practical one that interweaves planning at the abstract level and learning at the concrete level. In our experiments, we demonstrate that our approach outperforms state-of-the-art hierarchical reinforcement learning algorithms on several challenging benchmarks.",0
"We introduce a new framework for hierarchical reinforcement learning that can handle continuous state and action spaces. Our approach involves defining subgoal regions, which are subsets of states specified by the user. We then (i) learn options that transition between these subgoal regions and (ii) create a high-level plan using an abstract decision process (ADP). The ADP may not be Markov, which poses a challenge that we tackle by proposing two planning algorithms. The first algorithm is conservative and gives us theoretical guarantees on performance, which helps to inform the subgoal region design. The second algorithm is practical and combines planning at the abstract level with learning at the concrete level. Our experiments show that our approach outperforms other hierarchical reinforcement learning algorithms on challenging benchmarks.",1
"Safety is a critical feature of controller design for physical systems. When designing control policies, several approaches to guarantee this aspect of autonomy have been proposed, such as robust controllers or control barrier functions. However, these solutions strongly rely on the model of the system being available to the designer. As a parallel development, reinforcement learning provides model-agnostic control solutions but in general, it lacks the theoretical guarantees required for safety. Recent advances show that under mild conditions, control policies can be learned via reinforcement learning, which can be guaranteed to be safe by imposing these requirements as constraints of an optimization problem. However, to transfer from learning safety to learning safely, there are two hurdles that need to be overcome: (i) it has to be possible to learn the policy without having to re-initialize the system; and (ii) the rollouts of the system need to be in themselves safe. In this paper, we tackle the first issue, proposing an algorithm capable of operating in the continuing task setting without the need of restarts. We evaluate our approach in a numerical example, which shows the capabilities of the proposed approach in learning safe policies via safe exploration.",0
"Designing controllers for physical systems that ensure safety is of utmost importance. Robust controllers or control barrier functions have been proposed to achieve this, but they rely heavily on the availability of the system model to the designer. Reinforcement learning, on the other hand, offers model-agnostic control solutions, but they lack the necessary theoretical guarantees for safety. Recent research suggests that control policies learned via reinforcement learning can be made safe by imposing safety requirements as constraints in an optimization problem. However, two challenges must be overcome to achieve this: (i) the policy must be learned without requiring system re-initialization, and (ii) system rollouts must be safe. In this paper, we address the first challenge by proposing an algorithm that operates in a continuing task setting without the need for restarts. Our approach enables safe exploration and is evaluated through a numerical example.",1
"Communication is a important factor that enables agents work cooperatively in multi-agent reinforcement learning (MARL). Most previous work uses continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete message communication protocol emerged from a variety of domains can increase the interpretability for human designers and other agents.This paper proposes a method to generate discrete messages analogous to human languages, and achieve communication by a broadcast-and-listen mechanism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with much a much smaller vocabulary size.Furthermore, we propose an approach that allows humans to interactively send discrete messages to agents.",0
"In multi-agent reinforcement learning (MARL), effective communication is crucial for agents to collaborate successfully. However, previous research has mainly employed continuous message communication, which sacrifices interpretability for enhanced representational capacity. To improve interpretability for both human designers and agents, this study suggests allowing agents to learn their own discrete message communication protocol from various domains. Specifically, we propose a self-attention-based broadcast-and-listen mechanism that generates discrete messages resembling human languages. Our results demonstrate that discrete message communication yields comparable performance to continuous message communication, but with a much smaller vocabulary size. Additionally, we introduce an interactive approach that enables humans to send discrete messages to agents.",1
"For many reinforcement learning (RL) applications, specifying a reward is difficult. In this paper, we consider an RL setting where the agent can obtain information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a model of the reward function that allows standard RL algorithms to achieve high expected return with as few expert queries as possible. For this purpose, we propose Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward function and selects queries that maximize the information gain about the difference in return between potentially optimal policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. Moreover, by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model, it achieves similar or better performance with significantly fewer queries. We support our findings with extensive evaluations in multiple environments and with different types of queries.",0
"The process of specifying a reward for reinforcement learning (RL) applications can be challenging. This study examines an RL scenario where the agent can only obtain reward information by consulting an expert who can evaluate individual states or provide binary preferences for trajectories. Our objective is to learn a reward function model that will enable standard RL algorithms to achieve a high expected return while using the fewest possible expert queries. To this end, we introduce Information Directed Reward Learning (IDRL), which employs a Bayesian reward function model and selects queries that maximize the information gain about the difference in return between potentially optimal policies. Unlike previous active reward learning approaches that are tailored to specific query types, IDRL is versatile and can accommodate various query types. By prioritizing the improvement of the policy induced by the reward model over reducing the reward approximation error, we demonstrate that IDRL achieves comparable or superior performance with significantly fewer queries. We validate our results through extensive evaluations in multiple environments and query types.",1
"Since the earliest days of reinforcement learning, the workhorse method for assigning credit to actions over time has been temporal-difference (TD) learning, which propagates credit backward timestep-by-timestep. This approach suffers when delays between actions and rewards are long and when intervening unrelated events contribute variance to long-term returns. We propose state-associative (SA) learning, where the agent learns associations between states and arbitrarily distant future rewards, then propagates credit directly between the two. In this work, we use SA-learning to model the contribution of past states to the current reward. With this model we can predict each state's contribution to the far future, a quantity we call ""synthetic returns"". TD-learning can then be applied to select actions that maximize these synthetic returns (SRs). We demonstrate the effectiveness of augmenting agents with SRs across a range of tasks on which TD-learning alone fails. We show that the learned SRs are interpretable: they spike for states that occur after critical actions are taken. Finally, we show that our IMPALA-based SR agent solves Atari Skiing -- a game with a lengthy reward delay that posed a major hurdle to deep-RL agents -- 25 times faster than the published state-of-the-art.",0
"Temporal-difference (TD) learning has been the go-to method for assigning credit to actions over time in reinforcement learning since its inception. However, this approach is hindered by long delays between actions and rewards, as well as unrelated events that contribute to long-term returns. In order to tackle these challenges, we introduce state-associative (SA) learning, which involves the agent learning associations between states and rewards that are arbitrarily distant in the future, and propagating credit directly between them. Our study focuses on utilizing SA-learning to model the contribution of past states to the current reward and predict each state's contribution to the far future, which we call ""synthetic returns"" (SRs). By maximizing these SRs using TD-learning, we demonstrate the effectiveness of augmenting agents with SRs across various tasks where TD-learning alone fails. Furthermore, the learned SRs are interpretable, with spikes occurring for states that follow critical actions. Finally, we show that our IMPALA-based SR agent is capable of solving Atari Skiing, which has posed a significant challenge to deep-RL agents due to its lengthy reward delay, 25 times faster than the current state-of-the-art.",1
"Recent reinforcement learning studies extensively explore the interplay between cooperative and competitive behaviour in mixed environments. Unlike cooperative environments where agents strive towards a common goal, mixed environments are notorious for the conflicts of selfish and social interests. As a consequence, purely rational agents often struggle to achieve and maintain cooperation. A prevalent approach to induce cooperative behaviour is to assign additional rewards based on other agents' well-being. However, this approach suffers from the issue of multi-agent credit assignment, which can hinder performance. This issue is efficiently alleviated in cooperative setting with such state-of-the-art algorithms as QMIX and COMA. Still, when applied to mixed environments, these algorithms may result in unfair allocation of rewards. We propose BAROCCO, an extension of these algorithms capable to balance individual and social incentives. The mechanism behind BAROCCO is to train two distinct but interwoven components that jointly affect each agent's decisions. Our meta-algorithm is compatible with both Q-learning and Actor-Critic frameworks. We experimentally confirm the advantages over the existing methods and explore the behavioural aspects of BAROCCO in two mixed multi-agent setups.",0
"Current studies on reinforcement learning extensively investigate the relationship between cooperative and competitive behavior in mixed environments. In contrast to cooperative environments where agents work towards a shared goal, mixed environments are notorious for conflicting selfish and social interests, making it difficult for purely rational agents to maintain cooperation. One common approach to promote cooperative behavior is to provide additional rewards based on other agents' well-being. However, this method can hinder performance due to the issue of multi-agent credit assignment. While state-of-the-art algorithms such as QMIX and COMA efficiently alleviate this issue in cooperative settings, they may result in unfair reward allocation in mixed environments. Therefore, we introduce BAROCCO, an extension of these algorithms that balances individual and social incentives. BAROCCO includes two interwoven components that jointly influence each agent's decision-making. We demonstrate the advantages of BAROCCO over existing methods through experimental results and explore its behavioral aspects in two mixed multi-agent scenarios. Our meta-algorithm is compatible with both Q-learning and Actor-Critic frameworks.",1
"Reward decomposition is a critical problem in centralized training with decentralized execution~(CTDE) paradigm for multi-agent reinforcement learning. To take full advantage of global information, which exploits the states from all agents and the related environment for decomposing Q values into individual credits, we propose a general meta-learning-based Mixing Network with Meta Policy Gradient~(MNMPG) framework to distill the global hierarchy for delicate reward decomposition. The excitation signal for learning global hierarchy is deduced from the episode reward difference between before and after ""exercise updates"" through the utility network. Our method is generally applicable to the CTDE method using a monotonic mixing network. Experiments on the StarCraft II micromanagement benchmark demonstrate that our method just with a simple utility network is able to outperform the current state-of-the-art MARL algorithms on 4 of 5 super hard scenarios. Better performance can be further achieved when combined with a role-based utility network.",0
"In the paradigm of centralized training with decentralized execution (CTDE) for multi-agent reinforcement learning, reward decomposition is a crucial issue. To fully utilize global information, which involves the states of all agents and the corresponding environment in order to break down Q values into individual credits, we introduce a Mixing Network with Meta Policy Gradient (MNMPG) framework based on meta-learning. This framework distills the global hierarchy for delicate reward decomposition. The excitation signal for learning the global hierarchy is derived from the difference in episode reward before and after ""exercise updates"" through the utility network. Our method is generally applicable to the CTDE approach using a monotonic mixing network. We conducted experiments on the StarCraft II micromanagement benchmark and found that our method, with a simple utility network, outperforms the current state-of-the-art MARL algorithms on 4 out of 5 super hard scenarios. Combining our method with a role-based utility network can further improve performance.",1
"6D pose estimation from a single RGB image is a challenging and vital task in computer vision. The current mainstream deep model methods resort to 2D images annotated with real-world ground-truth 6D object poses, whose collection is fairly cumbersome and expensive, even unavailable in many cases. In this work, to get rid of the burden of 6D annotations, we formulate the 6D pose refinement as a Markov Decision Process and impose on the reinforcement learning approach with only 2D image annotations as weakly-supervised 6D pose information, via a delicate reward definition and a composite reinforced optimization method for efficient and effective policy training. Experiments on LINEMOD and T-LESS datasets demonstrate that our Pose-Free approach is able to achieve state-of-the-art performance compared with the methods without using real-world ground-truth 6D pose labels.",0
"Computational vision faces a daunting challenge in estimating the 6D pose from a single RGB image. Most deep model methods rely on 2D images labeled with real-world ground-truth 6D object poses, which can be difficult and costly to obtain, or even unavailable. This study proposes a Markov Decision Process to refine the 6D pose without the need for 6D annotations. Instead, a reinforcement learning approach uses 2D image annotations as a weakly supervised source of 6D pose information. A well-defined reward system and optimized policy training method ensure efficiency and effectiveness. Experiments on LINEMOD and T-LESS datasets show that this Pose-Free approach surpasses methods that require real-world ground-truth 6D pose labels in achieving state-of-the-art performance.",1
"Thermal power generation plays a dominant role in the world's electricity supply. It consumes large amounts of coal worldwide, and causes serious air pollution. Optimizing the combustion efficiency of a thermal power generating unit (TPGU) is a highly challenging and critical task in the energy industry. We develop a new data-driven AI system, namely DeepThermal, to optimize the combustion control strategy for TPGUs. At its core, is a new model-based offline reinforcement learning (RL) framework, called MORE, which leverages logged historical operational data of a TPGU to solve a highly complex constrained Markov decision process problem via purely offline training. MORE aims at simultaneously improving the long-term reward (increase combustion efficiency and reduce pollutant emission) and controlling operational risks (safety constraints satisfaction). In DeepThermal, we first learn a data-driven combustion process simulator from the offline dataset. The RL agent of MORE is then trained by combining real historical data as well as carefully filtered and processed simulation data through a novel restrictive exploration scheme. DeepThermal has been successfully deployed in four large coal-fired thermal power plants in China. Real-world experiments show that DeepThermal effectively improves the combustion efficiency of a TPGU. We also report and demonstrate the superior performance of MORE by comparing with the state-of-the-art algorithms on the standard offline RL benchmarks. To the best knowledge of the authors, DeepThermal is the first AI application that has been used to solve real-world complex mission-critical control tasks using the offline RL approach.",0
"The use of thermal power generation is widespread in providing electricity globally, but it also has negative consequences, such as contributing to air pollution and consuming large amounts of coal. Optimizing combustion efficiency in thermal power generating units (TPGUs) is a challenging and crucial task in the energy industry. To address this issue, we have developed DeepThermal, a new AI system that uses data-driven methods to optimize the combustion control strategy for TPGUs. The system is based on a model-based offline reinforcement learning (RL) framework called MORE, which uses historical operational data to solve a complex constrained Markov decision process problem. MORE aims to improve long-term rewards while controlling operational risks, such as safety constraints. DeepThermal includes a data-driven combustion process simulator and an RL agent trained using real historical data and filtered simulation data. We have successfully deployed DeepThermal in four large coal-fired thermal power plants in China, where it has improved combustion efficiency. We have also compared MORE's performance to state-of-the-art algorithms on standard offline RL benchmarks and found it to be superior. DeepThermal is the first AI application to use offline RL to solve complex mission-critical control tasks in the real world.",1
"For the traveling salesman problem (TSP), the existing supervised learning based algorithms suffer seriously from the lack of generalization ability. To overcome this drawback, this paper tries to train (in supervised manner) a small-scale model, which could be repetitively used to build heat maps for TSP instances of arbitrarily large size, based on a series of techniques such as graph sampling, graph converting and heat maps merging. Furthermore, the heat maps are fed into a reinforcement learning approach (Monte Carlo tree search), to guide the search of high-quality solutions. Experimental results based on a large number of instances (with up to 10,000 vertices) show that, this new approach clearly outperforms the existing machine learning based TSP algorithms, and significantly improves the generalization ability of the trained model.",0
"The current supervised learning algorithms for the traveling salesman problem (TSP) have a significant limitation in their ability to generalize. To address this issue, this study proposes using a supervised learning approach to train a small-scale model that can generate heat maps for TSP instances of varying sizes by employing techniques such as graph sampling, graph conversion, and heat map merging. In addition, the heat maps are used in a reinforcement learning approach (Monte Carlo tree search) to guide the search for high-quality solutions. Based on a large number of instances, including those with up to 10,000 vertices, the experimental results demonstrate that this new approach outperforms existing TSP algorithms based on machine learning and significantly enhances the generalization ability of the trained model.",1
"Constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multipliers evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, while primal-dual methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from the optimal policy.",0
"The concept of constrained reinforcement learning involves the accumulation of multiple rewards towards specific thresholds. Within this category, there are instances where the desired optimal policy cannot be achieved through a linear combination of rewards. As a result, neither regularized nor classical primal-dual methods can produce optimal policies for these constrained reinforcement learning problems. To address this problem, this study proposes augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the driving force behind the evolution of the multipliers. This approach offers a systematic procedure for state augmentation that ensures the successful resolution of reinforcement learning problems with constraints. Although primal-dual methods may not always discover optimal policies, executing the augmented policy while running the dual dynamics produces an algorithm that samples actions from the optimal policy with certainty.",1
"Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.",0
"The ability of humans to link different stimuli and use them to solve problems in new situations can be emulated by our novel neural network model. Our model uses Fast Weight Memory (FWM), which is an associative memory, to learn state representations of facts that can be combined to make these associations. By incorporating FWM into the LSTM model and using differentiable operations throughout the input sequence, we are able to maintain and update compositional associations. This end-to-end training by gradient descent results in excellent performance in compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.",1
"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",0
"It is challenging to design reward functions for reinforcement learning because not only do they need to specify the desired behavior for a task, but they also need to discourage undesired outcomes to avoid unintended negative consequences and unsafe behavior. One solution proposed is to add an impact regularizer to the reward function, which discourages behavior that has a significant effect on the environment. While this approach has shown promising results in reducing certain types of side effects, there are still significant challenges to overcome. This paper examines the current challenges of impact regularizers and their relation to fundamental design decisions. It discusses which challenges have been addressed and which remain unsolved, and suggests potential solutions to prevent negative side effects using impact regularizers.",1
"In Cooperative Multi-Agent Reinforcement Learning (MARL) and under the setting of Centralized Training with Decentralized Execution (CTDE), agents observe and interact with their environment locally and independently. With local observation and random sampling, the randomness in rewards and observations leads to randomness in long-term returns. Existing methods such as Value Decomposition Network (VDN) and QMIX estimate the value of long-term returns as a scalar that does not contain the information of randomness. Our proposed model QR-MIX introduces quantile regression, modeling joint state-action values as a distribution, combining QMIX with Implicit Quantile Network (IQN). However, the monotonicity in QMIX limits the expression of joint state-action value distribution and may lead to incorrect estimation results in non-monotonic cases. Therefore, we proposed a flexible loss function to approximate the monotonicity found in QMIX. Our model is not only more tolerant of the randomness of returns, but also more tolerant of the randomness of monotonic constraints. The experimental results demonstrate that QR-MIX outperforms the previous state-of-the-art method QMIX in the StarCraft Multi-Agent Challenge (SMAC) environment.",0
"The Cooperative Multi-Agent Reinforcement Learning (MARL) framework uses Centralized Training with Decentralized Execution (CTDE) to allow agents to interact with their environment independently and locally. However, the randomness in rewards and observations can lead to randomness in long-term returns. Existing methods, such as Value Decomposition Network (VDN) and QMIX, estimate the value of long-term returns as a scalar that does not account for this randomness. To address this, we propose QR-MIX, which combines QMIX with Implicit Quantile Network (IQN) and introduces quantile regression to model joint state-action values as a distribution. However, the monotonicity in QMIX can limit the expression of joint state-action value distribution, leading to incorrect estimation results in non-monotonic cases. To solve this, we propose a flexible loss function that approximates the monotonicity found in QMIX. Our model is more tolerant of both the randomness of returns and monotonic constraints. Experimental results show that QR-MIX outperforms QMIX in the StarCraft Multi-Agent Challenge (SMAC) environment.",1
"Deep reinforcement learning has been applied successfully to solve various real-world problems and the number of its applications in the multi-agent settings has been increasing. Multi-agent learning distinctly poses significant challenges in the effort to allocate a concealed communication medium. Agents receive thorough knowledge from the medium to determine subsequent actions in a distributed nature. Apparently, the goal is to leverage the cooperation of multiple agents to achieve a designated objective efficiently. Recent studies typically combine a specialized neural network with reinforcement learning to enable communication between agents. This approach, however, limits the number of agents or necessitates the homogeneity of the system. In this paper, we have proposed a more scalable approach that not only deals with a great number of agents but also enables collaboration between dissimilar functional agents and compatibly combined with any deep reinforcement learning methods. Specifically, we create a global communication map to represent the status of each agent in the system visually. The visual map and the environmental state are fed to a shared-parameter network to train multiple agents concurrently. Finally, we select the Asynchronous Advantage Actor-Critic (A3C) algorithm to demonstrate our proposed scheme, namely Visual communication map for Multi-agent A3C (VMA3C). Simulation results show that the use of visual communication map improves the performance of A3C regarding learning speed, reward achievement, and robustness in multi-agent problems.",0
"The application of deep reinforcement learning has proved successful in solving various real-world problems, with an increasing number of applications in multi-agent settings. However, the allocation of a concealed communication medium poses significant challenges in multi-agent learning. In this setting, agents require thorough knowledge from the medium to determine subsequent actions in a distributed manner, with the aim of efficiently achieving a designated objective through the cooperation of multiple agents. Recent studies have combined reinforcement learning with a specialized neural network to enable communication between agents, but this approach has limitations in terms of agent number and system homogeneity. In this paper, we propose a scalable approach that enables collaboration between dissimilar functional agents, which can be combined with any deep reinforcement learning methods. Our approach involves creating a global communication map that visually represents the status of each agent in the system. This visual map, along with the environmental state, is fed to a shared-parameter network to train multiple agents concurrently. We demonstrate our proposed scheme, called Visual communication map for Multi-agent A3C (VMA3C), using the Asynchronous Advantage Actor-Critic (A3C) algorithm. Simulation results show that our approach improves the performance of A3C in terms of learning speed, reward achievement, and robustness in multi-agent problems.",1
"The integration of deep learning to reinforcement learning (RL) has enabled RL to perform efficiently in high-dimensional environments. Deep RL methods have been applied to solve many complex real-world problems in recent years. However, development of a deep RL-based system is challenging because of various issues such as the selection of a suitable deep RL algorithm, its network configuration, training time, training methods, and so on. This paper proposes a comprehensive software framework that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. We have designed and developed a deep RL-based software framework that strictly ensures flexibility, robustness, and scalability. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. To enforce generalization, the proposed architecture does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents. Using our framework, software developers can develop and integrate new RL algorithms or new types of agents, and can flexibly change network configuration or the number of agents.",0
"The combination of deep learning and reinforcement learning (RL) has greatly improved the performance of RL in complex environments. Despite its success, creating a deep RL-based system can be challenging due to issues such as selecting the appropriate algorithm, network configuration, training methods, and more. To address these challenges, this paper presents a comprehensive software framework that facilitates the design of a deep RL architecture and streamlines the development of realistic RL applications in a short time period. The proposed framework is flexible, robust, and scalable, providing software managers with the necessary tools to overcome any obstacles that may arise during the design process. Moreover, the framework is not dependent on any specific RL algorithm, network configuration, or agent type, enabling developers to easily integrate new algorithms or agents and adjust network configuration or the number of agents as needed. This approach ensures generalization and flexibility, which is particularly useful in agile development environments.",1
"This paper presents the first non-asymptotic result showing that a model-free algorithm can achieve a logarithmic cumulative regret for episodic tabular reinforcement learning if there exists a strictly positive sub-optimality gap in the optimal $Q$-function. We prove that the optimistic $Q$-learning studied in [Jin et al. 2018] enjoys a ${\mathcal{O}}\left(\frac{SA\cdot \mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$ cumulative regret bound, where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, $T$ is the total number of steps, and $\Delta_{\min}$ is the minimum sub-optimality gap. This bound matches the information theoretical lower bound in terms of $S,A,T$ up to a $\log\left(SA\right)$ factor. We further extend our analysis to the discounted setting and obtain a similar logarithmic cumulative regret bound.",0
"The article introduces a novel finding that demonstrates the effectiveness of a model-free algorithm in achieving logarithmic cumulative regret for episodic tabular reinforcement learning, as long as there is a strictly positive sub-optimality gap in the optimal $Q$-function. By analyzing optimistic $Q$-learning, previously studied in [Jin et al. 2018], we prove that it has a cumulative regret bound of ${\mathcal{O}}\left(\frac{SA\cdot \mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$, where $S$ denotes the number of states, $A$ denotes the number of actions, $H$ represents the planning horizon, $T$ indicates the total number of steps, and $\Delta_{\min}$ is the minimum sub-optimality gap. This bound is consistent with the information theoretical lower limit in terms of $S,A,T$, with the exception of a $\log\left(SA\right)$ factor. Additionally, we expand our analysis to the discounted setting, obtaining a similar logarithmic cumulative regret bound.",1
"A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating -- as form of meta-learning -- a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned ""student"" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective ""constructively adversarial"" objective, the teacher learns to propose increasingly challenging -- yet achievable -- goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.",0
"Reinforcement learning (RL) faces a significant obstacle in learning in environments that lack external rewards. In contrast, humans can acquire new skills without any rewards by employing various forms of intrinsic motivation. To address this issue, we propose a new agent called AMIGo that uses meta-learning to incorporate a goal-generating teacher that suggests Adversarially Motivated Intrinsic Goals. These goals are used to train a ""student"" policy that is conditioned on achieving them, even when no external rewards are available. By employing a ""constructively adversarial"" objective, the teacher is able to propose progressively more challenging yet attainable goals that allow the student to learn general skills that can be applied to novel environments. Our approach generates a natural curriculum of self-generated goals that enables the agent to successfully tackle complex procedurally-generated tasks where other intrinsic motivation methods and state-of-the-art RL techniques have failed.",1
"Pole-like landmark has received increasing attention as a domain-invariant visual cue for visual robot self-localization across domains (e.g., seasons, times of day, weathers). However, self-localization using pole-like landmarks can be ill-posed for a passive observer, as many viewpoints may not provide any pole-like landmark view. To alleviate this problem, we consider an active observer and explore a novel ""domain-invariant"" next-best-view (NBV) planner that attains consistent performance over different domains (i.e., maintenance-free), without requiring the expensive task of training data collection and retraining. In our approach, a novel multi-encoder deep convolutional neural network enables to detect domain invariant pole-like landmarks, which are then used as the sole input to a model-free deep reinforcement learning -based domain-invariant NBV planner. Further, we develop a practical system for active self-localization using sparse invariant landmarks and dense discriminative landmarks. In experiments, we demonstrate that the proposed method is effective both in efficient landmark detection and in discriminative self-localization.",0
"The use of pole-like landmarks has become increasingly popular as a means of achieving visual robot self-localization in various domains, such as different seasons, times of day, and weather conditions. However, relying solely on these landmarks for self-localization can be problematic for a passive observer, as not all viewpoints will offer a clear view of such landmarks. To address this issue, we propose an active observer approach and introduce a new ""domain-invariant"" next-best-view (NBV) planner that is consistent across different domains and does not require expensive training data collection or retraining. Our method employs a multi-encoder deep convolutional neural network to detect domain invariant pole-like landmarks, which are then used as the only input to a model-free deep reinforcement learning-based domain-invariant NBV planner. Additionally, we develop a practical system that utilizes both sparse invariant landmarks and dense discriminative landmarks for active self-localization. Our experiments demonstrate that this method is effective in both detecting landmarks efficiently and achieving accurate self-localization.",1
"Algorithms that tackle deep exploration -- an important challenge in reinforcement learning -- have relied on epistemic uncertainty representation through ensembles or other hypermodels, exploration bonuses, or visitation count distributions. An open question is whether deep exploration can be achieved by an incremental reinforcement learning algorithm that tracks a single point estimate, without additional complexity required to account for epistemic uncertainty. We answer this question in the affirmative. In particular, we develop Langevin DQN, a variation of DQN that differs only in perturbing parameter updates with Gaussian noise and demonstrate through a computational study that the presented algorithm achieves deep exploration. We also offer some intuition to how Langevin DQN achieves deep exploration. In addition, we present a modification of the Langevin DQN algorithm to improve the computational efficiency.",0
"Reinforcement learning algorithms have faced the challenge of deep exploration, which has been addressed through various methods such as ensembles or hypermodels, exploration bonuses, or visitation count distributions to represent epistemic uncertainty. The question remains whether a single point estimate can achieve deep exploration without additional complexity to account for epistemic uncertainty. We confirm that this is possible with our development of Langevin DQN, a variation of DQN that perturbs parameter updates with Gaussian noise. Our computational study proves that this algorithm achieves deep exploration, and we provide insight into how it achieves this. Additionally, we offer a modification of Langevin DQN to enhance computational efficiency.",1
"Reinforcement learning (RL) has great potential in sequential decision-making. At present, the mainstream RL algorithms are data-driven, relying on millions of iterations and a large number of empirical data to learn a policy. Although data-driven RL may have excellent asymptotic performance, it usually yields slow convergence speed. As a comparison, model-driven RL employs a differentiable transition model to improve convergence speed, in which the policy gradient (PG) is calculated by using the backpropagation through time (BPTT) technique. However, such methods suffer from numerical instability, model error sensitivity and low computing efficiency, which may lead to poor policies. In this paper, a mixed policy gradient (MPG) method is proposed, which uses both empirical data and the transition model to construct the PG, so as to accelerate the convergence speed without losing the optimality guarantee. MPG contains two types of PG: 1) data-driven PG, which is obtained by directly calculating the derivative of the learned Q-value function with respect to actions, and 2) model-driven PG, which is calculated using BPTT based on the model-predictive return. We unify them by revealing the correlation between the upper bound of the unified PG error and the predictive horizon, where the data-driven PG is regraded as 0-step model-predictive return. Relying on that, MPG employs a rule-based method to adaptively adjust the weights of data-driven and model-driven PGs. In particular, to get a more accurate PG, the weight of the data-driven PG is designed to grow along the learning process while the other to decrease. Besides, an asynchronous learning framework is proposed to reduce the wall-clock time needed for each update iteration. Simulation results show that the MPG method achieves the best asymptotic performance and convergence speed compared with other baseline algorithms.",0
"Sequential decision-making has great potential for Reinforcement Learning (RL). The existing mainstream RL algorithms rely on vast amounts of empirical data and millions of iterations to learn a policy. Although data-driven RL has excellent asymptotic performance, it has slow convergence. Model-driven RL, on the other hand, uses a differentiable transition model to enhance convergence speed. However, model-driven methods are prone to numerical instability, model error sensitivity, and low computing efficiency, which may lead to poor policies. To address these issues, a mixed policy gradient (MPG) method is proposed in this paper. The MPG method employs both empirical data and the transition model to construct the PG, which accelerates convergence speed without losing optimality guarantee. The MPG method includes two types of PG: data-driven PG and model-driven PG. The data-driven PG is obtained by directly calculating the derivative of the learned Q-value function with respect to actions, while the model-driven PG is calculated using BPTT based on the model-predictive return. The MPG method adaptsively adjusts the weights of data-driven and model-driven PGs using a rule-based method to obtain a more accurate PG. In particular, the weight of the data-driven PG grows along the learning process while the weight of the model-driven PG decreases. Additionally, an asynchronous learning framework is proposed to reduce the time required for each update iteration. Simulation results demonstrate that the MPG method has the best asymptotic performance and convergence speed compared to other baseline algorithms.",1
"Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg .",0
"The performance of Deep Reinforcement Learning (Deep RL) has gained attention due to its success in controlling various tasks. However, conventional regularization techniques for training neural networks, such as $L_2$ regularization and dropout, are often disregarded in RL methods. This may be due to the fact that agents are typically trained and evaluated in the same environment, and the deep RL community focuses more on high-level algorithm designs. This study presents the initial comprehensive examination of regularization techniques with several policy optimization algorithms on continuous control tasks. Surprisingly, conventional regularization techniques on policy networks can significantly improve performance, particularly on more challenging tasks, and remain stable when training hyperparameters vary. Additionally, we compare these techniques with entropy regularization and explore the impact of regularizing different components. Our findings suggest that only regularizing the policy network is typically optimal. We also analyze why regularization may promote generalization in RL from four perspectives: sample complexity, reward distribution, weight norm, and noise robustness. We aim to offer guidance for future practices in regularizing policy optimization algorithms and provide our code at https://github.com/xuanlinli17/iclr2021_rlreg.",1
"Reinforcement learning (RL) has shown great success in estimating sequential treatment strategies which take into account patient heterogeneity. However, health-outcome information, which is used as the reward for reinforcement learning methods, is often not well coded but rather embedded in clinical notes. Extracting precise outcome information is a resource intensive task, so most of the available well-annotated cohorts are small. To address this issue, we propose a semi-supervised learning (SSL) approach that efficiently leverages a small sized labeled data with true outcome observed, and a large unlabeled data with outcome surrogates. In particular, we propose a semi-supervised, efficient approach to Q-learning and doubly robust off policy value estimation. Generalizing SSL to sequential treatment regimes brings interesting challenges: 1) Feature distribution for Q-learning is unknown as it includes previous outcomes. 2) The surrogate variables we leverage in the modified SSL framework are predictive of the outcome but not informative to the optimal policy or value function. We provide theoretical results for our Q-function and value function estimators to understand to what degree efficiency can be gained from SSL. Our method is at least as efficient as the supervised approach, and moreover safe as it robust to mis-specification of the imputation models.",0
"Sequential treatment strategies that consider patient heterogeneity have been effectively estimated using reinforcement learning (RL). However, the health-outcome information required for RL methods to function is often not well-coded and is instead embedded in clinical notes, making it difficult to extract precise outcome information. Consequently, most well-annotated cohorts are small. In response to this issue, we suggest a semi-supervised learning (SSL) approach that makes use of a small labeled dataset with true outcome observations and a large unlabeled dataset with outcome surrogates. Our approach is efficient and specifically tailored to Q-learning and doubly robust off policy value estimation. Generalizing SSL to sequential treatment regimes is challenging, as we need to consider unknown feature distribution for Q-learning and leverage surrogate variables that are predictive but not informative for the optimal policy or value function. We provide theoretical results to evaluate the efficiency of our Q-function and value function estimators and demonstrate that our method is at least as efficient as the supervised approach and robust to mis-specification of the imputation models.",1
"Modern tasks in reinforcement learning have large state and action spaces. To deal with them efficiently, one often uses predefined feature mapping to represent states and actions in a low-dimensional space. In this paper, we study reinforcement learning for discounted Markov Decision Processes (MDPs), where the transition kernel can be parameterized as a linear function of certain feature mapping. We propose a novel algorithm that makes use of the feature mapping and obtains a $\tilde O(d\sqrt{T}/(1-\gamma)^2)$ regret, where $d$ is the dimension of the feature space, $T$ is the time horizon and $\gamma$ is the discount factor of the MDP. To the best of our knowledge, this is the first polynomial regret bound without accessing the generative model or making strong assumptions such as ergodicity of the MDP. By constructing a special class of MDPs, we also show that for any algorithms, the regret is lower bounded by $\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$. Our upper and lower bound results together suggest that the proposed reinforcement learning algorithm is near-optimal up to a $(1-\gamma)^{-0.5}$ factor.",0
"Reinforcement learning tasks in the modern era involve vast state and action spaces. To efficiently handle these, a common approach is to utilize a pre-defined feature mapping technique to represent states and actions in a low-dimensional space. This study focuses on reinforcement learning for discounted Markov Decision Processes (MDPs), where the transition kernel can be parameterized as a linear function of a specific feature mapping. An innovative algorithm is proposed that utilizes this feature mapping and achieves a regret of approximately $\tilde O(d\sqrt{T}/(1-\gamma)^2)$, where $d$ represents the feature space dimension, $T$ is the time horizon, and $\gamma$ is the MDP discount factor. This is the first polynomial regret bound achieved without the need for the generative model or strong assumptions such as MDP ergodicity. Furthermore, by constructing a unique class of MDPs, it is demonstrated that the regret for any algorithms is lower bounded by $\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$. These upper and lower bound results suggest that the proposed reinforcement learning algorithm is nearly optimal up to a $(1-\gamma)^{-0.5}$ factor.",1
"In this paper, we learn dynamics models for parametrized families of dynamical systems with varying properties. The dynamics models are formulated as stochastic processes conditioned on a latent context variable which is inferred from observed transitions of the respective system. The probabilistic formulation allows us to compute an action sequence which, for a limited number of environment interactions, optimally explores the given system within the parametrized family. This is achieved by steering the system through transitions being most informative for the context variable. We demonstrate the effectiveness of our method for exploration on a non-linear toy-problem and two well-known reinforcement learning environments.",0
"This article explores the dynamics models of parametrized families of dynamical systems with different features. The stochastic processes of these models are reliant on latent context variables, inferred from observed transitions in the corresponding system. This probabilistic approach allows for optimal exploration of the system within the given parametric family through the calculation of an action sequence. The system is steered towards transitions that provide the most information for the context variable, all within a limited number of environmental interactions. We validate our exploration method on a non-linear toy problem and two popular reinforcement learning environments.",1
"Humans learn compositional and causal abstraction, \ie, knowledge, in response to the structure of naturalistic tasks. When presented with a problem-solving task involving some objects, toddlers would first interact with these objects to reckon what they are and what can be done with them. Leveraging these concepts, they could understand the internal structure of this task, without seeing all of the problem instances. Remarkably, they further build cognitively executable strategies to \emph{rapidly} solve novel problems. To empower a learning agent with similar capability, we argue there shall be three levels of generalization in how an agent represents its knowledge: perceptual, conceptual, and algorithmic. In this paper, we devise the very first systematic benchmark that offers joint evaluation covering all three levels. This benchmark is centered around a novel task domain, HALMA, for visual concept development and rapid problem-solving. Uniquely, HALMA has a minimum yet complete concept space, upon which we introduce a novel paradigm to rigorously diagnose and dissect learning agents' capability in understanding and generalizing complex and structural concepts. We conduct extensive experiments on reinforcement learning agents with various inductive biases and carefully report their proficiency and weakness.",0
"The acquisition of knowledge, including compositional and causal abstraction, is a result of humans engaging with naturalistic tasks. Toddlers, when presented with a problem-solving task involving objects, first interact with them to gain an understanding of what they are and what can be done with them. This understanding enables them to comprehend the internal structure of the task, without having to see all the problem instances. They then develop cognitive strategies to solve novel problems quickly. We propose that to imbue a learning agent with similar capabilities, knowledge representation should occur at three levels: perceptual, conceptual, and algorithmic. Our paper introduces a systematic benchmark centered on a novel task domain, HALMA, for visual concept development and rapid problem-solving. HALMA has a minimal yet comprehensive concept space, and we introduce a novel paradigm to diagnose and analyze learning agents' ability to comprehend and generalize complex and structural concepts. We run comprehensive experiments on reinforcement learning agents with various inductive biases and report their strengths and weaknesses in detail.",1
"Offline reinforcement learning approaches can generally be divided to proximal and uncertainty-aware methods. In this work, we demonstrate the benefit of combining the two in a latent variational model. We impose a latent representation of states and actions and leverage its intrinsic Riemannian geometry to measure distance of latent samples to the data. Our proposed metrics measure both the quality of out of distribution samples as well as the discrepancy of examples in the data. We integrate our metrics in a model-based offline optimization framework, in which proximity and uncertainty can be carefully controlled. We illustrate the geodesics on a simple grid-like environment, depicting its natural inherent topology. Finally, we analyze our approach and improve upon contemporary offline RL benchmarks.",0
"The classification of offline reinforcement learning methods can generally be split into two categories: proximal and uncertainty-aware techniques. Our research showcases the advantages of combining these two methods into a latent variational model. We utilize a latent representation of states and actions and exploit its intrinsic Riemannian geometry to measure the distance between latent samples and data. Our metrics evaluate the quality of samples that are out of the distribution as well as the inconsistency of examples in the data. We merge our metrics into a model-based offline optimization framework, which enables us to carefully manage proximity and uncertainty. We demonstrate the geodesics on a straightforward grid-like environment, revealing its inherent natural topology. Lastly, we examine our approach and enhance contemporary offline RL benchmarks.",1
"Deep Reinforcement Learning (RL) methods rely on experience replay to approximate the minibatched supervised learning setting; however, unlike supervised learning where access to lots of training data is crucial to generalization, replay-based deep RL appears to struggle in the presence of extraneous data. Recent works have shown that the performance of Deep Q-Network (DQN) degrades when its replay memory becomes too large.   This suggests that outdated experiences somehow impact the performance of deep RL, which should not be the case for off-policy methods like DQN. Consequently, we re-examine the motivation for sampling uniformly over a replay memory, and find that it may be flawed when using function approximation. We show that -- despite conventional wisdom -- sampling from the uniform distribution does not yield uncorrelated training samples and therefore biases gradients during training. Our theory prescribes a special non-uniform distribution to cancel this effect, and we propose a stratified sampling scheme to efficiently implement it.",0
"Experience replay is a key component of Deep Reinforcement Learning (RL) methods, which approximates minibatched supervised learning. However, unlike supervised learning, which relies heavily on large amounts of training data for generalization, replay-based deep RL struggles in the presence of extraneous data. Recent research has shown that Deep Q-Network (DQN) performance deteriorates when its replay memory becomes too large, indicating that outdated experiences affect deep RL performance, which should not be the case for off-policy methods like DQN. As a result, we question the motivation for uniformly sampling from a replay memory, as it may be flawed when using function approximation. Our analysis reveals that sampling from a uniform distribution does not yield uncorrelated training samples, contrary to popular belief, and biases gradients during training. Our theory calls for a unique non-uniform distribution to counteract this effect, and we propose a stratified sampling approach to implement it effectively.",1
"Reinforcement learning (RL) is one of the most active fields of AI research. Despite the interest demonstrated by the research community in reinforcement learning, the development methodology still lags behind, with a severe lack of standard APIs to foster the development of RL applications. OpenAI Gym is probably the most used environment to develop RL applications and simulations, but most of the abstractions proposed in such a framework are still assuming a semi-structured methodology. This is particularly relevant for agent-based models whose purpose is to analyse adaptive behaviour displayed by self-learning agents in the simulation. In order to bridge this gap, we present a workflow and tools for the decoupled development and maintenance of multi-purpose agent-based models and derived single-purpose reinforcement learning environments, enabling the researcher to swap out environments with ones representing different perspectives or different reward models, all while keeping the underlying domain model intact and separate. The Sim-Env Python library generates OpenAI-Gym-compatible reinforcement learning environments that use existing or purposely created domain models as their simulation back-ends. Its design emphasizes ease-of-use, modularity and code separation.",0
"Reinforcement learning (RL) is a highly active area of AI research, but the development methodology has not kept pace with interest from the research community. The lack of standard APIs for RL application development is a major challenge. While OpenAI Gym is a popular environment for RL applications and simulations, its abstractions assume a semi-structured methodology. This is especially problematic for agent-based models that analyze adaptive behavior in self-learning agents. To address this gap, we propose a workflow and tools for decoupled development and maintenance of multi-purpose agent-based models and single-purpose RL environments. Our approach allows researchers to swap out RL environments with different perspectives or reward models while keeping the underlying domain model separate. Our Sim-Env Python library generates OpenAI-Gym-compatible RL environments using existing or custom domain models as simulation back-ends. We prioritize ease-of-use, modularity, and code separation in our design.",1
"Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. In low data regime, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks.",0
"In recent times, there have been suggestions for auxiliary tasks that can quicken representation learning and enhance sample efficiency in deep reinforcement learning. However, these tasks don't consider the features of RL issues and are not supervised. We recommend a new auxiliary task that takes advantage of the returns, which are the most important feedback signals in RL, to compel the learned representations to differentiate between state-action pairs with distinct returns. We have a theoretical basis for our auxiliary loss, which helps to learn representations that capture the structure of a new state-action abstraction form. This form aggregates state-action pairs with similar return distributions. In the low data regime, our algorithm performs better than strong baselines on intricate tasks in Atari games and DeepMind Control suite. Additionally, the algorithm achieves even better results when combined with existing auxiliary tasks.",1
"Reinforcement learning agents have been mostly developed and evaluated under the assumption that they will operate in a fully autonomous manner -- they will take all actions. In this work, our goal is to develop algorithms that, by learning to switch control between machine and human agents, allow existing reinforcement learning agents to operate under different automation levels. To this end, we first formally define the problem of learning to switch control among agents in a team via a 2-layer Markov decision process. Then, we develop an online learning algorithm that uses upper confidence bounds on the agents' policies and the environment's transition probabilities to find a sequence of switching policies. We prove that the total regret of our algorithm with respect to the optimal switching policy is sublinear in the number of learning steps. Moreover, we also show that our algorithm can be used to find multiple sequences of switching policies across several independent teams of agents operating in similar environments, where it greatly benefits from maintaining shared confidence bounds for the environments' transition probabilities. Simulation experiments in obstacle avoidance in a semi-autonomous driving scenario illustrate our theoretical findings and demonstrate that, by exploiting the specific structure of the problem, our proposed algorithm is superior to problem-agnostic algorithms.",0
"The majority of reinforcement learning agents have been created and assessed with the assumption that they will function entirely independently, without any human intervention. However, our aim is to create algorithms that enable existing reinforcement learning agents to operate under varying levels of automation by learning to alternate control between machine and human agents. To achieve this, we have established the issue of learning to switch control among agents in a team via a 2-layer Markov decision process. We have then devised an online learning algorithm that employs upper confidence bounds on the agents' policies and the environment's transition probabilities to identify a sequence of switching policies. Our algorithm has been proven to have sublinear total regret with respect to the optimal switching policy in terms of the number of learning steps. Additionally, we have shown that our algorithm can be utilised to discover multiple sequences of switching policies across numerous independent teams of agents operating in similar environments, and that it benefits significantly from maintaining shared confidence bounds for the environments' transition probabilities. Our theoretical findings have been validated by simulation experiments on obstacle avoidance in a semi-autonomous driving scenario, which demonstrate that our proposed algorithm is superior to problem-agnostic algorithms by taking advantage of the specific structure of the problem.",1
"Agents trained with deep reinforcement learning algorithms are capable of performing highly complex tasks including locomotion in continuous environments. We investigate transferring the learning acquired in one task to a set of previously unseen tasks. Generalization and overfitting in deep reinforcement learning are not commonly addressed in current transfer learning research. Conducting a comparative analysis without an intermediate regularization step results in underperforming benchmarks and inaccurate algorithm comparisons due to rudimentary assessments. In this study, we propose regularization techniques in deep reinforcement learning for continuous control through the application of sample elimination, early stopping and maximum entropy regularized adversarial learning. First, the importance of the inclusion of training iteration number to the hyperparameters in deep transfer reinforcement learning will be discussed. Because source task performance is not indicative of the generalization capacity of the algorithm, we start by acknowledging the training iteration number as a hyperparameter. In line with this, we introduce an additional step of resorting to earlier snapshots of policy parameters to prevent overfitting to the source task. Then, to generate robust policies, we discard the samples that lead to overfitting via a method we call strict clipping. Furthermore, we increase the generalization capacity in widely used transfer learning benchmarks by using maximum entropy regularization, different critic methods, and curriculum learning in an adversarial setup. Subsequently, we propose maximum entropy adversarial reinforcement learning to increase the domain randomization. Finally, we evaluate the robustness of these methods on simulated robots in target environments where the morphology of the robot, gravity, and tangential friction coefficient of the environment are altered.",0
"Our research focuses on the use of deep reinforcement learning algorithms to train agents to perform complex tasks such as continuous locomotion. We aim to explore the transferability of knowledge gained from one task to a new set of tasks. Currently, the issues of generalization and overfitting in deep reinforcement learning are not adequately addressed in transfer learning research, leading to inaccurate algorithm comparisons. Therefore, we propose regularization techniques such as sample elimination, early stopping, and maximum entropy regularized adversarial learning to improve the performance of deep transfer reinforcement learning for continuous control. To achieve this, we acknowledge the importance of the training iteration number as a hyperparameter and resort to earlier snapshots of policy parameters to avoid overfitting. Additionally, we use strict clipping to eliminate samples that lead to overfitting and increase generalization capacity by incorporating maximum entropy regularization, different critic methods, and curriculum learning in an adversarial setup. Finally, we evaluate the effectiveness of these methods on simulated robots in target environments with altered morphology, gravity, and tangential friction coefficient.",1
"Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many real-world applications. A popular solution to the problem is to infer task identity as augmented state using a context-based encoder, for which efficient learning of task representations remains an open challenge. In this work, we improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives for more effective task inference and learning of control. Theoretical analysis and experiments are presented to demonstrate the superior performance, efficiency and robustness of our end-to-end and model free method compared to prior algorithms across multiple meta-RL benchmarks.",0
"Offline reinforcement learning with meta-learning (OMRL) is a promising area that has not been extensively researched but could have a significant impact on real-world applications by enabling RL algorithms. To address this problem, one popular approach is to use a context-based encoder to identify the task, and although this method has potential, it still lacks efficient task representation learning. In this study, we enhance one of the best OMRL algorithms, FOCAL, by integrating an intra-task attention mechanism and inter-task contrastive learning objectives to enhance task inference and control learning. Our end-to-end and model-free method outperforms previous algorithms in multiple meta-RL benchmarks, as demonstrated by theoretical analysis and experiments. Our approach is superior in terms of effectiveness, efficiency, and robustness.",1
"Neural networks have now long been used for solving complex problems of image domain, yet designing the same needs manual expertise. Furthermore, techniques for automatically generating a suitable deep learning architecture for a given dataset have frequently made use of reinforcement learning and evolutionary methods which take extensive computational resources and time. We propose a new framework for neural architecture search based on a hill-climbing procedure using morphism operators that makes use of a novel gradient update scheme. The update is based on the aging of neural network layers and results in the reduction in the overall training time. This technique can search in a broader search space which subsequently yields competitive results. We achieve a 4.96% error rate on the CIFAR-10 dataset in 19.4 hours of a single GPU training.",0
"Although neural networks have been utilized for complex image problems for some time, their design still requires manual expertise. Additionally, techniques for automatically generating deep learning architectures for specific datasets often rely on reinforcement learning and evolutionary methods that demand significant computational resources and time. To address this, we propose a novel framework for neural architecture search that employs a hill-climbing procedure utilizing morphism operators and a new gradient update scheme. This update scheme ages neural network layers, resulting in reduced training time and a broader search space that yields competitive outcomes. Our method achieves a 4.96% error rate on the CIFAR-10 dataset with a single GPU training time of 19.4 hours.",1
"We consider the problem where $M$ agents interact with $M$ identical and independent environments with $S$ states and $A$ actions using reinforcement learning for $T$ rounds. The agents share their data with a central server to minimize their regret. We aim to find an algorithm that allows the agents to minimize the regret with infrequent communication rounds. We provide \NAM\ which runs at each agent and prove that the total cumulative regret of $M$ agents is upper bounded as $\Tilde{O}(DS\sqrt{MAT})$ for a Markov Decision Process with diameter $D$, number of states $S$, and number of actions $A$. The agents synchronize after their visitations to any state-action pair exceeds a certain threshold. Using this, we obtain a bound of $O\left(MSA\log(MT)\right)$ on the total number of communications rounds. Finally, we evaluate the algorithm against multiple environments and demonstrate that the proposed algorithm performs at par with an always communication version of the UCRL2 algorithm, while with significantly lower communication.",0
"The problem we address involves $M$ agents who interact with $M$ independent environments, each with $S$ states and $A$ actions, using reinforcement learning for $T$ rounds. To minimize regret, the agents share their data with a central server. Our goal is to develop an algorithm that allows the agents to minimize regret while communicating infrequently. We introduce \NAM\, which is implemented at each agent, and show that the total cumulative regret of $M$ agents is upper bounded as $\Tilde{O}(DS\sqrt{MAT})$ for a Markov Decision Process with diameter $D$, number of states $S$, and number of actions $A$. The agents synchronize after visiting a particular state-action pair a certain number of times. This approach limits the total number of communication rounds to $O\left(MSA\log(MT)\right)$. We evaluate our algorithm against multiple environments and demonstrate that it performs as well as the UCRL2 algorithm, which always communicates, but with significantly less communication.",1
"Non-stationarity is one thorny issue in multi-agent reinforcement learning, which is caused by the policy changes of agents during the learning procedure. Current works to solve this problem have their own limitations in effectiveness and scalability, such as centralized critic and decentralized actor (CCDA), population-based self-play, modeling of others and etc. In this paper, we novelly introduce a $\delta$-stationarity measurement to explicitly model the stationarity of a policy sequence, which is theoretically proved to be proportional to the joint policy divergence. However, simple policy factorization like mean-field approximation will mislead to larger policy divergence, which can be considered as trust region decomposition dilemma. We model the joint policy as a general Markov random field and propose a trust region decomposition network based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established with the purpose to satisfy $\delta$-stationarity. MAMT can adjust the trust region of the local policies adaptively in an end-to-end manner, thereby approximately constraining the divergence of joint policy to alleviate the non-stationary problem. Our method can bring noticeable and stable performance improvement compared with baselines in coordination tasks of different complexity.",0
"Non-stationarity is a challenging problem in multi-agent reinforcement learning, arising from policy changes during the learning procedure. Current approaches to address this issue, such as centralized critic and decentralized actor (CCDA), population-based self-play, and modeling of others, have limitations in terms of effectiveness and scalability. This paper presents a novel $\delta$-stationarity measurement to explicitly model the stationarity of a policy sequence, which is theoretically proven to be proportional to the joint policy divergence. However, simplistic policy factorization like mean-field approximation can lead to larger policy divergence and a trust region decomposition dilemma. To overcome this, we model the joint policy as a general Markov random field and propose a trust region decomposition network based on message passing to estimate the joint policy divergence more accurately. We establish the Multi-Agent Mirror descent policy algorithm with Trust region decomposition (MAMT), which adjusts the trust region of local policies adaptively in an end-to-end manner to satisfy $\delta$-stationarity. This approach alleviates the non-stationary problem by approximately constraining the divergence of the joint policy, resulting in noticeable and stable performance improvement compared to baselines in coordination tasks of varying complexity.",1
"Advances in computing resources have resulted in the increasing complexity of cyber-physical systems (CPS). As the complexity of CPS evolved, the focus has shifted from traditional control methods to deep reinforcement learning-based (DRL) methods for control of these systems. This is due to the difficulty of obtaining accurate models of complex CPS for traditional control. However, to securely deploy DRL in production, it is essential to examine the weaknesses of DRL-based controllers (policies) towards malicious attacks from all angles. In this work, we investigate targeted attacks in the action-space domain, also commonly known as actuation attacks in CPS literature, which perturbs the outputs of a controller. We show that a query-based black-box attack model that generates optimal perturbations with respect to an adversarial goal can be formulated as another reinforcement learning problem. Thus, such an adversarial policy can be trained using conventional DRL methods. Experimental results showed that adversarial policies that only observe the nominal policy's output generate stronger attacks than adversarial policies that observe the nominal policy's input and output. Further analysis reveals that nominal policies whose outputs are frequently at the boundaries of the action space are naturally more robust towards adversarial policies. Lastly, we propose the use of adversarial training with transfer learning to induce robust behaviors into the nominal policy, which decreases the rate of successful targeted attacks by 50%.",0
"The growing complexity of cyber-physical systems (CPS) is a result of advances in computing resources. As a result, traditional control methods have given way to deep reinforcement learning-based (DRL) methods in the control of CPS. This is because traditional control proves difficult in obtaining accurate models of complex CPS. However, to ensure the safe deployment of DRL in production, it is crucial to examine the vulnerabilities of DRL-based controllers towards malicious attacks comprehensively. In this regard, we investigate actuation attacks in CPS, which perturb the outputs of a controller. We demonstrate that an adversarial policy can be trained using conventional DRL methods by formulating a query-based black-box attack model that generates optimal perturbations with respect to an adversarial goal as another reinforcement learning problem. Our experiments reveal that adversarial policies that only observe the nominal policy's output generate stronger attacks than those that observe both input and output. Furthermore, nominal policies whose outputs frequently fall on the action space's boundaries are naturally more robust towards adversarial policies. Lastly, we suggest the use of adversarial training with transfer learning to induce robust behaviors into the nominal policy, which reduces the rate of successful targeted attacks by 50%.",1
"Most compilers for machine learning (ML) frameworks need to solve many correlated optimization problems to generate efficient machine code. Current ML compilers rely on heuristics based algorithms to solve these optimization problems one at a time. However, this approach is not only hard to maintain but often leads to sub-optimal solutions especially for newer model architectures. Existing learning based approaches in the literature are sample inefficient, tackle a single optimization problem, and do not generalize to unseen graphs making them infeasible to be deployed in practice. To address these limitations, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO), based on a scalable sequential attention mechanism over an inductive graph neural network. GO generates decisions on the entire graph rather than on each individual node autoregressively, drastically speeding up the search compared to prior methods. Moreover, we propose recurrent attention layers to jointly optimize dependent graph optimization tasks and demonstrate 33%-60% speedup on three graph optimization tasks compared to TensorFlow default optimization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21% improvement over human experts and 18% improvement over the prior state of the art with 15x faster convergence, on a device placement task evaluated in real systems.",0
"To generate efficient machine code, compilers for machine learning (ML) frameworks must solve multiple correlated optimization problems. However, current ML compilers rely on heuristics-based algorithms that solve these problems one at a time, which is difficult to maintain and often leads to suboptimal solutions for newer model architectures. Existing learning-based approaches in the literature are also not practical because they are sample inefficient, tackle only one optimization problem, and do not generalize to unseen graphs. To overcome these limitations, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO). Our approach utilizes a scalable sequential attention mechanism over an inductive graph neural network to generate decisions on the entire graph rather than on each individual node. This drastically speeds up the search compared to prior methods. Additionally, we introduce recurrent attention layers to jointly optimize dependent graph optimization tasks, resulting in a 33%-60% speedup on three graph optimization tasks compared to TensorFlow default optimization. We evaluate our method on a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet. GO achieves on average a 21% improvement over human experts and an 18% improvement over the prior state of the art, with 15x faster convergence on a device placement task evaluated in real systems.",1
"Bandit and reinforcement learning (RL) problems can often be framed as optimization problems where the goal is to maximize average performance while having access only to stochastic estimates of the true gradient. Traditionally, stochastic optimization theory predicts that learning dynamics are governed by the curvature of the loss function and the noise of the gradient estimates. In this paper we demonstrate that this is not the case for bandit and RL problems. To allow our analysis to be interpreted in light of multi-step MDPs, we focus on techniques derived from stochastic optimization principles (e.g., natural policy gradient and EXP3) and we show that some standard assumptions from optimization theory are violated in these problems. We present theoretical results showing that, at least for bandit problems, curvature and noise are not sufficient to explain the learning dynamics and that seemingly innocuous choices like the baseline can determine whether an algorithm converges. These theoretical findings match our empirical evaluation, which we extend to multi-state MDPs.",0
"Problems involving Bandit and reinforcement learning (RL) can be viewed as optimization problems that aim to maximize average performance. However, these problems only provide stochastic estimates of the true gradient. Typically, stochastic optimization theory predicts that the curvature of the loss function and the noise of gradient estimates control learning dynamics. In contrast, this paper shows that this is not the case for bandit and RL problems. We examine techniques derived from stochastic optimization principles, such as natural policy gradient and EXP3, and demonstrate that these problems violate some standard assumptions from optimization theory. Our theoretical results indicate that, at least for bandit problems, the learning dynamics are not adequately explained by curvature and noise alone. In fact, seemingly insignificant choices like the baseline can determine whether an algorithm converges. Our empirical evaluation supports our theoretical findings, which we extend to multi-state MDPs.",1
"We show how to construct variance-aware confidence sets for linear bandits and linear mixture Markov Decision Process (MDP). Our method yields the following new regret bounds:   * For linear bandits, we obtain an $\widetilde{O}(\mathrm{poly}(d)\sqrt{1 + \sum_{i=1}^{K}\sigma_i^2})$ regret bound, where $d$ is the feature dimension, $K$ is the number of rounds, and $\sigma_i^2$ is the (unknown) variance of the reward at the $i$-th round. This is the first regret bound that only scales with the variance and the dimension, with no explicit polynomial dependency on $K$.   * For linear mixture MDP, we obtain an $\widetilde{O}(\mathrm{poly}(d, \log H)\sqrt{K})$ regret bound, where $d$ is the number of base models, $K$ is the number of episodes, and $H$ is the planning horizon. This is the first regret bound that only scales logarithmically with $H$ in the reinforcement learning with linear function approximation setting, thus exponentially improving existing results.   Our methods utilize three novel ideas that may be of independent interest: 1) applications of the peeling techniques to the norm of input and the magnitude of variance, 2) a recursion-based approach to estimate the variance, and 3) a convex potential lemma that somewhat generalizes the seminal elliptical potential lemma.",0
"In this article, we present a technique for creating confidence sets that are sensitive to variance in both linear bandits and linear mixture Markov Decision Process (MDP). Our approach leads to new regret bounds, including an approximate upper bound of $\widetilde{O}(\mathrm{poly}(d)\sqrt{1 + \sum_{i=1}^{K}\sigma_i^2})$ for linear bandits. Here, $d$ refers to the feature dimension, $K$ denotes the number of rounds, and $\sigma_i^2$ refers to the unknown variance of the reward at round $i$. Notably, this bound only depends on the variance and dimension, not on $K$. For linear mixture MDP, we derive an approximate upper bound of $\widetilde{O}(\mathrm{poly}(d, \log H)\sqrt{K})$, where $d$ represents the number of base models, $K$ is the number of episodes, and $H$ refers to the planning horizon. This is the first regret bound to scale logarithmically with $H$ in the reinforcement learning with linear function approximation setting, which represents a significant improvement on previous results. We also introduce three innovative techniques: 1) the use of peeling techniques to analyze the norm of input and the magnitude of variance, 2) a recursion-based method for estimating variance, and 3) a convex potential lemma that generalizes the seminal elliptical potential lemma.",1
"Learning competitive behaviors in multi-agent settings such as racing requires long-term reasoning about potential adversarial interactions. This paper presents Deep Latent Competition (DLC), a novel reinforcement learning algorithm that learns competitive visual control policies through self-play in imagination. The DLC agent imagines multi-agent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction. Imagined self-play reduces costly sample generation in the real world, while the latent representation enables planning to scale gracefully with observation dimensionality. We demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multi-agent racing benchmark that requires planning from image observations. Code and videos available at https://sites.google.com/view/deep-latent-competition.",0
"In order to excel in competitive settings like racing, it is necessary to possess the ability to anticipate adversarial interactions and engage in long-term reasoning. The purpose of this research is to introduce a new reinforcement learning algorithm called Deep Latent Competition (DLC). This algorithm enables the learning of competitive visual control policies through self-play in a simulated environment. By imagining multi-agent interactions in a compact latent space created by a learned world model, which combines a joint transition function with opponent viewpoint prediction, the DLC agent can reduce the need for real-world sample generation. Additionally, the use of a latent representation allows for planning to scale smoothly with observation dimensionality. To demonstrate the effectiveness of this algorithm, we present results from a multi-agent racing benchmark that requires planning from image observations. Code and videos can be found at https://sites.google.com/view/deep-latent-competition.",1
"[Zhang, ICML 2018] provided the first decentralized actor-critic algorithm for multi-agent reinforcement learning (MARL) that offers convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to offer a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in real-world settings. To handle the lack of exploration inherent in deterministic policies, we consider both off-policy and on-policy settings. We provide the expression of a local deterministic policy gradient, decentralized deterministic actor-critic algorithms and convergence guarantees for linearly-approximated value functions. This work will help enable decentralized MARL in high-dimensional action spaces and pave the way for more widespread use of MARL.",0
"The first decentralized actor-critic algorithm for multi-agent reinforcement learning (MARL) was introduced by Zhang in ICML 2018 and it provided convergence guarantees. However, the policies were stochastic and only defined on finite action spaces. Therefore, we have extended that work to develop a decentralized actor-critic algorithm that can learn deterministic policies on continuous action spaces while ensuring provable convergence. Deterministic policies are crucial in real-world scenarios, but they lack exploration. To overcome this challenge, we have considered both off-policy and on-policy settings and provided the expression of a local deterministic policy gradient, decentralized deterministic actor-critic algorithms, and convergence guarantees for linearly-approximated value functions. By addressing the lack of exploration inherent in deterministic policies, our work will facilitate decentralized MARL in high-dimensional action spaces and pave the way for more extensive use of MARL.",1
"Reinforcement learning (RL) with linear function approximation has received increasing attention recently. However, existing work has focused on obtaining $\sqrt{T}$-type regret bound, where $T$ is the number of interactions with the MDP. In this paper, we show that logarithmic regret is attainable under two recently proposed linear MDP assumptions provided that there exists a positive sub-optimality gap for the optimal action-value function. More specifically, under the linear MDP assumption (Jin et al. 2019), the LSVI-UCB algorithm can achieve $\tilde{O}(d^{3}H^5/\text{gap}_{\text{min}}\cdot \log(T))$ regret; and under the linear mixture MDP assumption (Ayoub et al. 2020), the UCRL-VTR algorithm can achieve $\tilde{O}(d^{2}H^5/\text{gap}_{\text{min}}\cdot \log^3(T))$ regret, where $d$ is the dimension of feature mapping, $H$ is the length of episode, $\text{gap}_{\text{min}}$ is the minimal sub-optimality gap, and $\tilde O$ hides all logarithmic terms except $\log(T)$. To the best of our knowledge, these are the first logarithmic regret bounds for RL with linear function approximation. We also establish gap-dependent lower bounds for the two linear MDP models.",0
"Recently, there has been an increasing focus on reinforcement learning (RL) using linear function approximation. However, previous research has concentrated on achieving $\sqrt{T}$-type regret bounds, with $T$ representing the number of interactions with the MDP. This paper presents the possibility of logarithmic regret, achievable under two recently proposed linear MDP assumptions, provided there is a positive sub-optimality gap for the optimal action-value function. Specifically, the linear MDP assumption (Jin et al. 2019) can achieve $\tilde{O}(d^{3}H^5/\text{gap}_{\text{min}}\cdot \log(T))$ regret with the LSVI-UCB algorithm, while the linear mixture MDP assumption (Ayoub et al. 2020) can obtain $\tilde{O}(d^{2}H^5/\text{gap}_{\text{min}}\cdot \log^3(T))$ regret with the UCRL-VTR algorithm. Here, $d$ represents the dimension of feature mapping, $H$ is the length of episode, $\text{gap}_{\text{min}}$ is the minimal sub-optimality gap, and $\tilde O$ hides all logarithmic terms except $\log(T)$. These are the first logarithmic regret bounds for RL with linear function approximation. Additionally, gap-dependent lower bounds for the two linear MDP models are also established.",1
"We study the reinforcement learning problem for discounted Markov Decision Processes (MDPs) under the tabular setting. We propose a model-based algorithm named UCBVI-$\gamma$, which is based on the \emph{optimism in the face of uncertainty principle} and the Bernstein-type bonus. We show that UCBVI-$\gamma$ achieves an $\tilde{O}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$ regret, where $S$ is the number of states, $A$ is the number of actions, $\gamma$ is the discount factor and $T$ is the number of steps. In addition, we construct a class of hard MDPs and show that for any algorithm, the expected regret is at least $\tilde{\Omega}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$. Our upper bound matches the minimax lower bound up to logarithmic factors, which suggests that UCBVI-$\gamma$ is nearly minimax optimal for discounted MDPs.",0
"The tabular setting of the reinforcement learning problem for discounted Markov Decision Processes (MDPs) is the focus of our study. Our proposed model-based algorithm, UCBVI-$\gamma$, is grounded in the \emph{optimism in the face of uncertainty principle} and the application of a Bernstein-type bonus. Our findings demonstrate that UCBVI-$\gamma$ achieves an $\tilde{O}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$ regret, where $S$ represents the number of states, $A$ the number of actions, $\gamma$ the discount factor, and $T$ the number of steps. Additionally, we introduce a set of challenging MDPs and prove that for any algorithm, the anticipated regret is at least $\tilde{\Omega}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$. Our upper bound matches the minimax lower bound up to logarithmic factors, which indicates that UCBVI-$\gamma$ is nearly minimax optimal for discounted MDPs.",1
"Latest insights from biology show that intelligence does not only emerge from the connections between the neurons, but that individual neurons shoulder more computational responsibility. Current Neural Network architecture design and search are biased on fixed activation functions. Using more advanced learnable activation functions provide Neural Networks with higher learning capacity. However, general guidance for building such networks is still missing. In this work, we first explain why rationals offer an optimal choice for activation functions. We then show that they are closed under residual connections, and inspired by recurrence for residual networks we derive a self-regularized version of Rationals: Recurrent Rationals. We demonstrate that (Recurrent) Rational Networks lead to high performance improvements on Image Classification and Deep Reinforcement Learning.",0
"Recent findings in biology reveal that intelligence is not solely dependent on the connections between neurons, and individual neurons carry a greater computational responsibility. The current design and search for Neural Network architecture relies heavily on fixed activation functions, but the use of more advanced learnable activation functions can enhance the learning capacity of Neural Networks. However, there is still a lack of general guidance for constructing such networks. This study presents an explanation for why rationals are an ideal choice for activation functions and demonstrates that they are compatible with residual connections. Inspired by recurrence for residual networks, a self-regularized version of Rationals, called Recurrent Rationals, is derived. The study shows that (Recurrent) Rational Networks significantly improve performance in Image Classification and Deep Reinforcement Learning.",1
"One of the main challenges in real-world reinforcement learning is to learn successfully from limited training samples. We show that in certain settings, the available data can be dramatically increased through a form of multi-task learning, by exploiting an invariance property in the tasks. We provide a theoretical performance bound for the gain in sample efficiency under this setting. This motivates a new approach to multi-task learning, which involves the design of an appropriate neural network architecture and a prioritized task-sampling strategy. We demonstrate empirically the effectiveness of the proposed approach on two real-world sequential resource allocation tasks where this invariance property occurs: financial portfolio optimization and meta federated learning.",0
"Learning from limited training samples is a major obstacle for real-world reinforcement learning. However, we have found that in certain situations, multi-task learning can significantly increase the amount of available data by utilizing an invariance property in the tasks. We have established a theoretical performance limit for the sample efficiency gained in this scenario, which has led us to develop a new approach to multi-task learning. This approach involves creating a suitable neural network architecture and a task-sampling strategy that prioritizes the most important tasks. We have tested this approach on two real-world sequential resource allocation tasks where this invariance property is present, namely financial portfolio optimization and meta federated learning, and have successfully demonstrated its effectiveness.",1
"We approach the task of network congestion control in datacenters using Reinforcement Learning (RL). Successful congestion control algorithms can dramatically improve latency and overall network throughput. Until today, no such learning-based algorithms have shown practical potential in this domain. Evidently, the most popular recent deployments rely on rule-based heuristics that are tested on a predetermined set of benchmarks. Consequently, these heuristics do not generalize well to newly-seen scenarios. Contrarily, we devise an RL-based algorithm with the aim of generalizing to different configurations of real-world datacenter networks. We overcome challenges such as partial-observability, non-stationarity, and multi-objectiveness. We further propose a policy gradient algorithm that leverages the analytical structure of the reward function to approximate its derivative and improve stability. We show that this scheme outperforms alternative popular RL approaches, and generalizes to scenarios that were not seen during training. Our experiments, conducted on a realistic simulator that emulates communication networks' behavior, exhibit improved performance concurrently on the multiple considered metrics compared to the popular algorithms deployed today in real datacenters. Our algorithm is being productized to replace heuristics in some of the largest datacenters in the world.",0
"To improve latency and overall network throughput in datacenters, we have employed Reinforcement Learning (RL) to develop a congestion control algorithm. Presently, popular deployments rely on rule-based heuristics which lack practical potential in new scenarios. In contrast, our RL-based algorithm can generalize to a range of real-world datacenter network configurations and addresses challenges such as partial-observability, non-stationarity and multi-objectiveness. We have proposed a policy gradient algorithm that uses the analytical structure of the reward function to approximate its derivative and improve stability, outperforming alternative popular RL approaches. Our experiments on a realistic simulator demonstrate improved performance on multiple metrics compared to current algorithms in real datacenters. We are currently productizing our algorithm to replace heuristics in some of the world's largest datacenters.",1
"We consider the decision-making framework of online convex optimization with a very large number of experts. This setting is ubiquitous in contextual and reinforcement learning problems, where the size of the policy class renders enumeration and search within the policy class infeasible.   Instead, we consider generalizing the methodology of online boosting. We define a weak learning algorithm as a mechanism that guarantees multiplicatively approximate regret against a base class of experts. In this access model, we give an efficient boosting algorithm that guarantees near-optimal regret against the convex hull of the base class. We consider both full and partial (a.k.a. bandit) information feedback models. We also give an analogous efficient boosting algorithm for the i.i.d. statistical setting.   Our results simultaneously generalize online boosting and gradient boosting guarantees to contextual learning model, online convex optimization and bandit linear optimization settings.",0
"The focus of our study is the decision-making framework for online convex optimization involving a vast number of experts. This situation is commonplace in contextual and reinforcement learning problems, where the policy class is too large for enumeration and search. Instead, we propose extending the online boosting methodology. We define a weak learning algorithm, which guarantees approximate regret against a base class of experts. Using this access model, we present an efficient boosting algorithm that delivers near-optimal regret against the convex hull of the base class. Our approach applies to both full and partial (bandit) information feedback models, and we also provide an efficient boosting algorithm for the i.i.d. statistical setting. Our results generalize online boosting and gradient boosting guarantees to a range of settings, including contextual learning models, online convex optimization, and bandit linear optimization.",1
"Campbell-Goodhart's law relates to the causal inference error whereby decision-making agents aim to influence variables which are correlated to their goal objective but do not reliably cause it. This is a well known error in Economics and Political Science but not widely labelled in Artificial Intelligence research. Through a simple example, we show how off-the-shelf deep Reinforcement Learning (RL) algorithms are not necessarily immune to this cognitive error. The off-policy learning method is tricked, whilst the on-policy method is not. The practical implication is that naive application of RL to complex real life problems can result in the same types of policy errors that humans make. Great care should be taken around understanding the causal model that underpins a solution derived from Reinforcement Learning.",0
"The Campbell-Goodhart principle pertains to the mistake of inferring causation from correlation, where decision-makers attempt to affect variables that are linked to their desired goal but do not always cause it. This is a commonly recognized error in Political Science and Economics, though it is not frequently referred to in Artificial Intelligence research. To illustrate, we demonstrate how readily available deep Reinforcement Learning (RL) algorithms can fall prey to this cognitive fallacy using a straightforward example. The off-policy learning technique is deceived, whereas the on-policy technique is not. The practical outcome is that applying RL to intricate real-world issues without caution can lead to the same types of policy errors that humans commit. Therefore, it is crucial to comprehend the causal model supporting a solution derived from Reinforcement Learning.",1
"This paper presents a multi-agent reinforcement learning algorithm to represent strategic bidding behavior in freight transport markets. Using this algorithm, we investigate whether feasible market equilibriums arise without any central control or communication between agents. Studying behavior in such environments may serve as a stepping stone towards self-organizing logistics systems like the Physical Internet. We model an agent-based environment in which a shipper and a carrier actively learn bidding strategies using policy gradient methods, posing bid- and ask prices at the individual container level. Both agents aim to learn the best response given the expected behavior of the opposing agent. A neutral broker allocates jobs based on bid-ask spreads.   Our game-theoretical analysis and numerical experiments focus on behavioral insights. To evaluate system performance, we measure adherence to Nash equilibria, fairness of reward division and utilization of transport capacity. We observe good performance both in predictable, deterministic settings (~95% adherence to Nash equilibria) and highly stochastic environments (~85% adherence). Risk-seeking behavior may increase an agent's reward share, as long as the strategies are not overly aggressive. The results suggest a potential for full automation and decentralization of freight transport markets.",0
"In this article, we propose a multi-agent reinforcement learning algorithm for representing strategic bidding behavior in the freight transport market. The aim of our study is to examine the emergence of feasible market equilibriums without any central control or communication between agents. We believe that studying such environments can pave the way for self-organizing logistics systems like the Physical Internet. Our agent-based model comprises a shipper and a carrier who actively learn bidding strategies using policy gradient methods. Both agents set bid- and ask prices at the individual container level and aim to learn the best response considering the expected behavior of the opposing agent. A neutral broker allocates jobs based on bid-ask spreads. Our game-theoretical analysis and numerical experiments focus on the behavioral insights of the agents. We evaluate the performance of the system by measuring adherence to Nash equilibria, fairness of reward division, and utilization of transport capacity. We find that the agents perform well both in predictable, deterministic settings (~95% adherence to Nash equilibria) and highly stochastic environments (~85% adherence). The study also suggests the potential for full automation and decentralization of freight transport markets, as long as the strategies are not overly aggressive.",1
"Efficient exploration has presented a long-standing challenge in reinforcement learning, especially when rewards are sparse. A developmental system can overcome this difficulty by learning from both demonstrations and self-exploration. However, existing methods are not applicable to most real-world robotic controlling problems because they assume that environments follow Markov decision processes (MDP); thus, they do not extend to partially observable environments where historical observations are necessary for decision making. This paper builds on the idea of replaying demonstrations for memory-dependent continuous control, by proposing a novel algorithm, Recurrent Actor-Critic with Demonstration and Experience Replay (READER). Experiments involving several memory-crucial continuous control tasks reveal significantly reduce interactions with the environment using our method with a reasonably small number of demonstration samples. The algorithm also shows better sample efficiency and learning capabilities than a baseline reinforcement learning algorithm for memory-based control from demonstrations.",0
"Reinforcement learning faces a persistent challenge in achieving efficient exploration, particularly when rewards are sparse. To overcome this issue, a developmental system can learn from both self-exploration and demonstrations. However, current techniques are not suitable for most real-world robotic control problems because they assume Markov decision processes and cannot be extended to partially observable environments. This paper introduces a new algorithm called Recurrent Actor-Critic with Demonstration and Experience Replay (READER) that builds on the concept of replaying demonstrations for memory-dependent continuous control. The experiments demonstrate that our method significantly reduces interactions with the environment with a relatively small number of demonstration samples and exhibits better sample efficiency and learning capabilities than a baseline reinforcement learning algorithm for memory-based control from demonstrations.",1
"In the industrial interior design process, professional designers plan the furniture layout to achieve a satisfactory 3D design for selling. In this paper, we explore the interior graphics scenes design task as a Markov decision process (MDP) in 3D simulation, which is solved by multi-agent reinforcement learning. The goal is to produce furniture layout in the 3D simulation of the indoor graphics scenes. In particular, we firstly transform the 3D interior graphic scenes into two 2D simulated scenes. We then design the simulated environment and apply two reinforcement learning agents to learn the optimal 3D layout for the MDP formulation in a cooperative way. We conduct our experiments on a large-scale real-world interior layout dataset that contains industrial designs from professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts as compared with the state-of-art model. The developed simulator and codes are available at \url{https://github.com/CODE-SUBMIT/simulator2}.",0
"The process of industrial interior design involves designing furniture layout for the purpose of selling. In this study, we focus on the task of designing interior graphics scenes using a Markov decision process (MDP) in a 3D simulation. To achieve this, we utilize multi-agent reinforcement learning. Our aim is to create a furniture layout in the 3D simulation of indoor graphics scenes. We begin by converting the 3D interior graphics scenes into two 2D simulated scenes. Then, we design the simulated environment and utilize two reinforcement learning agents to learn the optimal 3D layout in a cooperative manner. Our experiments are conducted using a large-scale real-world interior layout dataset that contains industrial designs from professional designers. Our numerical results indicate that the proposed model produces higher-quality layouts compared to the state-of-the-art model. The simulator and codes developed for this study are available at \url{https://github.com/CODE-SUBMIT/simulator2}.",1
"We study the reinforcement learning for finite-horizon episodic Markov decision processes with adversarial reward and full information feedback, where the unknown transition probability function is a linear function of a given feature mapping. We propose an optimistic policy optimization algorithm with Bernstein bonus and show that it can achieve $\tilde{O}(dH\sqrt{T})$ regret, where $H$ is the length of the episode, $T$ is the number of interaction with the MDP and $d$ is the dimension of the feature mapping. Furthermore, we also prove a matching lower bound of $\tilde{\Omega}(dH\sqrt{T})$ up to logarithmic factors. To the best of our knowledge, this is the first computationally efficient, nearly minimax optimal algorithm for adversarial Markov decision processes with linear function approximation.",0
"Our focus is on finite-horizon episodic Markov decision processes with adversarial reward and full information feedback. The transition probability function is unknown, but is a linear function of a feature mapping. To address this challenge, we propose an optimistic policy optimization algorithm with Bernstein bonus. Our approach achieves $\tilde{O}(dH\sqrt{T})$ regret, where $H$ is the length of the episode, $T$ is the number of interactions with the MDP, and $d$ is the dimension of the feature mapping. We also prove a lower bound of $\tilde{\Omega}(dH\sqrt{T})$, up to logarithmic factors. This is the first algorithm that is computationally efficient and nearly minimax optimal for adversarial Markov decision processes with linear function approximation.",1
"Due to its perceptual limitations, an agent may have too little information about the state of the environment to act optimally. In such cases, it is important to keep track of the observation history to uncover hidden state. Recent deep reinforcement learning methods use recurrent neural networks (RNN) to memorize past observations. However, these models are expensive to train and have convergence difficulties, especially when dealing with high dimensional input spaces. In this paper, we propose influence-aware memory (IAM), a theoretically inspired memory architecture that tries to alleviate the training difficulties by restricting the input of the recurrent layers to those variables that influence the hidden state information. Moreover, as opposed to standard RNNs, in which every piece of information used for estimating Q values is inevitably fed back into the network for the next prediction, our model allows information to flow without being necessarily stored in the RNN's internal memory. Results indicate that, by letting the recurrent layers focus on a small fraction of the observation variables while processing the rest of the information with a feedforward neural network, we can outperform standard recurrent architectures both in training speed and policy performance. This approach also reduces runtime and obtains better scores than methods that stack multiple observations to remove partial observability.",0
"When an agent lacks sufficient information about the environment, due to perceptual limitations, it cannot act optimally. To uncover hidden state, it is essential to keep track of the observation history. However, recent deep reinforcement learning methods utilizing recurrent neural networks (RNN) are expensive to train and experience convergence difficulties, particularly with high-dimensional input spaces. In this study, we present influence-aware memory (IAM), a memory architecture that restricts the input of the recurrent layers to variables that affect the hidden state information, inspired by theoretical concepts. Our model enables information flow without being stored in the RNN's internal memory, unlike standard RNNs. The results indicate that IAM surpasses standard recurrent architectures in both training speed and policy performance by allowing the recurrent layers to focus on a small portion of the observation variables while handling the rest of the information with a feedforward neural network. This approach also reduces runtime and achieves better scores than methods that stack multiple observations to address partial observability.",1
"Finding an effective medical treatment often requires a search by trial and error. Making this search more efficient by minimizing the number of unnecessary trials could lower both costs and patient suffering. We formalize this problem as learning a policy for finding a near-optimal treatment in a minimum number of trials using a causal inference framework. We give a model-based dynamic programming algorithm which learns from observational data while being robust to unmeasured confounding. To reduce time complexity, we suggest a greedy algorithm which bounds the near-optimality constraint. The methods are evaluated on synthetic and real-world healthcare data and compared to model-free reinforcement learning. We find that our methods compare favorably to the model-free baseline while offering a more transparent trade-off between search time and treatment efficacy.",0
"In order to minimize both costs and patient suffering, it is important to find an efficient medical treatment without unnecessary trials. To achieve this, we propose a solution that utilizes a causal inference framework to learn a policy for finding a near-optimal treatment in the minimum number of trials. Our model-based dynamic programming algorithm is designed to be robust to unmeasured confounding and can learn from observational data. To further reduce time complexity, we suggest a greedy algorithm that places a constraint on near-optimality. We evaluate these methods on both synthetic and real-world healthcare data, comparing them to model-free reinforcement learning. Our results show that our methods perform favorably to the model-free baseline and offer a more transparent trade-off between search time and treatment efficacy.",1
"Data augmentation technique from computer vision has been widely considered as a regularization method to improve data efficiency and generalization performance in vision-based reinforcement learning. We variate the timing of using augmentation, which is, in turn, critical depending on tasks to be solved in training and testing. According to our experiments on Open AI Procgen Benchmark, if the regularization imposed by augmentation is helpful only in testing, it is better to procrastinate the augmentation after training than to use it during training in terms of sample and computation complexity. We note that some of such augmentations can disturb the training process. Conversely, an augmentation providing regularization useful in training needs to be used during the whole training period to fully utilize its benefit in terms of not only generalization but also data efficiency. These phenomena suggest a useful timing control of data augmentation in reinforcement learning.",0
"The use of data augmentation in computer vision is commonly acknowledged as a means of regularization to enhance data efficiency and generalization performance in reinforcement learning based on visual inputs. However, the timing of applying this technique is crucial and varies depending on the task at hand during training and testing. Our research on Open AI Procgen Benchmark demonstrates that if augmentation only aids in testing, it is preferable to postpone it after training to reduce sample and computation complexity. It should be noted that certain augmentations can hinder the training process. Conversely, if an augmentation is beneficial for training regularization, it should be applied throughout the entire training period to maximize its benefits for generalization and data efficiency. These findings indicate the importance of timing control when using data augmentation in reinforcement learning.",1
"Safety is essential for reinforcement learning (RL) applied in real-world tasks like autonomous driving. Chance constraints which guarantee the satisfaction of state constraints at a high probability are suitable to represent the requirements in real-world environment with uncertainty. Existing chance constrained RL methods like the penalty method and the Lagrangian method either exhibit periodic oscillations or cannot satisfy the constraints. In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. Taking a control perspective, we first interpret the penalty method and the Lagrangian method as proportional feedback and integral feedback control, respectively. Then, a proportional-integral Lagrangian method is proposed to steady learning process while improving safety. To prevent integral overshooting and reduce conservatism, we introduce the integral separation technique inspired by PID control. Finally, an analytical gradient of the chance constraint is utilized for model-based policy optimization. The effectiveness of SPIL is demonstrated by a narrow car-following task. Experiments indicate that compared with previous methods, SPIL improves the performance while guaranteeing safety, with a steady learning process.",0
"For RL to be useful in real-world applications such as self-driving cars, safety is crucial. Chance constraints are ideal for representing the requirements of uncertain real-world environments and ensuring the satisfaction of state constraints with high probability. However, existing chance constrained RL methods like the penalty method and the Lagrangian method have limitations. They either exhibit periodic oscillations or cannot meet the constraints. This paper proposes a solution to these shortcomings with the separated proportional-integral Lagrangian (SPIL) algorithm. The penalty method and the Lagrangian method are viewed as proportional feedback and integral feedback control, respectively, from a control perspective. The proposed method uses the proportional-integral Lagrangian to improve safety while maintaining a stable learning process. To prevent integral overshooting and reduce conservatism, the integral separation technique inspired by PID control is introduced. Finally, the analytical gradient of the chance constraint is used for model-based policy optimization. The effectiveness of the SPIL algorithm is demonstrated in a narrow car-following task, showing that it improves performance while ensuring safety and maintaining a stable learning process compared to previous methods.",1
"Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.",0
"The lack of knowledge of the learner contributes to the out-of-sample prediction error known as epistemic uncertainty. Prior research has focused on model variance, but we suggest a more systematic approach to estimate epistemic uncertainty. This involves learning to predict generalization error and subtracting an estimate of intrinsic unpredictability, referred to as aleatoric uncertainty. Our method considers the impact of model bias and is applicable in non-stationary learning environments such as active learning or reinforcement learning. We demonstrate the effectiveness of Direct Epistemic Uncertainty Prediction (DEUP) in various downstream tasks like sequential model optimization and reinforcement learning. Furthermore, we evaluate the quality of uncertainty estimates from DEUP for probabilistic image classification and estimating uncertainty of synergistic drug combinations.",1
"We study black-box reward poisoning attacks against reinforcement learning (RL), in which an adversary aims to manipulate the rewards to mislead a sequence of RL agents with unknown algorithms to learn a nefarious policy in an environment unknown to the adversary a priori. That is, our attack makes minimum assumptions on the prior knowledge of the adversary: it has no initial knowledge of the environment or the learner, and neither does it observe the learner's internal mechanism except for its performed actions. We design a novel black-box attack, U2, that can provably achieve a near-matching performance to the state-of-the-art white-box attack, demonstrating the feasibility of reward poisoning even in the most challenging black-box setting.",0
"Our focus is on studying black-box reward poisoning attacks on reinforcement learning (RL). These attacks involve an adversary attempting to manipulate rewards to deceive a sequence of RL agents with unknown algorithms into learning a harmful policy in an environment that the adversary has no prior knowledge of. The attack assumes minimal prior knowledge on the part of the adversary, with no understanding of the environment or the learner, and with no access to the learner's internal workings other than observing its actions. To demonstrate the viability of reward poisoning in the most difficult black-box setting, we have developed a new black-box attack, U2, that can achieve a performance close to that of the state-of-the-art white-box attack.",1
"Planning future operational scenarios of bulk power systems that meet security and economic constraints typically requires intensive labor efforts in performing massive simulations. To automate this process and relieve engineers' burden, a novel multi-stage control approach is presented in this paper to train centralized and decentralized reinforcement learning agents that can automatically adjust grid controllers for regulating transmission line flows at normal condition and under contingencies. The power grid flow control problem is formulated as Markov Decision Process (MDP). At stage one, centralized soft actor-critic (SAC) agent is trained to control generator active power outputs in a wide area to control transmission line flows against specified security limits. If line overloading issues remain unresolved, stage two is used to train decentralized SAC agent via load throw-over at local substations. The effectiveness of the proposed approach is verified on a series of actual planning cases used for operating the power grid of SGCC Zhejiang Electric Power Company.",0
"To plan for future operational scenarios of bulk power systems, extensive simulations are typically required to ensure both security and economic constraints are met. However, this process can be labor-intensive and burdensome for engineers. To address this, a new approach is presented in this paper that utilizes multi-stage control and reinforcement learning agents to automatically adjust grid controllers for regulating transmission line flows. The power grid flow control problem is formulated as a Markov Decision Process. In stage one, a centralized soft actor-critic (SAC) agent is trained to control generator active power outputs in a wide area to control transmission line flows against specified security limits. If line overloading issues remain unresolved, stage two is used to train a decentralized SAC agent via load throw-over at local substations. The proposed approach is tested on actual planning cases used for operating the power grid of SGCC Zhejiang Electric Power Company to demonstrate its effectiveness.",1
"Model-based algorithms, which learn a dynamics model from logged experience and perform some sort of pessimistic planning under the learned model, have emerged as a promising paradigm for offline reinforcement learning (offline RL). However, practical variants of such model-based algorithms rely on explicit uncertainty quantification for incorporating pessimism. Uncertainty estimation with complex models, such as deep neural networks, can be difficult and unreliable. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. This results in a conservative estimate of the value function for out-of-support state-action tuples, without requiring explicit uncertainty estimation. We theoretically show that our method optimizes a lower bound on the true policy value, that this bound is tighter than that of prior methods, and our approach satisfies a policy improvement guarantee in the offline setting. Through experiments, we find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.",0
"The use of model-based algorithms in offline reinforcement learning has shown promise. These algorithms learn a dynamics model through logged experience and undertake pessimistic planning based on the learned model. However, practical versions of these algorithms require uncertainty quantification to incorporate pessimism, which can be challenging when using complex models like deep neural networks. To address this limitation, we introduce COMBO, a new model-based offline RL algorithm that regularizes the value function using out-of-support state-action tuples generated through rollouts under the learned model. This allows for a conservative estimate of the value function without requiring explicit uncertainty estimation. Our theoretical analysis reveals that our approach optimizes a lower bound on the true policy value, which is tighter than that of previous methods. Furthermore, we demonstrate that COMBO consistently performs as well as or better than prior offline model-free and model-based methods on established offline RL benchmarks, including image-based tasks, while satisfying a policy improvement guarantee in the offline setting.",1
"Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a significant impact on the performance. In this paper, we exploit the fact that the underlying Markov decision process (MDP) represents a graph, which enables us to incorporate the topological information for effective state representation learning.   Motivated by the recent success of node representations for several graph analytical tasks we specifically investigate the capability of node representation learning methods to effectively encode the topology of the underlying MDP in Deep RL. To this end we perform a comparative analysis of several models chosen from 4 different classes of representation learning algorithms for policy learning in grid-world navigation tasks, which are representative of a large class of RL problems. We find that all embedding methods outperform the commonly used matrix representation of grid-world environments in all of the studied cases. Moreoever, graph convolution based methods are outperformed by simpler random walk based methods and graph linear autoencoders.",0
"The success of Deep RL approaches largely depends on the deep neural network's ability to create valuable internal representations. However, these approaches require a large number of samples and a good input representation can significantly improve performance. In this study, we take advantage of the fact that the underlying Markov decision process (MDP) is represented as a graph, allowing us to incorporate topological information for effective state representation learning. Inspired by the success of node representations in graph analytical tasks, we investigate the effectiveness of node representation learning methods in encoding the MDP topology in Deep RL. We compare several models from 4 different classes of representation learning algorithms for policy learning in grid-world navigation tasks, which represent a wide range of RL problems. Our results show that all embedding methods outperform the commonly used matrix representation of grid-world environments. Additionally, simpler random walk based methods and graph linear autoencoders outperform graph convolution based methods.",1
"Reinforcement learning is a promising paradigm for solving sequential decision-making problems, but low data efficiency and weak generalization across tasks are bottlenecks in real-world applications. Model-based meta reinforcement learning addresses these issues by learning dynamics and leveraging knowledge from prior experience. In this paper, we take a closer look at this framework, and propose a new Thompson-sampling based approach that consists of a new model to identify task dynamics together with an amortized policy optimization step. We show that our model, called a graph structured surrogate model (GSSM), outperforms state-of-the-art methods in predicting environment dynamics. Additionally, our approach is able to obtain high returns, while allowing fast execution during deployment by avoiding test time policy gradient optimization.",0
"Although reinforcement learning shows promise in solving sequential decision-making problems, limited data efficiency and weak generalization across tasks hinder its real-world applications. However, model-based meta reinforcement learning can address these challenges by learning dynamics and utilizing knowledge from previous experiences. In this article, we examine this framework in detail and introduce a novel Thompson-sampling based approach that involves a new model for identifying task dynamics and an amortized policy optimization step. Our proposed graph structured surrogate model (GSSM) surpasses current methods in predicting environment dynamics and enables us to achieve high returns while avoiding test time policy gradient optimization, resulting in quick execution during deployment.",1
"Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequential decision making. The recent AlphaGo and AlphaZero algorithms have shown how to successfully combine these two paradigms in order to solve large scale sequential decision problems. These methodologies exploit a variant of the well-known UCT algorithm to trade off exploitation of good actions and exploration of unvisited states, but their empirical success comes at the cost of poor sample-efficiency and high computation time. In this paper, we overcome these limitations by considering convex regularization in Monte-Carlo Tree Search (MCTS), which has been successfully used in RL to efficiently drive exploration. First, we introduce a unifying theory on the use of generic convex regularizers in MCTS, deriving the regret analysis and providing guarantees of exponential convergence rate. Second, we exploit our theoretical framework to introduce novel regularized backup operators for MCTS, based on the relative entropy of the policy update, and on the Tsallis entropy of the policy. Finally, we empirically evaluate the proposed operators in AlphaGo and AlphaZero on problems of increasing dimensionality and branching factor, from a toy problem to several Atari games, showing their superiority w.r.t. representative baselines.",0
"Sequential decision making requires the use of Monte-Carlo planning and Reinforcement Learning (RL). The AlphaGo and AlphaZero algorithms have demonstrated the success of combining these two paradigms to solve large scale decision problems. However, the UCT algorithm used in these methodologies sacrifices sample-efficiency and computation time for empirical success. To overcome these limitations, we propose the use of convex regularization in Monte-Carlo Tree Search (MCTS), which has been proven effective in RL for exploration. Our paper presents a theory on the use of generic convex regularizers in MCTS, establishing the regret analysis and exponential convergence rate guarantees. We also introduce regularized backup operators for MCTS based on relative entropy of policy update and Tsallis entropy of policy. Our empirical evaluation on problems of increasing complexity, including Atari games, demonstrates the superiority of our proposed operators over existing baselines in AlphaGo and AlphaZero.",1
"Reinforcement learning has shown great potential in developing high-level autonomous driving. However, for high-dimensional tasks, current RL methods suffer from low data efficiency and oscillation in the training process. This paper proposes an algorithm called Learn to drive with Virtual Memory (LVM) to overcome these problems. LVM compresses the high-dimensional information into compact latent states and learns a latent dynamic model to summarize the agent's experience. Various imagined latent trajectories are generated as virtual memory by the latent dynamic model. The policy is learned by propagating gradient through the learned latent model with the imagined latent trajectories and thus leads to high data efficiency. Furthermore, a double critic structure is designed to reduce the oscillation during the training process. The effectiveness of LVM is demonstrated by an image-input autonomous driving task, in which LVM outperforms the existing method in terms of data efficiency, learning stability, and control performance.",0
"The potential of reinforcement learning in developing high-level autonomous driving has been established, but it faces challenges in low data efficiency and training process oscillation for high-dimensional tasks. To address these problems, this paper proposes an algorithm known as Learn to drive with Virtual Memory (LVM). LVM compresses high-dimensional information into compact latent states and learns a latent dynamic model to summarize the agent's experience. The latent dynamic model generates various imagined latent trajectories as virtual memory. The policy is learned by propagating gradient through the learned latent model with the imagined latent trajectories, leading to high data efficiency. Additionally, a double critic structure is designed to reduce oscillation during the training process. The effectiveness of LVM is demonstrated by an image-input autonomous driving task, where LVM outperforms existing methods in terms of data efficiency, learning stability, and control performance.",1
"Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.",0
"Computational design methods, known as Generative design, can explore design possibilities within designer-defined constraints. Topology optimization-based generative designs are one of the methods that aim to explore a diverse range of topology designs that cannot be achieved through standard parametric design methods. Data-driven topology optimization research has started to incorporate artificial intelligence such as deep learning or machine learning to improve the design exploration capabilities. This study presents a reinforcement learning (RL)-based generative design process that maximizes the diversity of topology designs. Design exploration is formulated as a sequential problem of finding optimal design parameters in line with a reference design. The learning framework used is the Proximal Policy Optimization, demonstrated in an automotive wheel design case study. The optimization process is approximated using neural networks to reduce the computational burden. The neural networks show a generalized performance and symmetry-preserving characteristics. The RL-based generative design produces numerous diverse designs within a short inference time by utilizing GPU in a fully automated manner. This method is different from the previous approach that uses CPU, which involves human intervention and takes much longer processing time.",1
"Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior. IRL is useful for automated control in situations where the reward function is difficult to specify manually, which impedes reinforcement learning. We provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. We provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm.",0
"The task of Inverse Reinforcement Learning (IRL) involves identifying a reward function that explains expert behavior that is already known or observed. IRL can be advantageous in scenarios where it is challenging to manually determine the reward function, thereby hindering reinforcement learning. Our new IRL algorithm caters to the setting of continuous state space with unfamiliar transition dynamics by utilizing a basis of orthonormal functions to model the system. We present a demonstration of correctness and formal assurances regarding the sample and time complexity of our algorithm.",1
"Humans quickly solve tasks in novel systems with complex dynamics, without requiring much interaction. While deep reinforcement learning algorithms have achieved tremendous success in many complex tasks, these algorithms need a large number of samples to learn meaningful policies. In this paper, we present a task for navigating a marble to the center of a circular maze. While this system is very intuitive and easy for humans to solve, it can be very difficult and inefficient for standard reinforcement learning algorithms to learn meaningful policies. We present a model that learns to move a marble in the complex environment within minutes of interacting with the real system. Learning consists of initializing a physics engine with parameters estimated using data from the real system. The error in the physics engine is then corrected using Gaussian process regression, which is used to model the residual between real observations and physics engine simulations. The physics engine augmented with the residual model is then used to control the marble in the maze environment using a model-predictive feedback over a receding horizon. To the best of our knowledge, this is the first time that a hybrid model consisting of a full physics engine along with a statistical function approximator has been used to control a complex physical system in real-time using nonlinear model-predictive control (NMPC).",0
"Although humans can solve complex tasks in new systems quickly and with minimal interaction, deep reinforcement learning algorithms require a significant number of samples to learn effective policies, despite their success in many complex tasks. This article introduces a navigation task that challenges standard reinforcement learning algorithms to learn meaningful policies for guiding a marble to the center of a circular maze. Our proposed model employs a physics engine initialized with data from the physical system and corrected using Gaussian process regression. This augmented physics engine is used to control the marble's movement in the maze using a model-predictive feedback approach. To our knowledge, this is the first time a hybrid model combining a full physics engine and a statistical function approximator has been used to control a complex physical system in real-time using nonlinear model-predictive control (NMPC).",1
"The success of deep learning in the computer vision and natural language processing communities can be attributed to training of very deep neural networks with millions or billions of parameters which can then be trained with massive amounts of data. However, similar trend has largely eluded training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of 1) wider networks with DenseNet connection, 2) decoupling representation learning from training of RL, 3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.",0
"Deep learning has proven successful in computer vision and natural language processing due to the training of deep neural networks with millions or billions of parameters using massive amounts of data. However, training deep reinforcement learning (RL) algorithms with larger networks has not led to performance improvement as seen in these other fields. This is mainly due to instability during training of deep RL agents. To address this issue, we propose a novel method that involves wider networks with DenseNet connection, decoupling representation learning from RL training, and a distributed training method to mitigate overfitting problems. Our three-fold technique results in significant performance gains as demonstrated through ablation studies and outperforms other baseline algorithms on challenging locomotion tasks.",1
"We consider constrained policy optimization in Reinforcement Learning, where the constraints are in form of marginals on state visitations and global action executions. Given these distributions, we formulate policy optimization as unbalanced optimal transport over the space of occupancy measures. We propose a general purpose RL objective based on Bregman divergence and optimize it using Dykstra's algorithm. The approach admits an actor-critic algorithm for when the state or action space is large, and only samples from the marginals are available. We discuss applications of our approach and provide demonstrations to show the effectiveness of our algorithm.",0
"In Reinforcement Learning, we examine the optimization of policies under specific constraints that are reflected in the form of state visitations and global action executions. Our proposed solution involves the use of unbalanced optimal transport applied to occupancy measures. To achieve this, we introduce a general RL objective that utilizes Bregman divergence and optimize it using Dykstra's algorithm. This enables us to implement an actor-critic algorithm when dealing with large state or action spaces and limited sampling from the marginals. We also showcase the practical applications of our approach and present demonstrations that illustrate the proficiency of our algorithm.",1
"Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires \textit{no explicit} uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves state-of-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks.",0
"Model-free reinforcement learning is hindered by sample complexity, and model-based reinforcement learning is a promising solution. However, the theoretical understanding of model-based RL has been limited. This paper presents a new algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. Our meta-algorithm guarantees improved rewards by iteratively building a lower bound of the expected reward based on estimated models and sample trajectories. The framework extends the optimism-in-face-of-uncertainty principle to nonlinear models without requiring explicit uncertainty quantification. We apply the framework to create the Stochastic Lower Bounds Optimization (SLBO) algorithm, which achieves state-of-the-art performance on continuous control benchmark tasks using only one million or fewer samples.",1
"We introduce causal Markov Decision Processes (C-MDPs), a new formalism for sequential decision making which combines the standard MDP formulation with causal structures over state transition and reward functions. Many contemporary and emerging application areas such as digital healthcare and digital marketing can benefit from modeling with C-MDPs due to the causal mechanisms underlying the relationship between interventions and states/rewards. We propose the causal upper confidence bound value iteration (C-UCBVI) algorithm that exploits the causal structure in C-MDPs and improves the performance of standard reinforcement learning algorithms that do not take causal knowledge into account. We prove that C-UCBVI satisfies an $\tilde{O}(HS\sqrt{ZT})$ regret bound, where $T$ is the the total time steps, $H$ is the episodic horizon, and $S$ is the cardinality of the state space. Notably, our regret bound does not scale with the size of actions/interventions ($A$), but only scales with a causal graph dependent quantity $Z$ which can be exponentially smaller than $A$. By extending C-UCBVI to the factored MDP setting, we propose the causal factored UCBVI (CF-UCBVI) algorithm, which further reduces the regret exponentially in terms of $S$. Furthermore, we show that RL algorithms for linear MDP problems can also be incorporated in C-MDPs. We empirically show the benefit of our causal approaches in various settings to validate our algorithms and theoretical results.",0
"Our new formalism for sequential decision making, called causal Markov Decision Processes (C-MDPs), combines standard MDP formulation with causal structures over state transition and reward functions. Applications in digital healthcare and digital marketing can benefit from modeling with C-MDPs due to the causal mechanisms underlying the relationship between interventions and states/rewards. To improve the performance of standard reinforcement learning algorithms that do not take causal knowledge into account, we propose the causal upper confidence bound value iteration (C-UCBVI) algorithm that exploits the causal structure in C-MDPs. We demonstrate that C-UCBVI satisfies an $\tilde{O}(HS\sqrt{ZT})$ regret bound, where $T$ is the total time steps, $H$ is the episodic horizon, and $S$ is the cardinality of the state space. Our regret bound does not scale with the size of actions/interventions ($A$), but only scales with a causal graph dependent quantity $Z$ which can be exponentially smaller than $A$. We also introduce the causal factored UCBVI (CF-UCBVI) algorithm, which further reduces the regret exponentially in terms of $S$, by extending C-UCBVI to the factored MDP setting. Additionally, we demonstrate that RL algorithms for linear MDP problems can also be incorporated in C-MDPs. We empirically validate our algorithms and theoretical results in various settings.",1
"In this paper, we consider a type of image quality assessment as a task-specific measurement, which can be used to select images that are more amenable to a given target task, such as image classification or segmentation. We propose to train simultaneously two neural networks for image selection and a target task using reinforcement learning. A controller network learns an image selection policy by maximising an accumulated reward based on the target task performance on the controller-selected validation set, whilst the target task predictor is optimised using the training set. The trained controller is therefore able to reject those images that lead to poor accuracy in the target task. In this work, we show that the controller-predicted image quality can be significantly different from the task-specific image quality labels that are manually defined by humans. Furthermore, we demonstrate that it is possible to learn effective image quality assessment without using a ``clean'' validation set, thereby avoiding the requirement for human labelling of images with respect to their amenability for the task. Using $6712$, labelled and segmented, clinical ultrasound images from $259$ patients, experimental results on holdout data show that the proposed image quality assessment achieved a mean classification accuracy of $0.94\pm0.01$ and a mean segmentation Dice of $0.89\pm0.02$, by discarding $5\%$ and $15\%$ of the acquired images, respectively. The significantly improved performance was observed for both tested tasks, compared with the respective $0.90\pm0.01$ and $0.82\pm0.02$ from networks without considering task amenability. This enables image quality feedback during real-time ultrasound acquisition among many other medical imaging applications.",0
"The aim of our study is to evaluate image quality as a task-specific metric for selecting images that are more suitable for a particular target task, such as image segmentation or classification. We propose a reinforcement learning approach that trains two neural networks simultaneously for image selection and the target task. The controller network learns an image selection policy by maximizing the target task's performance on the validation set selected by the controller, while the target task predictor is optimized using the training set. By doing so, the trained controller is capable of rejecting images that lead to poor accuracy in the target task. We demonstrate that the controller's predicted image quality differs significantly from the task-specific image quality labels that are manually defined by humans. Moreover, we show that effective image quality assessment can be learned without requiring a ""clean"" validation set, thus avoiding the need for human labeling of images regarding their amenability for the task. Our experiments on $6712$ labeled and segmented clinical ultrasound images from $259$ patients demonstrate that our proposed image quality assessment achieved a mean classification accuracy of $0.94\pm0.01$ and a mean segmentation Dice of $0.89\pm0.02$ by discarding $5\%$ and $15\%$ of the acquired images, respectively. Our approach outperforms networks that do not consider task amenability, which achieved a respective $0.90\pm0.01$ and $0.82\pm0.02$ for the tested tasks. This enables real-time image quality feedback during ultrasound acquisition and has numerous applications in medical imaging.",1
"We study reinforcement learning for two-player zero-sum Markov games with simultaneous moves in the finite-horizon setting, where the transition kernel of the underlying Markov games can be parameterized by a linear function over the current state, both players' actions and the next state. In particular, we assume that we can control both players and aim to find the Nash Equilibrium by minimizing the duality gap. We propose an algorithm Nash-UCRL-VTR based on the principle ""Optimism-in-Face-of-Uncertainty"". Our algorithm only needs to find a Coarse Correlated Equilibrium (CCE), which is computationally very efficient. Specifically, we show that Nash-UCRL-VTR can provably achieve an $\tilde{O}(dH\sqrt{T})$ regret, where $d$ is the linear function dimension, $H$ is the length of the game and $T$ is the total number of steps in the game. To access the optimality of our algorithm, we also prove an $\tilde{\Omega}( dH\sqrt{T})$ lower bound on the regret. Our upper bound matches the lower bound up to logarithmic factors, which suggests the optimality of our algorithm.",0
"Our focus is on studying reinforcement learning in finite-horizon, two-player zero-sum Markov games with simultaneous moves. These games have a transition kernel that can be parameterized by a linear function over the current state, both players' actions, and the next state. We aim to control both players and find the Nash Equilibrium by minimizing the duality gap. Our algorithm, Nash-UCRL-VTR, is based on the principle of ""Optimism-in-Face-of-Uncertainty"" and only requires finding a computationally efficient Coarse Correlated Equilibrium (CCE). We have shown that Nash-UCRL-VTR can achieve an $\tilde{O}(dH\sqrt{T})$ regret, where $d$ is the linear function dimension, $H$ is the length of the game, and $T$ is the total number of steps in the game. To assess the optimality of our algorithm, we have also proven an $\tilde{\Omega}(dH\sqrt{T})$ lower bound on the regret. Our upper bound matches the lower bound up to logarithmic factors, indicating the optimality of our algorithm.",1
"Electric Vehicle (EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose a framework, named Multi-Agent Spatio-Temporal Reinforcement Learning (Master), for intelligently recommending public accessible charging stations by jointly considering various long-term spatiotemporal factors. Specifically, by regarding each charging station as an individual agent, we formulate this problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with the centralized attentive critic to coordinate the recommendation between geo-distributed agents. Moreover, to quantify the influence of future potential charging competition, we introduce a delayed access strategy to exploit the knowledge of future charging competition during training. After that, to effectively optimize multiple learning objectives, we extend the centralized attentive critic to multi-critics and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. Finally, extensive experiments on two real-world datasets demonstrate that Master achieves the best comprehensive performance compared with nine baseline approaches.",0
"Due to its energy and environmental sustainability, the Electric Vehicle (EV) has become a popular choice in modern transportation systems. However, EV drivers often encounter challenges finding appropriate charging spots in large cities due to limited charging infrastructure and the imbalance of charging demands. The emergence of deep reinforcement learning presents an opportunity to improve the charging experience in various aspects over a long-term horizon. This paper proposes the Multi-Agent Spatio-Temporal Reinforcement Learning (Master) framework to intelligently recommend publicly accessible charging stations by considering various long-term spatiotemporal factors. The problem is formulated as a multi-objective multi-agent reinforcement learning task, treating each charging station as an individual agent. A multi-agent actor-critic framework is developed with the centralized attentive critic to coordinate recommendation between geo-distributed agents. To quantify the influence of future potential charging competition, a delayed access strategy is introduced during training. Furthermore, the centralized attentive critic is extended to multi-critics to effectively optimize multiple learning objectives, using a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. Extensive experiments on two real-world datasets show that Master outperforms nine baseline approaches, achieving the best comprehensive performance.",1
"Reinforcement learning with sparse rewards is challenging because an agent can rarely obtain non-zero rewards and hence, gradient-based optimization of parameterized policies can be incremental and slow. Recent work demonstrated that using a memory buffer of previous successful trajectories can result in more effective policies. However, existing methods may overly exploit past successful experiences, which can encourage the agent to adopt sub-optimal and myopic behaviors. In this work, instead of focusing on good experiences with limited diversity, we propose to learn a trajectory-conditioned policy to follow and expand diverse past trajectories from a memory buffer. Our method allows the agent to reach diverse regions in the state space and improve upon the past trajectories to reach new states. We empirically show that our approach significantly outperforms count-based exploration methods (parametric approach) and self-imitation learning (parametric approach with non-parametric memory) on various complex tasks with local optima. In particular, without using expert demonstrations or resetting to arbitrary states, we achieve the state-of-the-art scores under five billion number of frames, on challenging Atari games such as Montezuma's Revenge and Pitfall.",0
"Learning through reinforcement with sparse rewards can be difficult due to the infrequency of non-zero rewards, resulting in slow gradient-based optimization of parameterized policies. Previous research has shown that using a memory buffer to store successful trajectories can lead to more effective policies. However, current methods may overuse past experiences, leading to suboptimal and narrow-minded behaviors. Our proposed approach is to learn a trajectory-conditioned policy that expands upon diverse past trajectories from the memory buffer, allowing the agent to reach diverse regions in the state space and improve upon past trajectories. Empirical results demonstrate that our method outperforms count-based exploration methods and self-imitation learning on challenging tasks, achieving state-of-the-art scores on Atari games such as Montezuma's Revenge and Pitfall without expert demonstrations or arbitrary state resetting in under five billion frames.",1
"We study reinforcement learning in an infinite-horizon average-reward setting with linear function approximation, where the transition probability function of the underlying Markov Decision Process (MDP) admits a linear form over a feature mapping of the current state, action, and next state. We propose a new algorithm UCRL2-VTR, which can be seen as an extension of the UCRL2 algorithm with linear function approximation. We show that UCRL2-VTR with Bernstein-type bonus can achieve a regret of $\tilde{O}(d\sqrt{DT})$, where $d$ is the dimension of the feature mapping, $T$ is the horizon, and $\sqrt{D}$ is the diameter of the MDP. We also prove a matching lower bound $\tilde{\Omega}(d\sqrt{DT})$, which suggests that the proposed UCRL2-VTR is minimax optimal up to logarithmic factors. To the best of our knowledge, our algorithm is the first nearly minimax optimal RL algorithm with function approximation in the infinite-horizon average-reward setting.",0
"Our focus is on reinforcement learning within an environment of infinite-horizon average-reward using linear function approximation. The transition probability function of the Markov Decision Process (MDP) underlying this setting has a linear structure over a feature mapping of the current state, action, and next state. Our proposed UCRL2-VTR algorithm builds upon the UCRL2 algorithm with linear function approximation. By implementing a Bernstein-type bonus, UCRL2-VTR achieves a regret of $\tilde{O}(d\sqrt{DT})$, where $d$ is the dimension of the feature mapping, $T$ is the horizon, and $\sqrt{D}$ is the diameter of the MDP. We prove that UCRL2-VTR is nearly minimax optimal up to logarithmic factors, supported by our matching lower bound of $\tilde{\Omega}(d\sqrt{DT})$. Our algorithm represents the first nearly minimax optimal RL algorithm with function approximation in the infinite-horizon average-reward setting, to the best of our knowledge.",1
"Training deep reinforcement learning agents on environments with multiple levels / scenes from the same task, has become essential for many applications aiming to achieve generalization and domain transfer from simulation to the real world. While such a strategy is helpful with generalization, the use of multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods, effectively continue to view this collection of scenes as a single Markov decision process (MDP), and thus learn a scene-generic value function V(s). However, we argue that the sample variance for a multi-scene environment is best minimized by treating each scene as a distinct MDP, and then learning a joint value function V(s,M) dependent on both state s and MDP M. We further demonstrate that the true joint value function for a multi-scene environment, follows a multi-modal distribution which is not captured by traditional CNN / LSTM based critic networks. To this end, we propose a dynamic value estimation (DVE) technique, which approximates the true joint value function through a sparse attention mechanism over multiple value function hypothesis / modes. The resulting agent not only shows significant improvements in the final reward score across a range of OpenAI ProcGen environments, but also exhibits enhanced navigation efficiency and provides an implicit mechanism for unsupervised state-space skill decomposition.",0
"Training deep reinforcement learning agents to perform well in environments with multiple levels or scenes of the same task has become crucial for many applications. This is because it aims to achieve generalization and domain transfer from simulation to the real world. However, using multiple scenes significantly increases the variance of samples collected for policy gradient computations, despite aiding in generalization. Current methods treat the collection of scenes as a single Markov decision process (MDP) and learn a scene-generic value function V(s). We argue that a better approach would be to consider each scene as a distinct MDP and learn a joint value function V(s,M) that depends on both state s and MDP M. We show that the true joint value function for a multi-scene environment follows a multi-modal distribution that traditional CNN / LSTM-based critic networks cannot capture. Therefore, we propose a dynamic value estimation (DVE) technique that approximates the true joint value function using a sparse attention mechanism over multiple value function hypothesis / modes. The resulting agent shows significant improvements in the final reward score across various OpenAI ProcGen environments, exhibits enhanced navigation efficiency, and provides an implicit mechanism for unsupervised state-space skill decomposition.",1
"The number of connected smart devices has been increasing exponentially for different Internet-of-Things (IoT) applications. Security has been a long run challenge in the IoT systems which has many attack vectors, security flaws and vulnerabilities. Securing billions of B connected devices in IoT is a must task to realize the full potential of IoT applications. Recently, researchers have proposed many security solutions for IoT. Machine learning has been proposed as one of the emerging solutions for IoT security and Reinforcement learning is gaining more popularity for securing IoT systems. Reinforcement learning, unlike other machine learning techniques, can learn the environment by having minimum information about the parameters to be learned. It solves the optimization problem by interacting with the environment adapting the parameters on the fly. In this paper, we present an comprehensive survey of different types of cyber-attacks against different IoT systems and then we present reinforcement learning and deep reinforcement learning based security solutions to combat those different types of attacks in different IoT systems. Furthermore, we present the Reinforcement learning for securing CPS systems (i.e., IoT with feedback and control) such as smart grid and smart transportation system. The recent important attacks and countermeasures using reinforcement learning B in IoT are also summarized in the form of tables. With this paper, readers can have a more thorough understanding of IoT security attacks and countermeasures using Reinforcement Learning, as well as research trends in this area.",0
"The number of smart devices connected to the Internet-of-Things (IoT) has grown exponentially, leading to security challenges due to multiple attack vectors, vulnerabilities, and security flaws. Securing billions of devices is crucial to fully realize the potential of IoT applications. Researchers have proposed several security solutions for IoT, with machine learning and Reinforcement learning gaining popularity. Unlike other techniques, Reinforcement learning can learn the environment with minimal information and adapt parameters on-the-fly. This paper provides a comprehensive survey of cyber-attacks against various IoT systems and presents Reinforcement learning solutions to combat them. It also highlights the use of Reinforcement learning in securing IoT systems with feedback and control, such as smart grids and transportation systems. The paper summarizes recent attacks and countermeasures using Reinforcement learning in the form of tables, enabling readers to understand IoT security issues and current research trends.",1
"Combinatorial optimization problem (COP) over graphs is a fundamental challenge in optimization. Reinforcement learning (RL) has recently emerged as a new framework to tackle these problems and has demonstrated promising results. However, most RL solutions employ a greedy manner to construct the solution incrementally, thus inevitably pose unnecessary dependency on action sequences and need a lot of problem-specific designs. We propose a general RL framework that not only exhibits state-of-the-art empirical performance but also generalizes to a variety class of COPs. Specifically, we define state as a solution to a problem instance and action as a perturbation to this solution. We utilize graph neural networks (GNN) to extract latent representations for given problem instances for state-action encoding, and then apply deep Q-learning to obtain a policy that gradually refines the solution by flipping or swapping vertex labels. Experiments are conducted on Maximum $k$-Cut and Traveling Salesman Problem and performance improvement is achieved against a set of learning-based and heuristic baselines.",0
"Optimization over graphs, known as combinatorial optimization problem (COP), is a challenging task. Reinforcement learning (RL) has recently emerged as a promising framework to address COPs. However, most existing RL solutions adopt a greedy approach to construct the solution incrementally, which leads to a reliance on action sequences and requires problem-specific designs. To overcome these limitations, we propose a general RL framework that achieves state-of-the-art empirical performance and generalizes to various COPs. Our approach defines state as a solution to a problem instance and action as a perturbation to this solution. We utilize graph neural networks (GNN) to extract latent representations for a given problem instance for state-action encoding. We then apply deep Q-learning to obtain a policy that gradually refines the solution by flipping or swapping vertex labels. We tested our framework on Maximum $k$-Cut and Traveling Salesman Problem and achieved better performance compared to learning-based and heuristic baselines.",1
"We study multi-marginal optimal transport, a generalization of optimal transport that allows us to define discrepancies between multiple measures. It provides a framework to solve multi-task learning problems and to perform barycentric averaging. However, multi-marginal distances between multiple measures are typically challenging to compute because they require estimating a transport plan with $N^P$ variables. In this paper, we address this issue in the following way: 1) we efficiently solve the one-dimensional multi-marginal Monge-Wasserstein problem for a classical cost function in closed form, and 2) we propose a higher-dimensional multi-marginal discrepancy via slicing and study its generalized metric properties. We show that computing the sliced multi-marginal discrepancy is massively scalable for a large number of probability measures with support as large as $10^7$ samples. Our approach can be applied to solving problems such as barycentric averaging, multi-task density estimation and multi-task reinforcement learning.",0
"Our focus is on multi-marginal optimal transport, which is an extension of optimal transport that enables us to measure differences between several sets of measures. This method is useful in solving multi-task learning problems and performing barycentric averaging. However, calculating multi-marginal distances between multiple measures can be challenging as it involves estimating a transport plan with a large number of variables. In this study, we tackle this issue by: 1) solving the one-dimensional multi-marginal Monge-Wasserstein problem for a classical cost function in a closed form, and 2) proposing a higher-dimensional multi-marginal discrepancy by slicing and examining its generalized metric properties. We demonstrate that computing the sliced multi-marginal discrepancy is highly efficient, even with a large number of probability measures with support as large as 10^7 samples. Our technique can be applied to various problems, including barycentric averaging, multi-task density estimation, and multi-task reinforcement learning.",1
"We consider the problem of generalization in reinforcement learning where visual aspects of the observations might differ, e.g. when there are different backgrounds or change in contrast, brightness, etc. We assume that our agent has access to only a few of the MDPs from the MDP distribution during training. The performance of the agent is then reported on new unknown test domains drawn from the distribution (e.g. unseen backgrounds). For this ""zero-shot RL"" task, we enforce invariance of the learned representations to visual domains via a domain adversarial optimization process. We empirically show that this approach allows achieving a significant generalization improvement to new unseen domains.",0
"The issue of generalization in reinforcement learning is explored, taking into account the potential variations in visual observations, such as changes in background, contrast, and brightness. Throughout the training process, our agent is limited to just a few of the MDPs from the distribution, with performance evaluated on previously unseen test domains. To address this ""zero-shot RL"" challenge, we utilize a domain adversarial optimization procedure to ensure that the learned representations remain invariant across visual domains. Our experimental results demonstrate that this approach significantly enhances generalization to new, unfamiliar domains.",1
"Due to recent empirical successes, the options framework for hierarchical reinforcement learning is gaining increasing popularity. Rather than learning from rewards which suffers from the curse of dimensionality, we consider learning an options-type hierarchical policy from expert demonstrations. Such a problem is referred to as hierarchical imitation learning. Converting this problem to parameter inference in a latent variable model, we theoretically characterize the EM approach proposed by Daniel et al. (2016). The population level algorithm is analyzed as an intermediate step, which is nontrivial due to the samples being correlated. If the expert policy can be parameterized by a variant of the options framework, then under regularity conditions, we prove that the proposed algorithm converges with high probability to a norm ball around the true parameter. To our knowledge, this is the first performance guarantee for an hierarchical imitation learning algorithm that only observes primitive state-action pairs.",0
"The options framework for hierarchical reinforcement learning is on the rise due to recent empirical successes. Rather than relying on reward-based learning, which is limited by high dimensionality, we opt for a hierarchical approach that learns from expert demonstrations. This is known as hierarchical imitation learning, which we convert to parameter inference in a latent variable model. We examine the EM approach of Daniel et al. (2016) and analyze the algorithm at the population level, which is complicated by correlated samples. If the expert policy can be parameterized using the options framework, we prove with high probability that the proposed algorithm will converge to a norm ball around the true parameter under certain conditions. This is the first performance guarantee for an hierarchical imitation learning algorithm that only observes primitive state-action pairs, to our knowledge.",1
"The low rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments.",0
"The low rank MDP is a crucial model for exploring reinforcement learning techniques in representation learning and exploration. If a representation is known, various model-free exploration strategies are available, but for an unknown representation setting, model-based algorithms are necessary, which require the ability to model all dynamics. This study introduces the first model-free representation learning algorithms for low rank MDPs. They involve a new minimax representation learning objective, which includes different tradeoffs in their statistical and computational properties. Furthermore, this learning process is combined with an exploration strategy to cover the state space without a reward. These algorithms are sample efficient, and they can handle general function approximation to scale to more complex environments.",1
"Machine learning, especially deep learning, is dramatically changing the methods associated with optical thin-film inverse design. The vast majority of this research has focused on the parameter optimization (layer thickness, and structure size) of optical thin-films. A challenging problem that arises is an automated material search. In this work, we propose a new end-to-end algorithm for optical thin-film inverse design. This method combines the ability of unsupervised learning, reinforcement learning(RL) and includes a genetic algorithm to design an optical thin-film without any human intervention. Furthermore, with several concrete examples, we have shown how one can use this technique to optimize the spectra of a multi-layer solar absorber device.",0
"The process of optical thin-film inverse design is being revolutionized by machine learning, particularly deep learning. The main focus of research has been on optimizing the parameters of optical thin-films, such as layer thickness and structure size. However, a major challenge has been automating the search for suitable materials. This study presents a novel end-to-end algorithm that utilizes unsupervised learning, reinforcement learning (RL), and a genetic algorithm to enable the design of an optical thin-film without any human involvement. The effectiveness of this technique is demonstrated through various practical examples, including the optimization of spectra for a multi-layer solar absorber device.",1
"Ubiquitous mobile computing have enabled ride-hailing services to collect vast amounts of behavioral data of riders and drivers and optimize supply and demand matching in real time. While these mobility service providers have some degree of control over the market by assigning vehicles to requests, they need to deal with the uncertainty arising from self-interested driver behavior since workers are usually free to drive when they are not assigned tasks. In this work, we formulate the problem of passenger-vehicle matching in a sparsely connected graph and proposed an algorithm to derive an equilibrium policy in a multi-agent environment. Our framework combines value iteration methods to estimate the optimal policy given expected state visitation and policy propagation to compute multi-agent state visitation frequencies. Furthermore, we developed a method to learn the driver's reward function transferable to an environment with significantly different dynamics from training data. We evaluated the robustness to changes in spatio-temporal supply-demand distributions and deterioration in data quality using a real-world taxi trajectory dataset; our approach significantly outperforms several baselines in terms of imitation accuracy. The computational time required to obtain an equilibrium policy shared by all vehicles does not depend on the number of agents, and even on the scale of real-world services, it takes only a few seconds on a single CPU.",0
"The prevalence of mobile computing has allowed ride-hailing services to accumulate substantial amounts of data on the behavior of both drivers and passengers, which they can then use to optimize the allocation of supply and demand in real-time. Despite their ability to assign vehicles to requests, these service providers must contend with the uncertainty that arises from the self-interest of drivers who are free to drive when not on a job. To address this issue, we have developed an algorithm that formulates the problem of passenger-vehicle matching in a sparsely connected graph and derives an equilibrium policy in a multi-agent environment. Our framework employs value iteration methods to estimate the optimal policy and policy propagation to calculate the multi-agent state visitation frequencies. Additionally, we have created a method for learning a driver's reward function that can be applied to environments with vastly different dynamics from the original training data. Our approach was evaluated using a real-world taxi trajectory dataset, and was found to outperform several baselines in terms of imitation accuracy, even when faced with changes in spatio-temporal supply-demand distributions and a decrease in data quality. Finally, we note that the computational time required to obtain an equilibrium policy is not dependent on the number of agents and takes only a few seconds on a single CPU, even in large-scale real-world services.",1
"Adversarial Machine Learning has emerged as a substantial subfield of Computer Science due to a lack of robustness in the models we train along with crowdsourcing practices that enable attackers to tamper with data. In the last two years, interest has surged in adversarial attacks on graphs yet the Graph Classification setting remains nearly untouched. Since a Graph Classification dataset consists of discrete graphs with class labels, related work has forgone direct gradient optimization in favor of an indirect Reinforcement Learning approach. We will study the novel problem of Data Poisoning (training time) attack on Neural Networks for Graph Classification using Reinforcement Learning Agents.",0
"Due to both the vulnerability of machine learning models and the ease with which attackers can interfere with data through crowdsourcing, Adversarial Machine Learning has become a significant subfield within Computer Science. In recent years, there has been a growing interest in adversarial attacks on graphs. However, the Graph Classification setting has remained largely unexplored. This is because Graph Classification datasets consist of discrete graphs with class labels, which makes direct gradient optimization challenging. Instead, previous research has favored an indirect Reinforcement Learning approach. Our study aims to tackle the new challenge of a Data Poisoning (training time) attack on Neural Networks for Graph Classification, using Reinforcement Learning Agents.",1
"Sample efficiency and performance in the offline setting have emerged as significant challenges of deep reinforcement learning. We introduce Q-Value Weighted Regression (QWR), a simple RL algorithm that excels in these aspects. QWR is an extension of Advantage Weighted Regression (AWR), an off-policy actor-critic algorithm that performs very well on continuous control tasks, also in the offline setting, but has low sample efficiency and struggles with high-dimensional observation spaces. We perform an analysis of AWR that explains its shortcomings and use these insights to motivate QWR. We show experimentally that QWR matches the state-of-the-art algorithms both on tasks with continuous and discrete actions. In particular, QWR yields results on par with SAC on the MuJoCo suite and - with the same set of hyperparameters - yields results on par with a highly tuned Rainbow implementation on a set of Atari games. We also verify that QWR performs well in the offline RL setting.",0
"Deep reinforcement learning faces significant challenges regarding sample efficiency and performance in offline settings. To address these issues, we introduce Q-Value Weighted Regression (QWR), an RL algorithm that excels in both aspects. QWR extends Advantage Weighted Regression (AWR), an off-policy actor-critic algorithm that performs well on continuous control tasks in offline settings, but struggles with high-dimensional observation spaces and has low sample efficiency. AWR's shortcomings are analyzed, and the insights gained are used to motivate the development of QWR. Our experimental results show that QWR matches the performance of state-of-the-art algorithms on tasks with both continuous and discrete actions. QWR produces results on par with SAC in the MuJoCo suite and produces results similar to a highly tuned Rainbow implementation on a set of Atari games using the same set of hyperparameters. We also verify that QWR performs well in offline RL settings.",1
"Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.",0
"The use of temporal abstractions in the form of options has been proven to be beneficial in speeding up the learning process of reinforcement learning (RL) agents. However, despite previous research on the subject, the challenge of discovering options by interacting with the environment still exists. This paper presents a new meta-gradient approach for discovering useful options in multi-task RL environments. Our approach involves a manager-worker decomposition of the RL agent, where the manager learns a task-dependent policy over a set of task-independent discovered-options and primitive actions to maximize rewards from the environment. The option-reward and termination functions, which define subgoals for each option, are trained using neural networks and meta-gradients to enhance their effectiveness. Empirical analysis conducted on gridworld and DeepMind Lab tasks demonstrates that our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, and the discovered options are frequently used by the agent to solve the training tasks. Additionally, the discovered options help a randomly initialized manager learn faster in completely new tasks.",1
"The principle of optimism in the face of uncertainty is one of the most widely used and successful ideas in multi-armed bandits and reinforcement learning. However, existing optimistic algorithms (primarily UCB and its variants) are often unable to deal with large context spaces. Essentially all existing well performing algorithms for general contextual bandit problems rely on weighted action allocation schemes; and theoretical guarantees for optimism-based algorithms are only known for restricted formulations. In this paper we study general contextual bandits under the realizability condition, and propose a simple generic principle to design optimistic algorithms, dubbed ""Upper Counterfactual Confidence Bounds"" (UCCB). We show that these algorithms are provably optimal and efficient in the presence of large context spaces. Key components of UCCB include: 1) a systematic analysis of confidence bounds in policy space rather than in action space; and 2) the potential function perspective that is used to express the power of optimism in the contextual setting. We further show how the UCCB principle can be extended to infinite action spaces, by constructing confidence bounds via the newly introduced notion of ""counterfactual action divergence.""",0
"Optimism is a widely used and successful principle in multi-armed bandits and reinforcement learning, but existing optimistic algorithms struggle with large context spaces. Weighted action allocation schemes are commonly used for general contextual bandit problems and optimism-based algorithms only have theoretical guarantees for restricted formulations. This paper proposes a new principle called ""Upper Counterfactual Confidence Bounds"" (UCCB) for designing optimistic algorithms in general contextual bandits under the realizability condition. UCCB is optimal and efficient for large context spaces and involves analyzing confidence bounds in policy space and utilizing the potential function perspective to express the power of optimism in the contextual setting. The UCCB principle can also be extended to infinite action spaces by constructing confidence bounds through the concept of ""counterfactual action divergence.""",1
"Offline methods for reinforcement learning have a potential to help bridge the gap between reinforcement learning research and real-world applications. They make it possible to learn policies from offline datasets, thus overcoming concerns associated with online data collection in the real-world, including cost, safety, or ethical concerns. In this paper, we propose a benchmark called RL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes data from a diverse range of domains including games (e.g., Atari benchmark) and simulated motor control problems (e.g., DM Control Suite). The datasets include domains that are partially or fully observable, use continuous or discrete actions, and have stochastic vs. deterministic dynamics. We propose detailed evaluation protocols for each domain in RL Unplugged and provide an extensive analysis of supervised learning and offline RL methods using these protocols. We will release data for all our tasks and open-source all algorithms presented in this paper. We hope that our suite of benchmarks will increase the reproducibility of experiments and make it possible to study challenging tasks with a limited computational budget, thus making RL research both more systematic and more accessible across the community. Moving forward, we view RL Unplugged as a living benchmark suite that will evolve and grow with datasets contributed by the research community and ourselves. Our project page is available on https://git.io/JJUhd.",0
"Reinforcement learning research can benefit from offline methods, which offer a solution to bridge the gap between research and real-world applications. Offline methods allow for learning from pre-existing datasets, eliminating concerns around the cost, safety, and ethical implications of online data collection. This paper introduces RL Unplugged, a benchmark designed to evaluate and compare offline RL methods across a diverse range of domains. The datasets in RL Unplugged include partially or fully observable domains, use continuous or discrete actions, and have stochastic or deterministic dynamics. Detailed evaluation protocols are provided for each domain, and an extensive analysis of supervised learning and offline RL methods is presented. The data for all tasks will be released, and all algorithms will be open-sourced. The aim of RL Unplugged is to increase reproducibility, make RL research systematic and accessible, and evolve and grow with datasets contributed by the research community. The project page can be accessed on https://git.io/JJUhd.",1
"Reinforcement Learning (RL) algorithms have led to recent successes in solving complex games, such as Atari or Starcraft, and to a huge impact in real-world applications, such as cybersecurity or autonomous driving. In the side of the drawbacks, recent works have shown how the performance of RL algorithms decreases under the influence of soft changes in the reward function. However, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning exploration strategy. In this paper, we propose to fill this gap in the literature analyzing the effects of different attack strategies based on reward perturbations, and studying the effect in the learner depending on its exploration strategy. In order to explain all the behaviors, we choose a sub-class of MDPs: episodic, stochastic goal-only-rewards MDPs, and in particular, an intelligible grid domain as a benchmark. In this domain, we demonstrate that smoothly crafting adversarial rewards are able to mislead the learner, and that using low exploration probability values, the policy learned is more robust to corrupt rewards. Finally, in the proposed learning scenario, a counterintuitive result arises: attacking at each learning episode is the lowest cost attack strategy.",0
"RL algorithms have achieved success in complex games and real-world applications. However, soft changes in the reward function can negatively affect the performance of these algorithms. There is a lack of research on the sensitivity of these disturbances based on the attack aggressiveness and learning exploration strategy. This paper aims to address this gap by analyzing the effects of different attack strategies on reward perturbations and their impact on the learner's exploration strategy. The study uses a sub-class of MDPs and an intelligible grid domain as a benchmark. The results show that adversarial rewards can mislead the learner and low exploration probability values make the policy learned more robust to corrupt rewards. Surprisingly, the lowest cost attack strategy is to attack at each learning episode.",1
"A central capability of a long-lived reinforcement learning (RL) agent is to incrementally adapt its behavior as its environment changes, and to incrementally build upon previous experiences to facilitate future learning in real-world scenarios. In this paper, we propose LifeLong Incremental Reinforcement Learning (LLIRL), a new incremental algorithm for efficient lifelong adaptation to dynamic environments. We develop and maintain a library that contains an infinite mixture of parameterized environment models, which is equivalent to clustering environment parameters in a latent space. The prior distribution over the mixture is formulated as a Chinese restaurant process (CRP), which incrementally instantiates new environment models without any external information to signal environmental changes in advance. During lifelong learning, we employ the expectation maximization (EM) algorithm with online Bayesian inference to update the mixture in a fully incremental manner. In EM, the E-step involves estimating the posterior expectation of environment-to-cluster assignments, while the M-step updates the environment parameters for future learning. This method allows for all environment models to be adapted as necessary, with new models instantiated for environmental changes and old models retrieved when previously seen environments are encountered again. Experiments demonstrate that LLIRL outperforms relevant existing methods, and enables effective incremental adaptation to various dynamic environments for lifelong learning.",0
"The ability to progressively adjust its actions according to changes in the environment and to build upon past experiences for future learning in real-world situations is a crucial skill for a long-lasting reinforcement learning (RL) agent. This paper introduces LifeLong Incremental Reinforcement Learning (LLIRL), an innovative incremental algorithm that facilitates efficient lifelong adaptation to dynamic environments. A library that contains an infinite blend of parameterized environment models is developed and maintained, which is the equivalent of clustering environment parameters in a latent space. The prior distribution over the mixture is formulated as a Chinese restaurant process (CRP), which creates new environment models incrementally without any external information to signal environmental changes in advance. During lifelong learning, the expectation maximization (EM) algorithm with online Bayesian inference is employed to update the mixture in a fully incremental manner. In EM, the E-step involves estimating the posterior expectation of environment-to-cluster assignments, while the M-step updates the environment parameters for future learning. This approach enables all environment models to be adjusted as necessary, with new models created for environmental changes and old models retrieved when previously seen environments are encountered again. Experiments demonstrate that LLIRL outperforms relevant existing methods and enables effective incremental adaptation to a variety of dynamic environments for lifelong learning.",1
"Consider the following instance of the Offline Meta Reinforcement Learning (OMRL) problem: given the complete training logs of $N$ conventional RL agents, trained on $N$ different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. Building on the recent VariBAD BRL approach, we develop an off-policy BRL method that learns to plan an exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from offline data brings a new identifiability issue we term MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modification procedures. Finally, we evaluate our framework on a diverse set of domains, including difficult sparse reward tasks, and demonstrate learning of effective exploration behavior that is qualitatively different from the exploration used by any RL agent in the data.",0
"The Offline Meta Reinforcement Learning (OMRL) problem can be exemplified in the following scenario: N conventional RL agents have been trained on N different tasks, and their training logs are available. The objective is to design a meta-agent that can efficiently maximize the reward in a new task that has not been seen before, but belongs to the same task distribution. While each conventional RL agent explored and exploited its own unique task, the meta-agent must identify patterns in the data that lead to effective exploration/exploitation in the new task. A Bayesian RL (BRL) perspective is adopted, and a Bayes-optimal policy is learned from the offline data. To achieve this, an off-policy BRL method is developed using an adaptive neural belief estimate to plan an exploration strategy. However, learning to infer this belief from offline data presents a new challenge referred to as MDP ambiguity. The problem is characterized, and potential solutions are proposed using data collection and modification procedures. The framework is evaluated on various domains, including difficult sparse reward tasks, and the results indicate that the meta-agent learns effective exploration behavior that differs from the exploration used by any RL agent in the data.",1
"Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings. However, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces. In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We derive instantiations of this new framework for both multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work that use the same environment assumptions. To further demonstrate the efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines.",0
"There are many control tasks with similar dynamics that can be modeled with a shared latent structure. One approach to improve efficiency in multi-task settings is to use Hidden-Parameter Markov Decision Processes (HiP-MDPs), but this makes strong assumptions about state observability which limit its use in real-world scenarios with complex observation spaces. This paper proposes a new framework that extends the common structure ideas from HiP-MDPs, inspired by Block MDPs, to enable more robust state abstractions. The framework is applicable to both multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings, and provides transfer and generalization bounds based on task and state similarity. The sample complexity bounds depend on the aggregate number of samples across tasks, rather than the number of tasks, which is a significant improvement over prior work. The proposed method is empirically compared to multi-task and meta-reinforcement learning baselines, and shown to be effective.",1
"A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over 4x improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",0
"This study introduces an AutoML framework called RT3, which is based on pruning and enables dynamic reconfiguration during runtime. Its purpose is to facilitate the efficient running of large Natural Language Processing (NLP) models on mobile devices with limited resources while allowing for model switching in response to changing hardware conditions. The ability to switch models is crucial for battery-powered devices, which often use dynamic voltage and frequency scaling (DVFS) to save energy and extend battery life. The study proposes a hybrid block-structured pruning (BP) and pattern pruning (PP) approach for Transformer-based models and combines hardware and software reconfiguration to maximize energy savings. RT3 consists of two optimization levels: BP for the compression of resource-constrained devices and PP to search multiple pattern sets with diverse sparsity for lightweight software reconfiguration. The latter corresponds to DVFS frequency levels for hardware reconfiguration. During runtime, RT3 can switch pattern sets within 45ms to meet real-time constraints at different frequency levels. The results show that RT3 can prolong battery life by over 4x with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",1
"The actor-critic (AC) algorithm is a popular method to find an optimal policy in reinforcement learning. In the infinite horizon scenario, the finite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms has been established recently, but under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. In contrast, this paper characterizes the convergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with actor having general policy class approximation. We show that the overall sample complexity for a mini-batch AC to attain an $\epsilon$-accurate stationary point improves the best known sample complexity of AC by an order of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$, and the overall sample complexity for a mini-batch NAC to attain an $\epsilon$-accurate globally optimal point improves the existing sample complexity of NAC by an order of $\mathcal{O}(\epsilon^{-1}/\log(1/\epsilon))$. Moreover, the sample complexity of AC and NAC characterized in this work outperforms that of policy gradient (PG) and natural policy gradient (NPG) by a factor of $\mathcal{O}((1-\gamma)^{-3})$ and $\mathcal{O}((1-\gamma)^{-4}\epsilon^{-1}/\log(1/\epsilon))$, respectively. This is the first theoretical study establishing that AC and NAC attain orderwise performance improvement over PG and NPG under infinite horizon due to the incorporation of critic.",0
"The actor-critic (AC) algorithm is a well-known approach utilized in reinforcement learning to determine the optimal policy. Recent research has established the finite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms in an infinite horizon scenario, but only under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. This paper, however, investigates the convergence rate and sample complexity of AC and NAC when Markovian sampling is employed, mini-batch data is utilized for each iteration, and the actor implements a general policy class approximation. Our findings reveal that the mini-batch AC's overall sample complexity in achieving an $\epsilon$-accurate stationary point improves the best-known sample complexity of AC by an order of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$. Similarly, the overall sample complexity of a mini-batch NAC in achieving an $\epsilon$-accurate globally optimal point improves the pre-existing sample complexity of NAC by an order of $\mathcal{O}(\epsilon^{-1}/\log(1/\epsilon))$. Furthermore, the sample complexity of AC and NAC surpasses that of policy gradient (PG) and natural policy gradient (NPG) by a factor of $\mathcal{O}((1-\gamma)^{-3})$ and $\mathcal{O}((1-\gamma)^{-4}\epsilon^{-1}/\log(1/\epsilon))$, respectively. This study is the first to demonstrate that AC and NAC exhibit orderwise performance enhancement over PG and NPG in infinite horizon due to the incorporation of critic.",1
"This paper addresses the problem of model-free reinforcement learning for Robust Markov Decision Process (RMDP) with large state spaces. The goal of the RMDP framework is to find a policy that is robust against the parameter uncertainties due to the mismatch between the simulator model and real-world settings. We first propose the Robust Least Squares Policy Evaluation algorithm, which is a multi-step online model-free learning algorithm for policy evaluation. We prove the convergence of this algorithm using stochastic approximation techniques. We then propose Robust Least Squares Policy Iteration (RLSPI) algorithm for learning the optimal robust policy. We also give a general weighted Euclidean norm bound on the error (closeness to optimality) of the resulting policy. Finally, we demonstrate the performance of our RLSPI algorithm on some standard benchmark problems.",0
"The problem of model-free reinforcement learning for Robust Markov Decision Process (RMDP) with large state spaces is addressed in this paper. The aim of the RMDP framework is to develop a policy that can withstand parameter uncertainties resulting from the differences between the simulator model and real-world situations. The paper proposes the Robust Least Squares Policy Evaluation algorithm, a multi-step online model-free learning algorithm for policy evaluation, and proves its convergence using stochastic approximation techniques. The paper also introduces the Robust Least Squares Policy Iteration (RLSPI) algorithm for learning the optimal robust policy and provides a general weighted Euclidean norm bound on the error. Finally, the RLSPI algorithm's effectiveness is demonstrated on some standard benchmark problems.",1
"Many reinforcement learning algorithms can be seen as versions of approximate policy iteration (API). While standard API often performs poorly, it has been shown that learning can be stabilized by regularizing each policy update by the KL-divergence to the previous policy. Popular practical algorithms such as TRPO, MPO, and VMPO replace regularization by a constraint on KL-divergence of consecutive policies, arguing that this is easier to implement and tune. In this work, we study this implementation choice in more detail. We compare the use of KL divergence as a constraint vs. as a regularizer, and point out several optimization issues with the widely-used constrained approach. We show that the constrained algorithm is not guaranteed to converge even on simple problem instances where the constrained problem can be solved exactly, and in fact incurs linear expected regret. With approximate implementation using softmax policies, we show that regularization can improve the optimization landscape of the original objective. We demonstrate these issues empirically on several bandit and RL environments.",0
"Various reinforcement learning algorithms can be considered as a form of approximate policy iteration (API). However, the standard API method often performs poorly. To stabilize learning, it has been suggested that each policy update be regularized using the KL-divergence to the previous policy. Nevertheless, practical algorithms like TRPO, MPO, and VMPO have replaced regularization with a constraint on the KL-divergence of consecutive policies, as it is deemed simpler to implement and tune. This study delves deeper into this implementation choice. Specifically, it compares the use of KL divergence as a constraint versus a regularizer, highlighting optimization issues with the widely-used constrained approach. Results indicate that the constrained algorithm is not guaranteed to converge, even on simple problem instances where the constrained problem can be solved exactly, and in fact incurs linear expected regret. On the other hand, regularization can enhance the optimization landscape of the original objective, especially with approximate implementation using softmax policies. These findings are supported by empirical tests on various bandit and RL environments.",1
"In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation. Project: http://vision.cs.utexas.edu/projects/audio_visual_waypoints.",0
"The audio-visual navigation process involves an agent traveling through a complex, unmapped 3D environment to locate a sound source using both visual and auditory cues. Current models have limitations in terms of the granularity of agent motion and their reliance on basic recurrent aggregations of audio observations. Our approach to audio-visual navigation utilizes reinforcement learning and introduces two new elements: dynamically set and learned waypoints within the navigation policy, and an acoustic memory that records what the agent hears as it moves and provides spatial context. These elements work together to reveal the geometry of the unmapped space. We tested our approach on two challenging datasets, Replica and Matterport3D, and found that it significantly outperforms existing models. Our project highlights the importance of learning the connections between visual and auditory data and their relation to space in audio-visual navigation. More information can be found at http://vision.cs.utexas.edu/projects/audio_visual_waypoints.",1
"Model-free reinforcement learning algorithms combined with value function approximation have recently achieved impressive performance in a variety of application domains. However, the theoretical understanding of such algorithms is limited, and existing results are largely focused on episodic or discounted Markov decision processes (MDPs). In this work, we present adaptive approximate policy iteration (AAPI), a learning scheme which enjoys a $\tilde{O}(T^{2/3})$ regret bound for undiscounted, continuing learning in uniformly ergodic MDPs. This is an improvement over the best existing bound of $\tilde{O}(T^{3/4})$ for the average-reward case with function approximation. Our algorithm and analysis rely on online learning techniques, where value functions are treated as losses. The main technical novelty is the use of a data-dependent adaptive learning rate coupled with a so-called optimistic prediction of upcoming losses. In addition to theoretical guarantees, we demonstrate the advantages of our approach empirically on several environments.",0
"Recently, the combination of model-free reinforcement learning algorithms and value function approximation has proven to be highly effective in various application domains. However, there is a limited understanding of the theoretical aspects of these algorithms, with most existing results being focused on episodic or discounted Markov decision processes (MDPs). This paper introduces adaptive approximate policy iteration (AAPI), a learning scheme that achieves a regret bound of $\tilde{O}(T^{2/3})$ for undiscounted, continuing learning in uniformly ergodic MDPs. This is a significant improvement over the best existing bound of $\tilde{O}(T^{3/4})$ for the average-reward case with function approximation. Our approach employs online learning techniques, where value functions are treated as losses, and utilizes an adaptive learning rate based on data-dependent optimistic predictions of upcoming losses. In addition to demonstrating empirical advantages in several environments, we also provide theoretical guarantees for our approach.",1
"Model-based meta-reinforcement learning (RL) methods have recently shown to be a promising approach to improving the sample efficiency of RL in multi-task settings. However, the theoretical understanding of those methods is yet to be established, and there is currently no theoretical guarantee of their performance in a real-world environment. In this paper, we analyze the performance guarantee of model-based meta-RL methods by extending the theorems proposed by Janner et al. (2019). On the basis of our theoretical results, we propose Meta-Model-Based Meta-Policy Optimization (M3PO), a model-based meta-RL method with a performance guarantee. We demonstrate that M3PO outperforms existing meta-RL methods in continuous-control benchmarks.",0
"Recently, model-based meta-reinforcement learning (RL) methods have emerged as a possible solution to enhance the sample efficiency of RL in multi-task scenarios. However, the theoretical understanding of these methods is still lacking, and there is currently no assurance of their performance in real-world environments. This paper examines the performance assurance of model-based meta-RL methods by expanding on the theorems introduced by Janner et al. (2019). Based on our theoretical findings, we introduce Meta-Model-Based Meta-Policy Optimization (M3PO), a model-based meta-RL method that guarantees performance. We demonstrate that M3PO surpasses existing meta-RL methods in continuous-control benchmarks.",1
"Differential games, in particular two-player sequential zero-sum games (a.k.a. minimax optimization), have been an important modeling tool in applied science and received renewed interest in machine learning due to many recent applications, such as adversarial training, generative models and reinforcement learning. However, existing theory mostly focuses on convex-concave functions with few exceptions. In this work, we propose two novel Newton-type algorithms for nonconvex-nonconcave minimax optimization. We prove their local convergence at strict local minimax points, which are surrogates of global solutions. We argue that our Newton-type algorithms nicely complement existing ones in that (a) they converge faster to strict local minimax points; (b) they are much more effective when the problem is ill-conditioned; (c) their computational complexity remains similar. We verify the effectiveness of our Newton-type algorithms through experiments on training GANs which are intrinsically nonconvex and ill-conditioned.",0
"Differential games, specifically two-player sequential zero-sum games, have been widely used in applied science and have recently gained attention in machine learning for their applications in adversarial training, generative models, and reinforcement learning. However, the current theory mostly focuses on convex-concave functions, with only a few exceptions. In this research, we introduce two new Newton-type algorithms for nonconvex-nonconcave minimax optimization. We demonstrate their local convergence at strict local minimax points, which are substitutes for global solutions. Our Newton-type algorithms offer several advantages over existing ones. They converge more quickly to strict local minimax points, are more effective in ill-conditioned problems, and have similar computational complexity. We validate the effectiveness of our Newton-type algorithms through experiments on training GANs, which are inherently nonconvex and ill-conditioned.",1
"In this work, we address risk-averse Bayesadaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). We show that a policy optimising CVaR in this setting is risk-averse to both the parametric uncertainty due to the prior distribution over MDPs, and the internal uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach significantly outperforms baseline approaches for this problem.",0
"This piece of research focuses on Bayes-adaptive reinforcement learning with a risk-averse approach. The aim is to optimize the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). The proposed policy optimizes CVaR, taking into account both parametric uncertainty due to the prior distribution over MDPs and internal uncertainty due to the stochastic nature of MDPs. To tackle this problem, the researchers reshape it as a two-player stochastic game and introduce an approximate algorithm that uses Monte Carlo tree search and Bayesian optimization. The experiments show that this approach outperforms other baseline methods significantly.",1
"We study the global convergence of policy optimization for finding the Nash equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end, we first investigate the landscape of LQ games, viewing it as a nonconvex-nonconcave saddle-point problem in the policy space. Specifically, we show that despite its nonconvexity and nonconcavity, zero-sum LQ games have the property that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Building upon this, we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Moreover, we show that all of these algorithms enjoy both globally sublinear and locally linear convergence rates. Simulation results are also provided to illustrate the satisfactory convergence properties of the algorithms. To the best of our knowledge, this work appears to be the first one to investigate the optimization landscape of LQ games, and provably show the convergence of policy optimization methods to the Nash equilibria. Our work serves as an initial step toward understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general.",0
"Our research focuses on examining the global convergence of policy optimization in zero-sum linear quadratic (LQ) games to find the Nash equilibria (NE). We begin by analyzing the landscape of LQ games, which we view as a nonconvex-nonconcave saddle-point problem in the policy space. Despite the nonconvexity and nonconcavity of zero-sum LQ games, we demonstrate that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Based on this finding, we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Furthermore, these algorithms demonstrate both globally sublinear and locally linear convergence rates. We also provide simulation results to showcase the successful convergence properties of the algorithms. Our research is the first to investigate the optimization landscape of LQ games and prove the convergence of policy optimization methods to the Nash equilibria. It serves as an initial step in understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general.",1
"Deep reinforcement learning (RL) algorithms are powerful tools for solving visuomotor decision tasks. However, the trained models are often difficult to interpret, because they are represented as end-to-end deep neural networks. In this paper, we shed light on the inner workings of such trained models by analyzing the pixels that they attend to during task execution, and comparing them with the pixels attended to by humans executing the same tasks. To this end, we investigate the following two questions that, to the best of our knowledge, have not been previously studied. 1) How similar are the visual features learned by RL agents and humans when performing the same task? and, 2) How do similarities and differences in these learned features explain RL agents' performance on these tasks? Specifically, we compare the saliency maps of RL agents against visual attention models of human experts when learning to play Atari games. Further, we analyze how hyperparameters of the deep RL algorithm affect the learned features and saliency maps of the trained agents. The insights provided by our results have the potential to inform novel algorithms for the purpose of closing the performance gap between human experts and deep RL agents.",0
"Visuomotor decision tasks can be solved effectively with deep reinforcement learning (RL) algorithms, but interpreting the resulting models can be challenging as they are represented as end-to-end deep neural networks. This paper aims to provide insight into the inner workings of these trained models by examining the pixels they attend to during task execution and comparing them to those attended to by humans performing the same tasks. Through this investigation, the paper addresses two important questions that have not been previously studied: 1) How similar are the visual features learned by RL agents and humans when performing the same task? and 2) How do these learned features explain the performance of RL agents on these tasks? To answer these questions, the saliency maps of RL agents are compared against visual attention models of human experts while learning to play Atari games. Additionally, the paper analyzes how hyperparameters of the deep RL algorithm affect the learned features and saliency maps of the trained agents. By shedding light on the similarities and differences between RL agents and human experts, this research has the potential to inform the development of novel algorithms that can bridge the performance gap between these two groups.",1
"Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classifier systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and efficient approaches.",0
"Reinforcement learning involves teaching agents to make optimal decisions in unfamiliar environments by exploring and exploiting collected information. This can be a complex problem to solve. Derivative-free optimization is good at solving complex problems and typically uses a sampling-and-updating framework to improve solutions iteratively, requiring a balance between exploration and exploitation. As such, derivative-free optimization shares a core issue with reinforcement learning and has been incorporated into reinforcement learning approaches, such as learning classifier systems and neuroevolution/evolutionary reinforcement learning. While such methods have been developed for decades, there is still a lack of recent surveys on the topic. This article provides a summary of derivative-free reinforcement learning methods to date, organized by parameter updating, model selection, exploration, and parallel/distributed methods. It also discusses current limitations and potential future directions, with the aim of drawing greater attention to this topic and inspiring the development of new, efficient approaches.",1
"Large-scale Web-based services present opportunities for improving UI policies based on observed user interactions. We address challenges of learning such policies through model-free offline Reinforcement Learning (RL) with off-policy training. Deployed in a production system for user authentication in a major social network, it significantly improves long-term objectives. We articulate practical challenges, compare several ML techniques, provide insights on training and evaluation of RL models, and discuss generalizations.",0
"The potential for enhancing user interface policies based on user interactions is presented by the availability of large-scale Web-based services. We tackle the obstacles of acquiring such policies through off-policy training using model-free offline Reinforcement Learning (RL). This approach has been successfully implemented in a social network's production system for user authentication and has resulted in significant improvements in long-term goals. We also highlight the practical difficulties, compare various machine learning techniques, offer guidance on RL model training and evaluation, and discuss broader applications.",1
"Using a model of the environment, reinforcement learning agents can plan their future moves and achieve superhuman performance in board games like Chess, Shogi, and Go, while remaining relatively sample-efficient. As demonstrated by the MuZero Algorithm, the environment model can even be learned dynamically, generalizing the agent to many more tasks while at the same time achieving state-of-the-art performance. Notably, MuZero uses internal state representations derived from real environment states for its predictions. In this paper, we bind the model's predicted internal state representation to the environment state via two additional terms: a reconstruction model loss and a simpler consistency loss, both of which work independently and unsupervised, acting as constraints to stabilize the learning process. Our experiments show that this new integration of reconstruction model loss and simpler consistency loss provide a significant performance increase in OpenAI Gym environments. Our modifications also enable self-supervised pretraining for MuZero, so the algorithm can learn about environment dynamics before a goal is made available.",0
"Reinforcement learning agents can utilize an environmental model to plan their future moves and achieve exceptional performance in games like Chess, Shogi, and Go, while maintaining their sample efficiency. The MuZero Algorithm showcased how a dynamically learned environmental model can broaden the agent's capabilities to handle multiple tasks while still achieving a top-notch performance. The MuZero Algorithm derives its internal state representations from actual environmental states for predictions. In this article, we introduce two additional constraints, namely, a reconstruction model loss and a simpler consistency loss, which bind the model's predicted internal state representation to the environmental state. These constraints work independently and unsupervised, stabilizing the learning process. Our experiments show that the integration of these constraints enhances the performance of MuZero in OpenAI Gym environments. Our modifications also allow for self-supervised pretraining, enabling the algorithm to learn about environmental dynamics before a goal is presented.",1
"Deep reinforcement learning (RL) has proved successful at solving challenging environments but often requires scaling to large sampling and computing resources. Furthermore, advancing RL requires tools that are flexible enough to easily prototype new methods, yet avoiding impractically slow experimental turnaround times. To this end, we present PyTorchRL, a PyTorch-based library for RL with a modular design that allows composing agents from a set of reusable and easily extendable modules. Additionally, PyTorchRL permits the definition of distributed training architectures with flexibility and independence of the Agent components. In combination, these two features can accelerate the pace at which ideas are implemented and tested, simplifying research and enabling to tackle more challenging RL problems. We present several interesting use-cases of PyTorchRL and showcase the library by obtaining the highest to-date test performance on the Obstacle Tower Unity3D challenge environment.",0
"Although deep reinforcement learning (RL) has succeeded in tackling difficult environments, it often necessitates extensive sampling and computing resources. Moreover, developing RL necessitates adaptable tools that permit the easy prototyping of new methods while avoiding slow experimental turnaround times. To address this, we introduce PyTorchRL, which is a modular PyTorch-based library for RL that allows agents to be composed from a set of reusable and easily extendable modules. In addition, PyTorchRL allows for the creation of distributed training architectures that are versatile and independent of Agent components. By combining these two features, PyTorchRL can hasten the implementation and testing of ideas, streamline research, and enable tackling more challenging RL issues. We demonstrate PyTorchRL's effectiveness by showcasing several interesting use-cases and achieving the highest test performance to date on the Obstacle Tower Unity3D challenge environment.",1
"We develop Stratified Shortest Solution Imitation Learning (3SIL) to learn equational theorem proving in a deep reinforcement learning (RL) setting. The self-trained models achieve state-of-the-art performance in proving problems generated by one of the top open conjectures in quasigroup theory, the Abelian Inner Mapping (AIM) Conjecture. To develop the methods, we first use two simpler arithmetic rewriting tasks that share tree-structured proof states and sparse rewards with the AIM problems. On these tasks, 3SIL is shown to significantly outperform several established RL and imitation learning methods. The final system is then evaluated in a standalone and cooperative mode on the AIM problems. The standalone 3SIL-trained system proves in 60 seconds more theorems (70.2%) than the complex, hand-engineered Waldmeister system (65.5%). In the cooperative mode, the final system is combined with the Prover9 system, proving in 2 seconds what standalone Prover9 proves in 60 seconds.",0
"To learn equational theorem proving in a deep reinforcement learning (RL) environment, we created Stratified Shortest Solution Imitation Learning (3SIL). Our self-trained models achieved exceptional performance in solving problems from the Abelian Inner Mapping (AIM) Conjecture, one of the most challenging open conjectures in quasigroup theory. We began by using two simpler arithmetic rewriting tasks that share similar proof states and rewards with the AIM problems to develop our methods. 3SIL outperformed several established RL and imitation learning methods on these tasks. We then evaluated the final system in standalone and cooperative modes on the AIM problems. The 3SIL-trained system solved more theorems (70.2%) in 60 seconds than the complex, hand-engineered Waldmeister system (65.5%) in standalone mode. In cooperative mode, the final system combined with the Prover9 system solved in 2 seconds what standalone Prover9 took 60 seconds to solve.",1
"The problem of finding K-nearest neighbors in the given dataset for a given query point has been worked upon since several years. In very high dimensional spaces the K-nearest neighbor search (KNNS) suffers in terms of complexity in computation of high dimensional distances. With the issue of curse of dimensionality, it gets quite tedious to reliably bank on the results of variety approximate nearest neighbor search approaches. In this paper, we survey some novel K-Nearest Neighbor Search approaches that tackles the problem of Search from the perspectives of computations, the accuracy of approximated results and leveraging parallelism to speed-up computations. We attempt to derive a relationship between the true positive and false points for a given KNNS approach. Finally, in order to evaluate the robustness of a KNNS approach against adversarial points, we propose a generic Reinforcement Learning based framework for the same.",0
"For several years, researchers have been working on the issue of identifying K-nearest neighbors in a given dataset for a specific query point. However, in high dimensional spaces, the complexity of computing high dimensional distances poses a challenge for K-nearest neighbor search (KNNS). This problem is further exacerbated by the curse of dimensionality, making it difficult to rely on the results of various approximate nearest neighbor search approaches. This paper presents several innovative approaches to KNNS that address the problem of search complexity, accuracy of approximated results, and parallelism for faster computations. The paper also explores the relationship between true positive and false points for a given KNNS approach and proposes a Reinforcement Learning based framework to evaluate the approach's robustness against adversarial points.",1
"We investigate the hardness of online reinforcement learning in fixed horizon, sparse linear Markov decision process (MDP), with a special focus on the high-dimensional regime where the ambient dimension is larger than the number of episodes. Our contribution is two-fold. First, we provide a lower bound showing that linear regret is generally unavoidable in this case, even if there exists a policy that collects well-conditioned data. The lower bound construction uses an MDP with a fixed number of states while the number of actions scales with the ambient dimension. Note that when the horizon is fixed to one, the case of linear stochastic bandits, the linear regret can be avoided. Second, we show that if the learner has oracle access to a policy that collects well-conditioned data then a variant of Lasso fitted Q-iteration enjoys a nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$ where $N$ is the number of episodes and $s$ is the sparsity level. This shows that in the large-action setting, the difficulty of learning can be attributed to the difficulty of finding a good exploratory policy.",0
"Our research focuses on the challenge of online reinforcement learning in fixed horizon, sparse linear Markov decision processes (MDPs), with a particular emphasis on the high-dimensional scenario where the ambient dimension exceeds the number of episodes. We make two significant contributions. Firstly, we demonstrate that linear regret is inevitable in this context, even when a policy is in place that collects well-conditioned data. Our lower bound construction employs an MDP with a fixed number of states and an increasing number of actions with ambient dimension scaling. It is worth noting that when the horizon is one, as in the case of linear stochastic bandits, linear regret can be avoided. Secondly, we prove that if the learner has oracle access to a policy that collects well-conditioned data, then a variant of Lasso fitted Q-iteration can achieve nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$, where $N$ denotes the number of episodes and $s$ is the sparsity level. This demonstrates that in the large-action setting, the challenge of learning is due to the difficulty of identifying a suitable exploratory policy.",1
"The human visual system contains a hierarchical sequence of modules that take part in visual perception at superordinate, basic, and subordinate categorization levels. During the last decades, various computational models have been proposed to mimic the hierarchical feed-forward processing of visual cortex, but many critical characteristics of the visual system, such actual neural processing and learning mechanisms, are ignored. Pursuing the line of biological inspiration, we propose a computational model for object recognition in different categorization levels, in which a spiking neural network equipped with the reinforcement learning rule is used as a module at each categorization level. Each module solves the object recognition problem at each categorization level, solely based on the earliest spike of class-specific neurons at its last layer, without using any external classifier. According to the required information at each categorization level, the relevant band-pass filtered images are utilized. The performance of our proposed model is evaluated by various appraisal criteria with three benchmark datasets and significant improvement in recognition accuracy of our proposed model is achieved in all experiments.",0
"The process of visual perception in humans involves a series of hierarchical modules that categorize information at different levels. While various computational models have attempted to replicate this process, they often overlook important factors such as actual neural processing and learning mechanisms. Our proposed model for object recognition takes inspiration from biology and uses a spiking neural network with reinforcement learning at each categorization level. This model requires only the earliest spike of class-specific neurons at its last layer to solve the recognition problem, without the need for external classifiers. Each module utilizes relevant band-pass filtered images based on the required information at each categorization level. We evaluated the performance of our model on three benchmark datasets and observed a significant improvement in recognition accuracy.",1
"Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance.",0
"Teaching Reinforcement Learning (RL) agents for crucial uses may not be feasible due to the dangers involved in exploration. Consequently, the agent can only use secure policies to gather previously collected data. While previous studies focused on optimizing average performance using offline data, our study concentrates on optimizing a risk-averse standard, namely the CVaR. Our study introduces the Offline Risk-Averse Actor-Critic (O-RAAC), which is a model-free RL algorithm that can learn risk-averse policies in a fully offline environment. We demonstrate that O-RAAC acquires policies with higher CVaR than risk-neutral approaches in various robot control tasks. Moreover, considering risk-averse standards guarantees distributional robustness of the average performance concerning specific distribution shifts. Our empirical findings show that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance.",1
"We propose a novel framework for value function factorization in multi-agent deep reinforcement learning (MARL) using graph neural networks (GNNs). In particular, we consider the team of agents as the set of nodes of a complete directed graph, whose edge weights are governed by an attention mechanism. Building upon this underlying graph, we introduce a mixing GNN module, which is responsible for i) factorizing the team state-action value function into individual per-agent observation-action value functions, and ii) explicit credit assignment to each agent in terms of fractions of the global team reward. Our approach, which we call GraphMIX, follows the centralized training and decentralized execution paradigm, enabling the agents to make their decisions independently once training is completed. We show the superiority of GraphMIX as compared to the state-of-the-art on several scenarios in the StarCraft II multi-agent challenge (SMAC) benchmark. We further demonstrate how GraphMIX can be used in conjunction with a recent hierarchical MARL architecture to both improve the agents' performance and enable fine-tuning them on mismatched test scenarios with higher numbers of agents and/or actions.",0
"A new framework for factorizing value functions in multi-agent deep reinforcement learning (MARL) is presented using graph neural networks (GNNs). The team of agents is seen as a complete directed graph with edge weights determined by an attention mechanism. This graph is used to introduce a mixing GNN module which performs two tasks: it factorizes the team state-action value function into individual per-agent observation-action value functions, and it assigns explicit credit to each agent by fractioning the global team reward. This approach, GraphMIX, follows a centralized training and decentralized execution paradigm, allowing the agents to act independently after training. GraphMIX is shown to outperform the state-of-the-art in various scenarios in the StarCraft II multi-agent challenge (SMAC) benchmark. Additionally, GraphMIX is used in combination with a recent hierarchical MARL architecture to improve the agents' performance and enable fine-tuning on mismatched test scenarios with higher numbers of agents and/or actions.",1
"Despite advancements in deep reinforcement learning algorithms, developing an effective exploration strategy is still an open problem. Most existing exploration strategies either are based on simple heuristics, or require the model of the environment, or train additional deep neural networks to generate imagination-augmented paths. In this paper, a revolutionary algorithm, called Policy Augmentation, is introduced. Policy Augmentation is based on a newly developed inductive matrix completion method. The proposed algorithm augments the values of unexplored state-action pairs, helping the agent take actions that will result in high-value returns while the agent is in the early episodes. Training deep reinforcement learning algorithms with high-value rollouts leads to the faster convergence of deep reinforcement learning algorithms. Our experiments show the superior performance of Policy Augmentation. The code can be found at: https://github.com/arashmahyari/PolicyAugmentation.",0
"Although deep reinforcement learning algorithms have made significant progress, the issue of creating a successful exploration strategy remains unresolved. The current exploration strategies either rely on simplistic heuristics, require an environmental model, or entail training additional deep neural networks to generate imagination-augmented paths. In this article, a revolutionary algorithm, Policy Augmentation, is introduced. Policy Augmentation employs a newly developed inductive matrix completion method to enhance the values of unexplored state-action pairs. This helps the agent make actions that will result in high-value returns, particularly in the early episodes. By training deep reinforcement learning algorithms with high-value rollouts, the convergence rate is accelerated. Our experiments illustrate the superior performance of Policy Augmentation. The code for this algorithm can be found at https://github.com/arashmahyari/PolicyAugmentation.",1
"This graduate textbook on machine learning tells a story of how patterns in data support predictions and consequential actions. Starting with the foundations of decision making, we cover representation, optimization, and generalization as the constituents of supervised learning. A chapter on datasets as benchmarks examines their histories and scientific bases. Self-contained introductions to causality, the practice of causal inference, sequential decision making, and reinforcement learning equip the reader with concepts and tools to reason about actions and their consequences. Throughout, the text discusses historical context and societal impact. We invite readers from all backgrounds; some experience with probability, calculus, and linear algebra suffices.",0
"The focus of this graduate textbook is machine learning, which highlights how data patterns can be used to make predictions and inform actions. The book covers various topics, starting with the fundamentals of decision making and moving on to explore representation, optimization, and generalization as essential components of supervised learning. Additionally, a chapter is dedicated to datasets and their historical and scientific significance as benchmarks. The book also includes comprehensive introductions to concepts such as causality, causal inference, sequential decision making, and reinforcement learning, providing readers with the necessary tools to reason about actions and their effects. Throughout the book, contextual and societal implications of machine learning are discussed. The book is designed to be accessible to readers with a basic understanding of probability, calculus, and linear algebra, irrespective of their background.",1
"The stringent requirements of mobile edge computing (MEC) applications and functions fathom the high capacity and dense deployment of MEC hosts to the upcoming wireless networks. However, operating such high capacity MEC hosts can significantly increase energy consumption. Thus, a base station (BS) unit can act as a self-powered BS. In this paper, an effective energy dispatch mechanism for self-powered wireless networks with edge computing capabilities is studied. First, a two-stage linear stochastic programming problem is formulated with the goal of minimizing the total energy consumption cost of the system while fulfilling the energy demand. Second, a semi-distributed data-driven solution is proposed by developing a novel multi-agent meta-reinforcement learning (MAMRL) framework to solve the formulated problem. In particular, each BS plays the role of a local agent that explores a Markovian behavior for both energy consumption and generation while each BS transfers time-varying features to a meta-agent. Sequentially, the meta-agent optimizes (i.e., exploits) the energy dispatch decision by accepting only the observations from each local agent with its own state information. Meanwhile, each BS agent estimates its own energy dispatch policy by applying the learned parameters from meta-agent. Finally, the proposed MAMRL framework is benchmarked by analyzing deterministic, asymmetric, and stochastic environments in terms of non-renewable energy usages, energy cost, and accuracy. Experimental results show that the proposed MAMRL model can reduce up to 11% non-renewable energy usage and by 22.4% the energy cost (with 95.8% prediction accuracy), compared to other baseline methods.",0
"Mobile edge computing (MEC) applications require high capacity and dense deployment of MEC hosts in upcoming wireless networks, which can result in increased energy consumption. To address this issue, this paper proposes using a self-powered base station (BS) unit and studying an effective energy dispatch mechanism for self-powered wireless networks with edge computing capabilities. The proposed solution involves a two-stage linear stochastic programming problem that aims to minimize the total energy consumption cost while fulfilling energy demand. A semi-distributed data-driven solution is then proposed using a multi-agent meta-reinforcement learning (MAMRL) framework. Each BS acts as a local agent, exploring a Markovian behavior for energy consumption and generation, while transferring time-varying features to a meta-agent. The meta-agent optimizes energy dispatch decisions by accepting observations from each local agent with its own state information. The proposed MAMRL framework is benchmarked using deterministic, asymmetric, and stochastic environments in terms of non-renewable energy usage, energy cost, and accuracy. Experimental results show that the proposed MAMRL model can reduce non-renewable energy usage by up to 11% and energy cost by 22.4% (with 95.8% prediction accuracy) compared to other baseline methods.",1
"Value-based methods of multi-agent reinforcement learning (MARL), especially the value decomposition methods, have been demonstrated on a range of challenging cooperative tasks. However, current methods pay little attention to the interaction between agents, which is essential to teamwork in games or real life. This limits the efficiency of value-based MARL algorithms in the two aspects: collaborative exploration and value function estimation. In this paper, we propose a novel cooperative MARL algorithm named as interactive actor-critic~(IAC), which models the interaction of agents from the perspectives of policy and value function. On the policy side, a multi-agent joint stochastic policy is introduced by adopting a collaborative exploration module, which is trained by maximizing the entropy-regularized expected return. On the value side, we use the shared attention mechanism to estimate the value function of each agent, which takes the impact of the teammates into consideration. At the implementation level, we extend the value decomposition methods to continuous control tasks and evaluate IAC on benchmark tasks including classic control and multi-agent particle environments. Experimental results indicate that our method outperforms the state-of-the-art approaches and achieves better performance in terms of cooperation.",0
"The effectiveness of value-based methods in multi-agent reinforcement learning (MARL), particularly those involving value decomposition, has been demonstrated in various challenging cooperative tasks. However, current methods do not adequately consider the interaction between agents, which is crucial in games or real-life teamwork. This deficiency results in suboptimal performance in two areas: collaborative exploration and value function estimation. To address this issue, we propose a new cooperative MARL algorithm called Interactive Actor-Critic (IAC), which models the interaction between agents by focusing on policy and value function. The policy component introduces a multi-agent joint stochastic policy that employs a collaborative exploration module to maximize the entropy-regularized expected return. The value component uses a shared attention mechanism to estimate the value function of each agent while accounting for the impact of teammates. We extend the value decomposition methods to continuous control tasks and evaluate IAC on benchmark tasks, including classic control and multi-agent particle environments. Our experimental results demonstrate that IAC outperforms state-of-the-art approaches and improves cooperation performance.",1
"With three complexes spread evenly across the Earth, NASA's Deep Space Network (DSN) is the primary means of communications as well as a significant scientific instrument for dozens of active missions around the world. A rapidly rising number of spacecraft and increasingly complex scientific instruments with higher bandwidth requirements have resulted in demand that exceeds the network's capacity across its 12 antennae. The existing DSN scheduling process operates on a rolling weekly basis and is time-consuming; for a given week, generation of the final baseline schedule of spacecraft tracking passes takes roughly 5 months from the initial requirements submission deadline, with several weeks of peer-to-peer negotiations in between. This paper proposes a deep reinforcement learning (RL) approach to generate candidate DSN schedules from mission requests and spacecraft ephemeris data with demonstrated capability to address real-world operational constraints. A deep RL agent is developed that takes mission requests for a given week as input, and interacts with a DSN scheduling environment to allocate tracks such that its reward signal is maximized. A comparison is made between an agent trained using Proximal Policy Optimization and its random, untrained counterpart. The results represent a proof-of-concept that, given a well-shaped reward signal, a deep RL agent can learn the complex heuristics used by experts to schedule the DSN. A trained agent can potentially be used to generate candidate schedules to bootstrap the scheduling process and thus reduce the turnaround cycle for DSN scheduling.",0
"NASA's Deep Space Network (DSN) comprises three complexes that are distributed across the globe, serving as a crucial communication channel and scientific instrument for numerous active missions worldwide. However, the increasing number of spacecraft and sophisticated scientific instruments with higher bandwidth requirements has led to a demand that surpasses the network's capacity across its 12 antennae. The existing DSN scheduling process is time-consuming and operates on a rolling weekly basis, taking approximately five months to generate the final baseline schedule of spacecraft tracking passes from the initial requirements submission deadline, including several weeks of peer-to-peer negotiations. This paper suggests a deep reinforcement learning (RL) approach to generate DSN schedules from mission requests and spacecraft ephemeris data, which can address real-world operational constraints. The deep RL agent is designed to receive mission requests for a given week and interact with a DSN scheduling environment to allocate tracks, maximizing its reward signal. The paper compares a Proximal Policy Optimization-trained agent with its untrained counterpart, demonstrating that a well-shaped reward signal can help a deep RL agent learn the complex heuristics used by experts to schedule the DSN. A trained agent can potentially generate candidate schedules to initiate the scheduling process, thereby reducing the turnaround cycle for DSN scheduling.",1
"In recent years, reinforcement learning has seen interest because of deep Q-Learning, where the model is a convolutional neural network. Deep Q-Learning has shown promising results in games such as Atari and AlphaGo. Instead of learning the entire Q-table, it learns an estimate of the Q function that determines a state's policy action. We use Q-Learning and deep Q-learning, to learn control policies of four constraint satisfaction games (15-Puzzle, Minesweeper, 2048, and Sudoku). 15-Puzzle is a sliding permutation puzzle and provides a challenge in addressing its large state space. Minesweeper and Sudoku involve partially observable states and guessing. 2048 is also a sliding puzzle but allows for easier state representation (compared to 15-Puzzle) and uses interesting reward shaping to solve the game. These games offer unique insights into the potential and limits of reinforcement learning. The Q agent is trained with no rules of the game, with only the reward corresponding to each state's action. Our unique contribution is in choosing the reward structure, state representation, and formulation of the deep neural network. For low shuffle, 15-Puzzle, achieves a 100% win rate, the medium and high shuffle achieve about 43% and 22% win rates respectively. On a standard 16x16 Minesweeper board, both low and high-density boards achieve close to 45% win rate, whereas medium density boards have a low win rate of 15%. For 2048, the 1024 win rate was achieved with significant ease (100%) with high win rates for 2048, 4096, 8192 and 16384 as 40%, 0.05%, 0.01% and 0.004% , respectively. The easy Sudoku games had a win rate of 7%, while medium and hard games had 2.1% and 1.2% win rates, respectively. This paper explores the environment complexity and behavior of a subset of constraint games using reward structures which can get us closer to understanding how humans learn.",0
"Reinforcement learning has gained attention due to its use of deep Q-Learning, which employs a convolutional neural network as a model. Games like Atari and AlphaGo have shown promising results with Deep Q-Learning. Rather than learning the entire Q-table, it estimates the Q function that determines a state's policy action. In this study, Q-Learning and Deep Q-Learning were employed to learn control policies for four constraint satisfaction games: 15-Puzzle, Minesweeper, 2048, and Sudoku. These games offer unique insights into the potential and limits of reinforcement learning. The Q agent was trained without any rules of the game, only using the reward corresponding to each state's action. Our contribution is in the reward structure, state representation, and formulation of the deep neural network. The results show that the 15-Puzzle had a 100% win rate for low shuffle, while medium and high shuffle had 43% and 22% win rates respectively. For Minesweeper, both low and high-density boards achieved close to a 45% win rate, whereas medium density boards had a low win rate of 15%. The 1024 win rate for 2048 was achieved with ease (100%), while the win rates for 2048, 4096, 8192, and 16384 were 40%, 0.05%, 0.01%, and 0.004% respectively. For Sudoku, easy games had a win rate of 7%, while medium and hard games had 2.1% and 1.2% win rates respectively. This study explores the complexities and behaviors of a subset of constraint games using reward structures that can help us understand how humans learn.",1
"Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.",0
"The use of reinforcement learning has allowed agents to tackle complex tasks in unfamiliar surroundings. However, manually creating reward functions can be a time-consuming, costly, and prone to human error. While competing objectives have been proposed to help agents learn without external supervision, it remains unclear how well they reflect task rewards or human behavior. To expedite the development of intrinsic objectives, we have retrospectively calculated potential objectives using pre-collected datasets of agent behavior instead of optimizing them online, and measured their correlations by analyzing them. We studied input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We discovered that all three intrinsic objectives have a stronger correlation with a human behavior similarity metric than with task reward. Furthermore, input entropy and information gain have a stronger correlation with human similarity than task reward, suggesting that intrinsic objectives can be useful in designing agents that behave similarly to human players.",1
"In this paper, we investigate the problem of scheduling and resource allocation over a time varying set of clients with heterogeneous demands.In this context, a service provider has to schedule traffic destined to users with different classes of requirements and to allocate bandwidth resources over time as a means to efficiently satisfy service demands within a limited time horizon. This is a highly intricate problem, in particular in wireless communication systems, and solutions may involve tools stemming from diverse fields, including combinatorics and constrained optimization. Although recent work has successfully proposed solutions based on Deep Reinforcement Learning (DRL), the challenging setting of heterogeneous user traffic and demands has not been addressed. We propose a deep deterministic policy gradient algorithm that combines state-of-the-art techniques, namely Distributional RL and Deep Sets, to train a model for heterogeneous traffic scheduling. We test on diverse scenarios with different time dependence dynamics, users' requirements, and resources available, demonstrating consistent results using both synthetic and real data. We evaluate the algorithm on a wireless communication setting using both synthetic and real data and show significant gains in terms of Quality of Service (QoS) defined by the classes, against state-of-the-art conventional algorithms from combinatorics, optimization and scheduling metric(e.g. Knapsack, Integer Linear Programming, Frank-Wolfe, Exponential Rule).",0
"The paper explores the challenge of efficient scheduling and allocation of resources for a diverse group of clients with varying demands over time. This issue is particularly complex in wireless communication systems and requires solutions from various fields, such as combinatorics and constrained optimization. While Deep Reinforcement Learning (DRL) has been used to address this problem, it has not yet been applied to the context of heterogeneous user traffic and demands. Therefore, the authors propose a deep deterministic policy gradient algorithm that combines Distributional RL and Deep Sets to train a model for scheduling traffic with varying requirements. The algorithm is tested on different scenarios with synthetic and real data and shows consistent results in terms of Quality of Service (QoS), surpassing traditional algorithms such as Knapsack, Integer Linear Programming, Frank-Wolfe, and Exponential Rule.",1
"How much credit (or blame) should an action taken in a state get for a future reward? This is the fundamental temporal credit assignment problem in Reinforcement Learning (RL). One of the earliest and still most widely used heuristics is to assign this credit based on a scalar coefficient $\lambda$ (treated as a hyperparameter) raised to the power of the time interval between the state-action and the reward. In this empirical paper, we explore heuristics based on more general pairwise weightings that are functions of the state in which the action was taken, the state at the time of the reward, as well as the time interval between the two. Of course it isn't clear what these pairwise weight functions should be, and because they are too complex to be treated as hyperparameters we develop a metagradient procedure for learning these weight functions during the usual RL training of a policy. Our empirical work shows that it is often possible to learn these pairwise weight functions during learning of the policy to achieve better performance than competing approaches.",0
"Reinforcement Learning (RL) faces the challenge of determining the appropriate credit or blame for a state's action in relation to a future reward. This is known as the temporal credit assignment problem. The traditional heuristic involves assigning a scalar coefficient, $\lambda$, raised to the power of the time interval between the state-action and reward. However, this paper explores alternative heuristics based on pairwise weightings that consider the state of the action and reward, as well as the time interval. Developing effective weight functions is complex and cannot be treated as hyperparameters. Therefore, a metagradient procedure is proposed to learn these functions during policy training. Empirical results demonstrate that this approach can lead to superior performance compared to other approaches.",1
"In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of $M$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least $\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, {\it i.e.,} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.",0
"The focus of our research is on minimizing regret in reinforcement learning within latent Markov Decision Processes (LMDP). In an LMDP, one of M possible MDPs is randomly selected for interaction without disclosing its identity to the agent. Our study reveals that an LMDP instance necessitates a minimum of $\Omega((SA)^M)$ episodes to obtain an approximate optimal policy. However, we establish that with satisfactory assumptions, learning effective policies can be accomplished within a polynomial number of episodes. The key factor is the separation between the MDP system dynamics. We present an efficient algorithm that provides a sublinear regret guarantee with a good initialization. Furthermore, if standard statistical sufficiency assumptions as described in the Predictive State Representation literature are met, and a reachability assumption is added, the necessity for initialization can be eliminated.",1
"Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the number of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.",0
"The input of sampled environment transitions is crucial for deep reinforcement learning (DRL) algorithms. However, current DRL benchmarks often allow for the generation of large amounts of samples, leading to perceived progress that does not necessarily correlate with improved sample efficiency. As simulating real-world processes is difficult and obtaining real-world experience is expensive, sample efficiency is a significant indicator for the practical use of DRL. To investigate progress in sample efficiency, we compared the number of samples needed by various algorithms to achieve a certain performance level in Atari games and continuous control tasks. We observed an exponential improvement in sample efficiency, with estimated doubling times ranging from 10 to 18 months on Atari, 5 to 24 months on state-based continuous control, and 4 to 9 months on pixel-based continuous control, depending on the task and performance level.",1
"When solving a complex task, humans will spontaneously form teams and to complete different parts of the whole task, respectively. Meanwhile, the cooperation between teammates will improve efficiency. However, for current cooperative MARL methods, the cooperation team is constructed through either heuristics or end-to-end blackbox optimization. In order to improve the efficiency of cooperation and exploration, we propose a structured diversification emergence MARL framework named {\sc{Rochico}} based on reinforced organization control and hierarchical consensus learning. {\sc{Rochico}} first learns an adaptive grouping policy through the organization control module, which is established by independent multi-agent reinforcement learning. Further, the hierarchical consensus module based on the hierarchical intentions with consensus constraint is introduced after team formation. Simultaneously, utilizing the hierarchical consensus module and a self-supervised intrinsic reward enhanced decision module, the proposed cooperative MARL algorithm {\sc{Rochico}} can output the final diversified multi-agent cooperative policy. All three modules are organically combined to promote the structured diversification emergence. Comparative experiments on four large-scale cooperation tasks show that {\sc{Rochico}} is significantly better than the current SOTA algorithms in terms of exploration efficiency and cooperation strength.",0
"Humans tend to form teams to tackle complex tasks and improve efficiency through cooperation. However, current cooperative MARL methods construct teams through heuristics or end-to-end blackbox optimization, which can limit their effectiveness. To address this, we propose a structured diversification emergence MARL framework called {\sc{Rochico}} that uses reinforced organization control and hierarchical consensus learning. Our framework first learns an adaptive grouping policy through the organization control module, established through independent multi-agent reinforcement learning. After team formation, we introduce the hierarchical consensus module based on hierarchical intentions with consensus constraint. Using this module and a self-supervised intrinsic reward enhanced decision module, {\sc{Rochico}} can output the final diversified multi-agent cooperative policy. Our experiments on four large-scale cooperation tasks demonstrate that {\sc{Rochico}} outperforms current SOTA algorithms in terms of exploration efficiency and cooperation strength.",1
"Bootstrapping provides a flexible and effective approach for assessing the quality of batch reinforcement learning, yet its theoretical property is less understood. In this paper, we study the use of bootstrapping in off-policy evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE) that is known to be minimax-optimal in the tabular and linear-model cases. We propose a bootstrapping FQE method for inferring the distribution of the policy evaluation error and show that this method is asymptotically efficient and distributionally consistent for off-policy statistical inference. To overcome the computation limit of bootstrapping, we further adapt a subsampling procedure that improves the runtime by an order of magnitude. We numerically evaluate the bootrapping method in classical RL environments for confidence interval estimation, estimating the variance of off-policy evaluator, and estimating the correlation between multiple off-policy evaluators.",0
"The effectiveness and flexibility of bootstrapping in assessing the quality of batch reinforcement learning is well-established, but its theoretical properties are not fully understood. This paper delves into the use of bootstrapping in off-policy evaluation (OPE), with a specific focus on the fitted Q-evaluation (FQE) method that is minimax-optimal in the tabular and linear-model cases. We introduce a bootstrapping FQE approach to determine the distribution of policy evaluation error, which is proven to be asymptotically efficient and distributionally consistent for off-policy statistical inference. To address the computational limitations of bootstrapping, we also implement a subsampling procedure that significantly improves runtime. Our numerical experiments demonstrate the effectiveness of the bootstrapping method in classical RL environments, such as confidence interval estimation, off-policy evaluator variance estimation, and correlation estimation between multiple off-policy evaluators.",1
"A central component of training in Reinforcement Learning (RL) is Experience: the data used for training. The mechanisms used to generate and consume this data have an important effect on the performance of RL algorithms.   In this paper, we introduce Reverb: an efficient, extensible, and easy to use system designed specifically for experience replay in RL. Reverb is designed to work efficiently in distributed configurations with up to thousands of concurrent clients.   The flexible API provides users with the tools to easily and accurately configure the replay buffer. It includes strategies for selecting and removing elements from the buffer, as well as options for controlling the ratio between sampled and inserted elements. This paper presents the core design of Reverb, gives examples of how it can be applied, and provides empirical results of Reverb's performance characteristics.",0
"Experience, the data utilized for training, is a crucial aspect of Reinforcement Learning (RL) training. The way in which this data is generated and consumed has a significant impact on the performance of RL algorithms. In this study, we introduce Reverb, a system specifically designed for experience replay in RL that is efficient, adaptable, and user-friendly. Reverb is capable of operating efficiently in distributed environments with numerous simultaneous users. The versatile API enables users to effortlessly configure the replay buffer by providing techniques for selecting and removing elements from the buffer and controlling the ratio between sampled and inserted elements. This paper presents the fundamental design of Reverb, showcases its applications, and presents empirical evidence of its performance characteristics.",1
"Two-dimensional nanomaterials, such as graphene, have been extensively studied because of their outstanding physical properties. Structure and geometry optimization of nanopores on such materials is beneficial for their performances in real-world engineering applications, like water desalination. However, the optimization process often involves very large number of experiments or simulations which are expensive and time-consuming. In this work, we propose a graphene nanopore optimization framework via the combination of deep reinforcement learning (DRL) and convolutional neural network (CNN) for efficient water desalination. The DRL agent controls the growth of nanopore by determining the atom to be removed at each timestep, while the CNN predicts the performance of nanoporus graphene for water desalination: the water flux and ion rejection at a certain external pressure. With the synchronous feedback from CNN-accelerated desalination performance prediction, our DRL agent can optimize the nanoporous graphene efficiently in an online manner. Molecular dynamics (MD) simulations on promising DRL-designed graphene nanopores show that they have higher water flux while maintaining rival ion rejection rate compared to the normal circular nanopores. Semi-oval shape with rough edges geometry of DRL-designed pores is found to be the key factor for their high water desalination performance. Ultimately, this study shows that DRL can be a powerful tool for material design.",0
"The physical properties of two-dimensional nanomaterials, such as graphene, have been extensively researched. Improving the structure and geometry of nanopores on these materials can enhance their performance in engineering applications, particularly in water desalination. However, the optimization process can be expensive and time-consuming due to the large number of experiments or simulations involved. This study proposes a framework for optimizing graphene nanopores using deep reinforcement learning (DRL) and convolutional neural network (CNN) to enhance water desalination efficiency. The DRL agent determines the atom to be removed at each timestep, while the CNN predicts the performance of the nanoporous graphene for water desalination. The DRL agent is optimized efficiently in an online manner, with feedback from CNN-accelerated performance prediction, resulting in promising graphene nanopores with higher water flux and comparable ion rejection rate to normal circular nanopores. The semi-oval shape with rough edges is the key to their high water desalination performance. Ultimately, this study shows that DRL can be a powerful tool for material design.",1
"Active inference may be defined as Bayesian modeling of a brain with a biologically plausible model of the agent. Its primary idea relies on the free energy principle and the prior preference of the agent. An agent will choose an action that leads to its prior preference for a future observation. In this paper, we claim that active inference can be interpreted using reinforcement learning (RL) algorithms and find a theoretical connection between them. We extend the concept of expected free energy (EFE), which is a core quantity in active inference, and claim that EFE can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical connection, we propose a simple but novel method for learning a prior preference from experts. This illustrates that the problem with inverse RL can be approached with a new perspective of active inference. Experimental results of prior preference learning show the possibility of active inference with EFE-based rewards and its application to an inverse RL problem.",0
"The process of active inference involves utilizing a biologically plausible model of the brain and implementing Bayesian modeling. Its main principle is based on the free energy principle and an agent's prior preference. Essentially, an agent selects an action that leads to the preferred future observation. This paper aims to establish a theoretical link between active inference and reinforcement learning (RL) algorithms. We propose that active inference can be interpreted through RL algorithms by expanding the concept of expected free energy (EFE) and treating it as a negative value function. We also present a new method for learning a prior preference from experts, which demonstrates that active inference can potentially address the issue with inverse RL. Our experiments on prior preference learning using EFE-based rewards provide evidence for the practical application of active inference to an inverse RL problem.",1
"Intelligent agents can cope with sensory-rich environments by learning task-agnostic state abstractions. In this paper, we propose an algorithm to approximate causal states, which are the coarsest partition of the joint history of actions and observations in partially-observable Markov decision processes (POMDP). Our method learns approximate causal state representations from RNNs trained to predict subsequent observations given the history. We demonstrate that these learned state representations are useful for learning policies efficiently in reinforcement learning problems with rich observation spaces. We connect causal states with causal feature sets from the causal inference literature, and also provide theoretical guarantees on the optimality of the continuous version of this causal state representation under Lipschitz assumptions by proving equivalence to bisimulation, a relation between behaviorally equivalent systems. This allows for lower bounds on the optimal value function of the learned representation, which is tight given certain assumptions. Finally, we empirically evaluate causal state representations using multiple partially observable tasks and compare with prior methods.",0
"This paper proposes an algorithm that enables intelligent agents to handle environments with a lot of sensory information by learning state abstractions that are not specific to any particular task. Specifically, we introduce a method for approximating causal states, which are the most general partition of the joint history of actions and observations in partially-observable Markov decision processes (POMDP). Our approach involves training RNNs to predict subsequent observations based on the history and using these models to learn approximate causal state representations. We show that these representations can effectively be used to learn policies in reinforcement learning problems with rich observation spaces. We also establish a connection between causal states and causal feature sets from the causal inference literature, and prove the optimality of the continuous version of this causal state representation under Lipschitz assumptions by establishing equivalence to bisimulation. This enables us to derive lower bounds on the optimal value function of the learned representation, which are tight under certain conditions. Finally, we evaluate the proposed causal state representations on multiple partially observable tasks and compare against existing methods.",1
"Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.",0
"The concept of Centralized Training for Decentralized Execution has become increasingly popular in the multi-agent reinforcement learning community. This approach involves training agents offline using centralized information, but allowing them to execute in a decentralized manner online. One common example of this is actor-critic methods with a centralized critic and decentralized actors. However, the implications of using a centralized critic in this context are not fully understood, despite being the standard choice for many algorithms. To address this, we conduct a formal analysis of both centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. We also empirically compare the two methods across a variety of environments to validate our theories and offer practical advice. Our results challenge current misconceptions regarding centralized critics and suggest that algorithm designers should consider the pros and cons of both centralized and decentralized critics.",1
"Exploration in environments with sparse rewards is difficult for artificial agents. Curiosity driven learning -- using feed-forward prediction errors as intrinsic rewards -- has achieved some success in these scenarios, but fails when faced with action-dependent noise sources. We present aleatoric mapping agents (AMAs), a neuroscience inspired solution modeled on the cholinergic system of the mammalian brain. AMAs aim to explicitly ascertain which dynamics of the environment are unpredictable, regardless of whether those dynamics are induced by the actions of the agent. This is achieved by generating separate forward predictions for the mean and variance of future states and reducing intrinsic rewards for those transitions with high aleatoric variance. We show AMAs are able to effectively circumvent action-dependent stochastic traps that immobilise conventional curiosity driven agents. The code for all experiments presented in this paper is open sourced: http://github.com/self-supervisor/Escaping-Stochastic-Traps-With-Aleatoric-Mapping-Agents.",0
"It is challenging for artificial agents to explore environments that have few rewards. Although curiosity-driven learning has been somewhat successful in these situations by using feed-forward prediction errors as intrinsic rewards, it is not effective when faced with noise sources that depend on the actions taken. To address this issue, we introduce aleatoric mapping agents (AMAs), a neuroscience-inspired solution based on the cholinergic system of mammalian brains. The goal of AMAs is to determine which dynamics of the environment are unpredictable, regardless of whether they are induced by the agent's actions. This is accomplished by generating separate predictions for the mean and variance of future states and reducing intrinsic rewards for transitions with high aleatoric variance. Our experiments demonstrate that AMAs can effectively avoid stochastic traps that immobilize conventional curiosity-driven agents. The code for all experiments conducted in this study is available at http://github.com/self-supervisor/Escaping-Stochastic-Traps-With-Aleatoric-Mapping-Agents.",1
"Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.",0
"Although actor-critic algorithms have demonstrated considerable success in deep reinforcement learning problems, they still face challenges with sample inefficiency in complex environments, particularly in situations where efficient exploration is difficult. These algorithms involve using a policy (the actor) and a value function (the critic), each with their own distinct approaches and motivations. However, a new approach has been introduced in this paper, which introduces a third element: the adversary. The adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, while the actor aims to differentiate itself from the adversary predictions while also learning to solve the task. This new approach encourages the actor to adopt strategies that could not have been predicted from previous trajectories, resulting in innovative behavior in tasks where rewards are scarce. Our experimental analysis demonstrates that the Adversarially Guided Actor-Critic (AGAC) algorithm leads to more thorough exploration and outperforms current state-of-the-art methods on a range of difficult exploration and procedurally-generated tasks.",1
"This work explores learning agent-agnostic synthetic environments (SEs) for Reinforcement Learning. SEs act as a proxy for target environments and allow agents to be trained more efficiently than when directly trained on the target environment. We formulate this as a bi-level optimization problem and represent an SE as a neural network. By using Natural Evolution Strategies and a population of SE parameter vectors, we train agents in the inner loop on evolving SEs while in the outer loop we use the performance on the target task as a score for meta-updating the SE population. We show empirically that our method is capable of learning SEs for two discrete-action-space tasks (CartPole-v0 and Acrobot-v1) that allow us to train agents more robustly and with up to 60% fewer steps. Not only do we show in experiments with 4000 evaluations that the SEs are robust against hyperparameter changes such as the learning rate, batch sizes and network sizes, we also show that SEs trained with DDQN agents transfer in limited ways to a discrete-action-space version of TD3 and very well to Dueling DDQN.",0
"The focus of this research is on exploring synthetic environments (SEs) that are agnostic to learning agents for Reinforcement Learning. SEs act as a substitute for target environments, making agent training more efficient. The study frames this as a bi-level optimization problem and uses a neural network to represent the SE. Natural Evolution Strategies are employed, along with a population of SE parameter vectors, to train agents on evolving SEs in the inner loop. In the outer loop, the performance on the target task is used to score meta-updating the SE population. The research demonstrates that this method can effectively learn SEs for two discrete-action-space tasks, leading to more robust agent training with up to 60% fewer steps. Furthermore, the SEs prove to be robust against hyperparameter changes, and those trained with DDQN agents transfer well to Dueling DDQN.",1
"Although reinforcement learning has been successfully applied in many domains in recent years, we still lack agents that can systematically generalize. While relational inductive biases that fit a task can improve generalization of RL agents, these biases are commonly hard-coded directly in the agent's neural architecture. In this work, we show that we can incorporate relational inductive biases, encoded in the form of relational graphs, into agents. Based on this insight, we propose Grid-to-Graph (GTG), a mapping from grid structures to relational graphs that carry useful spatial relational inductive biases when processed through a Relational Graph Convolution Network (R-GCN). We show that, with GTG, R-GCNs generalize better both in terms of in-distribution and out-of-distribution compared to baselines based on Convolutional Neural Networks and Neural Logic Machines on challenging procedurally generated environments and MinAtar. Furthermore, we show that GTG produces agents that can jointly reason over observations and environment dynamics encoded in knowledge bases.",0
"Despite the success of reinforcement learning in various domains, the agents are still lacking in their ability to generalize systematically. Although relational inductive biases can improve the generalization of RL agents, they are often hardcoded in the agent's neural architecture. This study proposes a solution to this issue by incorporating relational inductive biases in the form of relational graphs into agents. The Grid-to-Graph (GTG) mapping, which converts grid structures to relational graphs, is used to carry spatial relational inductive biases when processed through a Relational Graph Convolution Network (R-GCN). The study demonstrates that using GTG improves generalization performance compared to baselines based on Convolutional Neural Networks and Neural Logic Machines on challenging procedurally generated environments and MinAtar. Additionally, GTG produces agents that can reason over observations and environment dynamics encoded in knowledge bases.",1
"The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD($\lambda$). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",0
"The problem of determining which actions and states are responsible for a certain outcome is referred to as the credit assignment problem, which is a key research question in artificial intelligence and reinforcement learning. Although eligibility traces are useful in assigning credit to the immediate sequence of states and actions encountered by the agent, they cannot be used to assign credit to hypothetical sequences that could have led to the current state. Our study introduces expected eligibility traces, which enable updating of states and actions that could have preceded the current state, even if they did not do so on the current occasion. We explore the benefits of using expected traces over instantaneous traces in temporal-difference learning, and demonstrate that significant improvements can sometimes be achieved. We also propose a technique for smoothly transitioning between instantaneous and expected traces, using a bootstrapping mechanism that guarantees the resulting algorithm is a strict generalization of TD($\lambda$). Finally, we examine potential extensions and connections to related concepts, such as successor features.",1
"Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running a highly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be well validated before deployment. In this paper, we present a near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for policy validation. We evaluate existing offline RL algorithms on NeoRL and argue that the performance of a policy should also be compared with the deterministic version of the behavior policy, instead of the dataset reward. The empirical results demonstrate that the tested offline RL algorithms become less competitive to the deterministic policy on many datasets, and the offline policy evaluation hardly helps. The NeoRL suit can be found at http://polixir.ai/research/neorl. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.",0
"The objective of offline reinforcement learning (RL) is to develop a proficient policy by using a set of collected data without further interactions with the environment during training. Nonetheless, current offline RL benchmarks possess a substantial reality gap as they make use of large datasets gathered by highly exploratory policies, and the trained policy is assessed directly in the environment. In real-life scenarios, the use of a highly exploratory policy is not permissible for system safety, and the data available is usually limited. Furthermore, a trained policy must be appropriately validated before deployment. In this study, we introduce NeoRL, which is a nearly real-world offline RL benchmark. It encompasses datasets from different domains with controlled sizes, and additional test datasets for policy validation. We assess existing offline RL algorithms on NeoRL and argue that the performance of a policy should be evaluated against the deterministic version of the behavior policy, instead of the dataset reward. According to the empirical findings, the tested offline RL algorithms become less competitive than the deterministic policy on many datasets, and offline policy evaluation hardly proves helpful. The NeoRL package can be accessed at http://polixir.ai/research/neorl. We anticipate that this research will provide some direction for future studies and attract more attention when implementing RL in actual systems.",1
"Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.",0
"The struggle to maintain order in the face of disruptive environmental forces is a common challenge for all living organisms. We propose that this principle can be applied to the development of useful behaviors in artificial agents. Our approach, called surprise minimizing reinforcement learning (SMiRL), is an unsupervised method that involves learning a density model to evaluate stimulus surprise and improving the policy to seek out more predictable stimuli. The policy of SMiRL seeks stable and repeatable situations that counteract the environment's entropy, such as avoiding hostile agents or finding a stable pose for a bipedal robot. We demonstrate the success of SMiRL in playing games like Tetris and Doom, controlling a humanoid to avoid falls, and navigating mazes to escape enemies, without any specific task rewards. Additionally, we show that SMiRL can be combined with standard task rewards to speed up reward-driven learning.",1
"We consider convolutional neural network (CNN) ensemble learning with the objective function inspired by least action principle; it includes resource consumption component. We teach an agent to perceive images through the set of pre-trained classifiers and want the resulting dynamically configured system to unfold the computational graph with the trajectory that refers to the minimal number of operations and maximal expected accuracy. The proposed agent's architecture implicitly approximates the required classifier selection function with the help of reinforcement learning. Our experimental results prove, that if the agent exploits the dynamic (and context-dependent) structure of computations, it outperforms conventional ensemble learning.",0
"Our approach involves utilizing a CNN ensemble learning technique, with an objective function influenced by the least action principle and incorporating a resource consumption component. Our goal is to train an agent to comprehend images using pre-trained classifiers and subsequently configure a dynamic system that produces a computational graph with the least possible operations and highest expected accuracy. The agent's architecture approximates the necessary classifier selection function through reinforcement learning. Our experiments demonstrate that by leveraging the dynamic and context-specific nature of computations, our approach surpasses traditional ensemble learning methods.",1
"In recent years, $Q$-learning has become indispensable for model-free reinforcement learning (MFRL). However, it suffers from well-known problems such as under- and overestimation bias of the value, which may adversely affect the policy learning. To resolve this issue, we propose a MFRL framework that is augmented with the components of model-based RL. Specifically, we propose to estimate not only the $Q$-values but also both the transition and the reward with a shared network. We further utilize the estimated reward from the model estimators for $Q$-learning, which promotes interaction between the estimators. We show that the proposed scheme, called Model-augmented $Q$-learning (MQL), obtains a policy-invariant solution which is identical to the solution obtained by learning with true reward. Finally, we also provide a trick to prioritize past experiences in the replay buffer by utilizing model-estimation errors. We experimentally validate MQL built upon state-of-the-art off-policy MFRL methods, and show that MQL largely improves their performance and convergence. The proposed scheme is simple to implement and does not require additional training cost.",0
"$Q$-learning has become an essential tool for model-free reinforcement learning (MFRL) in recent times. However, it is plagued with well-known issues such as value under- and overestimation bias, which can have detrimental effects on policy learning. To address this problem, we propose a MFRL model that incorporates elements of model-based RL. Specifically, we recommend estimating not only the $Q$-values, but also the transition and reward using a shared network. We also use the estimated reward from model estimators for $Q$-learning, which encourages interaction between the estimators. Our Model-augmented $Q$-learning (MQL) strategy attains a policy-invariant solution that is identical to the solution obtained by learning with the actual reward. Finally, we present a method for prioritizing past experiences in the replay buffer by utilizing model-estimation errors. We experimentally demonstrate that MQL significantly improves the performance and convergence of state-of-the-art off-policy MFRL techniques. The proposed approach is simple to implement and does not necessitate additional training expenses.",1
"Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).",0
"Multi-agent reinforcement learning has faced limitations in training one model for each new task due to the fixed input and output dimensions of the model architecture. This restricts experience accumulation and transfer of learning between tasks with varying difficulties. To address this, a universal multi-agent reinforcement learning pipeline was developed by designing a single architecture to fit tasks with different observation and action configurations. A transformer-based model, UPDeT, was used to generate a flexible policy by separating the policy distribution from the input observation with an importance weight measured by the self-attention mechanism. UPDeT is a general model that can be used in any multi-agent reinforcement learning pipeline for strong generalization abilities that enable the handling of multiple tasks simultaneously. Large-scale SMAC multi-agent competitive games were used to demonstrate the superiority of UPDeT-based multi-agent reinforcement learning over state-of-the-art approaches in terms of both performance and training speed.",1
"Thompson sampling is a well-known approach for balancing exploration and exploitation in reinforcement learning. It requires the posterior distribution of value-action functions to be maintained; this is generally intractable for tasks that have a high dimensional state-action space. We derive a variational Thompson sampling approximation for DQNs which uses a deep network whose parameters are perturbed by a learned variational noise distribution. We interpret the successful NoisyNets method \cite{fortunato2018noisy} as an approximation to the variational Thompson sampling method that we derive. Further, we propose State Aware Noisy Exploration (SANE) which seeks to improve on NoisyNets by allowing a non-uniform perturbation, where the amount of parameter perturbation is conditioned on the state of the agent. This is done with the help of an auxiliary perturbation module, whose output is state dependent and is learnt end to end with gradient descent. We hypothesize that such state-aware noisy exploration is particularly useful in problems where exploration in certain \textit{high risk} states may result in the agent failing badly. We demonstrate the effectiveness of the state-aware exploration method in the off-policy setting by augmenting DQNs with the auxiliary perturbation module.",0
"The Thompson sampling method is widely used in reinforcement learning to balance exploration and exploitation. However, maintaining the posterior distribution of value-action functions can be challenging for tasks with a high dimensional state-action space. To address this issue, we propose a variational Thompson sampling approximation for DQNs, which leverages a deep network that is perturbed by a learned variational noise distribution. We show that NoisyNets can be seen as an approximation of our method. Moreover, we introduce State Aware Noisy Exploration (SANE), which improves upon NoisyNets by allowing a non-uniform perturbation based on the agent's state. This is achieved with the help of an auxiliary perturbation module that is trained end-to-end with gradient descent. We believe that this state-aware noisy exploration approach is particularly useful for problems where exploration in high-risk states can lead to significant failures. To demonstrate the effectiveness of our method, we augment DQNs with the auxiliary perturbation module and evaluate it in the off-policy setting.",1
"In the practice of sequential decision making, agents are often designed to sense state at regular intervals of $d$ time steps, $d > 1$, ignoring state information in between sensing steps. While it is clear that this practice can reduce sensing and compute costs, recent results indicate a further benefit. On many Atari console games, reinforcement learning (RL) algorithms deliver substantially better policies when run with $d > 1$ -- in fact with $d$ even as high as $180$. In this paper, we investigate the role of the parameter $d$ in RL; $d$ is called the ""frame-skip"" parameter, since states in the Atari domain are images. For evaluating a fixed policy, we observe that under standard conditions, frame-skipping does not affect asymptotic consistency. Depending on other parameters, it can possibly even benefit learning. To use $d > 1$ in the control setting, one must first specify which $d$-step open-loop action sequences can be executed in between sensing steps. We focus on ""action-repetition"", the common restriction of this choice to $d$-length sequences of the same action. We define a task-dependent quantity called the ""price of inertia"", in terms of which we upper-bound the loss incurred by action-repetition. We show that this loss may be offset by the gain brought to learning by a smaller task horizon. Our analysis is supported by experiments on different tasks and learning algorithms.",0
"Sequential decision making often involves designing agents to sense state at regular intervals of $d$ time steps, with $d > 1, and ignoring state information between sensing steps. This practice can reduce sensing and compute costs, and recent results show that reinforcement learning algorithms deliver better policies on many Atari console games when run with $d > 1$, with $d$ sometimes as high as 180. In this paper, we investigate the role of the ""frame-skip"" parameter, which is $d$ in Atari games since states are images. We find that for evaluating a fixed policy, frame-skipping does not affect asymptotic consistency under standard conditions and may even benefit learning depending on other parameters. To use $d > 1$ in the control setting, one must specify which $d$-step open-loop action sequences can be executed between sensing steps, with ""action-repetition"" being the common restriction of this choice to $d$-length sequences of the same action. We introduce the task-dependent ""price of inertia"" and upper-bound the loss incurred by action-repetition. We show that this loss may be offset by the gain brought to learning by a smaller task horizon and support our analysis with experiments on different tasks and learning algorithms.",1
"Model-based algorithms -- algorithms that explore the environment through building and utilizing an estimated model -- are widely used in reinforcement learning practice and theoretically shown to achieve optimal sample efficiency for single-agent reinforcement learning in Markov Decision Processes (MDPs). However, for multi-agent reinforcement learning in Markov games, the current best known sample complexity for model-based algorithms is rather suboptimal and compares unfavorably against recent model-free approaches. In this paper, we present a sharp analysis of model-based self-play algorithms for multi-agent Markov games. We design an algorithm -- Optimistic Nash Value Iteration (Nash-VI) for two-player zero-sum Markov games that is able to output an $\epsilon$-approximate Nash policy in $\tilde{\mathcal{O}}(H^3SAB/\epsilon^2)$ episodes of game playing, where $S$ is the number of states, $A,B$ are the number of actions for the two players respectively, and $H$ is the horizon length. This significantly improves over the best known model-based guarantee of $\tilde{\mathcal{O}}(H^4S^2AB/\epsilon^2)$, and is the first that matches the information-theoretic lower bound $\Omega(H^3S(A+B)/\epsilon^2)$ except for a $\min\{A,B\}$ factor. In addition, our guarantee compares favorably against the best known model-free algorithm if $\min \{A,B\}=o(H^3)$, and outputs a single Markov policy while existing sample-efficient model-free algorithms output a nested mixture of Markov policies that is in general non-Markov and rather inconvenient to store and execute. We further adapt our analysis to designing a provably efficient task-agnostic algorithm for zero-sum Markov games, and designing the first line of provably sample-efficient algorithms for multi-player general-sum Markov games.",0
"Algorithms that build and use an estimated model of the environment, called model-based algorithms, are commonly used in reinforcement learning for single-agent Markov Decision Processes (MDPs) and are theoretically proven to be highly sample efficient. However, these algorithms are not as effective for multi-agent reinforcement learning in Markov games and are outperformed by recent model-free approaches. In this paper, we introduce the Optimistic Nash Value Iteration (Nash-VI) algorithm for two-player zero-sum Markov games, which can output an $\epsilon$-approximate Nash policy in $\tilde{\mathcal{O}}(H^3SAB/\epsilon^2)$ episodes. This is a significant improvement over the previous best-known model-based guarantee of $\tilde{\mathcal{O}}(H^4S^2AB/\epsilon^2)$ and matches the information-theoretic lower bound except for a $\min\{A,B\}$ factor. Our algorithm also outperforms the best-known model-free algorithm if $\min\{A,B\}=o(H^3)$ and produces a single Markov policy, which is easier to store and execute than the nested mixture of Markov policies produced by existing model-free algorithms. Additionally, we adapt our analysis to design a task-agnostic algorithm for zero-sum Markov games and develop the first line of provably sample-efficient algorithms for multi-player general-sum Markov games.",1
"Using the policy gradient algorithm, we train a single-hidden-layer neural network to balance a physically accurate simulation of a single inverted pendulum. The trained weights and biases can then be transferred to a physical agent, where they are robust enough to to balance a real inverted pendulum. This hybrid approach of training a simulation allows thousands of trial runs to be completed orders of magnitude faster than would be possible in the real world, resulting in greatly reduced training time and more iterations, producing a more robust model. When compared with existing reinforcement learning methods, the resulting control is smoother, learned faster, and able to withstand forced disturbances.",0
"By employing the policy gradient algorithm, we educate a neural network with a single hidden layer to stabilize a precise simulation of a sole inverted pendulum. Once the weights and biases have been trained, they can be conveniently transferred to an actual agent, rendering them sufficiently robust to stabilize a factual inverted pendulum. This method of simulating and educating allows for the completion of thousands of trial runs much faster than what is achievable in reality, thus reducing training time significantly and enhancing iterations, leading to a more resilient model. When compared with current reinforcement learning methods, the outcome of the control is more fluent, learned at a quicker pace, and has the capability to endure forced disturbances.",1
"We study online learning in unknown Markov games, a problem that arises in episodic multi-agent reinforcement learning where the actions of the opponents are unobservable. We show that in this challenging setting, achieving sublinear regret against the best response in hindsight is statistically hard. We then consider a weaker notion of regret by competing with the \emph{minimax value} of the game, and present an algorithm that achieves a sublinear $\tilde{\mathcal{O}}(K^{2/3})$ regret after $K$ episodes. This is the first sublinear regret bound (to our knowledge) for online learning in unknown Markov games. Importantly, our regret bound is independent of the size of the opponents' action spaces. As a result, even when the opponents' actions are fully observable, our regret bound improves upon existing analysis (e.g., (Xie et al., 2020)) by an exponential factor in the number of opponents.",0
"The focus of our study is on online learning in unknown Markov games, which is a problem that arises in multi-agent reinforcement learning when the actions of opponents are not observable. We demonstrate that this is a challenging scenario where achieving sublinear regret against the best response in hindsight is statistically difficult. However, we introduce a weaker form of regret by competing with the minimax value of the game and present an algorithm that attains a sublinear regret of approximately $K^{2/3}$ after $K$ episodes. This is the first sublinear regret bound that we know of for online learning in unknown Markov games. Notably, our regret bound is independent of the size of the opponents' action spaces. Hence, even when opponents' actions are entirely observable, our regret bound surpasses the existing analyses by an exponential factor concerning the number of opponents.",1
"The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.",0
"The full batch RL, also known as the offline reinforcement learning setting, has become increasingly attractive due to the progress of RL methods, which can take advantage of large, previously-collected datasets, similar to how supervised learning has benefited from the rise of large datasets. However, current online RL benchmarks are not suitable for the offline setting, and existing offline RL benchmarks only use data generated by partially-trained agents, making it difficult to measure progress in offline RL. To address this issue, we have introduced new benchmarks that are specifically tailored for the offline setting, guided by key properties of datasets relevant to real-world applications. These benchmarks include datasets generated by hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By going beyond simple benchmark tasks and data collected by partially-trained RL agents, we have revealed significant deficiencies in existing algorithms that were previously unappreciated. To aid research in this area, we have released our benchmark tasks and datasets, along with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This provides a common starting point for the community to identify shortcomings in existing offline RL methods and collaborate towards progress in this emerging field.",1
"Experience replay enables off-policy reinforcement learning (RL) agents to utilize past experiences to maximize the cumulative reward. Prioritized experience replay that weighs experiences by the magnitude of their temporal-difference error ($|\text{TD}|$) significantly improves the learning efficiency. But how $|\text{TD}|$ is related to the importance of experience is not well understood. We address this problem from an economic perspective, by linking $|\text{TD}|$ to value of experience, which is defined as the value added to the cumulative reward by accessing the experience. We theoretically show the value metrics of experience are upper-bounded by $|\text{TD}|$ for Q-learning. Furthermore, we successfully extend our theoretical framework to maximum-entropy RL by deriving the lower and upper bounds of these value metrics for soft Q-learning, which turn out to be the product of $|\text{TD}|$ and ""on-policyness"" of the experiences. Our framework links two important quantities in RL: $|\text{TD}|$ and value of experience. We empirically show that the bounds hold in practice, and experience replay using the upper bound as priority improves maximum-entropy RL in Atari games.",0
"Off-policy reinforcement learning agents can use experience replay to optimize their cumulative reward by drawing on past experiences. The efficiency of this process can be enhanced by prioritizing experiences based on the magnitude of their temporal-difference error ($|\text{TD}|$). However, the relationship between $|\text{TD}|$ and the importance of an experience is not fully understood. To address this issue, we adopt an economic perspective and link $|\text{TD}|$ to the value of an experience, which refers to the benefit it adds to the cumulative reward. Our theoretical analysis demonstrates that the value metrics of experiences are limited by $|\text{TD}|$ in Q-learning. We also extend this framework to maximum-entropy RL and derive lower and upper bounds for the value metrics of experiences in soft Q-learning, which are determined by the product of $|\text{TD}|$ and the ""on-policyness"" of the experiences. Our approach connects two crucial aspects of RL - $|\text{TD}|$ and the value of experience - and we demonstrate through empirical evidence that experience replay using the upper bound as priority can enhance maximum-entropy RL in Atari games.",1
"Model compression is an essential technique for deploying deep neural networks (DNNs) on power and memory-constrained resources. However, existing model-compression methods often rely on human expertise and focus on parameters' local importance, ignoring the rich topology information within DNNs. In this paper, we propose a novel multi-stage graph embedding technique based on graph neural networks (GNNs) to identify the DNNs' topology and use reinforcement learning (RL) to find a suitable compression policy. We performed resource-constrained (i.e., FLOPs) channel pruning and compared our approach with state-of-the-art compression methods using over-parameterized DNNs (e.g., ResNet and VGG-16) and mobile-friendly DNNs (e.g., MobileNet and ShuffleNet). We evaluated our method on various models from typical to mobile-friendly networks, such as ResNet family, VGG-16, MobileNet-v1/v2, and ShuffleNet. The results demonstrate that our method can prune dense networks (e.g., VGG-16) by up to 80% of their original FLOPs. More importantly, our method outperformed state-of-the-art methods and achieved a higher accuracy by up to 1.84% for ShuffleNet-v1. Furthermore, following our approach, the pruned VGG-16 achieved a noticeable 1.38$\times$ speed up and 141 MB GPU memory reduction.",0
"The compression of deep neural networks (DNNs) is crucial to enable their deployment on limited power and memory resources. However, current methods for model compression rely heavily on human expertise and concentrate on the importance of individual parameters, disregarding the valuable topology information within DNNs. This paper introduces a new approach that utilizes graph neural networks (GNNs) to identify the topology of DNNs and applies reinforcement learning (RL) to determine an effective compression policy. The proposed method involves multi-stage graph embedding and has been tested on various models, including over-parameterized DNNs such as ResNet and VGG-16, and mobile-friendly DNNs like MobileNet and ShuffleNet. The results indicate that our method can reduce the original FLOPs of dense networks like VGG-16 by up to 80%, and outperforms the state-of-the-art models by achieving a higher accuracy of up to 1.84% for ShuffleNet-v1. Moreover, implementing our approach led to a notable 1.38$\times$ increase in speed and a 141 MB reduction in GPU memory for the pruned VGG-16.",1
"We study multi-objective reinforcement learning (RL) where an agent's reward is represented as a vector. In settings where an agent competes against opponents, its performance is measured by the distance of its average return vector to a target set. We develop statistically and computationally efficient algorithms to approach the associated target set. Our results extend Blackwell's approachability theorem (Blackwell, 1956) to tabular RL, where strategic exploration becomes essential. The algorithms presented are adaptive; their guarantees hold even without Blackwell's approachability condition. If the opponents use fixed policies, we give an improved rate of approaching the target set while also tackling the more ambitious goal of simultaneously minimizing a scalar cost function. We discuss our analysis for this special case by relating our results to previous works on constrained RL. To our knowledge, this work provides the first provably efficient algorithms for vector-valued Markov games and our theoretical guarantees are near-optimal.",0
"Our focus is on multi-objective reinforcement learning (RL) and the representation of an agent's reward as a vector. In situations where an agent competes against opponents, its performance is assessed based on the proximity of its average return vector to a target set. We have devised algorithms that are both statistically and computationally efficient for approaching the target set. Our study applies Blackwell's approachability theorem (1956) to tabular RL, where strategic exploration is crucial. Our algorithms are adaptable and their guarantees are valid even without Blackwell's approachability condition. When opponents use fixed policies, we have improved the rate of approaching the target set while simultaneously minimizing a scalar cost function. We have analyzed this scenario and correlated our results with prior research on constrained RL. This work is the first to provide provably efficient algorithms for vector-valued Markov games, with our theoretical guarantees being almost optimal.",1
"In this paper, we study the problem of deceptive reinforcement learning to preserve the privacy of a reward function. Reinforcement learning is the problem of finding a behaviour policy based on rewards received from exploratory behaviour. A key ingredient in reinforcement learning is a reward function, which determines how much reward (negative or positive) is given and when. However, in some situations, we may want to keep a reward function private; that is, to make it difficult for an observer to determine the reward function used. We define the problem of privacy-preserving reinforcement learning, and present two models for solving it. These models are based on dissimulation -- a form of deception that `hides the truth'. We evaluate our models both computationally and via human behavioural experiments. Results show that the resulting policies are indeed deceptive, and that participants can determine the true reward function less reliably than that of an honest agent.",0
"The focus of this paper is on the issue of deceptive reinforcement learning, which aims to maintain the confidentiality of a reward function. Reinforcement learning involves discovering a behavior policy by analyzing received rewards from exploratory behavior. A reward function is an essential component of reinforcement learning, as it determines the amount of reward given and when it is given. However, in certain cases, it may be desirable to keep the reward function private, making it challenging for an observer to determine its use. We introduce the concept of privacy-preserving reinforcement learning and propose two dissimulation-based models to address this problem. Our models are assessed through both computational and human behavioral experiments, demonstrating that the resulting policies are genuinely deceptive and that participants are less able to identify the true reward function than with an honest agent.",1
"We offer a theoretical characterization of off-policy evaluation (OPE) in reinforcement learning using function approximation for marginal importance weights and $q$-functions when these are estimated using recent minimax methods. Under various combinations of realizability and completeness assumptions, we show that the minimax approach enables us to achieve a fast rate of convergence for weights and quality functions, characterized by the critical inequality \citep{bartlett2005}. Based on this result, we analyze convergence rates for OPE. In particular, we introduce novel alternative completeness conditions under which OPE is feasible and we present the first finite-sample result with first-order efficiency in non-tabular environments, i.e., having the minimal coefficient in the leading term.",0
"Using function approximation for marginal importance weights and $q$-functions, we provide a theoretical description of off-policy evaluation (OPE) in reinforcement learning. Our analysis employs recent minimax techniques and considers various combinations of realizability and completeness assumptions. By applying the minimax approach, we can achieve a rapid convergence rate for weights and quality functions as defined by the critical inequality \citep{bartlett2005}. Using this finding, we investigate the convergence rates for OPE, including the introduction of new alternative completeness conditions that make OPE feasible. Additionally, we present the first finite-sample result with first-order efficiency in non-tabular environments, which means having the minimum coefficient in the leading term.",1
"There has been rapidly growing interest in meta-learning as a method for increasing the flexibility and sample efficiency of reinforcement learning. One problem in this area of research, however, has been a scarcity of adequate benchmark tasks. In general, the structure underlying past benchmarks has either been too simple to be inherently interesting, or too ill-defined to support principled analysis. In the present work, we introduce a new benchmark for meta-RL research, which combines structural richness with structural transparency. Alchemy is a 3D video game, implemented in Unity, which involves a latent causal structure that is resampled procedurally from episode to episode, affording structure learning, online inference, hypothesis testing and action sequencing based on abstract domain knowledge. We evaluate a pair of powerful RL agents on Alchemy and present an in-depth analysis of one of these agents. Results clearly indicate a frank and specific failure of meta-learning, providing validation for Alchemy as a challenging benchmark for meta-RL. Concurrent with this report, we are releasing Alchemy as public resource, together with a suite of analysis tools and sample agent trajectories.",0
"Meta-learning has gained considerable attention as a means of enhancing the adaptability and efficiency of reinforcement learning. However, the shortage of suitable benchmark tasks has been a significant challenge in this field. Previous benchmarks were either too simplistic or lacked clear definitions for principled analysis. To address this issue, we introduce a new benchmark for meta-RL research called ""Alchemy."" This 3D video game, developed in Unity, incorporates a procedurally resampled causal structure that enables structure learning, online inference, hypothesis testing, and action sequencing based on abstract domain knowledge. We evaluate two powerful RL agents on Alchemy and conduct an in-depth analysis of one of them. Our findings demonstrate the failure of meta-learning, underscoring the difficulty of Alchemy as a challenging benchmark for meta-RL. We are making Alchemy available as a public resource, along with a suite of analysis tools and sample agent trajectories.",1
"Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage exploration in novel states or uncertain environment dynamics. Unfortunately, methods based on intrinsic rewards often fall short in procedurally-generated environments, where a different environment is generated in each episode so that the agent is not likely to visit the same state more than once. Motivated by how humans distinguish good exploration behaviors by looking into the entire episode, we introduce RAPID, a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behaviors. We demonstrate our method on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that RAPID significantly outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance. The code is available at https://github.com/daochenzha/rapid",0
"Model-free reinforcement learning faces a persistent challenge of exploration under sparse reward. To tackle this issue, current methods utilize intrinsic rewards to incentivize exploration in new or uncertain environments. However, intrinsic reward-based methods are often insufficient in procedurally-generated environments, where each episode features a unique environment, making it unlikely for the agent to revisit the same state. Inspired by how humans evaluate exploration behaviors based on the entire episode, we propose RAPID - a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID considers each episode as a whole and provides an episodic exploration score from both per-episode and long-term perspectives. Highly scored episodes are considered good exploration behaviors and are stored in a small ranking buffer. The agent then mimics the episodes in the buffer to reproduce past successful exploration behaviors. We assess RAPID's performance on several procedurally-generated MiniGrid environments, a MiniWorld maze navigation task, and various sparse MuJoCo tasks. Our results show that RAPID surpasses intrinsic reward-based strategies in terms of sample efficiency and final performance. The code is available at https://github.com/daochenzha/rapid.",1
"This paper is a study of reinforcement learning (RL) as an optimal-control strategy for control of nonlinear valves. It is evaluated against the PID (proportional-integral-derivative) strategy, using a unified framework. RL is an autonomous learning mechanism that learns by interacting with its environment. It is gaining increasing attention in the world of control systems as a means of building optimal-controllers for challenging dynamic and nonlinear processes. Published RL research often uses open-source tools (Python and OpenAI Gym environments). We use MATLAB's recently launched (R2019a) Reinforcement Learning Toolbox to develop the valve controller; trained using the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to simulate the nonlinear valve and create the experimental test-bench for evaluation. Simulink allows industrial engineers to quickly adapt and experiment with other systems of their choice. Results indicate that the RL controller is extremely good at tracking the signal with speed and produces a lower error with respect to the reference signal. The PID, however, is better at disturbance rejection and hence provides a longer life for the valves. Successful machine learning involves tuning many hyperparameters requiring significant investment of time and efforts. We introduce ""Graded Learning"" as a simplified, application oriented adaptation of the more formal and algorithmic ""Curriculum for Reinforcement Learning"". It is shown via experiments that it helps converge the learning task of complex non-linear real world systems. Finally, experiential learnings gained from this research are corroborated against published research.",0
"The aim of this research is to evaluate the effectiveness of reinforcement learning (RL) as an optimal-control approach for nonlinear valve control, using a unified framework and comparing it to the widely-used PID strategy. RL is an autonomous learning mechanism that interacts with the environment to build optimal controllers for challenging dynamic and nonlinear processes. While open-source tools such as Python and OpenAI Gym environments are commonly used in published RL research, we use MATLAB's Reinforcement Learning Toolbox launched in R2019a, along with the DDPG algorithm and Simulink to simulate the nonlinear valve and create the experimental test-bench for evaluation. Simulink enables industrial engineers to experiment with other systems easily. Results reveal that the RL controller is highly efficient in tracking the signal with speed and lower error compared to the reference signal, while PID is better at disturbance rejection, leading to longer valve life. Tuning hyperparameters for successful machine learning requires significant time and effort. Therefore, we propose ""Graded Learning"" as a simplified, application-oriented adaptation of the formal and algorithmic ""Curriculum for Reinforcement Learning"". It is demonstrated through experiments that it facilitates the learning task of complex real-world nonlinear systems. Finally, we compare our experiential learnings to published research.",1
"We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities present in the data and exploit them to create long-term profitable trading strategies. Indeed, agents learn trading strategies able to produce stable positive returns in spite of the highly stochastic and non-stationary environment.",0
"Our active high frequency trading framework is the first of its kind and incorporates Deep Reinforcement Learning (DRL). We utilized the Proximal Policy Optimization algorithm to train DRL agents to trade one unit of Intel Corporation stock. To train the agents, we used three months of high frequency Limit Order Book data, with the last month serving as validation data. We only selected training samples with the largest price changes to maximize the signal to noise ratio in the training data. The test was conducted on the following month of data, and we utilized the Sequential Model Based Optimization technique to tune the hyperparameters. We employed three different state characterizations, each differing in their LOB-based meta-features. Upon analyzing the agents' performance on test data, we concluded that they were able to create a dynamic representation of the underlying environment, identify occasional regularities present in the data, and exploit them to develop long-term profitable trading strategies. The agents learned trading strategies that produced stable positive returns despite the highly stochastic and non-stationary environment.",1
"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.",0
"In reinforcement learning (RL), tasks such as robustness, transfer learning, unsupervised RL, and emergent complexity often require a distribution of environments to train a policy. However, creating an effective distribution of environments can be error-prone and time-consuming for developers. As an alternative, we propose Unsupervised Environment Design (UED), where developers provide environments with unknown parameters and these parameters are used to automatically generate a distribution of valid, solvable environments. Existing techniques for generating environments have limitations, such as domain randomization's inability to create structure or adapt environment difficulty to an agent's learning progress, and minimax adversarial training's tendency to create unsolvable worst-case environments. To overcome these shortcomings, we introduce a second agent, the antagonist, to work with the environment-generating adversary in generating structured and solvable environments for our protagonist agent. The adversary seeks to maximize regret, defined as the difference between the returns of the protagonist and the antagonist. We call our approach Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments show that PAIRED generates a natural curriculum of increasingly complex environments, resulting in higher zero-shot transfer performance when tested in novel environments.",1
"Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However, they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative overgeneralization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.",0
"Coordination is essential for efficient task solving in Multi-agent reinforcement learning (MARL). However, fully centralized control is often impractical for certain tasks due to the immense size of joint action spaces. Coordination graph based formalization is a possible solution, but it necessitates domain expertise in its design. To address this issue, this paper presents the deep implicit coordination graph (DICG) architecture, which includes a module for predicting the dynamic coordination graph structure and a graph neural network based module for implicitly reasoning about joint actions or values. DICG enables learning the balance between full centralization and decentralization using standard actor-critic methods, significantly enhancing coordination for domains with numerous agents. DICG is applied to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes, and it solves the relative overgeneralization pathology in predatory-prey tasks while also outperforming several MARL baselines in challenging environments such as the StarCraft II Multi-agent Challenge (SMAC) and traffic junctions.",1
"In multi-agent reinforcement learning, the problem of learning to act is particularly difficult because the policies of co-players may be heavily conditioned on information only observed by them. On the other hand, humans readily form beliefs about the knowledge possessed by their peers and leverage beliefs to inform decision-making. Such abilities underlie individual success in a wide range of Markov games, from bluffing in Poker to conditional cooperation in the Prisoner's Dilemma, to convention-building in Bridge. Classical methods are usually not applicable to complex domains due to the intractable nature of hierarchical beliefs (i.e. beliefs of other agents' beliefs). We propose a scalable method to approximate these belief structures using recursive deep generative models, and to use the belief models to obtain representations useful to acting in complex tasks. Our agents trained with belief models outperform model-free baselines with equivalent representational capacity using common training paradigms. We also show that higher-order belief models outperform agents with lower-order models.",0
"Learning to act in multi-agent reinforcement learning is an arduous task because co-players' policies may rely heavily on information that only they have observed. Conversely, humans have an innate ability to deduce their peers' knowledge and utilize it to make decisions, which leads to their success in various Markov games such as Poker, Prisoner's Dilemma, and Bridge. However, classical methods are inadequate for complex domains because of the intricate nature of hierarchical beliefs. To overcome this, we present a scalable approach that employs recursive deep generative models to approximate these belief structures, allowing for useful representations for acting in complex tasks. Our agents trained with belief models surpass model-free baselines with the same representational capacity, and higher-order belief models outperform agents with lower-order models.",1
"The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, partially due to the substantial challenge of performing large-scale in silico experiments on evolution and learning. We introduce Deep Evolutionary Reinforcement Learning (DERL): a novel computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments using only low level egocentric sensory information. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the lifetime of their descendants. In agents that learn and evolve in complex environments, this result constitutes the first demonstration of a long-conjectured morphological Baldwin effect. Third, our experiments suggest a mechanistic basis for both the Baldwin effect and the emergence of morphological intelligence through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.",0
"The diversity of morphological forms in animals is a result of the intertwined processes of learning and evolution in complex environmental niches. Animal intelligence is deeply embodied in these evolved morphologies. However, the principles governing the relations between environmental complexity, evolved morphology, and the learnability of intelligent control are still not fully understood, partially due to the difficulty of performing large-scale experiments on evolution and learning. To address this challenge, we introduce a new computational framework called Deep Evolutionary Reinforcement Learning (DERL). This framework can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments using only low-level sensory information. By leveraging DERL, we demonstrate several important relationships between environmental complexity, morphological intelligence, and the learnability of control. First, we show that environmental complexity fosters the evolution of morphological intelligence, as measured by the ability of a morphology to facilitate the learning of novel tasks. Second, we show that evolution selects for morphologies that learn faster, allowing behaviors learned late in the lifetime of early ancestors to be expressed early in the lifetime of their descendants. This result is the first demonstration of a long-conjectured morphological Baldwin effect in agents that learn and evolve in complex environments. Third, our experiments suggest a mechanistic basis for both the Baldwin effect and the emergence of morphological intelligence through the evolution of more physically stable and energy-efficient morphologies that facilitate learning and control.",1
"Reinforcement learning algorithms are typically geared towards optimizing the expected return of an agent. However, in many practical applications, low variance in the return is desired to ensure the reliability of an algorithm. In this paper, we propose on-policy and off-policy actor-critic algorithms that optimize a performance criterion involving both mean and variance in the return. Previous work uses the second moment of return to estimate the variance indirectly. Instead, we use a much simpler recently proposed direct variance estimator which updates the estimates incrementally using temporal difference methods. Using the variance-penalized criterion, we guarantee the convergence of our algorithm to locally optimal policies for finite state action Markov decision processes. We demonstrate the utility of our algorithm in tabular and continuous MuJoCo domains. Our approach not only performs on par with actor-critic and prior variance-penalization baselines in terms of expected return, but also generates trajectories which have lower variance in the return.",0
"Reinforcement learning algorithms are designed to maximize the expected return of an agent, but in practical applications, it is often necessary to prioritize low variance in returns for the algorithm to be reliable. This study proposes on-policy and off-policy actor-critic algorithms that optimize a performance criterion which considers both the mean and variance in returns. Instead of using the second moment of return to estimate variance indirectly, the study uses a direct variance estimator that updates estimates incrementally using temporal difference methods. The variance-penalized criterion guarantees that the algorithm converges to locally optimal policies for finite state action Markov decision processes. The proposed algorithm is tested on tabular and continuous MuJoCo domains and outperforms prior variance-penalization baselines in generating trajectories with lower variance in returns while performing on par in terms of expected return.",1
"In online learning an algorithm plays against an environment with losses possibly picked by an adversary at each round. The generality of this framework includes problems that are not adversarial, for example offline optimization, or saddle point problems (i.e. min max optimization). However, online algorithms are typically not designed to leverage additional structure present in non-adversarial problems. Recently, slight modifications to well-known online algorithms such as optimism and adaptive step sizes have been used in several domains to accelerate online learning -- recovering optimal rates in offline smooth optimization, and accelerating convergence to saddle points or social welfare in smooth games. In this work we introduce optimism and adaptive stepsizes to Lagrangian hedging, a class of online algorithms that includes regret-matching, and hedge (i.e. multiplicative weights). Our results include: a general general regret bound; a path length regret bound for a fixed smooth loss, applicable to an optimistic variant of regret-matching and regret-matching+; optimistic regret bounds for $\Phi$ regret, a framework that includes external, internal, and swap regret; and optimistic bounds for a family of algorithms that includes regret-matching+ as a special case.",0
"The concept of online learning involves an algorithm competing against an environment that could potentially have losses selected by an adversary during each round. While this framework covers a wide range of problems, including those that are not adversarial, such as offline optimization or min max optimization, traditional online algorithms do not usually make use of the additional structure found in non-adversarial problems. However, recent modifications to well-known online algorithms, such as optimism and adaptive step sizes, have been implemented in various domains to speed up online learning. These modifications have been successful in achieving optimal rates in offline smooth optimization and improving convergence in smooth games. This paper focuses on applying optimism and adaptive step sizes to Lagrangian hedging, a class of online algorithms that includes regret-matching and hedge. Our study includes several results, such as a general regret bound, a path length regret bound for a fixed smooth loss, optimistic regret bounds for $\Phi$ regret, and optimistic bounds for a group of algorithms that includes regret-matching+.",1
"We consider the problem of offline reinforcement learning (RL) -- a well-motivated setting of RL that aims at policy optimization using only historical data. Despite its wide applicability, theoretical understandings of offline RL, such as its optimal sample complexity, remain largely open even in basic settings such as \emph{tabular} Markov Decision Processes (MDPs).   In this paper, we propose Off-Policy Double Variance Reduction (OPDVR), a new variance reduction based algorithm for offline RL. Our main result shows that OPDVR provably identifies an $\epsilon$-optimal policy with $\widetilde{O}(H^2/d_m\epsilon^2)$ episodes of offline data in the finite-horizon stationary transition setting, where $H$ is the horizon length and $d_m$ is the minimal marginal state-action distribution induced by the behavior policy. This improves over the best known upper bound by a factor of $H$. Moreover, we establish an information-theoretic lower bound of $\Omega(H^2/d_m\epsilon^2)$ which certifies that OPDVR is optimal up to logarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal sample complexity under alternative settings such as the finite-horizon MDPs with non-stationary transitions and the infinite horizon MDPs with discounted rewards.",0
"The focus of this paper is offline reinforcement learning (RL), which is a method of optimizing policies using historical data. Despite its broad usefulness, offline RL's optimal sample complexity, even in basic tabular Markov Decision Processes (MDPs), is not well understood. The authors present a new algorithm, Off-Policy Double Variance Reduction (OPDVR), that uses variance reduction to address this problem. The main result of the paper is that OPDVR identifies an $\epsilon$-optimal policy with $\widetilde{O}(H^2/d_m\epsilon^2)$ episodes of offline data in the finite-horizon stationary transition setting, where $H$ is the horizon length and $d_m$ is the minimal marginal state-action distribution induced by the behavior policy. This is an improvement over the best known upper bound by a factor of $H$, and an information-theoretic lower bound of $\Omega(H^2/d_m\epsilon^2)$ certifies that OPDVR is optimal up to logarithmic factors. Additionally, OPDVR achieves rate-optimal sample complexity under alternative settings such as the finite-horizon MDPs with non-stationary transitions and the infinite horizon MDPs with discounted rewards.",1
"Multi-agent reinforcement learning (MARL) has been increasingly used in a wide range of safety-critical applications, which require guaranteed safety (e.g., no unsafe states are ever visited) during the learning process.Unfortunately, current MARL methods do not have safety guarantees. Therefore, we present two shielding approaches for safe MARL. In centralized shielding, we synthesize a single shield to monitor all agents' joint actions and correct any unsafe action if necessary. In factored shielding, we synthesize multiple shields based on a factorization of the joint state space observed by all agents; the set of shields monitors agents concurrently and each shield is only responsible for a subset of agents at each step.Experimental results show that both approaches can guarantee the safety of agents during learning without compromising the quality of learned policies; moreover, factored shielding is more scalable in the number of agents than centralized shielding.",0
"The use of multi-agent reinforcement learning (MARL) has become more prevalent in applications with safety concerns, where it is essential to avoid unsafe states during the learning process. Unfortunately, current methods for MARL do not offer safety guarantees. To address this issue, we propose two approaches for safe MARL: centralized shielding and factored shielding. In centralized shielding, we create a single shield that monitors the joint actions of all agents and corrects any unsafe actions. In factored shielding, we create several shields based on a factorization of the joint state space observed by all agents. The set of shields monitors agents concurrently, and each shield is responsible for a subset of agents at each step. Our experiments prove that both approaches ensure the safety of agents during learning without compromising the quality of learned policies. Furthermore, factored shielding is more scalable in the number of agents than centralized shielding.",1
"This paper proposes a multi-agent reinforcement learning based medium access framework for wireless networks. The access problem is formulated as a Markov Decision Process (MDP), and solved using reinforcement learning with every network node acting as a distributed learning agent. The solution components are developed step by step, starting from a single-node access scenario in which a node agent incrementally learns to control MAC layer packet loads for reining in self-collisions. The strategy is then scaled up for multi-node fully-connected scenarios by using more elaborate reward structures. It also demonstrates preliminary feasibility for more general partially connected topologies. It is shown that by learning to adjust MAC layer transmission probabilities, the protocol is not only able to attain theoretical maximum throughput at an optimal load, but unlike classical approaches, it can also retain that maximum throughput at higher loading conditions. Additionally, the mechanism is agnostic to heterogeneous loading while preserving that feature. It is also shown that access priorities of the protocol across nodes can be parametrically adjusted. Finally, it is also shown that the online learning feature of reinforcement learning is able to make the protocol adapt to time-varying loading conditions.",0
"In this article, a framework for medium access in wireless networks is proposed, which is based on multi-agent reinforcement learning. The access issue is presented as a Markov Decision Process (MDP) and is resolved through reinforcement learning, with every node in the network working as a distributed learning agent. The solution is developed in stages, starting with a single-node access scenario where the agent learns to control MAC layer packet loads to prevent self-collisions. The approach is then extended to multi-node fully-connected scenarios, using more complex reward structures, and partial connectivity scenarios are also explored. The protocol is able to achieve maximum theoretical throughput by adjusting MAC layer transmission probabilities, and this throughput is maintained even at higher loading conditions. The protocol is insensitive to heterogeneous loading and enables access priorities to be adjusted parametrically. Furthermore, the protocol adapts to time-varying loading conditions through online learning.",1
"In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of ""well-behaved"" neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered.",0
"Reinforcement learning often involves impractical direct estimations of individual states, especially in continuous-state systems. Instead, researchers rely on state similarity, whether explicitly or implicitly, to create models that generalize beyond a limited set of samples. The choice of state similarity and the resulting neighborhoods and topologies are crucial to algorithm performance. Some recent work assumes well-behaved neighborhoods without specifying their topology, but this paper introduces a single formalism for defining these topologies using metrics. The paper establishes a hierarchy among these metrics and demonstrates their theoretical effects on the Markov Decision Process that defines the reinforcement learning problem. Empirical evaluations highlight the differences between the metrics considered.",1
"In many risk-aware and multi-objective reinforcement learning settings, the utility of the user is derived from the single execution of a policy. In these settings, making decisions based on the average future returns is not suitable. For example, in a medical setting a patient may only have one opportunity to treat their illness. When making a decision, just the expected return -- known in reinforcement learning as the value -- cannot account for the potential range of adverse or positive outcomes a decision may have. Our key insight is that we should use the distribution over expected future returns differently to represent the critical information that the agent requires at decision time. In this paper, we propose Distributional Monte Carlo Tree Search, an algorithm that learns a posterior distribution over the utility of the different possible returns attainable from individual policy executions, resulting in good policies for both risk-aware and multi-objective settings. Moreover, our algorithm outperforms the state-of-the-art in multi-objective reinforcement learning for the expected utility of the returns.",0
"The user's utility in risk-aware and multi-objective reinforcement learning is often based on a single execution of a policy. This approach does not work well when decisions must be made based on potential future outcomes, as in medical settings where patients may only have one chance to treat their illness. Simply relying on the expected return cannot capture the full range of possible outcomes. Our approach is to use the distribution over expected future returns to provide the agent with critical decision-making information. Our paper proposes the Distributional Monte Carlo Tree Search algorithm, which learns a posterior distribution over the utility of possible returns from policy executions. This algorithm performs better than the current state-of-the-art in multi-objective reinforcement learning by optimizing the expected utility of returns.",1
"Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn.   OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.",0
"Reinforcement Learning (RL) is a well-known machine learning method for sequential decision tasks such as game playing and robotics control. However, RL agents typically require large amounts of data and have lengthy learning times because they initially act randomly. To improve the RL agent's learning in complex tasks, this article suggests that an external teacher can be of significant help. OpenAI Gym is a widely-used framework for RL research, providing numerous standard environments and agents and making RL research more accessible. This article presents our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and its design choices. The framework aims to facilitate human-RL research by lowering the entry barrier so that more researchers can examine different ways in which human teachers could assist RL agents, such as learning from feedback, demonstrations, or curriculum learning.",1
"In this paper, we consider solving discounted Markov Decision Processes (MDPs) under the constraint that the resulting policy is stabilizing. In practice MDPs are solved based on some form of policy approximation. We will leverage recent results proposing to use Model Predictive Control (MPC) as a structured policy in the context of Reinforcement Learning to make it possible to introduce stability requirements directly inside the MPC-based policy. This will restrict the solution of the MDP to stabilizing policies by construction. The stability theory for MPC is most mature for the undiscounted MPC case. Hence, we will first show in this paper that stable discounted MDPs can be reformulated as undiscounted ones. This observation will entail that the MPC-based policy with stability requirements will produce the optimal policy for the discounted MDP if it is stable, and the best stabilizing policy otherwise.",0
"The objective of this study is to address the solution of discounted Markov Decision Processes (MDPs) while ensuring that the resultant policy is stabilizing. In practice, MDPs are resolved by approximating the policy. To introduce stability requirements into the MPC-based policy, we will employ recent research suggesting the use of Model Predictive Control (MPC) as a structured policy in the context of Reinforcement Learning. This will ensure that the solution of the MDP is restricted to stabilizing policies. However, the stability theory for MPC is most developed for the undiscounted MPC scenario. Therefore, we will initially demonstrate that stable discounted MDPs can be transformed into undiscounted ones. This observation will imply that the MPC-based policy with stability requirements will yield the optimal policy for the discounted MDP if it is stable, and the best stabilizing policy if it is not stable.",1
"The area of Smart Power Grids needs to constantly improve its efficiency and resilience, to pro-vide high quality electrical power, in a resistant grid, managing faults and avoiding failures. Achieving this requires high component reliability, adequate maintenance, and a studied failure occurrence. Correct system operation involves those activities, and novel methodologies to detect, classify, and isolate faults and failures, model and simulate processes with predictive algorithms and analytics (using data analysis and asset condition to plan and perform activities). We show-case the application of a complex-adaptive, self-organizing modeling method, Probabilistic Boolean Networks (PBN), as a way towards the understanding of the dynamics of smart grid devices, and to model and characterize their behavior. This work demonstrates that PBNs are is equivalent to the standard Reinforcement Learning Cycle, in which the agent/model has an inter-action with its environment and receives feedback from it in the form of a reward signal. Differ-ent reward structures were created in order to characterize preferred behavior. This information can be used to guide the PBN to avoid fault conditions and failures.",0
"The Smart Power Grids sector must continuously enhance its proficiency and robustness to deliver superior quality electricity via a resilient grid that manages faults and prevents failures. Accomplishing this necessitates reliable components, proper maintenance, and strategic fault management. To ensure optimal system operation, advanced techniques like predictive algorithms, analytics, and data analysis are used to detect, classify, and isolate faults and failures, as well as model and simulate processes. We present the utilization of Probabilistic Boolean Networks (PBN), a self-organizing modeling approach, to comprehend the dynamics of smart grid devices and analyze their behavior. Our study reveals that PBNs are equivalent to the Reinforcement Learning Cycle, where the model interacts with the environment and receives feedback in the form of a reward signal. Various reward structures were developed to define desired behavior, which can guide PBN to prevent fault situations and failures.",1
"We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits much faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the decision-making problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman residual minimization to establish the correct pointwise convergence guarantees. As specific cases, our results imply $O(1/n)$ regret rates in linear cases and $\exp(-\Omega(n))$ regret rates in tabular cases.",0
"The article explores the regret of reinforcement learning through offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process. Although previous analyses indicate a $O(1/\sqrt{n})$ convergence for regret in common approaches like fitted $Q$-iteration (FQI), the study observes faster empirical convergence rates. The research provides a more precise analysis of this phenomenon by determining faster rates for regret convergence. The study demonstrates that the policy's regret's convergence rate relies on the estimate of the optimal quality function $Q^*$ and its pointwise convergence rate. This rate's exponentiation relies on the decision-making problem's noise level rather than the estimation issue. The study establishes these noise levels for linear and tabular MDPs and provides new analyses of FQI and Bellman residual minimization to determine the correct pointwise convergence guarantees. The results indicate $O(1/n)$ regret rates in linear cases and $\exp(-\Omega(n))$ regret rates in tabular cases as specific instances.",1
"Injecting human knowledge is an effective way to accelerate reinforcement learning (RL). However, these methods are underexplored. This paper presents our discovery that an abstract forward model (Thought-game (TG)) combined with transfer learning is an effective way. We take StarCraft II as the study environment. With the help of a designed TG, the agent can learn a 99\% win-rate on a 64$\times$64 map against the Level-7 built-in AI, using only 1.08 hours in a single commercial machine. We also show that the TG method is not as restrictive as it was thought to be. It can work with roughly designed TGs, and can also be useful when the environment changes. Comparing with previous model-based RL, we show TG is more effective. We also present a TG hypothesis that gives the influence of fidelity levels of TG. For real games that have unequal state and action spaces, we proposed a novel XfrNet of which usefulness is validated while achieving a 90\% win-rate against the cheating Level-10 AI. We argue the TG method might shed light on further studies of efficient RL with human knowledge.",0
"Introducing human knowledge can expedite the process of reinforcement learning (RL), yet this approach remains largely unexplored. Our paper reveals that a conceptual forward model, known as a Thought-game (TG), combined with transfer learning, is a successful method for this purpose. Our study employs StarCraft II as the test environment, where the agent achieved a 99% win-rate on a 64x64 map against the Level-7 built-in AI, using only 1.08 hours on a single commercial machine. We demonstrate that TG is not as limiting as previously thought and can adapt to changes in the environment. In comparison to previous model-based RL, we find TG to be more effective. Additionally, we present the TG hypothesis, which outlines the impact of TG fidelity levels. To address games with uneven state and action spaces, we propose XfrNet, a novel approach that achieved a 90% win-rate against the cheating Level-10 AI. Our findings suggest that the TG method may lead to further research on efficient RL with human knowledge.",1
"This paper studies the exponential stability of random matrix products driven by a general (possibly unbounded) state space Markov chain. It is a cornerstone in the analysis of stochastic algorithms in machine learning (e.g. for parameter tracking in online learning or reinforcement learning). The existing results impose strong conditions such as uniform boundedness of the matrix-valued functions and uniform ergodicity of the Markov chains. Our main contribution is an exponential stability result for the $p$-th moment of random matrix product, provided that (i) the underlying Markov chain satisfies a super-Lyapunov drift condition, (ii) the growth of the matrix-valued functions is controlled by an appropriately defined function (related to the drift condition). Using this result, we give finite-time $p$-th moment bounds for constant and decreasing stepsize linear stochastic approximation schemes with Markovian noise on general state space. We illustrate these findings for linear value-function estimation in reinforcement learning. We provide finite-time $p$-th moment bound for various members of temporal difference (TD) family of algorithms.",0
"The focus of this paper is on examining the exponential stability of random matrix products that are driven by a state space Markov chain, which may be unbounded. This analysis is essential when it comes to stochastic algorithms used in machine learning, particularly for parameter tracking in online and reinforcement learning. However, current research imposes stringent conditions, such as uniform boundedness of matrix-valued functions and uniform ergodicity of Markov chains. Our contribution is an exponential stability outcome for the $p$-th moment of random matrix products. This result is possible if the Markov chain adheres to a super-Lyapunov drift condition and the growth of matrix-valued functions is controlled by an appropriately defined function linked to the drift condition. We use this result to provide finite-time $p$-th moment bounds for linear stochastic approximation schemes with Markovian noise on general state space that have constant and decreasing step sizes. We apply these findings to linear value-function estimation in reinforcement learning, providing finite-time $p$-th moment bounds for various members of the TD family of algorithms.",1
"Reinforcement learning typically assumes that the agent observes feedback from the environment immediately, but in many real-world applications (like recommendation systems) the feedback is observed in delay. Thus, we consider online learning in episodic Markov decision processes (MDPs) with unknown transitions, adversarially changing costs and unrestricted delayed feedback. That is, the costs and trajectory of episode $k$ are only available at the end of episode $k + d^k$, where the delays $d^k$ are neither identical nor bounded, and are chosen by an adversary. We present novel algorithms based on policy optimization that achieve near-optimal high-probability regret of $\widetilde O ( \sqrt{K} + \sqrt{D} )$ under full-information feedback, where $K$ is the number of episodes and $D = \sum_{k} d^k$ is the total delay. Under bandit feedback, we prove similar $\widetilde O ( \sqrt{K} + \sqrt{D} )$ regret assuming that the costs are stochastic, and $\widetilde O ( K^{2/3} + D^{2/3} )$ regret in the general case. To our knowledge, we are the first to consider the important setting of delayed feedback in adversarial MDPs.",0
"In reinforcement learning, it is often assumed that the agent receives immediate feedback from the environment. However, in real-world scenarios like recommendation systems, feedback is often delayed. To address this issue, we explore online learning in episodic Markov decision processes (MDPs) where transitions are unknown, costs change adversarially, and feedback is delayed. Specifically, the costs and trajectory of an episode are only available at the end of the episode, and the delays are chosen by an adversary without any bounds. We propose novel algorithms based on policy optimization that achieve near-optimal high-probability regret of $\widetilde O ( \sqrt{K} + \sqrt{D} )$ under full-information feedback, where $K$ is the number of episodes and $D$ is the total delay. We also prove similar regret bounds under bandit feedback assuming stochastic costs and a higher regret bound in the general case. Our work is the first to address the important issue of delayed feedback in adversarial MDPs.",1
"We study provably-efficient reinforcement learning in non-episodic factored Markov decision processes (FMDPs). All previous regret minimization algorithms in this setting made the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first algorithm that learns the structure of the FMDP while minimizing the regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. In addition, we give a variant of our algorithm that remains efficient even when the oracle is limited to non-factored actions, which is the case with almost all existing approximate planners. Finally, we also provide a novel lower bound for the known structure case that matches the best known regret bound of Chen et al. (2020).",0
"The focus of our research is on achieving provably-efficient reinforcement learning in non-episodic factored Markov decision processes (FMDPs). Previous regret minimization algorithms required the learner to have prior knowledge of the factored structure of the FMDP, which is a strong assumption. However, we present a new algorithm that allows for simultaneous learning of the FMDP structure while minimizing regret. Our algorithm utilizes the optimism in face of uncertainty principle and a straightforward statistical method for structure learning. It can be efficiently implemented with access to an FMDP planner. Furthermore, we propose a modified version of our algorithm that remains efficient when the planner only has access to non-factored actions. This modification is necessary as most existing approximate planners fall under this category. Lastly, we provide a novel lower bound for the known structure case that matches the best-known regret bound of Chen et al. (2020).",1
"Electric vehicles have been rapidly increasing in usage, but stations to charge them have not always kept up with demand, so efficient routing of vehicles to stations is critical to operating at maximum efficiency. Deciding which stations to recommend drivers to is a complex problem with a multitude of possible recommendations, volatile usage patterns and temporally extended consequences of recommendations. Reinforcement learning offers a powerful paradigm for solving sequential decision-making problems, but traditional methods may struggle with sample efficiency due to the high number of possible actions. By developing a model that allows complex representations of actions, we improve outcomes for users of our system by over 30% when compared to existing baselines in a simulation. If implemented widely, these better recommendations can globally save over 4 million person-hours of waiting and driving each year.",0
"The use of electric vehicles has grown rapidly, but the availability of charging stations has not kept pace with demand. Therefore, it is essential to efficiently direct vehicles to charging stations to maximize their usage. This decision is complicated by the numerous possible recommendations, fluctuating usage patterns, and the long-term consequences of our suggestions. Reinforcement learning is a powerful tool for solving sequential decision-making problems, but the high number of potential actions can hinder traditional methods' sample efficiency. By developing a model that can handle complex action representations, we have improved the system's outcomes by over 30% compared to existing baselines in a simulation. If these enhanced recommendations are widely adopted, they could lead to global savings of over 4 million person-hours per year in waiting and driving time.",1
"This paper offers a new hybrid probably approximately correct (PAC) reinforcement learning (RL) algorithm for Markov decision processes (MDPs) that intelligently maintains favorable features of its parents. The designed algorithm, referred to as the Dyna-Delayed Q-learning (DDQ) algorithm, combines model-free and model-based learning approaches while outperforming both in most cases. The paper includes a PAC analysis of the DDQ algorithm and a derivation of its sample complexity. Numerical results are provided to support the claim regarding the new algorithm's sample efficiency compared to its parents as well as the best known model-free and model-based algorithms in application.",0
"A novel hybrid probably approximately correct (PAC) reinforcement learning (RL) algorithm is presented in this paper for Markov decision processes (MDPs) that effectively incorporates the desirable features of its predecessors. The newly developed algorithm, named the Dyna-Delayed Q-learning (DDQ) algorithm, integrates both model-free and model-based learning approaches and showcases superior performance compared to both methods in many cases. The research also includes a PAC analysis and sample complexity derivation of the DDQ algorithm, along with numerical results that demonstrate its improved sample efficiency over its predecessors and the most effective model-free and model-based algorithms in practical applications.",1
"Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Despite recent advances in this field, current algorithms suffer from three major challenges: high sample complexity, learning only a single way of reaching the goals, and difficulties in solving complex motion planning tasks. In order to address these limitations, we introduce the concept of cumulative accessibility functions, which measure the reachability of a goal from a given state within a specified horizon. We show that these functions obey a recurrence relation, which enables learning from offline interactions. We also prove that optimal cumulative accessibility functions are monotonic in the planning horizon. Additionally, our method can trade off speed and reliability in goal-reaching by suggesting multiple paths to a single goal depending on the provided horizon. We evaluate our approach on a set of multi-goal discrete and continuous control tasks. We show that our method outperforms state-of-the-art goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code is available at https://github.com/layer6ai-labs/CAE, and additional visualizations can be found at https://sites.google.com/view/learning-cae/.",0
"Reinforcement learning faces the challenge of multi-goal reaching, which is necessary for algorithmic generalization. However, current algorithms encounter three major issues: high sample complexity, learning a single way of achieving goals, and difficulty in solving complex motion planning tasks. To overcome these limitations, we propose cumulative accessibility functions that gauge the reachability of a goal from a given state over a specific horizon. These functions follow a recurrence relation, allowing for offline learning. Furthermore, we demonstrate that optimal cumulative accessibility functions are monotonic in planning horizon. Our method can also vary speed and reliability in goal-reaching by suggesting various paths to a single goal based on the horizon. We assess our approach on discrete and continuous control tasks with multiple goals and show that it surpasses current goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code and additional visuals are accessible at https://github.com/layer6ai-labs/CAE and https://sites.google.com/view/learning-cae/.",1
"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",0
"Although Deep Neural Network-based systems are currently the leading approach for many robotics tasks, their application in safety-critical domains presents serious risks in the absence of formal guarantees on network robustness. Even slight deviations in sensor inputs caused by noise or adversarial examples can result in altered network-based decisions, as evidenced by an incident in which an autonomous vehicle swerved into a different lane. To address this issue, several defensive algorithms have been developed to protect against adversarial inputs, some of which offer formal robustness guarantees or certificates. This study utilizes research on certified adversarial robustness to create an online certifiably robust system for deep reinforcement learning algorithms. During implementation, the proposed defense computes guaranteed lower bounds on state-action values to determine a robust action in the event of a worst-case deviation in input space caused by possible adversaries or noise. Furthermore, the resulting policy includes a certificate of solution quality, despite the perturbations rendering the true state and optimal action unknown to the certifier. The approach is applied to a Deep Q-Network policy and is shown to enhance robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This study builds on previous work by broadening the range of performance guarantees, extending the approach to other RL algorithms, expanding the range of scenarios tested, including adversarial behavior, comparing the system to a more computationally intensive method, and offering visualizations to provide insight into the robustness algorithm.",1
"Actor-critic style two-time-scale algorithms are very popular in reinforcement learning, and have seen great empirical success. However, their performance is not completely understood theoretically. In this paper, we characterize the global convergence of an online natural actor-critic algorithm in the tabular setting using a single trajectory. Our analysis applies to very general settings, as we only assume that the underlying Markov chain is ergodic under all policies (the so-called Recurrence assumption). We employ $\epsilon$-greedy sampling in order to ensure enough exploration.   For a fixed exploration parameter $\epsilon$, we show that the natural actor critic algorithm is $\mathcal{O}(\frac{1}{\epsilon T^{1/4}}+\epsilon)$ close to the global optimum after $T$ iterations of the algorithm.   By carefully diminishing the exploration parameter $\epsilon$ as the iterations proceed, we also show convergence to the global optimum at a rate of $\mathcal{O}(1/T^{1/6})$.",0
"Reinforcement learning commonly employs actor-critic style two-time-scale algorithms, which have proven to be highly effective in practice. However, their theoretical performance remains somewhat unclear. This study presents an analysis of the convergence of an online natural actor-critic algorithm in the tabular setting, utilizing a single trajectory. Our approach applies to broad settings, assuming only that the underlying Markov chain is ergodic under all policies (known as the Recurrence assumption). To ensure sufficient exploration, we employ $\epsilon$-greedy sampling. Our results demonstrate that, for a fixed exploration parameter $\epsilon$, the natural actor-critic algorithm is within $\mathcal{O}(\frac{1}{\epsilon T^{1/4}}+\epsilon)$ of the global optimum after $T$ iterations. Furthermore, by appropriately reducing the exploration parameter $\epsilon$ as the iterations progress, we achieve convergence to the global optimum at a rate of $\mathcal{O}(1/T^{1/6})$.",1
"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",0
"Despite being feedforward networks, Transformers have proven to be effective for sequential, auto-regressive tasks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while still processing input tokens in parallel. This parallelization offers computational efficiency but limits the model's ability to fully utilize the sequential nature of the input. At each layer, the model can only access representations from lower layers, not the higher level ones already available. This study introduces the Feedback Transformer architecture, which allows all previous representations to be available to future representations. This means that the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. Through various benchmarks in language modeling, machine translation, and reinforcement learning, we demonstrate that this increased representation capacity allows for smaller, shallower models with significantly stronger performance than comparable Transformers.",1
"Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.",0
"The task of Novel Object Captioning involves describing objects that were not present in the training captions but can be detected by external object detectors. The main challenge is to accurately select and depict all the relevant novel objects in the input images. This paper primarily focuses on addressing this challenge by introducing the ECOL-R model, which is a copy-augmented transformer model that is trained to describe novel object labels. To achieve this, a specialized reward function is used in the SCST reinforcement learning framework, which encourages the model to mention the novel objects while maintaining the quality of the captions. The model is trained on images that already contain detected objects mentioned in reference captions. The model's copy mechanism is further improved through the use of Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which selects the appropriate inflected forms of novel object labels. As a result, the ECOL-R model outperforms previous models and sets a new state-of-the-art on the nocaps and held-out COCO benchmarks.",1
"Deep reinforcement learning (DRL) has shown remarkable success in sequential decision-making problems but suffers from a long training time to obtain such good performance. Many parallel and distributed DRL training approaches have been proposed to solve this problem, but it is difficult to utilize them on resource-limited devices. In order to accelerate DRL in real-world edge devices, memory bandwidth bottlenecks due to large weight transactions have to be resolved. However, previous iterative pruning not only shows a low compression ratio at the beginning of training but also makes DRL training unstable. To overcome these shortcomings, we propose a novel weight compression method for DRL training acceleration, named group-sparse training (GST). GST selectively utilizes block-circulant compression to maintain a high weight compression ratio during all iterations of DRL training and dynamically adapt target sparsity through reward-aware pruning for stable training. Thanks to the features, GST achieves a 25 \%p $\sim$ 41.5 \%p higher average compression ratio than the iterative pruning method without reward drop in Mujoco Halfcheetah-v2 and Mujoco humanoid-v2 environment with TD3 training.",0
"DRL has been successful in sequential decision-making but requires a long training time to achieve optimal performance. Although numerous parallel and distributed DRL training methods have been proposed, they are challenging to use on devices with limited resources. To enhance DRL on edge devices, memory bandwidth bottlenecks caused by large weight transactions must be eliminated. However, previous iterative pruning methods have drawbacks, including low compression ratio at the start of training and unstable DRL training. To overcome these issues, we suggest a new weight compression method for DRL training acceleration called group-sparse training (GST). GST uses block-circulant compression to maintain a high weight compression ratio throughout all iterations of DRL training and dynamically adjusts target sparsity via reward-aware pruning to ensure stable training. Thanks to these features, GST achieves a 25 \%p $\sim$ 41.5 \%p higher average compression ratio than the iterative pruning method without reward drop in the Mujoco Halfcheetah-v2 and Mujoco humanoid-v2 environments during TD3 training.",1
"With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.",0
"The emergence of deep representation learning has transformed reinforcement learning (RL) into a potent learning framework that can now master intricate policies in complex environments with high dimensions. This article summarizes deep reinforcement learning (DRL) algorithms and presents a classification of automated driving tasks where (D)RL techniques have been utilized. It also examines significant computational obstacles in the actual implementation of autonomous driving agents and identifies related domains such as behavior cloning, imitation learning, and inverse reinforcement learning that differ from classical RL algorithms. Additionally, the paper explores the use of simulators to train agents and outlines methods for validating, testing, and enhancing present solutions in RL.",1
"With the decreasing cost of data collection, the space of variables or features that can be used to characterize a particular predictor of interest continues to grow exponentially. Therefore, identifying the most characterizing features that minimizes the variance without jeopardizing the bias of our models is critical to successfully training a machine learning model. In addition, identifying such features is critical for interpretability, prediction accuracy and optimal computation cost. While statistical methods such as subset selection, shrinkage, dimensionality reduction have been applied in selecting the best set of features, some other approaches in literature have approached feature selection task as a search problem where each state in the search space is a possible feature subset. In this paper, we solved the feature selection problem using Reinforcement Learning. Formulating the state space as a Markov Decision Process (MDP), we used Temporal Difference (TD) algorithm to select the best subset of features. Each state was evaluated using a robust and low cost classifier algorithm which could handle any non-linearities in the dataset.",0
"As the cost of collecting data decreases, the number of variables that can be used to define a predictor increases exponentially. Therefore, it is crucial to identify the most significant features that reduce variance without compromising model bias when training a machine learning model. This is important for interpretability, prediction accuracy, and optimal computation cost. While statistical methods like subset selection, shrinkage, and dimensionality reduction have been used to choose the best features, other approaches treat feature selection as a search problem. In this study, we utilized Reinforcement Learning to address this problem. By formulating the state space as a Markov Decision Process (MDP), we employed the Temporal Difference (TD) algorithm to identify the best feature subset. Each state was assessed using a low-cost classifier algorithm that could handle non-linearities in the dataset.",1
"In this work, we study the system of interacting non-cooperative two Q-learning agents, where one agent has the privilege of observing the other's actions. We show that this information asymmetry can lead to a stable outcome of population learning, which generally does not occur in an environment of general independent learners. The resulting post-learning policies are almost optimal in the underlying game sense, i.e., they form a Nash equilibrium. Furthermore, we propose in this work a Q-learning algorithm, requiring predictive observation of two subsequent opponent's actions, yielding an optimal strategy given that the latter applies a stationary strategy, and discuss the existence of the Nash equilibrium in the underlying information asymmetrical game.",0
"The focus of our research is on the interaction between two Q-learning agents who are non-cooperative. One of these agents has the ability to observe the actions of the other agent. We have discovered that this uneven distribution of information can lead to stable population learning outcomes. This is a significant finding, as it is not typically observed in environments where independent learners are present. The post-learning policies that result from this asymmetry are close to optimal, as they form a Nash equilibrium in the game. In addition, we have developed a Q-learning algorithm that requires the prediction of the next two actions of an opponent. This algorithm yields an optimal strategy, assuming that the opponent applies a stationary strategy. We also discuss the existence of the Nash equilibrium in the information asymmetrical game.",1
"We present a novel Neural Embedding Spatio-Temporal (NEST) point process model for spatio-temporal discrete event data and develop an efficient imitation learning (a type of reinforcement learning) based approach for model fitting. Despite the rapid development of one-dimensional temporal point processes for discrete event data, the study of spatial-temporal aspects of such data is relatively scarce. Our model captures complex spatio-temporal dependence between discrete events by carefully design a mixture of heterogeneous Gaussian diffusion kernels, whose parameters are parameterized by neural networks. This new kernel is the key that our model can capture intricate spatial dependence patterns and yet still lead to interpretable results as we examine maps of Gaussian diffusion kernel parameters. The imitation learning model fitting for the NEST is more robust than the maximum likelihood estimate. It directly measures the divergence between the empirical distributions between the training data and the model-generated data. Moreover, our imitation learning-based approach enjoys computational efficiency due to the explicit characterization of the reward function related to the likelihood function; furthermore, the likelihood function under our model enjoys tractable expression due to Gaussian kernel parameterization. Experiments based on real data show our method's good performance relative to the state-of-the-art and the good interpretability of NEST's result.",0
"A new Neural Embedding Spatio-Temporal (NEST) point process model has been introduced for spatio-temporal discrete event data, along with an effective imitation learning method for model fitting. Although the development of one-dimensional temporal point processes for discrete event data has progressed quickly, research on the spatial-temporal aspects of such data is limited. The NEST model captures intricate spatio-temporal dependence between discrete events by employing a mixture of heterogeneous Gaussian diffusion kernels with parameters that are parameterized by neural networks. This new kernel facilitates the capture of spatial dependence patterns and generates interpretable results when maps of Gaussian diffusion kernel parameters are examined. The imitation learning model fitting for the NEST is more robust than maximum likelihood estimate as it measures the divergence between the empirical distributions of the training data and the model-generated data directly. The computational efficiency of our imitation learning-based approach is due to the explicit characterization of the reward function related to the likelihood function, while the likelihood function under our model enjoys tractable expression due to Gaussian kernel parameterization. Experiments using real data demonstrate the excellent performance of our method compared to the state-of-the-art and the good interpretability of NEST's result.",1
"This paper proposes a \emph{fully asynchronous} scheme for the policy evaluation problem of distributed reinforcement learning (DisRL) over directed peer-to-peer networks. Without waiting for any other node of the network, each node can locally update its value function at any time by using (possibly delayed) information from its neighbors. This is in sharp contrast to the gossip-based scheme where a pair of nodes concurrently update. Though the fully asynchronous setting involves a difficult multi-timescale decision problem, we design a novel stochastic average gradient (SAG) based distributed algorithm and develop a push-pull augmented graph approach to prove its exact convergence at a linear rate of $\mathcal{O}(c^k)$ where $c\in(0,1)$ and $k$ increases by one no matter on which node updates. Finally, numerical experiments validate that our method speeds up linearly with respect to the number of nodes, and is robust to straggler nodes.",0
"In this article, we present a new approach to solving the policy evaluation problem in distributed reinforcement learning (DisRL) over directed peer-to-peer networks. Our fully asynchronous method allows each node to update its value function independently and locally, using information from its neighbors, without waiting for other nodes to complete their updates. This is in contrast to the gossip-based scheme where pairs of nodes update concurrently. Despite the complexity of the multi-timescale decision problem posed in the fully asynchronous setting, we propose a novel stochastic average gradient (SAG) based distributed algorithm and augment it with a push-pull graph approach to demonstrate its exact convergence at a linear rate of $\mathcal{O}(c^k)$, where $c\in(0,1)$ and $k$ increases by one regardless of the node that updates. Our numerical experiments show that our method is robust to straggler nodes and scales linearly with the number of nodes.",1
"In safety-critical applications, autonomous agents may need to learn in an environment where mistakes can be very costly. In such settings, the agent needs to behave safely not only after but also while learning. To achieve this, existing safe reinforcement learning methods make an agent rely on priors that let it avoid dangerous situations during exploration with high probability, but both the probabilistic guarantees and the smoothness assumptions inherent in the priors are not viable in many scenarios of interest such as autonomous driving. This paper presents an alternative approach inspired by human teaching, where an agent learns under the supervision of an automatic instructor that saves the agent from violating constraints during learning. In this model, we introduce the monitor that neither needs to know how to do well at the task the agent is learning nor needs to know how the environment works. Instead, it has a library of reset controllers that it activates when the agent starts behaving dangerously, preventing it from doing damage. Crucially, the choices of which reset controller to apply in which situation affect the speed of agent learning. Based on observing agents' progress, the teacher itself learns a policy for choosing the reset controllers, a curriculum, to optimize the agent's final policy reward. Our experiments use this framework in two environments to induce curricula for safe and efficient learning.",0
"Autonomous agents may need to learn in an environment where errors can result in significant costs in safety-critical applications. In these situations, it is crucial that the agent behaves safely both during and after learning. Existing safe reinforcement learning methods rely on prior knowledge to help the agent avoid dangerous situations during exploration, but these priors are not always practical in real-life settings like autonomous driving. Instead, this paper proposes a new approach inspired by human teaching, where the agent learns under the supervision of an automatic instructor that prevents it from violating constraints during learning. The instructor has a library of reset controllers that it activates when the agent behaves dangerously, minimizing the potential for damage. The instructor learns from observing the agent's progress and develops a curriculum for choosing the reset controllers to optimize the final policy reward. Our experiments demonstrate the effectiveness of this approach in two environments for safe and efficient learning.",1
"We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.",0
"The focus of our research is on the resilience of reinforcement learning (RL) when state observations are perturbed adversarially. This scenario is relevant to deep reinforcement learning (DRL), where many adversarial attacks are targeted, and is also crucial for the deployment of RL agents in real-world settings with unpredictable sensing noise. Through our work, we demonstrate that an optimal adversary can be identified for perturbing state observations, which guarantees the worst case agent reward with a fixed policy. This enables a novel empirical adversarial attack on RL agents in DRL settings, which outperforms previous methods. To improve agent robustness, we propose a new framework called alternating training with learned adversaries (ATLA), which trains an adversary alongside the agent using policy gradient following the optimal adversarial attack framework. Furthermore, we show that considering the history of past states and actions can enhance agent robustness, and empirically find that an LSTM-based policy is more resilient under adversaries. Our evaluations on a range of continuous control environments demonstrate that ATLA achieves state-of-the-art performance against strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.",1
"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",0
"Many science fiction stories depict a future where robots coexist with humans, possessing the ability to sense and understand the world around them, assist in physical tasks, and communicate using natural language. This study focuses on designing artificial agents that can interact with humans naturally within a virtual environment, which presents challenges in areas such as visual perception, physical control, language processing, and social interaction. Although the ideal training for these agents would involve actual human interaction, it is currently not feasible, so the study employs another learned agent to approximate human behavior through inverse reinforcement learning. The study also develops various behavioral tests to evaluate the agents, including human observation and interaction. The results show that interactive training and auxiliary losses improve agent behavior beyond supervised learning, and that the agents' capabilities extend beyond the dataset. Additionally, evaluation models are trained to rate the agents, which agree with human judgement, making it easier to evaluate new agent models. Overall, the study suggests that large-scale human imitation can effectively create intelligent and interactive agents, and that evaluating these agents can be achieved reliably.",1
"This paper introduces MDP homomorphic networks for deep reinforcement learning. MDP homomorphic networks are neural networks that are equivariant under symmetries in the joint state-action space of an MDP. Current approaches to deep reinforcement learning do not usually exploit knowledge about such structure. By building this prior knowledge into policy and value networks using an equivariance constraint, we can reduce the size of the solution space. We specifically focus on group-structured symmetries (invertible transformations). Additionally, we introduce an easy method for constructing equivariant network layers numerically, so the system designer need not solve the constraints by hand, as is typically done. We construct MDP homomorphic MLPs and CNNs that are equivariant under either a group of reflections or rotations. We show that such networks converge faster than unstructured baselines on CartPole, a grid world and Pong.",0
"The article presents MDP homomorphic networks for deep reinforcement learning, which are neural networks that uphold symmetries within the joint state-action space of an MDP. The use of such structure is not currently common in deep reinforcement learning methods. By incorporating an equivariance constraint into the policy and value networks, we can limit the size of the solution space. Our focus is on invertible transformations and we provide a simple numerical method for creating equivariant network layers, rather than solving the constraints manually. We demonstrate the effectiveness of MDP homomorphic MLPs and CNNs, which are equivariant under reflections or rotations, in comparison to unstructured baselines on CartPole, a grid world and Pong.",1
"In this paper, we propose Broad Neural Architecture Search (BNAS) where we elaborately design broad scalable architecture dubbed Broad Convolutional Neural Network (BCNN) to solve the above issue. On one hand, the proposed broad scalable architecture has fast training speed due to its shallow topology. Moreover, we also adopt reinforcement learning and parameter sharing used in ENAS as the optimization strategy of BNAS. Hence, the proposed approach can achieve higher search efficiency. On the other hand, the broad scalable architecture extracts multi-scale features and enhancement representations, and feeds them into global average pooling layer to yield more reasonable and comprehensive representations. Therefore, the performance of broad scalable architecture can be promised. In particular, we also develop two variants for BNAS who modify the topology of BCNN. In order to verify the effectiveness of BNAS, several experiments are performed and experimental results show that 1) BNAS delivers 0.19 days which is 2.37x less expensive than ENAS who ranks the best in reinforcement learning-based NAS approaches, 2) compared with small-size (0.5 millions parameters) and medium-size (1.1 millions parameters) models, the architecture learned by BNAS obtains state-of-the-art performance (3.58% and 3.24% test error) on CIFAR-10, 3) the learned architecture achieves 25.3% top-1 error on ImageNet just using 3.9 millions parameters.",0
"The paper presents the concept of Broad Neural Architecture Search (BNAS) as a solution to the problem at hand. The proposed approach uses a Broad Convolutional Neural Network (BCNN) that has a shallow topology, leading to fast training. The optimization strategy used in BNAS involves reinforcement learning and parameter sharing, similar to ENAS, thereby improving its search efficiency. The BCNN extracts multi-scale features and enhancement representations and feeds them into a global average pooling layer to generate comprehensive representations, resulting in better performance. Additionally, two variants of BNAS that modify the topology of BCNN are developed. The effectiveness of BNAS is verified through several experiments, and the results indicate that BNAS is significantly less expensive than ENAS and achieves state-of-the-art performance on CIFAR-10, with a top-1 error of 25.3% on ImageNet using just 3.9 million parameters.",1
"One of the biggest challenges in multi-agent reinforcement learning is coordination, a typical application scenario of this is traffic signal control. Recently, it has attracted a rising number of researchers and has become a hot research field with great practical significance. In this paper, we propose a novel method called MetaVRS~(Meta Variational RewardShaping) for traffic signal coordination control. By heuristically applying the intrinsic reward to the environmental reward, MetaVRS can wisely capture the agent-to-agent interplay. Besides, latent variables generated by VAE are brought into policy for automatically tradeoff between exploration and exploitation to optimize the policy. In addition, meta learning was used in decoder for faster adaptation and better approximation. Empirically, we demonstate that MetaVRS substantially outperforms existing methods and shows superior adaptability, which predictably has a far-reaching significance to the multi-agent traffic signal coordination control.",0
"Multi-agent reinforcement learning poses a significant challenge, particularly in coordinating agents in scenarios like traffic signal control. This field has gained increasing attention from researchers due to its practical importance. This paper proposes a novel approach, MetaVRS, for traffic signal coordination control. MetaVRS applies intrinsic rewards to the environmental reward, enabling it to capture inter-agent interactions. VAE-generated latent variables are incorporated into the policy to balance exploration and exploitation, while meta-learning is implemented in the decoder to improve adaptation and approximation. Empirical evidence suggests that MetaVRS outperforms existing methods and offers superior adaptability, which holds significant implications for multi-agent traffic signal coordination control.",1
"We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge -- a meta-algorithm called PROPEL -- is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space. Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches. Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.",0
"The focus of our research is on programmatic reinforcement learning, where policies are represented as brief programs in a symbolic language. While neural policies have their advantages, programmatic policies are more easily understood, applicable to a wider range of scenarios, and can be verified with greater accuracy. However, devising effective learning approaches for programmatic policies presents a challenge. To address this issue, we introduce a meta-algorithm, PROPEL, based on three key principles. Firstly, we consider our learning task as an optimization problem in policy space, with the condition that the desired policy must have a programmatic representation. We solve this problem using a type of mirror descent that takes a gradient step into the unconstrained policy space before projecting back into the restricted space. Secondly, we view the unconstrained policy space as a combination of neural and programmatic representations, allowing us to use cutting-edge deep policy gradient techniques. Finally, we treat the projection step as program synthesis via imitation learning, utilizing up-to-date combinatorial methods. We provide theoretical convergence results for PROPEL and test the approach in three continuous control domains. Our experiments demonstrate that PROPEL outperforms current state-of-the-art approaches for learning programmatic policies.",1
"Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for meta-training such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree.",0
"Traditionally, important aspects of training parametric models such as loss functions and regularization mechanisms are chosen through a limited set of options using heuristic methods. In an effort to produce models that train faster and more robustly, this paper outlines the first step towards automating this process. Specifically, the authors present a method for meta-learning parametric loss functions that can be applied across various tasks and model architectures. They introduce a pipeline for meta-training these loss functions in order to maximize the model's performance. The results show that the loss landscape produced by the learned losses outperforms the original task-specific losses in both supervised and reinforcement learning tasks. Additionally, the authors demonstrate that their meta-learning framework can incorporate additional information at meta-train time, which shapes the learned loss function and eliminates the need for the environment to provide this information during meta-test time. The code for this work is available at https://sites.google.com/view/mlthree.",1
"Though deep reinforcement learning has led to breakthroughs in many difficult domains, these successes have required an ever-increasing number of samples. As state-of-the-art reinforcement learning (RL) systems require an exponentially increasing number of samples, their development is restricted to a continually shrinking segment of the AI community. Likewise, many of these systems cannot be applied to real-world problems, where environment samples are expensive. Resolution of these limitations requires new, sample-efficient methods. To facilitate research in this direction, we introduce the MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors.   The primary goal of the competition is to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, we introduce: (1) the Minecraft ObtainDiamond task, a sequential decision making environment requiring long-term planning, hierarchical control, and efficient exploration methods; and (2) the MineRL-v0 dataset, a large-scale collection of over 60 million state-action pairs of human demonstrations that can be resimulated into embodied trajectories with arbitrary modifications to game state and visuals.   Participants will compete to develop systems which solve the ObtainDiamond task with a limited number of samples from the environment simulator, Malmo. The competition is structured into two rounds in which competitors are provided several paired versions of the dataset and environment with different game textures. At the end of each round, competitors will submit containerized versions of their learning algorithms and they will then be trained/evaluated from scratch on a hold-out dataset-environment pair for a total of 4-days on a prespecified hardware platform.",0
"Despite the breakthroughs that deep reinforcement learning has achieved in challenging domains, these achievements have required an increasingly large number of samples. State-of-the-art reinforcement learning systems have an exponentially rising sample requirement, which limits their development to a shrinking subset of the AI community. Additionally, many of these systems are unsuitable for real-world problems that involve costly environment samples. To overcome these limitations, new methods that are sample-efficient are necessary. To encourage research in this direction, we are introducing the MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors. The competition aims to promote the development of algorithms that can effectively use human demonstrations to significantly reduce the number of samples required to solve complex, hierarchical, and sparse environments. To achieve this goal, we are introducing the Minecraft ObtainDiamond task, which requires long-term planning, hierarchical control, and efficient exploration methods. Furthermore, we are providing the MineRL-v0 dataset, which contains over 60 million state-action pairs of human demonstrations that can be resimulated into embodied trajectories with arbitrary modifications to game state and visuals. Participants in the competition will compete to develop systems that can solve the ObtainDiamond task with a limited number of samples from the Malmo environment simulator. The competition will have two rounds, and competitors will be given several paired versions of the dataset and environment with different game textures. At the end of each round, competitors will submit containerized versions of their learning algorithms, which will then be trained/evaluated from scratch on a hold-out dataset-environment pair for a total of 4-days on a prespecified hardware platform.",1
"In the industrial interior design process, professional designers plan the size and position of furniture in a room to achieve a satisfactory design for selling. In this paper, we explore the interior scene design task as a Markov decision process (MDP), which is solved by deep reinforcement learning. The goal is to produce an accurate position and size of the furniture simultaneously for the indoor layout task. In particular, we first formulate the furniture layout task as a MDP problem by defining the state, action, and reward function. We then design the simulated environment and train reinforcement learning agents to produce the optimal layout for the MDP formulation. We conduct our experiments on a large-scale real-world interior layout dataset that contains industrial designs from professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts as compared with the state-of-art model. The developed simulator and codes are available at \url{https://github.com/CODE-SUBMIT/simulator1}.",0
"The process of industrial interior design involves the careful planning of furniture size and placement to create an appealing aesthetic for potential buyers. This study examines the task of interior scene design by utilizing Markov decision process (MDP) and deep reinforcement learning to determine the optimal position and size of furniture for an indoor layout. To achieve this, the furniture layout task is first defined as an MDP problem with a specific state, action, and reward function. A simulated environment is then designed and reinforcement learning agents are trained to generate the best layout. The proposed model is evaluated on a large-scale real-world interior layout dataset consisting of professional designers' industrial designs and compared with the state-of-the-art model. Results show that the proposed model produces higher-quality layouts. The simulator and codes developed for this study are available at \url{https://github.com/CODE-SUBMIT/simulator1}.",1
"Reinforcement learning in complex environments is a challenging problem. In particular, the success of reinforcement learning algorithms depends on a well-designed reward function. Inverse reinforcement learning (IRL) solves the problem of recovering reward functions from expert demonstrations. In this paper, we solve a hierarchical inverse reinforcement learning problem within the options framework, which allows us to utilize intrinsic motivation of the expert demonstrations. A gradient method for parametrized options is used to deduce a defining equation for the Q-feature space, which leads to a reward feature space. Using a second-order optimality condition for option parameters, an optimal reward function is selected. Experimental results in both discrete and continuous domains confirm that our recovered rewards provide a solution to the IRL problem using temporal abstraction, which in turn are effective in accelerating transfer learning tasks. We also show that our method is robust to noises contained in expert demonstrations.",0
"The difficulty of reinforcement learning in intricate settings poses a challenge, with the effectiveness of reinforcement learning algorithms reliant on a well-crafted reward function. To address this, inverse reinforcement learning (IRL) is utilized to recover reward functions from expert demonstrations. This study presents a solution to a hierarchical IRL problem within the options framework, leveraging the expert demonstrations' intrinsic motivation. A gradient method for parametrized options is used to determine a defining equation for the Q-feature space, thus leading to a reward feature space. By employing a second-order optimality condition for option parameters, an optimal reward function is selected. Empirical results in both discrete and continuous domains validate that our recovered rewards offer a solution to the IRL problem using temporal abstraction, which ultimately accelerates transfer learning tasks. Moreover, we demonstrate that our method remains robust even in the presence of noise in expert demonstrations.",1
"Deep reinforcement learning has achieved significant milestones, however, the computational demands of reinforcement learning training and inference remain substantial. Quantization is an effective method to reduce the computational overheads of neural networks, though in the context of reinforcement learning, it is unknown whether quantization's computational benefits outweigh the accuracy costs introduced by the corresponding quantization error. To quantify this tradeoff we perform a broad study applying quantization to reinforcement learning. We apply standard quantization techniques such as post-training quantization (PTQ) and quantization aware training (QAT) to a comprehensive set of reinforcement learning tasks (Atari, Gym), algorithms (A2C, DDPG, DQN, D4PG, PPO), and models (MLPs, CNNs) and show that policies may be quantized to 8-bits without degrading reward, enabling significant inference speedups on resource-constrained edge devices. Motivated by the effectiveness of standard quantization techniques on reinforcement learning policies, we introduce a novel quantization algorithm, \textit{ActorQ}, for quantized actor-learner distributed reinforcement learning training. By leveraging full precision optimization on the learner and quantized execution on the actors, \textit{ActorQ} enables 8-bit inference while maintaining convergence. We develop a system for quantized reinforcement learning training around \textit{ActorQ} and demonstrate end to end speedups of $>$ 1.5 $\times$ - 2.5 $\times$ over full precision training on a range of tasks (Deepmind Control Suite). Finally, we break down the various runtime costs of distributed reinforcement learning training (such as communication time, inference time, model load time, etc) and evaluate the effects of quantization on these system attributes.",0
"Despite significant milestones achieved by deep reinforcement learning, the computational demands of training and inference remain substantial. While quantization is an effective method to reduce neural network computational overheads, it is uncertain in the context of reinforcement learning whether the benefits of quantization outweigh the accuracy costs resulting from quantization error. To evaluate this tradeoff, we conducted a comprehensive study of quantization applied to reinforcement learning, using standard techniques like post-training quantization and quantization-aware training across a range of algorithms, models, and tasks. Our results demonstrate that policies may be quantized to 8-bits without degrading reward, allowing for significant inference speedups on edge devices. Building on these findings, we introduce a novel quantization algorithm called ActorQ for quantized actor-learner distributed reinforcement learning training. By combining full precision optimization on the learner with quantized execution on the actors, ActorQ enables 8-bit inference without sacrificing convergence. We developed a system for quantized reinforcement learning training around ActorQ and show end-to-end speedups of over 1.5x to 2.5x compared to full precision training across a range of tasks. Finally, we analyze the various runtime costs of distributed reinforcement learning training and evaluate the impact of quantization on these system attributes.",1
"Exploration is one of the core challenges in reinforcement learning. A common formulation of curiosity-driven exploration uses the difference between the real future and the future predicted by a learned model. However, predicting the future is an inherently difficult task which can be ill-posed in the face of stochasticity. In this paper, we introduce an alternative form of curiosity that rewards novel associations between different senses. Our approach exploits multiple modalities to provide a stronger signal for more efficient exploration. Our method is inspired by the fact that, for humans, both sight and sound play a critical role in exploration. We present results on several Atari environments and Habitat (a photorealistic navigation simulator), showing the benefits of using an audio-visual association model for intrinsically guiding learning agents in the absence of external rewards. For videos and code, see https://vdean.github.io/audio-curiosity.html.",0
"Reinforcement learning faces a significant challenge in the form of exploration. One approach to curiosity-driven exploration relies on comparing the actual future with the future predicted by a learned model. However, predicting the future is a complex task that may not be suitable when faced with stochasticity. In this study, an alternative form of curiosity is proposed, which rewards the discovery of novel associations between different senses. By utilizing multiple modalities, this approach provides a more robust signal for efficient exploration. The inspiration for this method comes from the critical role that sight and sound play in human exploration. Results from testing this audio-visual association model on several Atari environments and Habitat (a photorealistic navigation simulator) demonstrate the advantages of using such a model to guide learning agents intrinsically, even in the absence of external rewards. For additional materials, including videos and code, please visit https://vdean.github.io/audio-curiosity.html.",1
"Policy specification is a process by which a human can initialize a robot's behaviour and, in turn, warm-start policy optimization via Reinforcement Learning (RL). While policy specification/design is inherently a collaborative process, modern methods based on Learning from Demonstration or Deep RL lack the model interpretability and accessibility to be classified as such. Current state-of-the-art methods for policy specification rely on black-box models, which are an insufficient means of collaboration for non-expert users: These models provide no means of inspecting policies learnt by the agent and are not focused on creating a usable modality for teaching robot behaviour. In this paper, we propose a novel machine learning framework that enables humans to 1) specify, through natural language, interpretable policies in the form of easy-to-understand decision trees, 2) leverage these policies to warm-start reinforcement learning and 3) outperform baselines that lack our natural language initialization mechanism. We train our approach by collecting a first-of-its-kind corpus mapping free-form natural language policy descriptions to decision tree-based policies. We show that our novel framework translates natural language to decision trees with a 96% and 97% accuracy on a held-out corpus across two domains, respectively. Finally, we validate that policies initialized with natural language commands are able to significantly outperform relevant baselines (p < 0.001) that do not benefit from our natural language-based warm-start technique.",0
"The process of policy specification involves initiating a robot's behavior and using Reinforcement Learning (RL) to optimize policies. Despite being a collaborative process, modern methods like Learning from Demonstration or Deep RL lack model interpretability and accessibility. Current methods rely on black-box models, which are inadequate for non-expert users as they do not allow for inspection of policies learned by the agent or facilitate teaching robot behavior. This paper introduces a novel machine learning framework that enables humans to specify interpretable policies in the form of decision trees using natural language. The framework also helps warm-start reinforcement learning and outperforms baselines that lack natural language initialization. A first-of-its-kind corpus is used to train the approach, mapping free-form natural language policy descriptions to decision tree-based policies. The framework achieves a 96% and 97% accuracy in translating natural language to decision trees across two domains, respectively. Policies initialized with natural language commands outperform relevant baselines and significantly benefit from the natural language-based warm-start technique (p < 0.001).",1
"In reinforcement learning, temporal difference-based algorithms can be sample-inefficient: for instance, with sparse rewards, no learning occurs until a reward is observed. This can be remedied by learning richer objects, such as a model of the environment, or successor states. Successor states model the expected future state occupancy from any given state for a given policy and are related to goal-dependent value functions, which learn how to reach arbitrary states. We formally derive the temporal difference algorithm for successor state and goal-dependent value function learning, either for discrete or for continuous environments with function approximation. Especially, we provide finite-variance estimators even in continuous environments, where the reward for exactly reaching a goal state becomes infinitely sparse. Successor states satisfy more than just the Bellman equation: a backward Bellman operator and a Bellman-Newton (BN) operator encode path compositionality in the environment. The BN operator is akin to second-order gradient descent methods and provides the true update of the value function when acquiring more observations, with explicit tabular bounds. In the tabular case and with infinitesimal learning rates, mixing the usual and backward Bellman operators provably improves eigenvalues for asymptotic convergence, and the asymptotic convergence of the BN operator is provably better than TD, with a rate independent from the environment. However, the BN method is more complex and less robust to sampling noise. Finally, a forward-backward (FB) finite-rank parameterization of successor states enjoys reduced variance and improved samplability, provides a direct model of the value function, has fully understood fixed points corresponding to long-range dependencies, approximates the BN method, and provides two canonical representations of states as a byproduct.",0
"Temporal difference-based algorithms in reinforcement learning can be inefficient when rewards are sparse, as no learning occurs until a reward is observed. To address this, richer objects such as a model of the environment or successor states can be used. Successor states show the expected future state occupancy from any given state and are related to goal-dependent value functions that enable learning how to reach arbitrary states. This article derives the temporal difference algorithm for successor state and goal-dependent value function learning, for both discrete and continuous environments with function approximation. It provides finite-variance estimators, even in environments with sparse rewards. Successor states satisfy more than the Bellman equation and encode path compositionality in the environment. The Bellman-Newton operator provides the true update of the value function when acquiring more observations, with explicit tabular bounds. In the tabular case, mixing the usual and backward Bellman operators improves eigenvalues for asymptotic convergence, and the asymptotic convergence of the BN operator is better than TD. The forward-backward finite-rank parameterization of successor states offers reduced variance and improved samplability, with fully understood fixed points corresponding to long-range dependencies, approximating the BN method and providing two canonical representations of states as a byproduct.",1
"Inverse reinforcement learning (IRL) aims to estimate the reward function of optimizing agents by observing their response (estimates or actions). This paper considers IRL when noisy estimates of the gradient of a reward function generated by multiple stochastic gradient agents are observed. We present a generalized Langevin dynamics algorithm to estimate the reward function $R(\theta)$; specifically, the resulting Langevin algorithm asymptotically generates samples from the distribution proportional to $\exp(R(\theta))$. The proposed IRL algorithms use kernel-based passive learning schemes. We also construct multi-kernel passive Langevin algorithms for IRL which are suitable for high dimensional data. The performance of the proposed IRL algorithms are illustrated on examples in adaptive Bayesian learning, logistic regression (high dimensional problem) and constrained Markov decision processes. We prove weak convergence of the proposed IRL algorithms using martingale averaging methods. We also analyze the tracking performance of the IRL algorithms in non-stationary environments where the utility function $R(\theta)$ jump changes over time as a slow Markov chain.",0
"The goal of Inverse Reinforcement Learning (IRL) is to determine the reward function of agents that optimize their behavior, based on observations of their actions or estimates. This study focuses on IRL in scenarios where multiple stochastic gradient agents generate noisy estimates of the reward function's gradient. To estimate the reward function $R(\theta)$, a generalized Langevin dynamics algorithm is proposed, which generates samples asymptotically from the distribution proportional to $\exp(R(\theta))$. Kernel-based passive learning methods are employed in the proposed IRL algorithms, and multi-kernel passive Langevin algorithms are developed to handle high-dimensional data. The effectiveness of these algorithms is demonstrated through examples in adaptive Bayesian learning, logistic regression, and constrained Markov decision processes. The study uses martingale averaging methods to prove weak convergence of the IRL algorithms and analyzes their tracking performance in non-stationary environments where the utility function $R(\theta)$ changes over time as a slow Markov chain.",1
"Flexibility design problems are a class of problems that appear in strategic decision-making across industries, where the objective is to design a ($e.g.$, manufacturing) network that affords flexibility and adaptivity. The underlying combinatorial nature and stochastic objectives make flexibility design problems challenging for standard optimization methods. In this paper, we develop a reinforcement learning (RL) framework for flexibility design problems. Specifically, we carefully design mechanisms with noisy exploration and variance reduction to ensure empirical success and show the unique advantage of RL in terms of fast-adaptation. Empirical results show that the RL-based method consistently finds better solutions compared to classical heuristics.",0
"The problem of designing flexible networks to meet strategic objectives is prevalent in various industries, and it poses a challenge to traditional optimization approaches due to its stochastic and combinatorial nature. This paper introduces a reinforcement learning framework that incorporates mechanisms for noisy exploration and variance reduction to overcome these obstacles and achieve empirical success. We show that RL offers unique advantages for fast adaptation, and our empirical findings demonstrate that our method consistently outperforms classical heuristics in finding better solutions.",1
"Entropic regularization of policies in Reinforcement Learning (RL) is a commonly used heuristic to ensure that the learned policy explores the state-space sufficiently before overfitting to a local optimal policy. The primary motivation for using entropy is for exploration and disambiguating optimal policies; however, the theoretical effects are not entirely understood. In this work, we study the more general regularized RL objective and using Fenchel duality; we derive the dual problem which takes the form of an adversarial reward problem. In particular, we find that the optimal policy found by a regularized objective is precisely an optimal policy of a reinforcement learning problem under a worst-case adversarial reward. Our result allows us to reinterpret the popular entropic regularization scheme as a form of robustification. Furthermore, due to the generality of our results, we apply to other existing regularization schemes. Our results thus give insights into the effects of regularization of policies and deepen our understanding of exploration through robust rewards at large.",0
"The use of entropy in Reinforcement Learning (RL) is a common heuristic to prevent overfitting to a local optimal policy by ensuring sufficient exploration of the state-space. While entropy is primarily used for exploration and disambiguating optimal policies, its theoretical effects are not fully comprehended. This study investigates the regularized RL objective and derives the dual problem using Fenchel duality, resulting in an adversarial reward problem. The optimal policy found by a regularized objective is precisely an optimal policy of a reinforcement learning problem under a worst-case adversarial reward. This interpretation allows entropic regularization to be viewed as a form of robustification, and our results are applicable to other existing regularization schemes. Our findings deepen our understanding of the effects of policy regularization and exploration through robust rewards.",1
"In recent years, a variety of tasks have been accomplished by deep reinforcement learning (DRL). However, when applying DRL to tasks in a real-world environment, designing an appropriate reward is difficult. Rewards obtained via actual hardware sensors may include noise, misinterpretation, or failed observations. The learning instability caused by these unstable signals is a problem that remains to be solved in DRL. In this work, we propose an approach that extends existing DRL models by adding a subtask to directly estimate the variance contained in the reward signal. The model then takes the feature map learned by the subtask in a critic network and sends it to the actor network. This enables stable learning that is robust to the effects of potential noise. The results of experiments in the Atari game domain with unstable reward signals show that our method stabilizes training convergence. We also discuss the extensibility of the model by visualizing feature maps. This approach has the potential to make DRL more practical for use in noisy, real-world scenarios.",0
"Deep reinforcement learning (DRL) has achieved success in various tasks in recent years. However, implementing DRL in real-world environments presents a challenge in designing appropriate rewards. The use of actual hardware sensors may result in rewards that are unstable due to noise, misinterpretation, or failed observations. This instability poses a problem in DRL that needs to be addressed. To address this issue, we propose a method that extends existing DRL models by adding a subtask to estimate the variance contained in the reward signal directly. The feature map learned by the subtask in a critic network is then sent to the actor network, enabling stable learning that is resilient to potential noise. Our experiments in the Atari game domain with unstable reward signals demonstrate that our approach stabilizes training convergence. We also explore the model's extensibility by visualizing feature maps. Our method has the potential to make DRL more practical for use in noisy, real-world scenarios.",1
"Training a multi-agent reinforcement learning (MARL) algorithm is more challenging than training a single-agent reinforcement learning algorithm, because the result of a multi-agent task strongly depends on the complex interactions among agents and their interactions with a stochastic and dynamic environment. We propose an algorithm that boosts MARL training using the biased action information of other agents based on a friend-or-foe concept. For a cooperative and competitive environment, there are generally two groups of agents: cooperative-agents and competitive-agents. In the proposed algorithm, each agent updates its value function using its own action and the biased action information of other agents in the two groups. The biased joint action of cooperative agents is computed as the sum of their actual joint action and the imaginary cooperative joint action, by assuming all the cooperative agents jointly maximize the target agent's value function. The biased joint action of competitive agents can be computed similarly. Each agent then updates its own value function using the biased action information, resulting in a biased value function and corresponding biased policy. Subsequently, the biased policy of each agent is inevitably subjected to recommend an action to cooperate and compete with other agents, thereby introducing more active interactions among agents and enhancing the MARL policy learning. We empirically demonstrate that our algorithm outperforms existing algorithms in various mixed cooperative-competitive environments. Furthermore, the introduced biases gradually decrease as the training proceeds and the correction based on the imaginary assumption vanishes.",0
"Teaching a multi-agent reinforcement learning (MARL) algorithm is a more difficult task compared to teaching a single-agent reinforcement learning algorithm. This is because the outcome of a multi-agent task relies heavily on intricate interactions among agents and how they interact with a dynamic and stochastic environment. We have developed an algorithm that enhances MARL training by using the partiality of other agents' actions based on a ""friend-or-foe"" concept. In a cooperative and competitive environment, there are two main groups of agents: cooperative-agents and competitive-agents. In our algorithm, each agent updates its value function using its own action and the partial action information of other agents in the two groups. The partial joint action of cooperative agents is calculated by adding their actual joint action and the hypothetical cooperative joint action, assuming that all cooperative agents are jointly maximizing the target agent's value function. The partial joint action of competitive agents can be computed similarly. Each agent then updates its own value function using the partial action information, leading to a partial value function and corresponding biased policy. Consequently, each agent's biased policy recommends actions to cooperate and compete with other agents, thereby increasing active interactions among agents and enhancing the MARL policy learning. We have demonstrated that our algorithm outperforms existing algorithms in various mixed cooperative-competitive environments. Moreover, the introduced biases gradually decrease as the training progresses, and the correction based on the hypothetical assumption disappears.",1
"Multi-Agent Reinforcement Learning (MARL) has demonstrated significant success in training decentralised policies in a centralised manner by making use of value factorization methods. However, addressing surprise across spurious states and approximation bias remain open problems for multi-agent settings. Towards this goal, we introduce the Energy-based MIXer (EMIX), an algorithm which minimizes surprise utilizing the energy across agents. Our contributions are threefold; (1) EMIX introduces a novel surprise minimization technique across multiple agents in the case of multi-agent partially-observable settings. (2) EMIX highlights a practical use of energy functions in MARL with theoretical guarantees and experiment validations of the energy operator. Lastly, (3) EMIX extends Maxmin Q-learning for addressing overestimation bias across agents in MARL. In a study of challenging StarCraft II micromanagement scenarios, EMIX demonstrates consistent stable performance for multiagent surprise minimization. Moreover, our ablation study highlights the necessity of the energy-based scheme and the need for elimination of overestimation bias in MARL. Our implementation of EMIX can be found at karush17.github.io/emix-web/.",0
"Multi-Agent Reinforcement Learning (MARL) has achieved notable success in training decentralized policies in a centralized manner, leveraging value factorization methods. However, multi-agent settings face unresolved issues, such as addressing surprise in spurious states and approximation bias. To address these challenges, we introduce the Energy-based MIXer (EMIX) algorithm, which uses energy across agents to minimize surprise. Our contributions are threefold: (1) EMIX introduces a novel technique for minimizing surprise across multiple agents in multi-agent partially-observable settings. (2) EMIX demonstrates the practical use of energy functions in MARL, supported by theoretical guarantees and experiment validations of the energy operator. Lastly, (3) EMIX expands on Maxmin Q-learning to tackle overestimation bias across agents in MARL. In challenging StarCraft II micromanagement scenarios, EMIX delivers consistent stable performance for multi-agent surprise minimization. Furthermore, our ablation study emphasizes the importance of the energy-based scheme and the need to eliminate overestimation bias in MARL. Visit karush17.github.io/emix-web/ for our implementation of EMIX.",1
"A hallmark of an AI agent is to mimic human beings to understand and interact with others. In this paper, we propose a collaborative multi-agent reinforcement learning algorithm to learn a \emph{joint} policy through the interactions over agents. To make a joint decision over the group, each agent makes an initial decision and tells its policy to its neighbors. Then each agent modifies its own policy properly based on received messages and spreads out its plan. As this intention propagation procedure goes on, we prove that it converges to a mean-field approximation of the joint policy with the framework of neural embedded probabilistic inference. We evaluate our algorithm on several large scale challenging tasks and demonstrate that it outperforms previous state-of-the-arts.",0
"The hallmark of AI agents is their ability to imitate humans in order to comprehend and engage with others. This study introduces a collaborative multi-agent reinforcement learning algorithm that facilitates the learning of a joint policy through agent interactions. Each agent initiates a decision and shares its policy with its neighboring agents, allowing them to modify their own policies based on received messages and disseminate their plans. We demonstrate that this intention propagation procedure leads to a mean-field approximation of the joint policy within the neural embedded probabilistic inference framework. Our algorithm is evaluated on various difficult tasks and is shown to outperform previous state-of-the-art methods.",1
"The Arcade Learning Environment (""ALE"") is a widely used library in the reinforcement learning community that allows easy programmatic interfacing with Atari 2600 games, via the Stella emulator. We introduce a publicly available extension to the ALE that extends its support to multiplayer games and game modes. This interface is additionally integrated with PettingZoo to allow for a simple Gym-like interface in Python to interact with these games. We additionally introduce experimental baselines for all environments included.",0
"An extensively used library among the reinforcement learning community is the Arcade Learning Environment (""ALE""). Through the Stella emulator, it enables easy programmatic interfacing with Atari 2600 games. Our contribution to this library is a publicly accessible extension that broadens its support to multiplayer games and game modes. Moreover, we have integrated this interface with PettingZoo, which provides a straightforward Python-based Gym-like interface to interact with these games. We have also introduced preliminary baselines for all the environments incorporated.",1
"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",0
"Adversarial perturbations to observations can render Deep reinforcement learning (RL) policies vulnerable, much like adversarial examples for classifiers. However, attackers cannot typically modify another agent's observations directly, prompting the question of whether attacking an RL agent is possible by selecting an adversarial policy in a multi-agent environment to create natural adversarial observations. Our study illustrates the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, pitted against state-of-the-art victims trained via self-play to be opponent-resistant. Adversarial policies consistently outperform victims while exhibiting seemingly uncoordinated and random behavior. The study also reveals that these policies are more successful in high-dimensional environments, causing substantial differences in victim policy network activations when compared to normal opponents. Videos are available at https://adversarialpolicies.github.io/.",1
"Advances in Reinforcement Learning (RL) have demonstrated data efficiency and optimal control over large state spaces at the cost of scalable performance. Genetic methods, on the other hand, provide scalability but depict hyperparameter sensitivity towards evolutionary operations. However, a combination of the two methods has recently demonstrated success in scaling RL agents to high-dimensional action spaces. Parallel to recent developments, we present the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. We abstract exploration from exploitation by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC). Through this lens, we enable dominant skill transfer between offsprings by making use of soft winner selections and genetic crossovers in hindsight and simultaneously improve hyperparameter sensitivity in evolutions using the novel Automatic Mutation Tuning (AMT). AMT gradually replaces the entropy framework of SAC allowing the population to succeed at the task while acting as randomly as possible, without making use of backpropagation updates. In a study of challenging locomotion tasks consisting of high-dimensional action spaces and sparse rewards, ESAC demonstrates improved performance and sample efficiency in comparison to the Maximum Entropy framework. Additionally, ESAC presents efficacious use of hardware resources and algorithm overhead. A complete implementation of ESAC can be found at karush17.github.io/esac-web/.",0
"Recent advancements in Reinforcement Learning (RL) have shown that optimal control over large state spaces and data efficiency can be achieved at the expense of scalable performance. Genetic methods, on the other hand, provide scalability but suffer from hyperparameter sensitivity towards evolutionary operations. However, a combination of both methods has proven successful in scaling RL agents to high-dimensional action spaces. In line with these developments, we introduce the Evolution-based Soft Actor-Critic (ESAC), which is a scalable RL algorithm. We separate exploration from exploitation by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), thereby enabling dominant skill transfer between offspring through the use of soft winner selections and genetic crossovers in hindsight. In addition, we improve hyperparameter sensitivity in evolutions using the novel Automatic Mutation Tuning (AMT), which gradually replaces the entropy framework of SAC. This allows the population to succeed at a task while acting as randomly as possible, without the use of backpropagation updates. In a study of challenging locomotion tasks with high-dimensional action spaces and sparse rewards, ESAC shows improved performance and sample efficiency when compared to the Maximum Entropy framework. Furthermore, ESAC effectively utilizes hardware resources and minimizes algorithm overhead. A complete implementation of ESAC can be found at karush17.github.io/esac-web/.",1
"The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the final performance of hyperparameters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efficient hyperparameter tuning. We propose to learn an evaluation function compressing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the benefit of assessing a hyperparameter setting over additional training steps against their computation cost. We further increase model efficiency by selectively including scores from different training steps for any evaluated hyperparameter set. We demonstrate the efficiency of our algorithm by tuning hyperparameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time.",0
"The effectiveness of deep (reinforcement) learning systems is heavily reliant on the selection of hyperparameters. The process of tuning these parameters is known to be time-consuming, requiring multiple iterations before convergence is reached. Conventional tuning methods only consider the final performance of hyperparameters, disregarding any intermediate data from previous training steps. This study presents a Bayesian optimization (BO) approach that capitalizes on the iterative structure of learning algorithms to optimize hyperparameters efficiently. An evaluation function is proposed to compress the learning progress at any stage of the training process into a single numeric score based on both training success and stability. The BO framework balances the benefit of assessing a hyperparameter setting over additional training steps and the computation cost. Additionally, scores from different training steps are selectively included to increase model efficiency for any evaluated hyperparameter set. The proposed algorithm demonstrates its efficiency by effectively tuning hyperparameters for deep reinforcement learning agents and convolutional neural networks, outperforming all existing baselines in identifying optimal hyperparameters in minimal time.",1
"In recent years, deep learning methods bring incredible progress to the field of object detection. However, in the field of remote sensing image processing, existing methods neglect the relationship between imaging configuration and detection performance, and do not take into account the importance of detection performance feedback for improving image quality. Therefore, detection performance is limited by the passive nature of the conventional object detection framework. In order to solve the above limitations, this paper takes adaptive brightness adjustment and scale adjustment as examples, and proposes an active object detection method based on deep reinforcement learning. The goal of adaptive image attribute learning is to maximize the detection performance. With the help of active object detection and image attribute adjustment strategies, low-quality images can be converted into high-quality images, and the overall performance is improved without retraining the detector.",0
"The field of object detection has made remarkable progress in recent years, thanks to the introduction of deep learning methods. However, when it comes to remote sensing image processing, current methods fail to acknowledge the correlation between imaging configuration and detection performance. Furthermore, they do not consider the significance of detection performance feedback for enhancing image quality. As a result, conventional object detection frameworks have limited detection performance due to their passive nature. To overcome these limitations, this study proposes an active object detection method that employs deep reinforcement learning and focuses on adaptive brightness and scale adjustments. The main objective of the proposed approach is to maximize detection performance by learning adaptive image attributes. By using active object detection and image attribute adjustment strategies, low-quality images can be transformed into high-quality ones, and the overall performance can be enhanced without the need for detector retraining.",1
"Meta Reinforcement Learning (RL) methods focus on automating the design of RL algorithms that generalize to a wide range of environments. The framework introduced in (Anonymous, 2020) addresses the problem by representing different RL algorithms as Directed Acyclic Graphs (DAGs), and using an evolutionary meta learner to modify these graphs and find good agent update rules. While the search language used to generate graphs in the paper serves to represent numerous already-existing RL algorithms (e.g., DQN, DDQN), it has limitations when it comes to representing Policy Gradient algorithms. In this work we try to close this gap by extending the original search language and proposing graphs for five different Policy Gradient algorithms: VPG, PPO, DDPG, TD3, and SAC.",0
"Meta Reinforcement Learning (RL) techniques aim to automate the creation of RL algorithms that can adapt to various environments. A framework presented in (Anonymous, 2020) tackles this challenge by utilizing Directed Acyclic Graphs (DAGs) to represent different RL algorithms and employing an evolutionary meta learner to modify these graphs and discover effective agent update rules. Although the search language used in the paper can represent many existing RL algorithms (such as DQN and DDQN), it has certain limitations when it comes to Policy Gradient algorithms. This study aims to address this issue by expanding the original search language and presenting graphs for five distinct Policy Gradient algorithms: VPG, PPO, DDPG, TD3, and SAC.",1
"In a sequential decision-making problem, off-policy evaluation estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. Reinforcement learning in high-stake environments, such as healthcare and education, is often limited to off-policy settings due to safety or ethical concerns, or inability of exploration. Hence it is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. In this paper, we propose a novel framework that provides robust and optimistic cumulative reward estimates using one or multiple logged trajectories data. Leveraging methodologies from distributionally robust optimization, we show that with proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with non-asymptotic and asymptotic guarantees under stochastic or adversarial environments. Our results are also generalized to batch reinforcement learning and are supported by empirical analysis.",0
"The process of off-policy evaluation in sequential decision-making problems involves estimating the expected cumulative reward of a target policy using logged trajectory data from a different behavior policy, without actual execution of the target policy. In high-stake environments, such as healthcare and education, reinforcement learning is often restricted to off-policy settings due to safety and ethical concerns, or the inability to explore. Therefore, it is crucial to quantify the off-policy estimate's uncertainty before deploying the target policy. This paper proposes a new framework that provides robust and optimistic cumulative reward estimates based on one or multiple logged trajectory data. By utilizing distributionally robust optimization methodologies, we demonstrate that these estimates serve as confidence bounds with non-asymptotic and asymptotic guarantees under stochastic or adversarial environments. Our results extend to batch reinforcement learning and are reinforced by empirical analysis.",1
"Remote Electrical Tilt (RET) optimization is an efficient method for adjusting the vertical tilt angle of Base Stations (BSs) antennas in order to optimize Key Performance Indicators (KPIs) of the network. Reinforcement Learning (RL) provides a powerful framework for RET optimization because of its self-learning capabilities and adaptivity to environmental changes. However, an RL agent may execute unsafe actions during the course of its interaction, i.e., actions resulting in undesired network performance degradation. Since the reliability of services is critical for Mobile Network Operators (MNOs), the prospect of performance degradation has prohibited the real-world deployment of RL methods for RET optimization. In this work, we model the RET optimization problem in the Safe Reinforcement Learning (SRL) framework with the goal of learning a tilt control strategy providing performance improvement guarantees with respect to a safe baseline. We leverage a recent SRL method, namely Safe Policy Improvement through Baseline Bootstrapping (SPIBB), to learn an improved policy from an offline dataset of interactions collected by the safe baseline. Our experiments show that the proposed approach is able to learn a safe and improved tilt update policy, providing a higher degree of reliability and potential for real-world network deployment.",0
"Optimizing the vertical tilt angle of Base Stations (BSs) antennas through Remote Electrical Tilt (RET) is an effective way to improve network Key Performance Indicators (KPIs). Reinforcement Learning (RL) is a suitable framework for RET optimization due to its adaptivity and self-learning capabilities. However, RL agents may take unsafe actions that negatively affect network performance, which is a concern for Mobile Network Operators (MNOs) who prioritize service reliability. As a result, RL methods for RET optimization have not been widely deployed in the real world. To address this issue, we propose a Safe Reinforcement Learning (SRL) framework that models RET optimization to learn a tilt control strategy that guarantees performance improvement. We employ the Safe Policy Improvement through Baseline Bootstrapping (SPIBB) method to improve policy learning from an offline dataset of interactions collected by the safe baseline. Our experiments demonstrate that our approach is able to learn a safe and improved tilt update policy, providing a higher level of reliability for potential real-world network deployment.",1
"Traditional autonomous vehicle pipelines that follow a modular approach have been very successful in the past both in academia and industry, which has led to autonomy deployed on road. Though this approach provides ease of interpretation, its generalizability to unseen environments is limited and hand-engineering of numerous parameters is required, especially in the prediction and planning systems. Recently, deep reinforcement learning has been shown to learn complex strategic games and perform challenging robotic tasks, which provides an appealing framework for learning to drive. In this work, we propose a deep reinforcement learning framework to learn optimal control policy using waypoints and low-dimensional visual representations, also known as affordances. We demonstrate that our agents when trained from scratch learn the tasks of lane-following, driving around inter-sections as well as stopping in front of other actors or traffic lights even in the dense traffic setting. We note that our method achieves comparable or better performance than the baseline methods on the original and NoCrash benchmarks on the CARLA simulator.",0
"In the past, the modular approach of traditional autonomous vehicle pipelines has been successful in both academia and industry, resulting in autonomy on the road. However, this approach has limitations in its ability to adapt to new environments and requires significant hand-engineering of parameters, particularly in prediction and planning systems. Recently, deep reinforcement learning has proven effective in learning complex strategic games and challenging robotic tasks, making it an attractive framework for learning to drive. Our work proposes a deep reinforcement learning framework that uses waypoints and low-dimensional visual representations (affordances) to learn the optimal control policy. Our agents, trained from scratch, can perform lane-following, navigate intersections, and stop in front of other actors or traffic lights, even in dense traffic settings. Our method performs comparably or better than baseline methods on the original and NoCrash benchmarks in the CARLA simulator.",1
"Several AutoML approaches have been proposed to automate the machine learning (ML) process, such as searching for the ML model architectures and hyper-parameters. However, these AutoML pipelines only focus on improving the learning accuracy of benign samples while ignoring the ML model robustness under adversarial attacks. As ML systems are increasingly being used in a variety of mission-critical applications, improving the robustness of ML systems has become of utmost importance. In this paper, we propose the first robust AutoML framework, Robusta--based on reinforcement learning (RL)--to perform feature selection, aiming to select features that lead to both accurate and robust ML systems. We show that a variation of the 0-1 robust loss can be directly optimized via an RL-based combinatorial search in the feature selection scenario. In addition, we employ heuristics to accelerate the search procedure based on feature scoring metrics, which are mutual information scores, tree-based classifiers feature importance scores, F scores, and Integrated Gradient (IG) scores, as well as their combinations. We conduct extensive experiments and show that the proposed framework is able to improve the model robustness by up to 22% while maintaining competitive accuracy on benign samples compared with other feature selection methods.",0
"Various approaches have been proposed for AutoML to automate machine learning (ML) processes, including searching for ML model architectures and hyper-parameters. However, these AutoML pipelines only concentrate on enhancing the learning accuracy of benign samples and overlook the robustness of the ML model under adversarial attacks. As ML systems are progressively utilized in mission-critical applications, ensuring the robustness of ML systems has become a top priority. This paper presents Robusta, the first robust AutoML framework based on reinforcement learning (RL), to perform feature selection that aims to select features that lead to both accurate and robust ML systems. The paper demonstrates that a variant of the 0-1 robust loss can be directly optimized through an RL-based combinatorial search in the feature selection scenario. Additionally, heuristics are utilized to expedite the search process based on feature scoring metrics, including mutual information scores, tree-based classifiers feature importance scores, F scores, and Integrated Gradient (IG) scores, as well as their combinations. Extensive experiments have been conducted, and the proposed framework has been shown to enhance the model's robustness by up to 22% while maintaining competitive accuracy on benign samples compared to other feature selection methods.",1
"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",0
"To enable deep reinforcement learning agents to operate effectively alongside humans in realistic environments, it is crucial to ensure their robustness. Given the diverse nature of the real world and the possibility of human behavior changing in response to agent deployment, these agents may encounter novel situations that were not seen during training, posing a challenge for evaluation. As such, relying solely on average training or validation reward as a metric may not be effective in evaluating robustness. To address this, we propose taking a cue from software engineering's unit testing practice. In designing AI agents that collaborate with humans, designers should identify potential edge cases in possible partner behavior and states encountered, and write tests to verify the agent's behavior in these cases. We apply this approach to Overcooked-AI and discover that our test suite provides valuable insight into the effects of proposed robustness improvements, which were not evident by solely examining average validation reward metrics.",1
"A major challenge in reinforcement learning is exploration, when local dithering methods such as epsilon-greedy sampling are insufficient to solve a given task. Many recent methods have proposed to intrinsically motivate an agent to seek novel states, driving the agent to discover improved reward. However, while state-novelty exploration methods are suitable for tasks where novel observations correlate well with improved reward, they may not explore more efficiently than epsilon-greedy approaches in environments where the two are not well-correlated. In this paper, we distinguish between exploration tasks in which seeking novel states aids in finding new reward, and those where it does not, such as goal-conditioned tasks and escaping local reward maxima. We propose a new exploration objective, maximizing the reward prediction error (RPE) of a value function trained to predict extrinsic reward. We then propose a deep reinforcement learning method, QXplore, which exploits the temporal difference error of a Q-function to solve hard exploration tasks in high-dimensional MDPs. We demonstrate the exploration behavior of QXplore on several OpenAI Gym MuJoCo tasks and Atari games and observe that QXplore is comparable to or better than a baseline state-novelty method in all cases, outperforming the baseline on tasks where state novelty is not well-correlated with improved reward.",0
"Exploration is a significant hurdle in reinforcement learning, as local methods like epsilon-greedy sampling may not suffice to solve specific tasks. To address this issue, some recent approaches have proposed intrinsic motivation to encourage agents to seek new states, leading to better rewards. However, while these methods work well for tasks where novel observations yield better rewards, they may not be as effective as epsilon-greedy approaches in environments where novel observations do not correlate with better rewards. In this paper, we distinguish between exploration tasks where seeking novel states helps in finding better rewards and those where it does not, such as goal-oriented tasks and escaping local reward peaks. We introduce a new exploration objective, maximizing the reward prediction error (RPE) of a value function trained to predict extrinsic reward. We then present QXplore, a deep reinforcement learning method that exploits the temporal difference error of a Q-function to solve challenging exploration tasks in high-dimensional MDPs. We demonstrate QXplore's exploration behavior on various OpenAI Gym MuJoCo tasks and Atari games and show that it performs comparably or better than a baseline state-novelty method in all cases, outperforming the baseline in tasks where state novelty does not correlate with better rewards.",1
"Scientifically evaluating soccer players represents a challenging Machine Learning problem. Unfortunately, most existing answers have very opaque algorithm training procedures; relevant data are scarcely accessible and almost impossible to generate. In this paper, we will introduce a two-part solution: an open-source Player Tracking model and a new approach to evaluate these players based solely on Deep Reinforcement Learning, without human data training nor guidance. Our tracking model was trained in a supervised fashion on datasets we will also release, and our Evaluation Model relies only on simulations of virtual soccer games. Combining those two architectures allows one to evaluate Soccer Players directly from a live camera without large datasets constraints. We term our new approach Expected Discounted Goal (EDG), as it represents the number of goals a team can score or concede from a particular state. This approach leads to more meaningful results than the existing ones that are based on real-world data, and could easily be extended to other sports.",0
"Assessing the skills of soccer players using scientific methods poses a complex challenge for Machine Learning. However, current solutions lack transparency in their algorithm training, while acquiring relevant data remains a difficult task. This study proposes a two-fold remedy: an open-source Player Tracking model and a novel approach to assess players based on Deep Reinforcement Learning, without the need for human data training or guidance. Our tracking model was trained using supervised learning on datasets that we will make publicly available, while our Evaluation Model relies on virtual soccer game simulations. The combination of these two models enables the evaluation of Soccer Players directly from a live camera, without the limitations of large datasets. We refer to our approach as Expected Discounted Goal (EDG), which calculates the number of goals a team can score or concede from a particular state. This method outperforms existing ones that rely on real-world data, and can be easily adapted to other sports.",1
"As power systems are undergoing a significant transformation with more uncertainties, less inertia and closer to operation limits, there is increasing risk of large outages. Thus, there is an imperative need to enhance grid emergency control to maintain system reliability and security. Towards this end, great progress has been made in developing deep reinforcement learning (DRL) based grid control solutions in recent years. However, existing DRL-based solutions have two main limitations: 1) they cannot handle well with a wide range of grid operation conditions, system parameters, and contingencies; 2) they generally lack the ability to fast adapt to new grid operation conditions, system parameters, and contingencies, limiting their applicability for real-world applications. In this paper, we mitigate these limitations by developing a novel deep meta reinforcement learning (DMRL) algorithm. The DMRL combines the meta strategy optimization together with DRL, and trains policies modulated by a latent space that can quickly adapt to new scenarios. We test the developed DMRL algorithm on the IEEE 300-bus system. We demonstrate fast adaptation of the meta-trained DRL polices with latent variables to new operating conditions and scenarios using the proposed method and achieve superior performance compared to the state-of-the-art DRL and model predictive control (MPC) methods.",0
"As power systems face greater uncertainties, less inertia, and operate closer to their limits, the risk of large outages increases. Therefore, it is crucial to improve grid emergency control to ensure system reliability and security. Deep reinforcement learning (DRL) has made significant progress in developing grid control solutions. However, current DRL solutions have two main limitations: they struggle with a wide range of grid conditions and lack the ability to quickly adapt to new scenarios, limiting their real-world application. To address these limitations, we propose a novel deep meta reinforcement learning (DMRL) algorithm that combines meta strategy optimization with DRL. The DMRL trains policies modulated by a latent space that enables quick adaptation to new scenarios. We test the DMRL algorithm on the IEEE 300-bus system and demonstrate superior performance compared to the state-of-the-art DRL and model predictive control (MPC) methods.",1
"Off-policy reinforcement learning holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting -- where a fixed collection of interactions are provided and no further interactions are allowed -- it has been shown that standard off-policy RL methods can significantly underperform. Recently proposed methods often aim to address this shortcoming by constraining learned policies to remain close to the given dataset of interactions. In this work, we closely investigate an important simplification of BCQ -- a prior approach for offline RL -- which removes a heuristic design choice and naturally restricts extracted policies to remain exactly within the support of a given behavior policy. Importantly, in contrast to their original theoretical considerations, we derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing us to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting -- the main focus of this work -- EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks. In the online RL setting, we demonstrate that EMaQ is competitive with Soft Actor Critic. The key contributions of our empirical findings are demonstrating the importance of careful generative model design for estimating behavior policies, and an intuitive notion of complexity for offline RL problems. With its simple interpretation and fewer moving parts, such as no explicit function approximator representing the policy, EMaQ serves as a strong yet easy to implement baseline for future work.",0
"The efficient learning of decision-making policies through off-policy reinforcement learning using past experience is promising. However, the standard off-policy RL methods can underperform significantly in the offline RL setting, where a fixed collection of interactions is available, and no further interactions are allowed. To address this issue, recently proposed methods restrict learned policies to remain close to the given dataset of interactions. In this study, we closely examine a simplified version of BCQ, which naturally restricts extracted policies to remain exactly within the support of a given behavior policy by removing a heuristic design choice. We derive the simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which considers the distribution support, number of samples, and proposal distribution. Our empirical findings demonstrate that careful generative model design for estimating behavior policies is essential, and we propose an intuitive measure of complexity for offline RL problems. EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks in the offline RL setting and is competitive with Soft Actor Critic in the online RL setting. With its simple interpretation and fewer moving parts, EMaQ serves as a strong yet easy to implement baseline for future work.",1
"This paper proves that the episodic learning environment of every finite-horizon decision task has a unique steady state under any behavior policy, and that the marginal distribution of the agent's input indeed converges to the steady-state distribution in essentially all episodic learning processes. This observation supports an interestingly reversed mindset against conventional wisdom: While the existence of unique steady states was often presumed in continual learning but considered less relevant in episodic learning, it turns out their existence is guaranteed for the latter. Based on this insight, the paper unifies episodic and continual RL around several important concepts that have been separately treated in these two RL formalisms. Practically, the existence of unique and approachable steady state enables a general way to collect data in episodic RL tasks, which the paper applies to policy gradient algorithms as a demonstration, based on a new steady-state policy gradient theorem. Finally, the paper also proposes and experimentally validates a perturbation method that facilitates rapid steady-state convergence in real-world RL tasks.",0
"In this paper, it is demonstrated that every finite-horizon decision task's episodic learning environment has a distinct steady state that is guaranteed under any behavior policy. Moreover, it is observed that the agent's input's marginal distribution converges to the steady-state distribution in almost all episodic learning processes. This finding challenges the conventional wisdom that unique steady states are more relevant in continual learning than in episodic learning. Consequently, the paper unifies episodic and continual RL by integrating various crucial concepts that have been separately dealt with in these two RL formalisms. The paper also presents a general approach to collect data in episodic RL tasks, which is enabled by the existence of the unique and accessible steady state. The proposed method is demonstrated using policy gradient algorithms based on a new steady-state policy gradient theorem. Additionally, the paper suggests and validates a perturbation technique that accelerates steady-state convergence in practical RL tasks.",1
"The study of exploration in the domain of decision making has a long history but remains actively debated. From the vast literature that addressed this topic for decades under various points of view (e.g., developmental psychology, experimental design, artificial intelligence), intrinsic motivation emerged as a concept that can practically be transferred to artificial agents. Especially, in the recent field of Deep Reinforcement Learning (RL), agents implement such a concept (mainly using a novelty argument) in the shape of an exploration bonus, added to the task reward, that encourages visiting the whole environment. This approach is supported by the large amount of theory on RL for which convergence to optimality assumes exhaustive exploration. Yet, Human Beings and mammals do not exhaustively explore the world and their motivation is not only based on novelty but also on various other factors (e.g., curiosity, fun, style, pleasure, safety, competition, etc.). They optimize for life-long learning and train to learn transferable skills in playgrounds without obvious goals. They also apply innate or learned priors to save time and stay safe. For these reasons, we propose to learn an exploration bonus from demonstrations that could transfer these motivations to an artificial agent with little assumptions about their rationale. Using an inverse RL approach, we show that complex exploration behaviors, reflecting different motivations, can be learnt and efficiently used by RL agents to solve tasks for which exhaustive exploration is prohibitive.",0
"The exploration of decision making has a rich history but remains a topic of active debate. The literature on this subject, spanning various perspectives such as developmental psychology, experimental design, and artificial intelligence, has identified intrinsic motivation as a practical concept that can be applied to artificial agents. In the field of Deep Reinforcement Learning (RL), agents use an exploration bonus, which is added to the task reward, to encourage visiting the entire environment. This approach is backed by RL theory, which assumes exhaustive exploration leads to optimality. However, humans and mammals do not explore the world exhaustively and are motivated by factors beyond novelty, such as curiosity, fun, style, pleasure, safety, and competition. They aim for lifelong learning and acquire transferable skills in playgrounds without clear objectives while also using innate or learned priors to save time and stay safe. To transfer these motivations to artificial agents, we suggest learning an exploration bonus from demonstrations. We demonstrate using an inverse RL approach that RL agents can learn complex exploration behaviors reflecting various motivations, which can solve tasks that require exhaustive exploration efficiently.",1
"Background and motivation: Deep Reinforcement Learning (Deep RL) is a rapidly developing field. Historically most application has been made to games (such as chess, Atari games, and go). Deep RL is now reaching the stage where it may offer value in real world problems, including optimisation of healthcare systems. One such problem is where to locate ambulances between calls in order to minimise time from emergency call to ambulance on-scene. This is known as the Ambulance Location problem.   Aim: To develop an OpenAI Gym-compatible framework and simulation environment for testing Deep RL agents.   Methods: A custom ambulance dispatch simulation environment was developed using OpenAI Gym and SimPy. Deep RL agents were built using PyTorch. The environment is a simplification of the real world, but allows control over the number of clusters of incident locations, number of possible dispatch locations, number of hospitals, and creating incidents that occur at different locations throughout each day.   Results: A range of Deep RL agents based on Deep Q networks were tested in this custom environment. All reduced time to respond to emergency calls compared with random allocation to dispatch points. Bagging Noisy Duelling Deep Q networks gave the most consistence performance. All methods had a tendency to lose performance if trained for too long, and so agents were saved at their optimal performance (and tested on independent simulation runs).   Conclusions: Deep RL agents, developed using simulated environments, have the potential to offer a novel approach to optimise the Ambulance Location problem. Creating open simulation environments should allow more rapid progress in this field.",0
"The area of Deep Reinforcement Learning (Deep RL) has experienced rapid growth, with most applications initially being applied to games like chess, Atari games, and go. However, Deep RL has now advanced to the point where it can be useful in addressing real-world issues, such as the optimization of healthcare systems. One such problem that can benefit from this technology is the Ambulance Location problem, which involves determining the optimal placement of ambulances between calls to minimize response time. The goal of this study was to develop a simulation environment compatible with OpenAI Gym to test Deep RL agents. The research utilized a custom ambulance dispatch simulation environment created using OpenAI Gym and SimPy, which allowed for control over various parameters, including the number of clusters of incident locations, the number of possible dispatch locations, and the number of hospitals. The study tested a range of Deep RL agents based on Deep Q networks in the custom environment, with all agents demonstrating a reduction in emergency response times compared to random allocation to dispatch points. The most consistent performance was achieved using Bagging Noisy Duelling Deep Q networks. However, it was found that all methods tended to lose performance if trained for too long, and agents were saved at their optimal performance level. The study concludes that simulated environments can help to advance the use of Deep RL agents in addressing the Ambulance Location problem and that open simulation environments can accelerate progress in this field.",1
"This paper introduces Fast Linearized Adaptive Policy (FLAP), a new meta-reinforcement learning (meta-RL) method that is able to extrapolate well to out-of-distribution tasks without the need to reuse data from training, and adapt almost instantaneously with the need of only a few samples during testing. FLAP builds upon the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent, such as in prior meta-RL methods like MAML, to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior Meta-RL methods fail to generalize to. Experiments on standard continuous-control meta-RL benchmarks show FLAP presenting significantly stronger performance on out-of-distribution tasks with up to double the average return and up to 8X faster adaptation run-time speeds when compared to prior methods.",0
"Introducing a new meta-reinforcement learning (meta-RL) approach named Fast Linearized Adaptive Policy (FLAP), this paper demonstrates the ability to excel in out-of-distribution tasks without having to reuse training data. FLAP adapts with only a few samples during testing and builds on the concept of learning a shared linear representation of the policy. This allows for quick prediction of linear weights when adapting to new tasks, eliminating the need for gradient descent. A separate adapter network is trained with the policy, enabling direct use during adaptation to predict linear weights. This approach not only speeds up the adaptation run-time, but also generalizes extremely well to different tasks. Standard continuous-control meta-RL benchmarks show FLAP outperforming prior methods with up to double the average return and up to 8X faster adaptation run-time speeds.",1
"End-to-end delay is a critical attribute of quality of service (QoS) in application domains such as cloud computing and computer networks. This metric is particularly important in tandem service systems, where the end-to-end service is provided through a chain of services. Service-rate control is a common mechanism for providing QoS guarantees in service systems. In this paper, we introduce a reinforcement learning-based (RL-based) service-rate controller that provides probabilistic upper-bounds on the end-to-end delay of the system, while preventing the overuse of service resources. In order to have a general framework, we use queueing theory to model the service systems. However, we adopt an RL-based approach to avoid the limitations of queueing-theoretic methods. In particular, we use Deep Deterministic Policy Gradient (DDPG) to learn the service rates (action) as a function of the queue lengths (state) in tandem service systems. In contrast to existing RL-based methods that quantify their performance by the achieved overall reward, which could be hard to interpret or even misleading, our proposed controller provides explicit probabilistic guarantees on the end-to-end delay of the system. The evaluations are presented for a tandem queueing system with non-exponential inter-arrival and service times, the results of which validate our controller's capability in meeting QoS constraints.",0
"Quality of service (QoS) in cloud computing and computer networks relies heavily on the end-to-end delay metric, especially in tandem service systems. Service-rate control is a prevalent method to ensure QoS guarantees. This paper introduces a reinforcement learning-based (RL-based) service-rate controller that provides probabilistic upper-bounds on the end-to-end delay while preventing service resource overuse. The service systems are modeled using queueing theory, but an RL-based approach is adopted to overcome the limitations. The Deep Deterministic Policy Gradient (DDPG) is used to learn the service rates as a function of queue lengths in tandem service systems. Unlike existing RL-based methods that measure performance through overall reward, our proposed controller provides explicit probabilistic guarantees on the end-to-end delay. The controller's effectiveness is validated through evaluations of a tandem queueing system with non-exponential inter-arrival and service times.",1
"The study and benchmarking of Deep Reinforcement Learning (DRL) models has become a trend in many industries, including aerospace engineering and communications. Recent studies in these fields propose these kinds of models to address certain complex real-time decision-making problems in which classic approaches do not meet time requirements or fail to obtain optimal solutions. While the good performance of DRL models has been proved for specific use cases or scenarios, most studies do not discuss the compromises and generalizability of such models during real operations. In this paper we explore the tradeoffs of different elements of DRL models and how they might impact the final performance. To that end, we choose the Frequency Plan Design (FPD) problem in the context of multibeam satellite constellations as our use case and propose a DRL model to address it. We identify 6 different core elements that have a major effect in its performance: the policy, the policy optimizer, the state, action, and reward representations, and the training environment. We analyze different alternatives for each of these elements and characterize their effect. We also use multiple environments to account for different scenarios in which we vary the dimensionality or make the environment nonstationary. Our findings show that DRL is a potential method to address the FPD problem in real operations, especially because of its speed in decision-making. However, no single DRL model is able to outperform the rest in all scenarios, and the best approach for each of the 6 core elements depends on the features of the operation environment. While we agree on the potential of DRL to solve future complex problems in the aerospace industry, we also reflect on the importance of designing appropriate models and training procedures, understanding the applicability of such models, and reporting the main performance tradeoffs.",0
"Deep Reinforcement Learning (DRL) models have become a popular choice for addressing complex real-time decision-making problems in various industries, such as aerospace engineering and communications. However, while the efficacy of DRL models has been demonstrated in specific use cases, their generalizability and performance compromises during real operations are often overlooked. Therefore, in this study, we examine the impact of different elements of DRL models on their final performance, using the Frequency Plan Design (FPD) problem in multibeam satellite constellations as a use case. We identify six core elements - the policy, policy optimizer, state, action, and reward representations, and training environment - that significantly influence DRL performance. We analyze various alternatives for each element and their effects, accounting for different scenarios by using multiple environments. Our findings suggest that DRL can effectively address the FPD problem in real operations due to its fast decision-making capabilities. However, the best approach for each core element depends on the operation environment's features, and there is no single DRL model that outperforms the rest in all scenarios. We highlight the importance of designing appropriate models and training procedures, understanding their applicability and reporting the main performance tradeoffs, to capitalize on DRL's potential in solving complex problems in the aerospace industry.",1
"This paper develops a Pontryagin Differentiable Programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP distinguishes from existing methods by two novel techniques: first, we differentiate through Pontryagin's Maximum Principle, and this allows to obtain the analytical derivative of a trajectory with respect to tunable parameters within an optimal control system, enabling end-to-end learning of dynamics, policies, or/and control objective functions; and second, we propose an auxiliary control system in the backward pass of the PDP framework, and the output of this auxiliary control system is the analytical derivative of the original system's trajectory with respect to the parameters, which can be iteratively solved using standard control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identification, and control/planning. We demonstrate the capability of the PDP in each learning mode on different high-dimensional systems, including multi-link robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.",0
"In this paper, a Pontryagin Differentiable Programming (PDP) methodology is presented as a comprehensive approach to solve various learning and control tasks. This method is unique due to two innovative techniques: firstly, the differentiation via Pontryagin's Maximum Principle, which allows for the analytical derivative of a trajectory to be obtained with respect to adjustable parameters in an optimal control system. This facilitates end-to-end learning of policies, dynamics, and/or control objective functions. Secondly, an auxiliary control system is introduced in the backward pass of the PDP framework. The output of this system is the analytical derivative of the original system's trajectory with respect to the parameters, which can be solved iteratively using standard control tools. The PDP is explored through three learning modes: inverse reinforcement learning, system identification, and control/planning, and is tested on diverse, high-dimensional systems such as a multi-link robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing to demonstrate its effectiveness.",1
"Mixup linearly interpolates pairs of examples to form new samples, which is easy to implement and has been shown to be effective in image classification tasks. However, there are two drawbacks in mixup: one is that more training epochs are needed to obtain a well-trained model; the other is that mixup requires tuning a hyper-parameter to gain appropriate capacity but that is a difficult task. In this paper, we find that mixup constantly explores the representation space, and inspired by the exploration-exploitation dilemma in reinforcement learning, we propose mixup Without hesitation (mWh), a concise, effective, and easy-to-use training algorithm. We show that mWh strikes a good balance between exploration and exploitation by gradually replacing mixup with basic data augmentation. It can achieve a strong baseline with less training time than original mixup and without searching for optimal hyper-parameter, i.e., mWh acts as mixup without hesitation. mWh can also transfer to CutMix, and gain consistent improvement on other machine learning and computer vision tasks such as object detection. Our code is open-source and available at https://github.com/yuhao318/mwh",0
"Mixup is a technique that generates new samples by creating linear interpolations of pairs of examples. It has been effective in image classification tasks, but has two disadvantages. Firstly, it requires more training epochs to obtain a well-trained model. Secondly, it is difficult to tune the hyper-parameter required for appropriate capacity. This paper proposes a solution to these issues with mixup Without hesitation (mWh), which explores the representation space without hesitation. Inspired by the exploration-exploitation dilemma in reinforcement learning, mWh strikes a balance between exploration and exploitation by gradually replacing mixup with basic data augmentation. It achieves a strong baseline with less training time and without searching for optimal hyper-parameters. mWh can also transfer to CutMix and consistently improve other machine learning and computer vision tasks, such as object detection. The code for mWh is open-source and available at https://github.com/yuhao318/mwh.",1
"We obtain global, non-asymptotic convergence guarantees for independent learning algorithms in competitive reinforcement learning settings with two agents (i.e., zero-sum stochastic games). We consider an episodic setting where in each episode, each player independently selects a policy and observes only their own actions and rewards, along with the state. We show that if both players run policy gradient methods in tandem, their policies will converge to a min-max equilibrium of the game, as long as their learning rates follow a two-timescale rule (which is necessary). To the best of our knowledge, this constitutes the first finite-sample convergence result for independent policy gradient methods in competitive RL; prior work has largely focused on centralized, coordinated procedures for equilibrium computation.",0
"Our study focuses on independent learning algorithms in competitive reinforcement learning scenarios with two agents. We provide global and non-asymptotic convergence guarantees in an episodic setting, where each player selects a policy and only observes their own actions, rewards, and state. We demonstrate that if both players use policy gradient methods simultaneously and adhere to a two-timescale learning rate rule, their policies will converge to the game's min-max equilibrium. This is the first finite-sample convergence outcome for independent policy gradient approaches in competitive RL, which differs from previous research that emphasized centralized, coordinated procedures for equilibrium computation.",1
"Most existing policy learning solutions require the learning agents to receive high-quality supervision signals, e.g., rewards in reinforcement learning (RL) or high-quality expert's demonstrations in behavioral cloning (BC). These quality supervisions are either infeasible or prohibitively expensive to obtain in practice. We aim for a unified framework that leverages the weak supervisions to perform policy learning efficiently. To handle this problem, we treat the ""weak supervisions"" as imperfect information coming from a peer agent, and evaluate the learning agent's policy based on a ""correlated agreement"" with the peer agent's policy (instead of simple agreements). Our way of leveraging peer agent's information offers us a family of solutions that learn effectively from weak supervisions with theoretical guarantees. Extensive evaluations on tasks including RL with noisy reward, BC with weak demonstrations and standard policy co-training (RL + BC) show that the proposed approach leads to substantial improvements, especially when the complexity or the noise of the learning environments grows.",0
"Current policy learning solutions typically require high-quality supervision signals, such as rewards in reinforcement learning or expert demonstrations in behavioral cloning. However, obtaining such signals can be impractical or expensive. Our goal is to develop a unified framework that can efficiently learn policies using weak supervisions. To achieve this, we treat weak supervisions as imperfect information from a peer agent and evaluate the learning agent's policy based on a correlated agreement with the peer agent's policy. This approach provides us with multiple effective solutions for learning from weak supervisions with theoretical guarantees. Our approach has been extensively evaluated on various tasks, including reinforcement learning with noisy rewards, behavioral cloning with weak demonstrations, and standard policy co-training. The results show significant improvements, particularly in complex or noisy learning environments.",1
"Abstraction is crucial for effective sequential decision making in domains with large state spaces. In this work, we propose an information bottleneck method for learning approximate bisimulations, a type of state abstraction. We use a deep neural encoder to map states onto continuous embeddings. We map these embeddings onto a discrete representation using an action-conditioned hidden Markov model, which is trained end-to-end with the neural network. Our method is suited for environments with high-dimensional states and learns from a stream of experience collected by an agent acting in a Markov decision process. Through this learned discrete abstract model, we can efficiently plan for unseen goals in a multi-goal Reinforcement Learning setting. We test our method in simplified robotic manipulation domains with image states. We also compare it against previous model-based approaches to finding bisimulations in discrete grid-world-like environments. Source code is available at https://github.com/ondrejba/discrete_abstractions.",0
"Effective decision making in domains with large state spaces requires abstraction. This study proposes an information bottleneck method for learning approximate bisimulations, a form of state abstraction. The approach utilizes a deep neural encoder to create continuous embeddings of states, which are then transformed into a discrete representation using an action-conditioned hidden Markov model. The model is trained end-to-end with the neural network and can handle high-dimensional states in a Markov decision process. The learned abstract model enables efficient planning for unseen goals in a multi-goal Reinforcement Learning setting. The method is tested in simplified robotic manipulation domains with image states and compared to previous model-based approaches for finding bisimulations in discrete grid-world-like environments. The source code for this work is available at https://github.com/ondrejba/discrete_abstractions.",1
"Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can \emph{also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.",0
"The use of meta-reinforcement learning algorithms can allow robots to learn new behaviors quickly by utilizing previous experiences in similar training tasks. However, the demanding data requirements of meta-training, combined with the difficulty of learning from sensory inputs such as images, have made applying meta-RL to real robotic systems a difficult task. One solution to this challenge is latent state models, which can speed up representation learning from visual inputs. In this study, we demonstrate that latent state models can also perform meta-learning by treating it as task inference, given an appropriate observation space. As a result, we have developed MELD, an algorithm for meta-RL from images that uses inference in a latent state model to quickly acquire new skills based on observations and rewards. MELD has outperformed previous meta-RL methods on multiple simulated image-based robotic control tasks and has allowed a real robotic arm to insert an Ethernet cable into new locations with only eight hours of meta-training in the real world. MELD is currently the only meta-RL algorithm trained in a real-world robotic control setting from images.",1
"Agents that interact with other agents often do not know a priori what the other agents' strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents' strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.",0
"When agents interact with other agents, they often don't know the strategies of the others beforehand. They need to maximize their own return while learning about the others. The Interactive Bayesian Reinforcement Learning framework can compute the optimal adaptive behavior under uncertainty over the other agents' strategies with respect to some prior. However, it is difficult to do so in most settings. Existing approximation methods are only suitable for small tasks. To overcome this, we suggest meta-learning approximate belief inference and Bayes-optimal behavior for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders. We meta-train this inference model with the policy. We demonstrate that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilize the known structure of the environment through empirical evidence.",1
"We propose a two-stage model called Depth Self-Optimized Learning (DSOL), which aims to realize ANN depth self-configuration, self-optimization as well as ANN training without manual intervention. In the first stage of DSOL, it will configure ANN of specific depth according to a specific dataset. In the second stage, DSOL will continuously optimize ANN based on Reinforcement Learning (RL). Finally, the optimal depth is returned to the first stage of DSOL for training, so that DSOL can configure the appropriate ANN depth and perform more reasonable optimization when processing similar datasets again. In the experiment, we ran DSOL on the Iris and Boston housing datasets, and the results showed that DSOL performed well. We have uploaded the experiment records and code to our Github.",0
"Our proposed model, Depth Self-Optimized Learning (DSOL), is designed to achieve automatic configuration, optimization, and training of artificial neural networks (ANN) without the need for manual intervention. DSOL operates in two stages, with the first stage configuring the appropriate ANN depth for a given dataset. The second stage continuously optimizes the ANN using Reinforcement Learning (RL). The optimal depth obtained from the second stage is then used in the first stage for training, enabling DSOL to perform more efficient optimization when processing similar datasets in the future. Our experiments using the Iris and Boston housing datasets demonstrated the effectiveness of DSOL, and we have made the experiment records and code available on our Github page.",1
"Multi-objective reinforcement learning (MORL) is an extension of ordinary, single-objective reinforcement learning (RL) that is applicable to many real-world tasks where multiple objectives exist without known relative costs. We study the problem of single policy MORL, which learns an optimal policy given the preference of objectives. Existing methods require strong assumptions such as exact knowledge of the multi-objective Markov decision process, and are analyzed in the limit of infinite data and time. We propose a new algorithm called model-based envelop value iteration (EVI), which generalizes the enveloped multi-objective $Q$-learning algorithm in Yang et al., 2019. Our method can learn a near-optimal value function with polynomial sample complexity and linear convergence speed. To the best of our knowledge, this is the first finite-sample analysis of MORL algorithms.",0
"MORL is an expansion of single-objective RL that is suitable for various real-world tasks with multiple objectives. However, the problem of single policy MORL, which determines the best policy based on objective preferences, has been limited by existing methods that require precise knowledge of the multi-objective Markov decision process and infinite data and time. To overcome these limitations, we propose a new algorithm, model-based envelop value iteration (EVI), which is a generalization of the enveloped multi-objective Q-learning algorithm in Yang et al., 2019. Our algorithm can learn a nearly optimal value function with polynomial sample complexity and linear convergence speed. This is the first finite-sample analysis of MORL algorithms, as far as we know.",1
"We propose a mechanism for distributed resource management and interference mitigation in wireless networks using multi-agent deep reinforcement learning (RL). We equip each transmitter in the network with a deep RL agent that receives delayed observations from its associated users, while also exchanging observations with its neighboring agents, and decides on which user to serve and what transmit power to use at each scheduling interval. Our proposed framework enables agents to make decisions simultaneously and in a distributed manner, unaware of the concurrent decisions of other agents. Moreover, our design of the agents' observation and action spaces is scalable, in the sense that an agent trained on a scenario with a specific number of transmitters and users can be applied to scenarios with different numbers of transmitters and/or users. Simulation results demonstrate the superiority of our proposed approach compared to decentralized baselines in terms of the tradeoff between average and $5^{th}$ percentile user rates, while achieving performance close to, and even in certain cases outperforming, that of a centralized information-theoretic baseline. We also show that our trained agents are robust and maintain their performance gains when experiencing mismatches between train and test deployments.",0
"Our proposal employs multi-agent deep reinforcement learning (RL) to manage resources and reduce interference in wireless networks in a distributed manner. Each transmitter in the network is equipped with a deep RL agent that obtains delayed observations from its users and exchanges observations with neighboring agents to determine which user to serve and what transmit power to use. Our framework enables agents to make independent decisions, without knowledge of decisions made by other agents. Additionally, our design of agents' observation and action spaces is scalable, allowing agents trained on a particular scenario to be applied to scenarios with varying numbers of transmitters and users. Simulation results demonstrate that our approach outperforms decentralized baselines in terms of average and $5^{th}$ percentile user rates, while achieving performance comparable to a centralized information-theoretic baseline. Furthermore, our trained agents prove to be resilient and maintain their performance even when encountering mismatches between train and test deployments.",1
"We present a framework to address a class of sequential decision making problems. Our framework features learning the optimal control policy with robustness to noisy data, determining the unknown state and action parameters, and performing sensitivity analysis with respect to problem parameters. We consider two broad categories of sequential decision making problems modelled as infinite horizon Markov Decision Processes (MDPs) with (and without) an absorbing state. The central idea underlying our framework is to quantify exploration in terms of the Shannon Entropy of the trajectories under the MDP and determine the stochastic policy that maximizes it while guaranteeing a low value of the expected cost along a trajectory. This resulting policy enhances the quality of exploration early on in the learning process, and consequently allows faster convergence rates and robust solutions even in the presence of noisy data as demonstrated in our comparisons to popular algorithms such as Q-learning, Double Q-learning and entropy regularized Soft Q-learning. The framework extends to the class of parameterized MDP and RL problems, where states and actions are parameter dependent, and the objective is to determine the optimal parameters along with the corresponding optimal policy. Here, the associated cost function can possibly be non-convex with multiple poor local minima. Simulation results applied to a 5G small cell network problem demonstrate successful determination of communication routes and the small cell locations. We also obtain sensitivity measures to problem parameters and robustness to noisy environment data.",0
"Our framework addresses a group of problems involving sequential decision making. It involves learning the optimal control policy while accounting for noisy data, identifying unknown state and action parameters, and performing sensitivity analysis with regard to problem parameters. We examine two categories of problems that are modeled as infinite horizon Markov Decision Processes (MDPs), with or without an absorbing state. Our framework focuses on measuring exploration through the Shannon Entropy of the MDP trajectories. We determine the stochastic policy that maximizes this exploration while ensuring low expected cost along the trajectory. This approach results in improved exploration during the learning process, leading to faster convergence rates and robust solutions even with noisy data. This approach outperforms popular algorithms such as Q-learning, Double Q-learning, and entropy regularized Soft Q-learning. Our framework is extendable to parameterized MDP and RL problems with parameter-dependent states and actions. In such cases, the cost function may not be convex and may have multiple poor local minima. We demonstrate the effectiveness of our approach with simulation results from a 5G small cell network problem, where we successfully identified communication routes and small cell locations. We also obtained sensitivity measures to problem parameters and robustness to noisy environment data.",1
"Many batch RL health applications first discretize time into fixed intervals. However, this discretization both loses resolution and forces a policy computation at each (potentially fine) interval. In this work, we develop a novel framework to compress continuous trajectories into a few, interpretable decision points --places where the batch data support multiple alternatives. We apply our approach to create recommendations from a cohort of hypotensive patients dataset. Our reduced state space results in faster planning and allows easy inspection by a clinical expert.",0
"Numerous health applications using batch reinforcement learning usually divide time into predetermined intervals. This method results in diminished resolution and necessitates a policy calculation at each interval, even when the intervals are fine. This study introduces an innovative framework that condenses continuous trajectories into a few, interpretable decision points that support numerous alternatives. We employ this approach to generate recommendations from a dataset of hypotensive patients. Our reduced state space facilitates quicker planning and enables easy examination by a medical professional.",1
"Learning multiple domains/tasks with a single model is important for improving data efficiency and lowering inference cost for numerous vision tasks, especially on resource-constrained mobile devices. However, hand-crafting a multi-domain/task model can be both tedious and challenging. This paper proposes a novel approach to automatically learn a multi-path network for multi-domain visual classification on mobile devices. The proposed multi-path network is learned from neural architecture search by applying one reinforcement learning controller for each domain to select the best path in the super-network created from a MobileNetV3-like search space. An adaptive balanced domain prioritization algorithm is proposed to balance optimizing the joint model on multiple domains simultaneously. The determined multi-path model selectively shares parameters across domains in shared nodes while keeping domain-specific parameters within non-shared nodes in individual domain paths. This approach effectively reduces the total number of parameters and FLOPS, encouraging positive knowledge transfer while mitigating negative interference across domains. Extensive evaluations on the Visual Decathlon dataset demonstrate that the proposed multi-path model achieves state-of-the-art performance in terms of accuracy, model size, and FLOPS against other approaches using MobileNetV3-like architectures. Furthermore, the proposed method improves average accuracy over learning single-domain models individually, and reduces the total number of parameters and FLOPS by 78% and 32% respectively, compared to the approach that simply bundles single-domain models for multi-domain learning.",0
"Improving data efficiency and reducing inference cost for various vision tasks, particularly on resource-constrained mobile devices, requires learning multiple domains/tasks with a single model. However, manually creating such a model can be a cumbersome and challenging task. In this paper, a new approach is proposed for automatic multi-domain visual classification on mobile devices. This involves learning a multi-path network using neural architecture search, with a reinforcement learning controller for each domain selecting the best path in the super-network created from a MobileNetV3-like search space. An adaptive balanced domain prioritization algorithm ensures optimization of the joint model on multiple domains simultaneously. The multi-path model selectively shares parameters across domains in shared nodes while keeping domain-specific parameters within non-shared nodes in individual domain paths. This approach reduces the total number of parameters and FLOPS, promoting positive knowledge transfer while minimizing negative interference across domains. Evaluations on the Visual Decathlon dataset demonstrate the superiority of the proposed multi-path model in terms of accuracy, model size, and FLOPS over other approaches using MobileNetV3-like architectures. Additionally, the proposed method outperforms learning single-domain models individually, while reducing the total number of parameters and FLOPS by 78% and 32%, respectively, as compared to the approach of bundling single-domain models for multi-domain learning.",1
"We study reinforcement learning (RL) with linear function approximation where the underlying transition probability kernel of the Markov decision process (MDP) is a linear mixture model (Jia et al., 2020; Ayoub et al., 2020; Zhou et al., 2020) and the learning agent has access to either an integration or a sampling oracle of the individual basis kernels. We propose a new Bernstein-type concentration inequality for self-normalized martingales for linear bandit problems with bounded noise. Based on the new inequality, we propose a new, computationally efficient algorithm with linear function approximation named $\text{UCRL-VTR}^{+}$ for the aforementioned linear mixture MDPs in the episodic undiscounted setting. We show that $\text{UCRL-VTR}^{+}$ attains an $\tilde O(dH\sqrt{T})$ regret where $d$ is the dimension of feature mapping, $H$ is the length of the episode and $T$ is the number of interactions with the MDP. We also prove a matching lower bound $\Omega(dH\sqrt{T})$ for this setting, which shows that $\text{UCRL-VTR}^{+}$ is minimax optimal up to logarithmic factors. In addition, we propose the $\text{UCLK}^{+}$ algorithm for the same family of MDPs under discounting and show that it attains an $\tilde O(d\sqrt{T}/(1-\gamma)^{1.5})$ regret, where $\gamma\in [0,1)$ is the discount factor. Our upper bound matches the lower bound $\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$ proved by Zhou et al. (2020) up to logarithmic factors, suggesting that $\text{UCLK}^{+}$ is nearly minimax optimal. To the best of our knowledge, these are the first computationally efficient, nearly minimax optimal algorithms for RL with linear function approximation.",0
"Our focus is on reinforcement learning (RL) utilizing linear function approximation, where the Markov decision process (MDP) is modeled with a linear mixture kernel (Jia et al., 2020; Ayoub et al., 2020; Zhou et al., 2020). The agent has either an integration or sampling oracle for individual basis kernels. We propose a new Bernstein-type concentration inequality for self-normalized martingales in linear bandit problems with bounded noise. Based on this, we introduce a computationally efficient algorithm with linear function approximation called $\text{UCRL-VTR}^{+}$, designed for episodic undiscounted linear mixture MDPs. Our algorithm achieves an $\tilde O(dH\sqrt{T})$ regret, where $d$ is the feature mapping dimension, $H$ is the episode length, and $T$ is the number of interactions with the MDP. A matching lower bound $\Omega(dH\sqrt{T})$ demonstrates that $\text{UCRL-VTR}^{+}$ is minimax optimal, up to logarithmic factors. Furthermore, we present the $\text{UCLK}^{+}$ algorithm for the same MDP family, incorporating discounting. Our algorithm achieves an $\tilde O(d\sqrt{T}/(1-\gamma)^{1.5})$ regret, where $\gamma\in [0,1)$ is the discount factor. This upper bound matches the $\Omega(d\sqrt{T}/(1-\gamma)^{1.5})$ lower bound proved by Zhou et al. (2020) up to logarithmic factors, indicating that $\text{UCLK}^{+}$ is nearly minimax optimal. Our algorithms are the first computationally efficient and nearly minimax optimal solutions for RL with linear function approximation.",1
"Despite the recent successes of reinforcement learning in games and robotics, it is yet to become broadly practical. Sample efficiency and unreliable performance in rare but challenging scenarios are two of the major obstacles. Drawing inspiration from the effectiveness of deliberate practice for achieving expert-level human performance, we propose a new adversarial sampling approach guided by a failure predictor named ""CoachNet"". CoachNet is trained online along with the agent to predict the probability of failure. This probability is then used in a stochastic sampling process to guide the agent to more challenging episodes. This way, instead of wasting time on scenarios that the agent has already mastered, training is focused on the agent's ""weak spots"". We present the design of CoachNet, explain its underlying principles, and empirically demonstrate its effectiveness in improving sample efficiency and test-time robustness in common continuous control tasks.",0
"Although reinforcement learning has shown promise in games and robotics, it has not yet been widely applicable due to issues with sample efficiency and unreliable performance in difficult scenarios. To address these obstacles, we propose a new approach called adversarial sampling, which is guided by a failure predictor named ""CoachNet."" This predictor is trained alongside the agent to predict the likelihood of failure, which is then used to guide the agent towards more challenging episodes. This approach allows for more efficient training by focusing on the agent's weaknesses rather than areas where it has already demonstrated proficiency. In this paper, we introduce the design of CoachNet, its underlying principles, and provide empirical evidence of its effectiveness in improving sample efficiency and test-time robustness in common continuous control tasks.",1
"Reinforcement Learning has been able to solve many complicated robotics tasks without any need for feature engineering in an end-to-end fashion. However, learning the optimal policy directly from the sensory inputs, i.e the observations, often requires processing and storage of a huge amount of data. In the context of robotics, the cost of data from real robotics hardware is usually very high, thus solutions that achieve high sample-efficiency are needed. We propose a method that aims at learning a mapping from the observations into a lower-dimensional state space. This mapping is learned with unsupervised learning using loss functions shaped to incorporate prior knowledge of the environment and the task. Using the samples from the state space, the optimal policy is quickly and efficiently learned. We test the method on several mobile robot navigation tasks in a simulation environment and also on a real robot.",0
"Reinforcement Learning has proven effective in solving complex robotics tasks in an end-to-end manner, eliminating the need for feature engineering. However, learning the optimal policy directly from sensory inputs requires immense data processing and storage. In robotics, obtaining data from real hardware is costly, necessitating solutions that are sample-efficient. Our proposed method involves unsupervised learning to create a mapping from observations to a lower-dimensional state space. We incorporate prior knowledge of the environment and task into the loss functions. The optimal policy can be learned quickly and efficiently using samples from the state space. We tested this approach on mobile robot navigation tasks in a simulation and a real robot scenario.",1
"In this paper, we consider the state estimation problem for nonlinear stochastic discrete-time systems. We combine Lyapunov's method in control theory and deep reinforcement learning to design the state estimator. We theoretically prove the convergence of the bounded estimate error solely using the data simulated from the model. An actor-critic reinforcement learning algorithm is proposed to learn the state estimator approximated by a deep neural network. The convergence of the algorithm is analysed. The proposed Lyapunov-based reinforcement learning state estimator is compared with a number of existing nonlinear filtering methods through Monte Carlo simulations, showing its advantage in terms of estimate convergence even under some system uncertainties such as covariance shift in system noise and randomly missing measurements. To the best of our knowledge, this is the first reinforcement learning based nonlinear state estimator with bounded estimate error performance guarantee.",0
"This paper examines the state estimation issue for nonlinear stochastic discrete-time systems. Our approach involves merging Lyapunov's control theory method with deep reinforcement learning to construct the state estimator. We demonstrate the convergence of the bounded estimate error by utilizing data generated from the model. We introduce an actor-critic reinforcement learning algorithm to train the state estimator, which is modeled by a deep neural network. We evaluate the algorithm's convergence and compare our Lyapunov-based reinforcement learning state estimator with other nonlinear filtering methods using Monte Carlo simulations. Our results demonstrate its superiority in terms of estimate convergence, even when dealing with system uncertainties like covariance shift in system noise and randomly missing measurements. We believe this is the first reinforcement learning-based nonlinear state estimator that provides a performance guarantee for bounded estimate error.",1
"Exploration is essential for solving complex Reinforcement Learning (RL) tasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration problem as a well-defined policy optimization problem whose solution aims at visiting all states as uniformly as possible. This is in contrast to standard uncertainty-based approaches where exploration is transient and eventually vanishes. However, existing approaches to MSVE are theoretically justified only for discrete state-spaces as they are oblivious to the geometry of continuous domains. We address this challenge by introducing Geometric Entropy Maximisation (GEM), a new algorithm that maximises the geometry-aware Shannon entropy of state-visits in both discrete and continuous domains. Our key theoretical contribution is casting geometry-aware MSVE exploration as a tractable problem of optimising a simple and novel noise-contrastive objective function. In our experiments, we show the efficiency of GEM in solving several RL problems with sparse rewards, compared against other deep RL exploration approaches.",0
"To solve complex Reinforcement Learning (RL) tasks, exploration is crucial. Maximum State-Visitation Entropy (MSVE) presents a well-defined policy optimization problem which aims to visit all states as uniformly as possible. This differs from traditional uncertainty-based approaches where exploration is temporary and eventually disappears. However, the existing approaches to MSVE only justify discrete state-spaces and ignore the geometry of continuous domains. To address this issue, we have developed Geometric Entropy Maximisation (GEM), which maximises the geometry-aware Shannon entropy of state-visits in both discrete and continuous domains. Our main theoretical contribution is transforming the geometry-aware MSVE exploration into a manageable problem of optimising a simple and innovative noise-contrastive objective function. In our experiments, we demonstrate the effectiveness of GEM in solving several RL problems with sparse rewards compared to other deep RL exploration approaches.",1
"This paper aims to mitigate straggler effects in synchronous distributed learning for multi-agent reinforcement learning (MARL) problems. Stragglers arise frequently in a distributed learning system, due to the existence of various system disturbances such as slow-downs or failures of compute nodes and communication bottlenecks. To resolve this issue, we propose a coded distributed learning framework, which speeds up the training of MARL algorithms in the presence of stragglers, while maintaining the same accuracy as the centralized approach. As an illustration, a coded distributed version of the multi-agent deep deterministic policy gradient(MADDPG) algorithm is developed and evaluated. Different coding schemes, including maximum distance separable (MDS)code, random sparse code, replication-based code, and regular low density parity check (LDPC) code are also investigated. Simulations in several multi-robot problems demonstrate the promising performance of the proposed framework.",0
"The objective of this study is to address the issue of straggler effects in synchronous distributed learning for multi-agent reinforcement learning (MARL) problems. The occurrence of stragglers is common in a distributed learning system due to factors such as compute node slow-downs, failures, and communication bottlenecks. To overcome this problem, the authors propose a coded distributed learning framework that accelerates the training of MARL algorithms while maintaining the same accuracy as the centralized approach. The study presents a coded distributed version of the multi-agent deep deterministic policy gradient(MADDPG) algorithm and evaluates various coding schemes, including maximum distance separable (MDS)code, random sparse code, replication-based code, and regular low density parity check (LDPC) code. The effectiveness of the proposed framework is demonstrated through simulations in several multi-robot problems.",1
"Transfer reinforcement learning aims to improve the sample efficiency of solving unseen new tasks by leveraging experiences obtained from previous tasks. We consider the setting where all tasks (MDPs) share the same environment dynamic except reward function. In this setting, the MDP dynamic is a good knowledge to transfer, which can be inferred by uniformly random policy. However, trajectories generated by uniform random policy are not useful for policy improvement, which impairs the sample efficiency severely. Instead, we observe that the binary MDP dynamic can be inferred from trajectories of any policy which avoids the need of uniform random policy. As the binary MDP dynamic contains the state structure shared over all tasks we believe it is suitable to transfer. Built on this observation, we introduce a method to infer the binary MDP dynamic on-line and at the same time utilize it to guide state embedding learning, which is then transferred to new tasks. We keep state embedding learning and policy learning separately. As a result, the learned state embedding is task and policy agnostic which makes it ideal for transfer learning. In addition, to facilitate the exploration over the state space, we propose a novel intrinsic reward based on the inferred binary MDP dynamic. Our method can be used out-of-box in combination with model-free RL algorithms. We show two instances on the basis of \algo{DQN} and \algo{A2C}. Empirical results of intensive experiments show the advantage of our proposed method in various transfer learning tasks.",0
"The objective of transfer reinforcement learning is to enhance the efficiency of solving new tasks by employing experiences obtained from previous tasks. The scenario considered in this study is where all tasks or Markov Decision Processes (MDPs) share the same environment dynamic, except for the reward function. In this situation, the MDP dynamic can be transferred as it is a good source of knowledge that can be inferred through a uniformly random policy. However, trajectories generated by this policy are not useful for policy improvement, which has a negative impact on sample efficiency. To overcome this problem, the researchers found that the binary MDP dynamic can be inferred from trajectories of any policy, thereby eliminating the need for a uniform random policy. As the binary MDP dynamic consists of the state structure shared across all tasks, it is deemed appropriate for transfer. Based on this observation, a method is introduced to infer the binary MDP dynamic online and simultaneously utilize it to guide state embedding learning, which can then be transferred to new tasks. The state embedding learning and policy learning are kept separate, resulting in a task and policy agnostic learned state embedding that is ideal for transfer learning. Additionally, to facilitate exploration over the state space, a novel intrinsic reward based on the inferred binary MDP dynamic is proposed. The method can be readily combined with model-free RL algorithms and has been shown to be advantageous in various transfer learning tasks through extensive experiments, including two instances based on DQN and A2C.",1
"We study reinforcement learning (RL) with linear function approximation under the adaptivity constraint. We consider two popular limited adaptivity models: batch learning model and rare policy switch model, and propose two efficient online RL algorithms for linear Markov decision processes. In specific, for the batch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\tilde O(\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature mapping, $H$ is the episode length, $T$ is the number of interactions and $B$ is the number of batches. Our result suggests that it suffices to use only $\sqrt{T/dH}$ batches to obtain $\tilde O(\sqrt{d^3H^3T})$ regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an $\tilde O(\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\log T$ policy switches suffice to obtain the $\tilde O(\sqrt{d^3H^3T})$ regret. Our algorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al., 2019), yet with a substantially smaller amount of adaptivity.",0
"Under the adaptivity constraint, we explore reinforcement learning (RL) with linear function approximation and focus on two well-known limited adaptivity models: the batch learning model and the rare policy switch model. To address these models, we propose two efficient online RL algorithms for linear Markov decision processes. Our LSVI-UCB-Batch algorithm achieves an $\tilde O(\sqrt{d^3H^3T} + dHT/B)$ regret for the batch learning model, where $d$ represents the feature mapping dimension, $H$ represents the episode length, $T$ represents the number of interactions, and $B$ represents the number of batches. Our findings suggest that only $\sqrt{T/dH}$ batches are required to obtain an $\tilde O(\sqrt{d^3H^3T})$ regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm achieves an $\tilde O(\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, implying that $dH\log T$ policy switches are sufficient to obtain the $\tilde O(\sqrt{d^3H^3T})$ regret. Our algorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al., 2019) with significantly less adaptivity.",1
"Recent Reinforcement Learning (RL) algorithms making use of Kullback-Leibler (KL) regularization as a core component have shown outstanding performance. Yet, only little is understood theoretically about why KL regularization helps, so far. We study KL regularization within an approximate value iteration scheme and show that it implicitly averages q-values. Leveraging this insight, we provide a very strong performance bound, the very first to combine two desirable aspects: a linear dependency to the horizon (instead of quadratic) and an error propagation term involving an averaging effect of the estimation errors (instead of an accumulation effect). We also study the more general case of an additional entropy regularizer. The resulting abstract scheme encompasses many existing RL algorithms. Some of our assumptions do not hold with neural networks, so we complement this theoretical analysis with an extensive empirical study.",0
"Reinforcement Learning (RL) algorithms that utilize Kullback-Leibler (KL) regularization have demonstrated exceptional performance. However, the theoretical understanding of why KL regularization is effective is limited. Our research examines KL regularization in an approximate value iteration framework, and we reveal that it implicitly averages q-values. By using this insight, we derive a powerful performance bound that has two desirable features: a linear dependency on the horizon and an error propagation term that involves an averaging effect of the estimation errors. This is the first time that these two aspects have been combined. We also investigate the more general case of an additional entropy regularizer, and our framework encompasses many existing RL algorithms. Although some of our assumptions are not applicable to neural networks, we supplement our theoretical analysis with a detailed empirical study.",1
"In this paper, a novel training paradigm inspired by quantum computation is proposed for deep reinforcement learning (DRL) with experience replay. In contrast to traditional experience replay mechanism in DRL, the proposed deep reinforcement learning with quantum-inspired experience replay (DRL-QER) adaptively chooses experiences from the replay buffer according to the complexity and the replayed times of each experience (also called transition), to achieve a balance between exploration and exploitation. In DRL-QER, transitions are first formulated in quantum representations, and then the preparation operation and the depreciation operation are performed on the transitions. In this progress, the preparation operation reflects the relationship between the temporal difference errors (TD-errors) and the importance of the experiences, while the depreciation operation is taken into account to ensure the diversity of the transitions. The experimental results on Atari 2600 games show that DRL-QER outperforms state-of-the-art algorithms such as DRL-PER and DCRL on most of these games with improved training efficiency, and is also applicable to such memory-based DRL approaches as double network and dueling network.",0
"This paper proposes a new approach to deep reinforcement learning (DRL) with experience replay, inspired by quantum computation. The method, called deep reinforcement learning with quantum-inspired experience replay (DRL-QER), differs from traditional experience replay mechanisms in that it selects experiences from the replay buffer based on their complexity and replayed times, striking a balance between exploration and exploitation. Transitions are first expressed in quantum representations, and then subjected to preparation and depreciation operations. The preparation operation reflects the relationship between temporal difference errors (TD-errors) and experience importance, while the depreciation operation ensures diversity of transitions. Experimental results on Atari 2600 games show that DRL-QER outperforms state-of-the-art algorithms such as DRL-PER and DCRL with improved training efficiency, and is also suitable for memory-based DRL approaches such as double network and dueling network.",1
"Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efficiency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the benefits of the parallelism in ES. To solve this challenge, asynchronous update scheme was introduced, which is capable of good time-efficiency and diverse policy exploration. In this paper, we introduce an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efficiency of ES and integrates it with policy gradient methods. Specifically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asynchronism, ES, and DRL, which are exploration and time efficiency, stability, and sample efficiency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efficiency compared to the previous methods.",0
"Various tasks have been successfully tackled by both Deep Reinforcement Learning (DRL) algorithms and Evolution Strategies (ES), each exhibiting contrasting properties: DRL has strong sample efficiency but poor stability, while ES has the opposite. Current attempts to combine these algorithms rely on synchronous update schemes which do not fully exploit the benefits of the parallelism in ES. A possible solution is the asynchronous update scheme, which facilitates diverse policy exploration and good time efficiency. To this end, we introduce Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL), which not only maximizes the parallel efficiency of ES but also integrates it with policy gradient methods. Our approach includes a novel framework that merges ES and DRL asynchronously, as well as various asynchronous update methods that leverage the advantages of asynchronism, ES, and DRL such as exploration and time efficiency, stability, and sample efficiency respectively. We evaluate our proposed framework and update methods in continuous control benchmark tasks, where they exhibit superior performance and time efficiency when compared to previous methods.",1
"Temporal information is essential to learning effective policies with Reinforcement Learning (RL). However, current state-of-the-art RL algorithms either assume that such information is given as part of the state space or, when learning from pixels, use the simple heuristic of frame-stacking to implicitly capture temporal information present in the image observations. This heuristic is in contrast to the current paradigm in video classification architectures, which utilize explicit encodings of temporal information through methods such as optical flow and two-stream architectures to achieve state-of-the-art performance. Inspired by leading video classification architectures, we introduce the Flow of Latents for Reinforcement Learning (Flare), a network architecture for RL that explicitly encodes temporal information through latent vector differences. We show that Flare (i) recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information, (ii) achieves state-of-the-art performance on pixel-based challenging continuous control tasks within the DeepMind control benchmark suite, namely quadruped walk, hopper hop, finger turn hard, pendulum swing, and walker run, and is the most sample efficient model-free pixel-based RL algorithm, outperforming the prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, and (iv), when augmented over rainbow DQN, outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari games at 100M time step benchmark.",0
"Temporal information is a crucial component for acquiring effective policies through Reinforcement Learning (RL). However, current state-of-the-art RL algorithms adopt one of two approaches: they either assume such information is included in the state space, or they rely on the frame-stacking heuristic to implicitly capture temporal information when learning from pixels. This heuristic differs from the video classification architecture paradigm, which utilizes explicit encodings of temporal information, such as optical flow and two-stream architectures, to achieve state-of-the-art performance. Drawing inspiration from leading video classification architectures, we present the Flow of Latents for Reinforcement Learning (Flare), a network architecture for RL that utilizes latent vector differences to explicitly encode temporal information. Our results demonstrate that Flare (i) achieves optimal performance in state-based RL without explicit access to state velocity, relying solely on positional state information; (ii) achieves state-of-the-art performance on challenging continuous control tasks within the DeepMind control benchmark suite, including quadruped walk, hopper hop, finger turn hard, pendulum swing, and walker run, while also being the most sample-efficient model-free pixel-based RL algorithm, outperforming the prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively; and (iv) outperforms the state-of-the-art level baseline on 5 of 8 challenging Atari games at the 100M time step benchmark when augmented over rainbow DQN.",1
"Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.",0
"The reduction of uncertainties in optimization and decision making processes is greatly aided by Uncertainty Quantification (UQ). This technique has been successfully applied to various real-world problems in science and engineering. Among the UQ methods, Bayesian approximation and ensemble learning have been widely used and tested in different applications such as computer vision, image processing, medical image analysis, natural language processing, and bioinformatics. In this paper, we review recent advances in UQ methods in deep learning and their application in reinforcement learning. Additionally, we highlight important applications of UQ methods and the fundamental research challenges they pose. Finally, we discuss future research directions in this field.",1
"The purpose of accounting audit is to have clear understanding on the financial activities of a company, which can be enhanced by machine learning or reinforcement learning as numeric analysis better than manual analysis can be made. For the purpose of assessment on the relevance, completeness and accuracy of the information produced by entity pertaining to the newly implemented International Financial Reporting Standard 16 Lease (IFRS 16) is one of such candidates as its characteristic of requiring the understanding on the nature of contracts and its complete analysis from listing up without omission, which can be enhanced by the digitalization of contracts for the purpose of creating the lists, still leaving the need of auditing cash flows of companies for the possible omission due to the potential error at the stage of data collection, especially for entities with various short or middle term business sites and related leases, such as construction entities.   The implementation of the reinforcement learning and its well-known code is to be made for the purpose of drawing the possibility and utilizability of interpreters from domain knowledge to numerical system, also can be called 'gamification interpreter' or 'numericalization interpreter' which can be referred or compared to the extrapolation with nondimensional numbers, such as Froude Number, in physics, which was a source of inspiration at this study. Studies on the interpreters can be able to empower the utilizability of artificial general intelligence in domain and commercial area.",0
"The accounting audit aims to gain a comprehensive understanding of a company's financial activities, which can be improved through the use of machine learning or reinforcement learning for more accurate numeric analysis. The newly implemented International Financial Reporting Standard 16 Lease (IFRS 16) is a prime candidate for assessing the relevance, completeness, and accuracy of a company's information, as it requires a thorough understanding of contracts and complete analysis without omission. Digitalizing contracts can aid in creating these lists, but auditing cash flows is still necessary to detect potential errors, especially for entities with multiple short or medium-term business sites and related leases. Reinforcement learning can be used to develop a ""gamification interpreter"" or ""numericalization interpreter"" that can translate domain knowledge into numerical systems, similar to the use of nondimensional numbers like the Froude Number in physics. This could potentially enhance the applications of artificial general intelligence in both the domain and commercial sectors.",1
"Reinforcement learning (RL) algorithms update an agent's parameters according to one of several possible rules, discovered manually through years of research. Automating the discovery of update rules from data could lead to more efficient algorithms, or algorithms that are better adapted to specific environments. Although there have been prior attempts at addressing this significant scientific challenge, it remains an open question whether it is feasible to discover alternatives to fundamental concepts of RL such as value functions and temporal-difference learning. This paper introduces a new meta-learning approach that discovers an entire update rule which includes both 'what to predict' (e.g. value functions) and 'how to learn from it' (e.g. bootstrapping) by interacting with a set of environments. The output of this method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical results show that our method discovers its own alternative to the concept of value functions. Furthermore it discovers a bootstrapping mechanism to maintain and use its predictions. Surprisingly, when trained solely on toy environments, LPG generalises effectively to complex Atari games and achieves non-trivial performance. This shows the potential to discover general RL algorithms from data.",0
"RL algorithms typically rely on manually discovered rules to update an agent's parameters. However, automating the discovery of these update rules from data may produce more efficient algorithms that are better suited to specific environments. Despite previous attempts to tackle this challenge, it remains unclear whether it is possible to find alternatives to RL's fundamental concepts, such as value functions and temporal-difference learning. This paper presents a novel meta-learning approach that identifies a complete update rule, encompassing both 'what to predict' (e.g. value functions) and 'how to learn from it' (e.g. bootstrapping), through interaction with a range of environments. The resulting RL algorithm, Learned Policy Gradient (LPG), demonstrates the ability to discover an alternative to value functions and a bootstrapping mechanism to maintain and utilize predictions. Remarkably, even when exclusively trained on toy environments, LPG exhibits effective generalization to complex Atari games and achieves noteworthy performance, highlighting the possibility of discovering general RL algorithms from data.",1
"The combination of high-resolution satellite imagery and machine learning have proven useful in many sustainability-related tasks, including poverty prediction, infrastructure measurement, and forest monitoring. However, the accuracy afforded by high-resolution imagery comes at a cost, as such imagery is extremely expensive to purchase at scale. This creates a substantial hurdle to the efficient scaling and widespread adoption of high-resolution-based approaches. To reduce acquisition costs while maintaining accuracy, we propose a reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on the high-resolution images. We apply this approach to the task of poverty prediction in Uganda, building on an earlier approach that used object detection to count objects and use these counts to predict poverty. Our approach exceeds previous performance benchmarks on this task while using 80% fewer high-resolution images. Our approach could have application in many sustainability domains that require high-resolution imagery.",0
"The utilization of high-resolution satellite imagery and machine learning has been valuable in various sustainability-related duties such as predicting poverty, measuring infrastructure, and monitoring forests. Nonetheless, the accuracy provided by high-resolution imagery carries a significant cost, making it challenging to implement high-resolution-based approaches on a large scale. To address this issue, we suggest a reinforcement learning method that employs free low-resolution imagery to identify where to obtain expensive high-resolution images before conducting deep learning tasks on them. We applied this method to predicting poverty in Uganda, improving on a previous approach that counted objects using object detection to forecast poverty. Our new approach surpasses previous benchmarks while using 80% fewer high-resolution images. This technique has the potential for various sustainability applications that require high-resolution imagery.",1
"Motion synthesis in a dynamic environment has been a long-standing problem for character animation. Methods using motion capture data tend to scale poorly in complex environments because of their larger capturing and labeling requirement. Physics-based controllers are effective in this regard, albeit less controllable. In this paper, we present CARL, a quadruped agent that can be controlled with high-level directives and react naturally to dynamic environments. Starting with an agent that can imitate individual animation clips, we use Generative Adversarial Networks to adapt high-level controls, such as speed and heading, to action distributions that correspond to the original animations. Further fine-tuning through the deep reinforcement learning enables the agent to recover from unseen external perturbations while producing smooth transitions. It then becomes straightforward to create autonomous agents in dynamic environments by adding navigation modules over the entire process. We evaluate our approach by measuring the agent's ability to follow user control and provide a visual analysis of the generated motion to show its effectiveness.",0
"Character animation has long struggled with motion synthesis in dynamic environments. Motion capture data methods have difficulty scaling in complex environments due to their extensive capturing and labeling requirements. While physics-based controllers are effective, they offer less control. To address this, we introduce CARL, a quadruped agent that responds naturally to dynamic environments and can be controlled with high-level directives. We use Generative Adversarial Networks to adapt high-level controls, such as speed and heading, to action distributions that correspond to original animations. Deep reinforcement learning allows the agent to recover from unseen external perturbations and produce smooth transitions. Adding navigation modules allows for autonomous agents in dynamic environments. Our approach is evaluated by measuring the agent's ability to follow user control and analyzing generated motion visually.",1
"Inspired by human visual attention, we introduce a Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework for modeling the visual attention allocation of drivers in imminent rear-end collisions. MEDIRL is composed of visual, driving, and attention modules. Given a front-view driving video and corresponding eye fixations from humans, the visual and driving modules extract generic and driving-specific visual features, respectively. Finally, the attention module learns the intrinsic task-sensitive reward functions induced by eye fixation policies recorded from attentive drivers. MEDIRL uses the learned policies to predict visual attention allocation of drivers. We also introduce EyeCar, a new driver visual attention dataset during accident-prone situations. We conduct comprehensive experiments and show that MEDIRL outperforms previous state-of-the-art methods on driving task-related visual attention allocation on the following large-scale driving attention benchmark datasets: DR(eye)VE, BDD-A, and DADA-2000. The code and dataset are provided for reproducibility.",0
"We present a framework called Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) that models the visual attention allocation of drivers in imminent rear-end collisions, drawing inspiration from human visual attention. MEDIRL consists of three modules: visual, driving, and attention. The visual and driving modules extract generic and driving-specific visual features, respectively, from a front-view driving video and corresponding eye fixations provided by humans. The attention module then learns intrinsic task-sensitive reward functions based on eye fixation policies recorded from attentive drivers. Using the learned policies, MEDIRL predicts the visual attention allocation of drivers. We also introduce a new driver visual attention dataset called EyeCar, which captures accident-prone situations. Our experiments demonstrate that MEDIRL outperforms previous state-of-the-art methods on driving task-related visual attention allocation for large-scale driving attention benchmark datasets, including DR(eye)VE, BDD-A, and DADA-2000. For reproducibility, we provide the code and dataset.",1
"This paper proposes a definition of system health in the context of multiple agents optimizing a joint reward function. We use this definition as a credit assignment term in a policy gradient algorithm to distinguish the contributions of individual agents to the global reward. The health-informed credit assignment is then extended to a multi-agent variant of the proximal policy optimization algorithm and demonstrated on particle and multiwalker robot environments that have characteristics such as system health, risk-taking, semi-expendable agents, continuous action spaces, and partial observability. We show significant improvement in learning performance compared to policy gradient methods that do not perform multi-agent credit assignment.",0
"In this paper, we suggest a definition of system health with reference to multiple agents working towards a common goal. This definition serves as a credit assignment term in a policy gradient algorithm, enabling us to differentiate the contributions of individual agents towards the global reward. We expand upon this health-based credit assignment method and implement it in a multi-agent version of the proximal policy optimization algorithm. We evaluate our approach on particle and multiwalker robot environments that possess attributes such as system health, risk-taking, semi-expendable agents, continuous action spaces, and partial observability. Our results show a significant improvement in learning performance when compared to policy gradient methods that do not include multi-agent credit assignment.",1
"In value-based reinforcement learning (RL), unlike in supervised learning, the agent faces not a single, stationary, approximation problem, but a sequence of value prediction problems. Each time the policy improves, the nature of the problem changes, shifting both the distribution of states and their values. In this paper we take a novel perspective, arguing that the value prediction problems faced by an RL agent should not be addressed in isolation, but rather as a single, holistic, prediction problem. An RL algorithm generates a sequence of policies that, at least approximately, improve towards the optimal policy. We explicitly characterize the associated sequence of value functions and call it the value-improvement path. Our main idea is to approximate the value-improvement path holistically, rather than to solely track the value function of the current policy. Specifically, we discuss the impact that this holistic view of RL has on representation learning. We demonstrate that a representation that spans the past value-improvement path will also provide an accurate value approximation for future policy improvements. We use this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test our hypothesis empirically, we augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path. In a study of Atari 2600 games, the augmented agent achieved approximately double the mean and median performance of the baseline agent.",0
"In value-based reinforcement learning, the agent is faced with a series of value prediction problems as opposed to a singular, fixed approximation problem like in supervised learning. With each policy improvement, the problem changes, causing a shift in the distribution of states and their values. This paper presents a new perspective, arguing that instead of addressing these value prediction problems in isolation, they should be viewed as a single, holistic prediction problem. The RL algorithm generates a sequence of policies that move towards the optimal policy, creating a value-improvement path. Rather than solely tracking the value function of the current policy, the paper proposes approximating the value-improvement path as a whole. This approach has significant ramifications for representation learning, as a representation that spans the past value-improvement path can accurately predict future policy improvements. The paper tests this hypothesis by adding an auxiliary task of learning the value-improvement path to a standard deep RL agent and achieves double the mean and median performance of the baseline agent in a study of Atari 2600 games.",1
"Offline reinforcement learning (RL), also known as batch RL, aims to optimize policy from a large pre-recorded dataset without interaction with the environment. This setting offers the promise of utilizing diverse, pre-collected datasets to obtain policies without costly, risky, active exploration. However, commonly used off-policy algorithms based on Q-learning or actor-critic perform poorly when learning from a static dataset. In this work, we study why off-policy RL methods fail to learn in offline setting from the value function view, and we propose a novel offline RL algorithm that we call Pessimistic Offline Policy Optimization (POPO), which learns a pessimistic value function to get a strong policy. We find that POPO performs surprisingly well and scales to tasks with high-dimensional state and action space, comparing or outperforming several state-of-the-art offline RL algorithms on benchmark tasks.",0
"Batch RL, also known as offline RL, seeks to optimize policy using a vast pre-recorded dataset without any environment interaction. This approach offers the advantage of utilizing diverse, pre-existing datasets to obtain policies without the need for expensive and risky active exploration. However, popular off-policy algorithms based on Q-learning or actor-critic are ineffective when learning from a static dataset. This study examines why off-policy RL methods struggle to learn in an offline setting from a value function perspective and proposes a new offline RL algorithm called Pessimistic Offline Policy Optimization (POPO). POPO learns a pessimistic value function to generate a robust policy and performs remarkably well, even in tasks with high-dimensional state and action spaces. It compares favorably or exceeds several state-of-the-art offline RL algorithms on benchmark tasks.",1
"Safety is a critical concern when deploying reinforcement learning agents for realistic tasks. Recently, safe reinforcement learning algorithms have been developed to optimize the agent's performance while avoiding violations of safety constraints. However, few studies have addressed the non-stationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the context-aware safe reinforcement learning (CASRL) method, a meta-learning framework to realize safe adaptation in non-stationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Safety constraints are then evaluated with uncertainty-aware trajectory sampling. The high cost of safety violations leads to the rareness of unsafe records in the dataset. We address this issue by enabling prioritized sampling during model training and formulating prior safety constraints with domain knowledge during constrained planning. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.",0
"Deploying reinforcement learning agents for realistic tasks requires prioritizing safety. Safe reinforcement learning algorithms have been developed to optimize agent performance while avoiding safety violations. However, few studies have considered non-stationary disturbances in the environment, which can lead to disastrous outcomes. To address this, we propose the Context-Aware Safe Reinforcement Learning (CASRL) method, a meta-learning framework that ensures safe adaptation in non-stationary environments. We use a probabilistic latent variable model to quickly infer the posterior environment transition distribution based on context data, and evaluate safety constraints with uncertainty-aware trajectory sampling. The rarity of unsafe records in the dataset is addressed by prioritized sampling during model training and formulating prior safety constraints based on domain knowledge during constrained planning. We evaluate the algorithm in realistic safety-critical environments with non-stationary disturbances and find that it significantly outperforms existing baselines in terms of safety and robustness.",1
"Many real-world applications, such as those in medical domains, recommendation systems, etc, can be formulated as large state space reinforcement learning problems with only a small budget of the number of policy changes, i.e., low switching cost. This paper focuses on the linear Markov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al 2020] where the linear function approximation is used for generalization on the large state space. We present the first algorithm for linear MDP with a low switching cost. Our algorithm achieves an $\widetilde{O}\left(\sqrt{d^3H^4K}\right)$ regret bound with a near-optimal $O\left(d H\log K\right)$ global switching cost where $d$ is the feature dimension, $H$ is the planning horizon and $K$ is the number of episodes the agent plays. Our regret bound matches the best existing polynomial algorithm by [Jin et al 2020] and our switching cost is exponentially smaller than theirs. When specialized to tabular MDP, our switching cost bound improves those in [Bai et al 2019, Zhang et al 20020]. We complement our positive result with an $\Omega\left(dH/\log d\right)$ global switching cost lower bound for any no-regret algorithm.",0
"This article focuses on the use of reinforcement learning in real-world applications, such as medicine and recommendation systems, where a large state space is present. Specifically, it examines the linear Markov Decision Process (MDP) and presents the first algorithm for this process with a low switching cost. The algorithm achieves a near-optimal global switching cost and a regret bound of $\widetilde{O}\left(\sqrt{d^3H^4K}\right)$, which matches the best existing polynomial algorithm. Additionally, the article shows that the presented algorithm's switching cost is exponentially smaller than that of previous algorithms. The article concludes with a lower bound of $\Omega\left(dH/\log d\right)$ for any no-regret algorithm.",1
"As reinforcement learning techniques are increasingly applied to real-world decision problems, attention has turned to how these algorithms use potentially sensitive information. We consider the task of training a policy that maximizes reward while minimizing disclosure of certain sensitive state variables through the actions. We give examples of how this setting covers real-world problems in privacy for sequential decision-making. We solve this problem in the policy gradients framework by introducing a regularizer based on the mutual information (MI) between the sensitive state and the actions at a given timestep. We develop a model-based stochastic gradient estimator for optimization of privacy-constrained policies. We also discuss an alternative MI regularizer that serves as an upper bound to our main MI regularizer and can be optimized in a model-free setting. We contrast previous work in differentially-private RL to our mutual-information formulation of information disclosure. Experimental results show that our training method results in policies which hide the sensitive state.",0
"The use of reinforcement learning techniques in practical decision-making situations has led to concerns about the algorithms' handling of sensitive information. Our focus is on training a policy that maximizes reward while minimizing the disclosure of specific sensitive state variables through actions. This approach is applicable to privacy concerns in real-world sequential decision-making scenarios. To address this problem, we introduce a regularizer based on mutual information (MI) between the sensitive state and actions at a given timestep within the policy gradients framework. We present a stochastic gradient estimator for optimizing privacy-constrained policies, and also offer an alternative MI regularizer that serves as an upper bound to the main MI regularizer and can be optimized in a model-free setting. We compare our mutual-information formulation to previous work in differentially-private RL, and our experimental results indicate that our training method produces policies that protect sensitive state information.",1
"There is a growing interest in the learning-to-learn paradigm, also known as meta-learning, where models infer on new tasks using a few training examples. Recently, meta-learning based methods have been widely used in few-shot classification, regression, reinforcement learning, and domain adaptation. The model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that obtains model parameter initialization at meta-training phase. In the meta-test phase, this initialization is rapidly adapted to new tasks by using gradient descent. However, meta-learning models are prone to overfitting since there are insufficient training tasks resulting in over-parameterized models with poor generalization performance for unseen tasks. In this paper, we propose a Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL algorithm. The proposed framework incorporates a sparse variational loss term alongside the loss function of MAML, which uses a sparsifying approximated KL divergence as a regularizer. We demonstrate the performance of B-MAML using classification and regression tasks, and highlight that training a sparsifying BNN using MAML indeed improves the parameter footprint of the model while performing at par or even outperforming the MAML approach. We also illustrate applicability of our approach in distributed sensor networks, where sparsity and meta-learning can be beneficial.",0
"Meta-learning, also known as the learning-to-learn paradigm, is gaining popularity as models can infer on new tasks with minimal training examples. Meta-learning techniques have been applied in various fields such as few-shot classification, regression, reinforcement learning, and domain adaptation. The model-agnostic meta-learning (MAML) algorithm is a well-known approach that acquires model parameter initialization during the meta-training phase and rapidly adapts to new tasks in the meta-test phase using gradient descent. However, meta-learning models are susceptible to overfitting due to a lack of training tasks, resulting in over-parameterized models with limited generalization performance for unseen tasks. To address this issue, we propose a Bayesian neural network-based MAML algorithm, known as the B-SMALL algorithm. Our approach includes a sparse variational loss term and an approximated KL divergence as a regularizer to the loss function of MAML. We show that B-MAML performs comparably or even better than MAML in classification and regression tasks. Additionally, we demonstrate the potential of our approach in distributed sensor networks, where sparsity and meta-learning can be advantageous.",1
"This paper focuses on inverse reinforcement learning for autonomous navigation using distance and semantic category observations. The objective is to infer a cost function that explains demonstrated behavior while relying only on the expert's observations and state-control trajectory. We develop a map encoder, that infers semantic category probabilities from the observation sequence, and a cost encoder, defined as a deep neural network over the semantic features. Since the expert cost is not directly observable, the model parameters can only be optimized by differentiating the error between demonstrated controls and a control policy computed from the cost estimate. We propose a new model of expert behavior that enables error minimization using a closed-form subgradient computed only over a subset of promising states via a motion planning algorithm. Our approach allows generalizing the learned behavior to new environments with new spatial configurations of the semantic categories. We analyze the different components of our model in a minigrid environment. We also demonstrate that our approach learns to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of buildings, sidewalks, and road lanes.",0
"The main focus of this paper is on using inverse reinforcement learning to achieve autonomous navigation by relying on distance and semantic category observations. The objective is to devise a cost function that explains the expert's behavior solely based on their observations and state-control trajectory. To achieve this, we present a map encoder that deduces semantic category probabilities from the observation sequence, and a cost encoder, which is a deep neural network over the semantic features. As the expert's cost cannot be directly observed, the model parameters are optimized by differentiating the error between the demonstrated controls and a control policy computed from the cost estimate. Our proposed model of expert behavior enables error minimization by using a closed-form subgradient computed over a subset of promising states via a motion planning algorithm. Our approach allows for generalizing the learned behavior to new environments with new spatial configurations of the semantic categories. We validate our model's performance in a mini-grid environment and demonstrate that it can learn to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of buildings, sidewalks, and road lanes.",1
"In the real world, people/entities usually find matches independently and autonomously, such as finding jobs, partners, roommates, etc. It is possible that this search for matches starts with no initial knowledge of the environment. We propose the use of a multi-agent reinforcement learning (MARL) paradigm for a spatially formulated decentralized two-sided matching market with independent and autonomous agents. Having autonomous agents acting independently makes our environment very dynamic and uncertain. Moreover, agents lack the knowledge of preferences of other agents and have to explore the environment and interact with other agents to discover their own preferences through noisy rewards. We think such a setting better approximates the real world and we study the usefulness of our MARL approach for it. Along with conventional stable matching case where agents have strictly ordered preferences, we check the applicability of our approach for stable matching with incomplete lists and ties. We investigate our results for stability, level of instability (for unstable results), and fairness. Our MARL approach mostly yields stable and fair outcomes.",0
"Individuals and entities typically seek out matches independently and autonomously in the real world, whether it be for job opportunities, romantic partners, or roommates. Such searches may begin without any prior knowledge of the environment. Our proposal involves using a multi-agent reinforcement learning (MARL) paradigm to create a spatially formulated, decentralized two-sided matching market with autonomous agents. This approach allows for a dynamic and uncertain environment, as well as for agents to learn about other agents' preferences through exploration and interaction. We believe this setting better mirrors real-life situations and have studied the effectiveness of our MARL approach. In addition to examining stable matching scenarios with strictly ordered preferences, we have also investigated its applicability for incomplete lists and ties. Our analysis has focused on the stability, level of instability (in the case of unstable results), and fairness of our MARL approach, which has mostly yielded stable and fair outcomes.",1
"Learning-to-rank (LTR) has become a key technology in E-commerce applications. Most existing LTR approaches follow a supervised learning paradigm from offline labeled data collected from the online system. However, it has been noticed that previous LTR models can have a good validation performance over offline validation data but have a poor online performance, and vice versa, which implies a possible large inconsistency between the offline and online evaluation. We investigate and confirm in this paper that such inconsistency exists and can have a significant impact on AliExpress Search. Reasons for the inconsistency include the ignorance of item context during the learning, and the offline data set is insufficient for learning the context. Therefore, this paper proposes an evaluator-generator framework for LTR with item context. The framework consists of an evaluator that generalizes to evaluate recommendations involving the context, and a generator that maximizes the evaluator score by reinforcement learning, and a discriminator that ensures the generalization of the evaluator. Extensive experiments in simulation environments and AliExpress Search online system show that, firstly, the classic data-based metrics on the offline dataset can show significant inconsistency with online performance, and can even be misleading. Secondly, the proposed evaluator score is significantly more consistent with the online performance than common ranking metrics. Finally, as the consequence, our method achieves a significant improvement (\textgreater$2\%$) in terms of Conversion Rate (CR) over the industrial-level fine-tuned model in online A/B tests.",0
"LTR technology is important for E-commerce applications. However, current LTR approaches rely on supervised learning from offline labeled data, which can lead to inconsistencies between offline and online evaluations. This paper confirms that such inconsistencies exist and can impact AliExpress Search. The root of the problem is the neglect of item context during learning, and the insufficiency of the offline dataset for learning context. To address this, the paper proposes an evaluator-generator framework that includes an evaluator for generalizing recommendations and a generator for maximizing evaluator score through reinforcement learning. Extensive experiments show that offline metrics can be misleading, and the proposed evaluator score is more consistent with online performance. As a result, the method achieves a significant improvement in Conversion Rate over the fine-tuned model in online A/B tests.",1
"Asynchronous and parallel implementation of standard reinforcement learning (RL) algorithms is a key enabler of the tremendous success of modern RL. Among many asynchronous RL algorithms, arguably the most popular and effective one is the asynchronous advantage actor-critic (A3C) algorithm. Although A3C is becoming the workhorse of RL, its theoretical properties are still not well-understood, including the non-asymptotic analysis and the performance gain of parallelism (a.k.a. speedup). This paper revisits the A3C algorithm with TD(0) for the critic update, termed A3C-TD(0), with provable convergence guarantees. With linear value function approximation for the TD update, the convergence of A3C-TD(0) is established under both i.i.d. and Markovian sampling. Under i.i.d. sampling, A3C-TD(0) obtains sample complexity of $\mathcal{O}(\epsilon^{-2.5}/N)$ per worker to achieve $\epsilon$ accuracy, where $N$ is the number of workers. Compared to the best-known sample complexity of $\mathcal{O}(\epsilon^{-2.5})$ for two-timescale AC, A3C-TD(0) achieves \emph{linear speedup}, which justifies the advantage of parallelism and asynchrony in AC algorithms theoretically for the first time. Numerical tests on synthetically generated instances and OpenAI Gym environments have been provided to verify our theoretical analysis.",0
"Modern reinforcement learning (RL) has achieved tremendous success due to the implementation of standard RL algorithms in an asynchronous and parallel manner. The asynchronous advantage actor-critic (A3C) algorithm is among the most popular and effective asynchronous RL algorithms. However, despite its widespread use, the theoretical properties of A3C, including non-asymptotic analysis and the performance gain of parallelism, are not well-understood. This paper introduces A3C-TD(0), which utilizes TD(0) for the critic update, and proves its convergence guarantees. A3C-TD(0) achieves linear speedup and reduces the sample complexity to $\mathcal{O}(\epsilon^{-2.5}/N)$ per worker, compared to the best-known sample complexity of $\mathcal{O}(\epsilon^{-2.5})$ for two-timescale AC. The theoretical advantage of parallelism and asynchrony in AC algorithms is thus justified for the first time, and numerical tests have been conducted to verify the theoretical analysis using synthetically generated instances and OpenAI Gym environments.",1
"Model compression aims to deploy deep neural networks (DNN) to mobile devices with limited computing power and storage resource. However, most of the existing model compression methods rely on manually defined rules, which requires domain expertise. In this paper, we propose an Auto Graph encoder-decoder Model Compression (AGMC) method combined with graph neural networks (GNN) and reinforcement learning (RL) to find the best compression policy. We model the target DNN as a graph and use GNN to learn the embeddings of the DNN automatically. In our experiments, we first compared our method with rule-based DNN embedding methods to show the graph auto encoder-decoder's effectiveness. Our learning-based DNN embedding achieved better performance and a higher compression ratio with fewer search steps. Moreover, we evaluated the AGMC on CIFAR-10 and ILSVRC-2012 datasets and compared handcrafted and learning-based model compression approaches. Our method outperformed handcrafted and learning-based methods on ResNet-56 with 3.6% and 1.8% higher accuracy, respectively. Furthermore, we achieved a higher compression ratio than state-of-the-art methods on MobileNet-V2 with just 0.93% accuracy loss.",0
"The objective of model compression is to utilize deep neural networks (DNN) on mobile devices that have restricted computing capacity and storage resources. However, the majority of the current model compression techniques rely on predefined rules that necessitate specialized knowledge. In this article, we suggest an Auto Graph encoder-decoder Model Compression (AGMC) approach that includes graph neural networks (GNN) and reinforcement learning (RL) to determine the optimal compression policy. We regard the target DNN as a graph and utilize GNN to automatically learn the DNN embeddings. Our method outperformed traditional and learning-based model compression techniques for ResNet-56, with 3.6% and 1.8% higher accuracy, respectively. Additionally, we attained a higher compression ratio than current methods for MobileNet-V2 with just a 0.93% accuracy loss. Our learning-based DNN embedding achieved superior performance and a higher compression ratio with fewer search steps compared to rule-based DNN embedding methods, as demonstrated by our experiments.",1
"A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods. Videos and visualizations are available here: http://sites.google.com/berkeley.edu/mbold.",0
"To complete various tasks in its environment, a generalist robot must possess the ability to do so. One way to specify each task is by setting a goal observation, but acquiring goal-reaching policies using reinforcement learning remains a difficult task, especially when there are no hand-engineered reward functions available. A promising approach is using learned dynamics models to learn about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We introduce a self-supervised method for model-based visual goal reaching that utilizes a visual dynamics model and a dynamical distance function learned through model-free reinforcement learning. Our technique learns entirely from offline, unlabeled data, making it practical to scale to large and diverse datasets. Our experiments demonstrate that our method can effectively learn models that perform a variety of tasks, such as moving objects amid distractors with a simulated robotic arm and learning to open and close a drawer using a real-world robot. Our approach substantially outperforms both model-free and model-based prior methods in comparisons. Visit http://sites.google.com/berkeley.edu/mbold for videos and visualizations.",1
"We consider the task of Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). In this setting, contexts, which define the reward and transition kernel, are sampled from a distribution. In addition, although the reward is a function of the context, it is not provided to the agent. Instead, the agent observes demonstrations from an optimal policy. The goal is to learn the reward mapping, such that the agent will act optimally even when encountering previously unseen contexts, also known as zero-shot transfer. We formulate this problem as a non-differential convex optimization problem and propose a novel algorithm to compute its subgradients. Based on this scheme, we analyze several methods both theoretically, where we compare the sample complexity and scalability, and empirically. Most importantly, we show both theoretically and empirically that our algorithms perform zero-shot transfer (generalize to new and unseen contexts). Specifically, we present empirical experiments in a dynamic treatment regime, where the goal is to learn a reward function which explains the behavior of expert physicians based on recorded data of them treating patients diagnosed with sepsis.",0
"Our focus is on Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). To achieve this, we extract contexts that determine the rewards and transition kernels from a distribution. Although the reward relies on the context, the agent does not receive it. Instead, the agent observes optimal policy demonstrations. The objective is to learn the reward mapping so that the agent can perform optimally even when presented with new contexts, also known as zero-shot transfer. To address this, we propose a non-differential convex optimization problem and introduce a unique algorithm to compute its subgradients. We compare sample complexity and scalability of several methods both theoretically and empirically. Most importantly, we show that our algorithms perform zero-shot transfer through theoretical and empirical analysis. Specifically, we conduct experiments in a dynamic treatment regime where we learn a reward function from expert physicians' recorded data treating patients diagnosed with sepsis.",1
"We present an approach for agents to learn representations of a global map from sensor data, to aid their exploration in new environments. To achieve this, we embed procedures mimicking that of traditional Simultaneous Localization and Mapping (SLAM) into the soft attention based addressing of external memory architectures, in which the external memory acts as an internal representation of the environment. This structure encourages the evolution of SLAM-like behaviors inside a completely differentiable deep neural network. We show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential. We validate our approach in both challenging grid-world environments and preliminary Gazebo experiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.",0
"Our approach enables agents to acquire a global map representation from sensor data, facilitating exploration in novel environments. We accomplish this by incorporating Simultaneous Localization and Mapping (SLAM) procedures into the soft attention-based addressing of external memory architectures. The external memory serves as an internal representation of the environment, promoting the development of SLAM-like behaviors within a differentiable deep neural network. Our method is effective in aiding reinforcement learning agents in navigating environments that require long-term memory. We demonstrate the effectiveness of our approach in both challenging grid-world settings and preliminary Gazebo experiments. To view a video of our experiments, please visit: https://goo.gl/G2Vu5y.",1
"We propose a framework for transferring any existing policy from a potentially unknown source MDP to a target MDP. This framework (1) enables reuse in the target domain of any form of source policy, including classical controllers, heuristic policies, or deep neural network-based policies, (2) attains optimality under suitable theoretical conditions, and (3) guarantees improvement over the source policy in the target MDP. These are achieved by packaging the source policy as a black-box option in the target MDP and providing a theoretically grounded way to learn the option's initiation set through general value functions. Our approach facilitates the learning of new policies by (1) maximizing the target MDP reward with the help of the black-box option, and (2) returning the agent to states in the learned initiation set of the black-box option where it is already optimal. We show that these two variants are equivalent in performance under some conditions. Through a series of experiments in simulated environments, we demonstrate that our framework performs excellently in sparse reward problems given (sub-)optimal source policies and improves upon prior art in transfer methods such as continual learning and progressive networks, which lack our framework's desirable theoretical properties.",0
"Our proposed framework allows for the transfer of existing policies from an unknown source MDP to a target MDP. This framework offers several benefits, including the ability to reuse any form of source policy, achieve optimality under certain conditions, and guarantee an improvement over the source policy in the target MDP. To accomplish this, we package the source policy as a black-box option in the target MDP and use general value functions to learn the option's initiation set. This allows for the learning of new policies by maximizing the target MDP reward with the black-box option and returning the agent to states in the learned initiation set where it is already optimal. Our experimental results demonstrate that our framework performs well in sparse reward problems and outperforms prior transfer methods such as continual learning and progressive networks, which lack our framework's desirable theoretical properties.",1
"The classical theory of reinforcement learning (RL) has focused on tabular and linear representations of value functions. Further progress hinges on combining RL with modern function approximators such as kernel functions and deep neural networks, and indeed there have been many empirical successes that have exploited such combinations in large-scale applications. There are profound challenges, however, in developing a theory to support this enterprise, most notably the need to take into consideration the exploration-exploitation tradeoff at the core of RL in conjunction with the computational and statistical tradeoffs that arise in modern function-approximation-based learning systems. We approach these challenges by studying an optimistic modification of the least-squares value iteration algorithm, in the context of the action-value function   represented by a kernel function or an overparameterized neural network. We establish both polynomial runtime complexity and polynomial sample complexity for this algorithm, without additional assumptions on the data-generating model. In particular, we prove that the algorithm incurs an $\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where $\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total number of episodes. Our regret bounds are independent of the number of states, a result which exhibits clearly the benefit of function approximation in RL.",0
"The traditional theory of reinforcement learning (RL) has primarily focused on using tabular and linear representations of value functions. However, to make further advancements in the field, it is necessary to combine RL with contemporary function approximators like deep neural networks and kernel functions. Despite the many successes achieved through this combination in large-scale applications, there are significant challenges in developing a theory that supports this approach, especially when factoring in the exploration-exploitation tradeoff and the computational and statistical tradeoffs that arise in modern function-approximation-based learning systems. To tackle these challenges, we studied an optimistic modification of the least-squares value iteration algorithm in the context of the action-value function represented by a kernel function or an overparameterized neural network. This algorithm achieves polynomial runtime complexity and polynomial sample complexity. Additionally, it incurs an $\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where $\delta_{\mathcal{F}}$ represents the intrinsic complexity of the function class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total number of episodes. Remarkably, our regret bounds are independent of the number of states, which highlights the advantages of using function approximation in RL.",1
"Imitation learning is well-suited for robotic tasks where it is difficult to directly program the behavior or specify a cost for optimal control. In this work, we propose a method for learning the reward function (and the corresponding policy) to match the expert state density. Our main result is the analytic gradient of any f-divergence between the agent and expert state distribution w.r.t. reward parameters. Based on the derived gradient, we present an algorithm, f-IRL, that recovers a stationary reward function from the expert density by gradient descent. We show that f-IRL can learn behaviors from a hand-designed target state density or implicitly through expert observations. Our method outperforms adversarial imitation learning methods in terms of sample efficiency and the required number of expert trajectories on IRL benchmarks. Moreover, we show that the recovered reward function can be used to quickly solve downstream tasks, and empirically demonstrate its utility on hard-to-explore tasks and for behavior transfer across changes in dynamics.",0
"Imitation learning is ideal for robotic tasks that are challenging to program directly or determine a cost for optimal control. Our proposed approach involves learning the reward function (and corresponding policy) to match the expert state density. The key contribution is the analytic gradient of any f-divergence between the agent and expert state distribution with respect to reward parameters. This gradient is used to develop an algorithm, f-IRL, that retrieves a stationary reward function from the expert density through gradient descent. Our method surpasses adversarial imitation learning techniques in terms of sample efficiency and required number of expert trajectories on IRL benchmarks. Additionally, we demonstrate that the retrieved reward function can solve downstream tasks quickly and is useful for behavior transfer across changes in dynamics, specifically on hard-to-explore tasks.",1
"Network architecture search (NAS) achieves state-of-the-art results in various tasks such as classification and semantic segmentation. Recently, a reinforcement learning-based approach has been proposed for Generative Adversarial Networks (GANs) search. In this work, we propose an alternative strategy for GAN search by using a method called DEGAS (Differentiable Efficient GenerAtor Search), which focuses on efficiently finding the generator in the GAN. Our search algorithm is inspired by the differential architecture search strategy and the Global Latent Optimization (GLO) procedure. This leads to both an efficient and stable GAN search. After the generator architecture is found, it can be plugged into any existing framework for GAN training. For CTGAN, which we use in this work, the new model outperforms the original inception score results by 0.25 for CIFAR-10 and 0.77 for STL. It also gets better results than the RL based GAN search methods in shorter search time.",0
"NAS has achieved impressive outcomes in tasks such as classification and semantic segmentation. Recently, a reinforcement learning-based approach was introduced for GANs search, which has shown promising results. However, in this study, we propose an alternative method for GAN search using DEGAS (Differentiable Efficient GenerAtor Search). Our approach efficiently finds the GAN's generator through a search algorithm that draws inspiration from the differential architecture search strategy and the Global Latent Optimization (GLO) procedure, resulting in a stable and efficient GAN search. Once the generator architecture is identified, it can be integrated into any existing GAN training framework. We demonstrate that our model outperforms the original inception score results by 0.25 for CIFAR-10 and 0.77 for STL in CTGAN and achieves better outcomes than RL-based GAN search techniques in a shorter search time.",1
"Previous work has shown the unreliability of existing algorithms in the batch Reinforcement Learning setting, and proposed the theoretically-grounded Safe Policy Improvement with Baseline Bootstrapping (SPIBB) fix: reproduce the baseline policy in the uncertain state-action pairs, in order to control the variance on the trained policy performance. However, in many real-world applications such as dialogue systems, pharmaceutical tests or crop management, data is collected under human supervision and the baseline remains unknown. In this paper, we apply SPIBB algorithms with a baseline estimate built from the data. We formally show safe policy improvement guarantees over the true baseline even without direct access to it. Our empirical experiments on finite and continuous states tasks support the theoretical findings. It shows little loss of performance in comparison with SPIBB when the baseline policy is given, and more importantly, drastically and significantly outperforms competing algorithms both in safe policy improvement, and in average performance.",0
"Prior research has identified the inadequacy of current algorithms in the batch Reinforcement Learning context and suggested a solution called Safe Policy Improvement with Baseline Bootstrapping (SPIBB), which is based on sound theoretical principles. The SPIBB approach involves replicating the baseline policy in uncertain state-action pairs to manage variations in the trained policy's performance. Nonetheless, in real-world scenarios such as pharmaceutical tests, crop management, and dialogue systems, data is typically gathered under human oversight, and the baseline remains unknown. This study employs SPIBB algorithms with a baseline estimation derived from data. We demonstrate, with formal evidence, that safe policy improvement guarantees can be obtained even without direct access to the true baseline. Our experimental results on tasks with finite and continuous states validate the theoretical findings. The findings reveal that the approach has a negligible loss of performance when compared to SPIBB, which has access to the baseline policy, and more importantly, it outperforms rival algorithms considerably in both safe policy improvement and average performance.",1
"The capability of imagining internally with a mental model of the world is vitally important for human cognition. If a machine intelligent agent can learn a world model to create a ""dream"" environment, it can then internally ask what-if questions -- simulate the alternative futures that haven't been experienced in the past yet -- and make optimal decisions accordingly. Existing world models are established typically by learning spatio-temporal regularities embedded from the past sensory signal without taking into account confounding factors that influence state transition dynamics. As such, they fail to answer the critical counterfactual questions about ""what would have happened"" if a certain action policy was taken. In this paper, we propose Causal World Models (CWMs) that allow unsupervised modeling of relationships between the intervened observations and the alternative futures by learning an estimator of the latent confounding factors. We empirically evaluate our method and demonstrate its effectiveness in a variety of physical reasoning environments. Specifically, we show reductions in sample complexity for reinforcement learning tasks and improvements in counterfactual physical reasoning.",0
"Human cognition heavily relies on the ability to internally imagine a mental model of the world. If a machine intelligent agent can acquire a world model to create a ""dream"" environment, it can simulate alternate futures and make optimal decisions. However, current world models only learn spatio-temporal regularities from past sensory signals and fail to answer counterfactual questions. In this study, we propose Causal World Models (CWMs) that allow unsupervised modeling of relationships between intervened observations and alternative futures by learning the latent confounding factors. Our empirical evaluation demonstrates the effectiveness of CWMs in physical reasoning environments, showing a reduction in sample complexity for reinforcement learning tasks and improvements in counterfactual physical reasoning.",1
"While deep reinforcement learning has achieved tremendous successes in various applications, most existing works only focus on maximizing the expected value of total return and thus ignore its inherent stochasticity. Such stochasticity is also known as the aleatoric uncertainty and is closely related to the notion of risk. In this work, we make the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. In particular, we focus on a variance-constrained policy optimization problem where the goal is to find a policy that maximizes the expected value of the long-run average reward, subject to a constraint that the long-run variance of the average reward is upper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we transform the original problem into an unconstrained saddle-point policy optimization problem, and propose an actor-critic algorithm that iteratively and efficiently updates the policy, the Lagrange multiplier, and the Fenchel dual variable. When both the value and policy functions are represented by multi-layer overparameterized neural networks, we prove that our actor-critic algorithm generates a sequence of policies that finds a globally optimal policy at a sublinear rate.",0
"While deep reinforcement learning has been successful in various applications, most current studies focus on maximizing the expected value of total return and ignore the stochastic nature of the system, also known as aleatoric uncertainty, which is related to risk. In this research, we investigate risk-sensitive deep reinforcement learning with the variance risk criteria. Specifically, we address the variance-constrained policy optimization problem, where the aim is to find a policy that maximizes the expected long-term average reward while ensuring that the long-term variance of the average reward stays within a threshold. We use Lagrangian and Fenchel dualities to transform the problem into an unconstrained saddle-point policy optimization problem. Then, we propose an actor-critic algorithm that updates the policy, Lagrange multiplier, and Fenchel dual variable iteratively and efficiently. We prove that our algorithm, which employs multi-layer overparameterized neural networks to represent both the value and policy functions, finds a globally optimal policy at a sublinear rate.",1
"Traffic signal controllers play an essential role in today's traffic system. However, the majority of them currently is not sufficiently flexible or adaptive to generate optimal traffic schedules. In this paper we present an approach to learning policies for signal controllers using deep reinforcement learning aiming for optimized traffic flow. Our method uses a novel formulation of the reward function that simultaneously considers efficiency and equity. We furthermore present a general approach to find the bound for the proposed equity factor and we introduce the adaptive discounting approach that greatly stabilizes learning and helps to maintain a high flexibility of green light duration. The experimental evaluations on both simulated and real-world data demonstrate that our proposed algorithm achieves state-of-the-art performance (previously held by traditional non-learning methods) on a wide range of traffic situations.",0
"Today's traffic system relies heavily on traffic signal controllers, but most of them lack flexibility and adaptability to create the best traffic schedules. This paper introduces a new method that uses deep reinforcement learning to teach controllers how to optimize traffic flow. Our approach considers both efficiency and equity by using a unique reward function formulation. We also provide a general approach to set the equity factor and an adaptive discounting approach to stabilize learning and maintain flexibility. Our experimental evaluations on simulated and real-world data show that our method outperforms traditional non-learning methods in a variety of traffic situations.",1
"Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.",0
"Ordered event data in continuous time is commonly produced by social goods, such as healthcare, smart city, and information networks. These event data have complex generative processes that require flexible models to capture their dynamics. Temporal point processes are an elegant framework for modeling event data without discretizing time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm necessitates hand-crafting the intensity function beforehand and cannot monitor the goodness-of-fit of the estimated model directly in the process of training. To mitigate the risk of model-misspecification in MLE, we propose generating samples from the generative model and monitoring the quality of the samples during training until they are indistinguishable from real data. We draw inspiration from reinforcement learning (RL) and treat each event's generation as an action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and progressively improve it to mimic the observed event distribution. Since the reward function is unknown, we derive an analytic and nonparametric form of it using an inverse reinforcement learning formulation. This new RL framework enables us to derive an efficient policy gradient algorithm for learning flexible point process models, which performs well in both synthetic and real data.",1
"Existing neural network-based autonomous systems are shown to be vulnerable against adversarial attacks, therefore sophisticated evaluation on their robustness is of great importance. However, evaluating the robustness only under the worst-case scenarios based on known attacks is not comprehensive, not to mention that some of them even rarely occur in the real world. In addition, the distribution of safety-critical data is usually multimodal, while most traditional attacks and evaluation methods focus on a single modality. To solve the above challenges, we propose a flow-based multimodal safety-critical scenario generator for evaluating decisionmaking algorithms. The proposed generative model is optimized with weighted likelihood maximization and a gradient-based sampling procedure is integrated to improve the sampling efficiency. The safety-critical scenarios are generated by querying the task algorithms and the log-likelihood of the generated scenarios is in proportion to the risk level. Experiments on a self-driving task demonstrate our advantages in terms of testing efficiency and multimodal modeling capability. We evaluate six Reinforcement Learning algorithms with our generated traffic scenarios and provide empirical conclusions about their robustness.",0
"Autonomous systems based on neural networks have been found to be vulnerable to attacks, highlighting the need for thorough assessments of their resilience. However, evaluating their stability solely under extreme circumstances and known attacks is insufficient, particularly since such scenarios rarely occur in real-life situations. Furthermore, safety-critical data is often distributed across multiple modes, but most evaluation methods and attacks focus on a single modality. To address these challenges, we propose a flow-based multimodal generator for creating safety-critical scenarios to evaluate decision-making algorithms. Our generative model is optimized with weighted likelihood maximization, and we have integrated a gradient-based sampling procedure to enhance sampling efficiency. We query task algorithms to generate safety-critical scenarios, with the log-likelihood of the generated scenarios being proportional to the level of risk. Our experiments on a self-driving task demonstrate our ability to efficiently test and model multimodal scenarios. We evaluated six Reinforcement Learning algorithms using our generated traffic scenarios and drew empirical conclusions about their resilience.",1
"We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.",0
"Our research focuses on reinforcement learning (RL) in the context of text-based games, which involve interactive simulations using natural language. Although various approaches have been developed to represent information about the environment and language actions, current RL agents lack reasoning capabilities to handle text-based games. In this study, we seek to use knowledge graphs to enable explicit reasoning for decision-making, allowing agents to generate actions supported by an interpretable inference procedure. To achieve this, we introduce a stacked hierarchical attention mechanism that constructs a clear representation of the reasoning process by leveraging the knowledge graph's structure. We extensively test our method on several benchmark games, and the results demonstrate its superior performance compared to existing text-based agents.",1
"AlphaGo's astonishing performance has ignited an explosive interest in developing deep reinforcement learning (DRL) for numerous real-world applications, such as intelligent robotics. However, the often prohibitive complexity of DRL stands at the odds with the required real-time control and constrained resources in many DRL applications, limiting the great potential of DRL powered intelligent devices. While substantial efforts have been devoted to compressing other deep learning models, existing works barely touch the surface of compressing DRL. In this work, we first identify that there exists an optimal model size of DRL that can maximize both the test scores and efficiency, motivating the need for task-specific DRL agents. We therefore propose an Auto-Agent-Distiller (A2D) framework, which to our best knowledge is the first neural architecture search (NAS) applied to DRL to automatically search for the optimal DRL agents for various tasks that optimize both the test scores and efficiency. Specifically, we demonstrate that vanilla NAS can easily fail in searching for the optimal agents, due to its resulting high variance in DRL training stability, and then develop a novel distillation mechanism to distill the knowledge from both the teacher agent's actor and critic to stabilize the searching process and improve the searched agents' optimality. Extensive experiments and ablation studies consistently validate our findings and the advantages and general applicability of our A2D, outperforming manually designed DRL in both the test scores and efficiency. All the codes will be released upon acceptance.",0
"The remarkable performance of AlphaGo has sparked a surge of interest in developing deep reinforcement learning (DRL) for a multitude of practical applications, including intelligent robotics. However, the complexity of DRL often poses a challenge for real-time control and limited resources, which hinders the potential of DRL-powered devices. Despite the efforts to compress other deep learning models, compressing DRL remains largely unexplored. This study identifies an optimal DRL model size that can maximize test scores and efficiency, highlighting the need for task-specific DRL agents. To address this, an Auto-Agent-Distiller (A2D) framework is proposed, which is the first neural architecture search (NAS) applied to DRL for automatically searching for the optimal DRL agents for various tasks. The vanilla NAS alone can fail due to high variance in DRL training stability, so a novel distillation mechanism is developed to distill knowledge from the teacher agent's actor and critic to stabilize the searching process and enhance the optimality of the searched agents. Extensive experiments and ablation studies confirm the effectiveness and general applicability of A2D, surpassing manually designed DRL in both test scores and efficiency. All codes will be available upon acceptance.",1
"In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations and mathematically characterize the non-stationary dynamics of each setting. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.",0
"The objective of this article is to present an overview of various formulations and methods for continual reinforcement learning (RL), which is also referred to as lifelong or non-stationary RL. Our perspective on why RL is well-suited for studying continual learning is discussed initially. We then classify the different continual RL formulations and describe the non-stationary dynamics of each setting mathematically. Additionally, we cover the evaluation of continual RL agents, including a summary of benchmarks used in the literature and important metrics for evaluating agent performance. Finally, we identify unresolved issues and challenges in connecting the current state of continual RL with neuroscience findings. Despite being in the early stages, the study of continual RL has the potential to create better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a crucial role, such as healthcare, education, logistics, and robotics.",1
"This paper proposes a new robust update rule of target network for deep reinforcement learning (DRL), to replace the conventional update rule, given as an exponential moving average. The target network is for smoothly generating the reference signals for a main network in DRL, thereby reducing learning variance. The problem with its conventional update rule is the fact that all the parameters are smoothly copied with the same speed from the main network, even when some of them are trying to update toward the wrong directions. This behavior increases the risk of generating the wrong reference signals. Although slowing down the overall update speed is a naive way to mitigate wrong updates, it would decrease learning speed. To robustly update the parameters while keeping learning speed, a t-soft update method, which is inspired by student-t distribution, is derived with reference to the analogy between the exponential moving average and the normal distribution. Through the analysis of the derived t-soft update, we show that it takes over the properties of the student-t distribution. Specifically, with a heavy-tailed property of the student-t distribution, the t-soft update automatically excludes extreme updates that differ from past experiences. In addition, when the updates are similar to the past experiences, it can mitigate the learning delay by increasing the amount of updates. In PyBullet robotics simulations for DRL, an online actor-critic algorithm with the t-soft update outperformed the conventional methods in terms of the obtained return and/or its variance. From the training process by the t-soft update, we found that the t-soft update is globally consistent with the standard soft update, and the update rates are locally adjusted for acceleration or suppression.",0
"This paper introduces a new update rule for the target network in deep reinforcement learning (DRL) that is more robust than the conventional exponential moving average method. The target network is responsible for producing reference signals for the main network in DRL, and the new method aims to reduce learning variance. The issue with the conventional method is that all parameters are copied at the same rate, even those moving in the wrong direction, which increases the risk of producing incorrect reference signals. Slowing down the update speed is not an effective solution as it decreases learning speed. To address this problem, the authors propose a t-soft update method based on the student-t distribution that robustly updates parameters while maintaining learning speed. The t-soft update automatically excludes extreme updates and can increase the amount of updates to mitigate learning delays. The authors demonstrate the effectiveness of the t-soft update in PyBullet robotics simulations for DRL, achieving better returns and/or variance compared to conventional methods. The t-soft update is globally consistent with the standard soft update, and its update rates are locally adjusted for acceleration or suppression during the training process.",1
"In this paper we consider the problem of learning an $\epsilon$-optimal policy for a discounted Markov Decision Process (MDP). Given an MDP with $S$ states, $A$ actions, the discount factor $\gamma \in (0,1)$, and an approximation threshold $\epsilon > 0$, we provide a model-free algorithm to learn an $\epsilon$-optimal policy with sample complexity $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{5.5}})$ (where the notation $\tilde{O}(\cdot)$ hides poly-logarithmic factors of $S,A,1/(1-\gamma)$, and $1/\epsilon$) and success probability $(1-p)$. For small enough $\epsilon$, we show an improved algorithm with sample complexity $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{3}})$. While the first bound improves upon all known model-free algorithms and model-based ones with tight dependence on $S$, our second algorithm beats all known sample complexity bounds and matches the information theoretic lower bound up to logarithmic factors.",0
"The focus of this paper is on the challenge of acquiring an $\epsilon$-optimal policy for a discounted Markov Decision Process (MDP). To this end, we present a model-free technique for learning an $\epsilon$-optimal policy with a sample complexity of $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{5.5}})$, where $\tilde{O}(\cdot)$ denotes that the poly-logarithmic factors of $S,A,1/(1-\gamma)$, and $1/\epsilon$ are concealed. The probability of success is $(1-p)$. For smaller values of $\epsilon$, we propose a better algorithm with a sample complexity of $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{3}})$. Our first approach surpasses all prevailing model-free and model-based algorithms with a strong dependence on $S$, whereas our second algorithm matches the information theoretic lower bound up to logarithmic factors and surpasses all existing sample complexity boundaries.",1
"Policy gradient methods are among the most effective methods for large-scale reinforcement learning, and their empirical success has prompted several works that develop the foundation of their global convergence theory. However, prior works have either required exact gradients or state-action visitation measure based mini-batch stochastic gradients with a diverging batch size, which limit their applicability in practical scenarios. In this paper, we consider classical policy gradient methods that compute an approximate gradient with a single trajectory or a fixed size mini-batch of trajectories under soft-max parametrization and log-barrier regularization, along with the widely-used REINFORCE gradient estimation procedure. By controlling the number of ""bad"" episodes and resorting to the classical doubling trick, we establish an anytime sub-linear high probability regret bound as well as almost sure global convergence of the average regret with an asymptotically sub-linear rate. These provide the first set of global convergence and sample efficiency results for the well-known REINFORCE algorithm and contribute to a better understanding of its performance in practice.",0
"Several works have developed the foundation of global convergence theory for policy gradient methods, which are highly effective for large-scale reinforcement learning. However, previous works have limitations, such as requiring exact gradients or state-action visitation measure based mini-batch stochastic gradients with a diverging batch size. This restricts their practical applicability. This paper focuses on classical policy gradient methods with soft-max parametrization and log-barrier regularization, using a single trajectory or a fixed size mini-batch of trajectories to compute an approximate gradient. By controlling the number of ""bad"" episodes and resorting to the classical doubling trick, this paper establishes anytime sub-linear high probability regret bound and almost sure global convergence of the average regret with an asymptotically sub-linear rate. These results provide the first set of global convergence and sample efficiency results for the well-known REINFORCE algorithm and contribute to a better understanding of its practical performance.",1
"Deep reinforcement learning (DRL) has been used to learn effective heuristics for solving complex combinatorial optimisation problem via policy networks and have demonstrated promising performance. Existing works have focused on solving (vehicle) routing problems as they have a nice balance between non-triviality and difficulty. State-of-the-art approaches learn a policy using reinforcement learning, and the learnt policy acts as a pseudo solver. These approaches have demonstrated good performance in some cases, but given the large search space typical combinatorial/routing problem, they can converge too quickly to poor policy. To prevent this, in this paper, we propose an approach name entropy regularised reinforcement learning (ERRL) that supports exploration by providing more stochastic policies, which tends to improve optimisation. Empirically, the low variance ERRL offers RL training fast and stable. We also introduce a combination of local search operators during test time, which significantly improves solution and complement ERRL. We qualitatively demonstrate that for vehicle routing problems, a policy with higher entropy can make the optimisation landscape smooth which makes it easier to optimise. The quantitative evaluation shows that the performance of the model is comparable with the state-of-the-art variants. In our evaluation, we experimentally illustrate that the model produces state-of-the-art performance on variants of Vehicle Routing problems such as Capacitated Vehicle Routing Problem (CVRP), Multiple Routing with Fixed Fleet Problems (MRPFF) and Travelling Salesman problem.",0
"Policy networks trained using deep reinforcement learning (DRL) have proven effective for solving complex combinatorial optimization problems, particularly in the context of vehicle routing problems. However, existing approaches can converge too quickly to suboptimal policies due to the large search space of these problems. To address this issue, we propose an entropy regularized reinforcement learning (ERRL) approach that promotes exploration and enables the learning of more stochastic policies to improve optimization. Our empirical evaluation shows that ERRL offers fast and stable RL training with low variance. Additionally, we introduce a combination of local search operators during test time to further enhance the quality of solutions. Our results demonstrate that policies with higher entropy can lead to smoother optimization landscapes, making the optimization task easier. We evaluate our model on several variants of vehicle routing problems, including Capacitated Vehicle Routing Problem (CVRP), Multiple Routing with Fixed Fleet Problems (MRPFF), and Travelling Salesman Problem, and show that our model achieves state-of-the-art performance.",1
"Many traditional algorithms for solving combinatorial optimization problems involve using hand-crafted heuristics that sequentially construct a solution. Such heuristics are designed by domain experts and may often be suboptimal due to the hard nature of the problems. Reinforcement learning (RL) proposes a good alternative to automate the search of these heuristics by training an agent in a supervised or self-supervised manner. In this survey, we explore the recent advancements of applying RL frameworks to hard combinatorial problems. Our survey provides the necessary background for operations research and machine learning communities and showcases the works that are moving the field forward. We juxtapose recently proposed RL methods, laying out the timeline of the improvements for each problem, as well as we make a comparison with traditional algorithms, indicating that RL models can become a promising direction for solving combinatorial problems.",0
"To solve combinatorial optimization problems, traditional algorithms often use manually designed heuristics that construct solutions sequentially. Due to the difficulty of these problems, these heuristics may not be optimal despite being crafted by experts in the field. Reinforcement learning (RL) offers an alternative solution by training an agent in a supervised or self-supervised manner to automate the search for heuristics. Our survey delves into recent advancements in using RL frameworks for challenging combinatorial problems. We provide essential background information for both operations research and machine learning communities and showcase the latest developments in the field. We present a timeline of recent RL methods and compare them to traditional algorithms, highlighting the potential of RL models for solving combinatorial problems.",1
"Off-policy evaluation (OPE) is the problem of evaluating new policies using historical data obtained from a different policy. In the recent OPE context, most studies have focused on single-player cases, and not on multi-player cases. In this study, we propose OPE estimators constructed by the doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. The proposed estimators project exploitability that is often used as a metric for determining how close a policy profile (i.e., a tuple of policies) is to a Nash equilibrium in two-player zero-sum games. We prove the exploitability estimation error bounds for the proposed estimators. We then propose the methods to find the best candidate policy profile by selecting the policy profile that minimizes the estimated exploitability from a given policy profile class. We prove the regret bounds of the policy profiles selected by our methods. Finally, we demonstrate the effectiveness and performance of the proposed estimators through experiments.",0
"The challenge of assessing new policies using historical data obtained from a different policy is known as off-policy evaluation (OPE). Current OPE research has largely focused on single-player scenarios, rather than multi-player ones. Our study introduces OPE estimators that utilize doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. These estimators are designed to project exploitability, which is commonly used to measure how closely a policy profile (a set of policies) approaches a Nash equilibrium in two-player zero-sum games. We provide error bounds for the exploitability estimates generated by our proposed estimators. Additionally, we present methods for identifying the best candidate policy profile by selecting the profile with the lowest estimated exploitability from a given profile class. We also demonstrate the policy profiles selected by our methods have regret bounds. Finally, we conduct experiments to illustrate the effectiveness and performance of our proposed estimators.",1
"Deep reinforcement learning (DRL) policies have been shown to be deceived by perturbations (e.g., random noise or intensional adversarial attacks) on state observations that appear at test time but are unknown during training. To increase the robustness of DRL policies, previous approaches assume that the knowledge of adversaries can be added into the training process to achieve the corresponding generalization ability on these perturbed observations. However, such an assumption not only makes the robustness improvement more expensive but may also leave a model less effective to other kinds of attacks in the wild. In contrast, we propose an adversary agnostic robust DRL paradigm that does not require learning from adversaries. To this end, we first theoretically derive that robustness could indeed be achieved independently of the adversaries based on a policy distillation setting. Motivated by this finding, we propose a new policy distillation loss with two terms: 1) a prescription gap maximization loss aiming at simultaneously maximizing the likelihood of the action selected by the teacher policy and the entropy over the remaining actions; 2) a corresponding Jacobian regularization loss that minimizes the magnitude of the gradient with respect to the input state. The theoretical analysis shows that our distillation loss guarantees to increase the prescription gap and the adversarial robustness. Furthermore, experiments on five Atari games firmly verify the superiority of our approach in terms of boosting adversarial robustness compared to other state-of-the-art methods.",0
"Perturbations, such as random noise or deliberate adversarial attacks, on state observations during test time can deceive Deep Reinforcement Learning (DRL) policies, even if they were not present during training. Prior methods to increase the robustness of DRL policies involved adding knowledge of adversaries into the training process to achieve generalization ability on perturbed observations. However, this approach is costly and may not protect against other types of attacks. In contrast, we propose an adversary agnostic robust DRL paradigm that does not require learning from adversaries. Based on a policy distillation setting, we derive that robustness can be achieved independently of adversaries. We propose a new policy distillation loss with two terms: a prescription gap maximization loss and a corresponding Jacobian regularization loss. Our theoretical analysis shows that our distillation loss guarantees to increase the prescription gap and adversarial robustness. Experiments on five Atari games demonstrate the superiority of our approach in terms of boosting adversarial robustness compared to other state-of-the-art methods.",1
"Multi-agent reinforcement learning (MARL) has been increasingly explored to learn the cooperative policy towards maximizing a certain global reward. Many existing studies take advantage of graph neural networks (GNN) in MARL to propagate critical collaborative information over the interaction graph, built upon inter-connected agents. Nevertheless, the vanilla GNN approach yields substantial defects in dealing with complex real-world scenarios since the generic message passing mechanism is ineffective between heterogeneous vertices and, moreover, simple message aggregation functions are incapable of accurately modeling the combinational interactions from multiple neighbors. While adopting complex GNN models with more informative message passing and aggregation mechanisms can obviously benefit heterogeneous vertex representations and cooperative policy learning, it could, on the other hand, increase the training difficulty of MARL and demand more intense and direct reward signals compared to the original global reward. To address these challenges, we propose a new cooperative learning framework with pre-trained heterogeneous observation representations. Particularly, we employ an encoder-decoder based graph attention to learn the intricate interactions and heterogeneous representations that can be more easily leveraged by MARL. Moreover, we design a pre-training with local actor-critic algorithm to ease the difficulty in cooperative policy learning. Extensive experiments over real-world scenarios demonstrate that our new approach can significantly outperform existing MARL baselines as well as operational research solutions that are widely-used in industry.",0
"MARL is being increasingly studied as a way to learn how to maximize a global reward through cooperative policies. To achieve this, many studies use GNNs to propagate collaborative information across interconnected agents in an interaction graph. However, the vanilla GNN approach is not effective in complex real-world scenarios due to its inability to deal with heterogeneous vertices and model combinational interactions accurately. Although more complex GNN models could improve vertex representations and policy learning, they could also make MARL training more challenging and require more direct reward signals. To overcome these challenges, we propose a new cooperative learning framework that uses pre-trained heterogeneous observation representations and an encoder-decoder based graph attention mechanism. Additionally, we design a pre-training process using a local actor-critic algorithm to make cooperative policy learning easier. Our experiments show that our approach outperforms existing MARL baselines and widely-used operational research solutions in industry.",1
"We propose a simple model selection approach for algorithms in stochastic bandit and reinforcement learning problems. As opposed to prior work that (implicitly) assumes knowledge of the optimal regret, we only require that each base algorithm comes with a candidate regret bound that may or may not hold during all rounds. In each round, our approach plays a base algorithm to keep the candidate regret bounds of all remaining base algorithms balanced, and eliminates algorithms that violate their candidate bound. We prove that the total regret of this approach is bounded by the best valid candidate regret bound times a multiplicative factor. This factor is reasonably small in several applications, including linear bandits and MDPs with nested function classes, linear bandits with unknown misspecification, and LinUCB applied to linear bandits with different confidence parameters. We further show that, under a suitable gap-assumption, this factor only scales with the number of base algorithms and not their complexity when the number of rounds is large enough. Finally, unlike recent efforts in model selection for linear stochastic bandits, our approach is versatile enough to also cover cases where the context information is generated by an adversarial environment, rather than a stochastic one.",0
"Our proposed model selection approach for stochastic bandit and reinforcement learning problems is straightforward. Unlike previous approaches that assume knowledge of the optimal regret, we only require each base algorithm to have a candidate regret bound that may or may not hold during all rounds. In each round, we play a base algorithm that keeps the candidate regret bounds of all remaining base algorithms balanced and eliminates those that violate their candidate bound. We prove that the total regret of this approach is limited by the best valid candidate regret bound multiplied by a reasonably small factor in various applications. These include linear bandits and MDPs with nested function classes, linear bandits with unknown misspecification, and LinUCB applied to linear bandits with different confidence parameters. Furthermore, we demonstrate that, under an appropriate gap-assumption, this factor only scales with the number of base algorithms and not their complexity when the number of rounds is large enough. Lastly, our approach is versatile enough to cover cases where the context information is generated by an adversarial environment instead of a stochastic one, unlike recent model selection efforts for linear stochastic bandits.",1
"We consider the problem of finding optimal policies for a Markov Decision Process with almost sure constraints on state transitions and action triplets. We define value and action-value functions that satisfy a barrier-based decomposition which allows for the identification of feasible policies independently of the reward process. We prove that, given a policy {\pi}, certifying whether certain state-action pairs lead to feasible trajectories under {\pi} is equivalent to solving an auxiliary problem aimed at finding the probability of performing an unfeasible transition. Using this interpretation,we develop a Barrier-learning algorithm, based on Q-Learning, that identifies such unsafe state-action pairs. Our analysis motivates the need to enhance the Reinforcement Learning (RL) framework with an additional signal, besides rewards, called here damage function that provides feasibility information and enables the solution of RL problems with model-free constraints. Moreover, our Barrier-learning algorithm wraps around existing RL algorithms, such as Q-Learning and SARSA, giving them the ability to solve almost-surely constrained problems.",0
"The aim of our study is to determine optimal policies for a Markov Decision Process that has almost certain constraints on state transitions and action triplets. We introduce value and action-value functions that follow a barrier-based breakdown, which allows for the identification of feasible policies without reference to the reward process. Our research demonstrates that determining whether certain state-action pairs are feasible under a given policy is equivalent to solving a supplementary problem that evaluates the probability of performing an unfeasible transition. We have developed a Barrier-learning algorithm, based on Q-Learning, that identifies hazardous state-action pairs. Our findings highlight the need to supplement the Reinforcement Learning (RL) framework with a damage function, in addition to rewards, which provides feasibility information and enables the solution of RL problems with model-free constraints. Additionally, our Barrier-learning algorithm can be integrated with existing RL algorithms, such as Q-Learning and SARSA, enabling them to solve almost-surely constrained problems.",1
"This paper gives a detailed review of reinforcement learning in combinatorial optimization, introduces the history of combinatorial optimization starting in the 1960s, and compares it with the reinforcement learning algorithms in recent years. We explicitly look at a famous combinatorial problem known as the Traveling Salesperson Problem (TSP). We compare the approach of the modern reinforcement learning algorithms on TSP with an approach published in 1970. Then, we discuss the similarities between these algorithms and how the approach of reinforcement learning changes due to the evolution of machine learning techniques and computing power. We also mention the deep learning approach on the TSP, which is named Deep Reinforcement Learning. We argue that deep learning is a generic approach that can be integrated with traditional reinforcement learning algorithms and optimize the outcomes of the TSP.",0
"In this paper, an in-depth analysis of reinforcement learning in combinatorial optimization is presented. The history of combinatorial optimization from the 1960s is introduced, and a comparison is made with the recent reinforcement learning algorithms. The Traveling Salesperson Problem (TSP), a well-known combinatorial problem, is explicitly examined, and the modern reinforcement learning algorithms' approach is compared with the approach published in 1970. The similarities between these algorithms are discussed, and the evolution of machine learning techniques and computing power is shown to affect the reinforcement learning approach. Additionally, the Deep Reinforcement Learning approach on TSP is mentioned, and it is argued that it is a generic approach that can optimize the outcomes of the TSP by integrating with traditional reinforcement learning algorithms.",1
"This paper introduces four new algorithms that can be used for tackling multi-agent reinforcement learning (MARL) problems occurring in cooperative settings. All algorithms are based on the Deep Quality-Value (DQV) family of algorithms, a set of techniques that have proven to be successful when dealing with single-agent reinforcement learning problems (SARL). The key idea of DQV algorithms is to jointly learn an approximation of the state-value function $V$, alongside an approximation of the state-action value function $Q$. We follow this principle and generalise these algorithms by introducing two fully decentralised MARL algorithms (IQV and IQV-Max) and two algorithms that are based on the centralised training with decentralised execution training paradigm (QVMix and QVMix-Max). We compare our algorithms with state-of-the-art MARL techniques on the popular StarCraft Multi-Agent Challenge (SMAC) environment. We show competitive results when QVMix and QVMix-Max are compared to well-known MARL techniques such as QMIX and MAVEN and show that QVMix can even outperform them on some of the tested environments, being the algorithm which performs best overall. We hypothesise that this is due to the fact that QVMix suffers less from the overestimation bias of the $Q$ function.",0
"In this paper, new algorithms are presented for addressing multi-agent reinforcement learning (MARL) problems in cooperative settings. These algorithms are based on the successful Deep Quality-Value (DQV) family of algorithms used for single-agent reinforcement learning (SARL). The DQV approach involves learning both state-value function V and state-action value function Q together. We have extended this approach by introducing two fully decentralised MARL algorithms (IQV and IQV-Max) and two algorithms based on the centralised training with decentralised execution training paradigm (QVMix and QVMix-Max). Our algorithms are compared to state-of-the-art MARL techniques on the StarCraft Multi-Agent Challenge (SMAC) environment, and we demonstrate competitive results. Specifically, QVMix and QVMix-Max perform as well as well-known techniques such as QMIX and MAVEN, and QVMix even outperforms them on some of the tested environments. We believe that this is because QVMix is less affected by the overestimation bias of the Q function.",1
"Self-imitation learning is a Reinforcement Learning (RL) method that encourages actions whose returns were higher than expected, which helps in hard exploration and sparse reward problems. It was shown to improve the performance of on-policy actor-critic methods in several discrete control tasks. Nevertheless, applying self-imitation to the mostly action-value based off-policy RL methods is not straightforward. We propose SAIL, a novel generalization of self-imitation learning for off-policy RL, based on a modification of the Bellman optimality operator that we connect to Advantage Learning. Crucially, our method mitigates the problem of stale returns by choosing the most optimistic return estimate between the observed return and the current action-value for self-imitation. We demonstrate the empirical effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard exploration games.",0
"The technique of self-imitation learning is a Reinforcement Learning (RL) approach that promotes actions that yield higher-than-anticipated returns, aiding in the resolution of tricky exploration and sparse reward issues. This method has been shown to enhance the performance of on-policy actor-critic methods in various discrete control tasks. However, applying self-imitation to off-policy RL methods, which are mostly action-value based, is not a simple task. To address this, we have developed SAIL, a novel form of self-imitation learning for off-policy RL, which is grounded in a modification of the Bellman optimality operator and linked to Advantage Learning. Our method effectively addresses the issue of stagnant returns by choosing the most optimistic return estimate between the observed return and the current action-value for self-imitation. We have tested the efficacy of SAIL empirically on the Arcade Learning Environment, concentrating on difficult exploration games.",1
"This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that our methodology can effectively recover the DRL models with high fidelity and accuracy. We also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal DRL models stealthily even they are protected by DNN watermarks. These pose a severe threat to the intellectual property and privacy protection of DRL applications.",0
"This article introduces the first model extraction attack aimed at Deep Reinforcement Learning (DRL), which allows an outside adversary to recover the black-box DRL model accurately through its interactions with the environment. Previous research has extensively studied model extraction attacks against supervised Deep Learning models, but these techniques are not applicable to the reinforcement learning scenario due to the complexity, stochasticity, and limited observable information of DRL models. To address these challenges, the authors propose a unique approach that views DRL model extraction as imitation learning, a proven solution to learning sequential decision-making policies. The methodology involves building a classifier to identify the training algorithm family of the targeted black-box DRL model based only on its predicted actions and then utilizing state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that the methodology can effectively recover DRL models with high fidelity and accuracy. The authors also demonstrate two use cases where the model extraction attack can improve the success rate of adversarial attacks and steal DRL models stealthily, even if they are protected by DNN watermarks. These findings pose a significant risk to intellectual property and privacy protection in DRL applications.",1
"Generating useful network summaries is a challenging and important problem with several applications like sensemaking, visualization, and compression. However, most of the current work in this space do not take human feedback into account while generating summaries. Consider an intelligence analysis scenario, where the analyst is exploring a similarity network between documents. The analyst can express her agreement/disagreement with the visualization of the network summary via iterative feedback, e.g. closing or moving documents (""nodes"") together. How can we use this feedback to improve the network summary quality? In this paper, we present NetReAct, a novel interactive network summarization algorithm which supports the visualization of networks induced by text corpora to perform sensemaking. NetReAct incorporates human feedback with reinforcement learning to summarize and visualize document networks. Using scenarios from two datasets, we show how NetReAct is successful in generating high-quality summaries and visualizations that reveal hidden patterns better than other non-trivial baselines.",0
"Generating effective network summaries is a challenging and significant problem with various applications such as visualization, sensemaking, and compression. However, most of the existing work in this area neglects human input when creating summaries. In a scenario where an intelligence analyst is exploring a similarity network between documents, the analyst can provide feedback on the visualization of the network summary by closing or moving documents (""nodes"") together. This feedback can be utilized to improve the quality of the network summary. This paper presents NetReAct, an innovative algorithm that generates interactive network summaries and supports text corpora-induced network visualization for sensemaking. NetReAct integrates human feedback with reinforcement learning to generate document network summaries and visualizations. Two datasets scenarios demonstrate how NetReAct generates superior quality summaries and visualizations that reveal hidden patterns better than other non-trivial baselines.",1
"While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.",0
"Despite the impressive results demonstrated by deep learning and deep reinforcement learning (RL) systems in image classification, game playing, and robotic control, the challenge of data efficiency remains. Multi-task learning is a promising approach for enhancing efficiency by sharing structure across multiple tasks; however, this approach presents several optimization challenges that hinder large efficiency gains. The reasons for the difficulty of multi-task learning compared to single-task learning are not fully understood. In this study, we identify three conditions of the multi-task optimization landscape that cause detrimental gradient interference and propose a general approach for avoiding such interference between task gradients. We introduce gradient surgery, which involves projecting a task's gradient onto the normal plane of the gradient of any other task with a conflicting gradient. This approach leads to significant improvements in efficiency and performance on challenging multi-task supervised and multi-task RL problems. Moreover, it is model-agnostic and can be combined with previously proposed multi-task architectures to enhance performance.",1
"Reinforcement learning (RL) has the potential to significantly improve clinical decision making. However, treatment policies learned via RL from observational data are sensitive to subtle choices in study design. We highlight a simple approach, trajectory inspection, to bring clinicians into an iterative design process for model-based RL studies. We identify where the model recommends unexpectedly aggressive treatments or expects surprisingly positive outcomes from its recommendations. Then, we examine clinical trajectories simulated with the learned model and policy alongside the actual hospital course. Applying this approach to recent work on RL for sepsis management, we uncover a model bias towards discharge, a preference for high vasopressor doses that may be linked to small sample sizes, and clinically implausible expectations of discharge without weaning off vasopressors. We hope that iterations of detecting and addressing the issues unearthed by our method will result in RL policies that inspire more confidence in deployment.",0
"The use of reinforcement learning (RL) has the potential to significantly enhance clinical decision making. However, the design of observational data-based treatment policies learned through RL is crucial, as even subtle choices in study design can affect the results. To address this issue, we propose a simple approach, trajectory inspection, that involves clinicians in an iterative design process for model-based RL studies. We first identify areas where the model recommends unexpectedly aggressive treatments or expects surprisingly positive outcomes from its recommendations. We then simulate clinical trajectories using the learned model and policy alongside the actual hospital course. By applying this approach to recent work on RL for sepsis management, we reveal a model bias towards discharge, a preference for high vasopressor doses that may be linked to small sample sizes, and clinically implausible expectations of discharge without weaning off vasopressors. We believe that by continually detecting and addressing the issues we uncover through our method, we can develop RL policies that inspire greater confidence in their deployment.",1
"Users can be supported to adopt healthy behaviors, such as regular physical activity, via relevant and timely suggestions on their mobile devices. Recently, reinforcement learning algorithms have been found to be effective for learning the optimal context under which to provide suggestions. However, these algorithms are not necessarily designed for the constraints posed by mobile health (mHealth) settings, that they be efficient, domain-informed and computationally affordable. We propose an algorithm for providing physical activity suggestions in mHealth settings. Using domain-science, we formulate a contextual bandit algorithm which makes use of a linear mixed effects model. We then introduce a procedure to efficiently perform hyper-parameter updating, using far less computational resources than competing approaches. Not only is our approach computationally efficient, it is also easily implemented with closed form matrix algebraic updates and we show improvements over state of the art approaches both in speed and accuracy of up to 99% and 56% respectively.",0
"To encourage healthy habits like regular physical activity, users can receive useful and timely suggestions on their mobile devices. Reinforcement learning algorithms have proven effective for determining the best context in which to provide these suggestions. However, these algorithms are not always suitable for use in mobile health (mHealth) settings, where efficiency, domain knowledge, and affordability are essential. Our proposal introduces an algorithm for delivering physical activity suggestions in mHealth settings. Using domain-specific knowledge, we have developed a contextual bandit algorithm that incorporates a linear mixed effects model. We have also developed a procedure for efficiently updating hyper-parameters, which requires fewer computational resources than other methods. Our approach is both computationally efficient and easily implemented, and we have demonstrated that it outperforms other methods in terms of speed and accuracy by up to 99% and 56%, respectively.",1
"Offline reinforcement learning (RL) refers to the problem of learning policies from a static dataset of environment interactions. Offline RL enables extensive use and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-world applicability of RL. Most prior work in offline RL has focused on tasks with compact state representations. However, the ability to learn directly from rich observation spaces like images is critical for real-world applications such as robotics. In this work, we build on recent advances in model-based algorithms for offline RL, and extend them to high-dimensional visual observation spaces. Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. Our approach is both tractable in practice and corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In experiments on a range of challenging image-based locomotion and manipulation tasks, we find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods. Moreover, we also find that our approach excels on an image-based drawer closing task on a real robot using a pre-existing dataset. All results including videos can be found online at https://sites.google.com/view/lompo/ .",0
"The concept of offline reinforcement learning involves learning policies from a fixed dataset of interactions with the environment. This approach allows for the repeated use of historical data and reduces safety concerns related to online exploration, making RL more applicable in real-world scenarios. However, previous research in this area has focused primarily on tasks with limited state representations, whereas learning directly from visual observations is critical for applications like robotics. This study builds on recent advances in offline RL algorithms and extends them to high-dimensional visual observation spaces. The proposed approach involves learning a latent-state dynamics model to represent uncertainty in the model predictions, which is particularly challenging with image observations. Experimental results demonstrate that the algorithm significantly outperforms previous offline model-free RL methods and state-of-the-art online visual model-based RL methods in a range of challenging image-based locomotion and manipulation tasks. Additionally, the proposed approach performs well on a real robot using a pre-existing dataset. More information, including videos, can be found at https://sites.google.com/view/lompo/.",1
"We propose a generic reward shaping approach for improving the rate of convergence in reinforcement learning (RL), called Self Improvement Based REwards, or SIBRE. The approach is designed for use in conjunction with any existing RL algorithm, and consists of rewarding improvement over the agent's own past performance. We prove that SIBRE converges in expectation under the same conditions as the original RL algorithm. The reshaped rewards help discriminate between policies when the original rewards are weakly discriminated or sparse. Experiments on several well-known benchmark environments with different RL algorithms show that SIBRE converges to the optimal policy faster and more stably. We also perform sensitivity analysis with respect to hyper-parameters, in comparison with baseline RL algorithms.",0
"Our proposed method for enhancing the speed of convergence in reinforcement learning (RL) is a general reward shaping technique named Self Improvement Based Rewards (SIBRE). SIBRE can be combined with any existing RL algorithm and involves rewarding progress based on the agent's past performance. We demonstrate that SIBRE is expected to converge under the same conditions as the original RL algorithm. By reshaping the rewards, SIBRE is useful in distinguishing between policies when the initial rewards are thinly distributed or weakly differentiated. We conducted experiments on various well-established benchmark environments using different RL algorithms, and the results showed that SIBRE reaches the optimal policy faster and more consistently. We also conducted a sensitivity analysis, comparing the hyper-parameters to the baseline RL algorithms.",1
"Frequency control is an important problem in modern recommender systems. It dictates the delivery frequency of recommendations to maintain product quality and efficiency. For example, the frequency of delivering promotional notifications impacts daily metrics as well as the infrastructure resource consumption (e.g. CPU and memory usage). There remain open questions on what objective we should optimize to represent business values in the long term best, and how we should balance between daily metrics and resource consumption in a dynamically fluctuating environment. We propose a personalized methodology for the frequency control problem, which combines long-term value optimization using reinforcement learning (RL) with a robust volume control technique we termed ""Effective Factor"". We demonstrate statistically significant improvement in daily metrics and resource efficiency by our method in several notification applications at a scale of billions of users. To our best knowledge, our study represents the first deep RL application on the frequency control problem at such an industrial scale.",0
"In present-day recommender systems, managing frequency is a crucial challenge. This involves deciding how often to provide recommendations to ensure product quality and operational efficiency. For instance, the rate at which promotional notifications are distributed influences daily metrics as well as the utilization of infrastructure resources such as CPU and memory. However, there are still unresolved issues regarding the optimization objective that best reflects long-term business values and the balancing of daily metrics and resource consumption in a constantly changing environment. To address this, we introduce a customized approach for frequency control that merges reinforcement learning (RL) for long-term value optimization and a robust volume control technique called ""Effective Factor."" Our method has produced significant enhancements in daily metrics and resource efficiency across various notification applications with billions of users. To our knowledge, this study is the first to employ deep RL in the frequency control problem on such a large industrial scale.",1
"In order for reinforcement learning techniques to be useful in real-world decision making processes, they must be able to produce robust performance from limited data. Deep policy optimization methods have achieved impressive results on complex tasks, but their real-world adoption remains limited because they often require significant amounts of data to succeed. When combined with small sample sizes, these methods can result in unstable learning due to their reliance on high-dimensional sample-based estimates. In this work, we develop techniques to control the uncertainty introduced by these estimates. We leverage these techniques to propose a deep policy optimization approach designed to produce stable performance even when data is scarce. The resulting algorithm, Uncertainty-Aware Trust Region Policy Optimization, generates robust policy updates that adapt to the level of uncertainty present throughout the learning process.",0
"For reinforcement learning techniques to be applicable in practical decision making scenarios, they must be capable of producing consistent results with minimal data. Although deep policy optimization methods have demonstrated exceptional performance on intricate tasks, their adoption in real-world contexts is limited due to their high data requirements. Moreover, when coupled with small sample sizes, these methods can lead to unstable learning as they heavily rely on estimations based on high-dimensional samples. This study aims to address this issue by introducing methods to manage the uncertainty introduced by these estimates. By utilizing these techniques, we propose a deep policy optimization approach, named Uncertainty-Aware Trust Region Policy Optimization, that can generate stable performance even when data is scarce. This algorithm generates dependable policy updates that adapt to the level of uncertainty present throughout the learning process.",1
"We present a novel deep reinforcement learning method to learn construction heuristics for vehicle routing problems. In specific, we propose a Multi-Decoder Attention Model (MDAM) to train multiple diverse policies, which effectively increases the chance of finding good solutions compared with existing methods that train only one policy. A customized beam search strategy is designed to fully exploit the diversity of MDAM. In addition, we propose an Embedding Glimpse layer in MDAM based on the recursive nature of construction, which can improve the quality of each policy by providing more informative embeddings. Extensive experiments on six different routing problems show that our method significantly outperforms the state-of-the-art deep learning based models.",0
"Our research introduces a new approach to deep reinforcement learning for constructing heuristics in vehicle routing problems. Our proposed method, the Multi-Decoder Attention Model (MDAM), trains multiple policies to increase the likelihood of discovering effective solutions in comparison to current methods that only train a single policy. We have also developed a customized beam search strategy that optimizes the diversity of MDAM. Additionally, we have created an Embedding Glimpse layer in MDAM that enhances each policy's quality by providing more informative embeddings, based on the recursive nature of construction. Our extensive experiments on six distinct routing problems demonstrate that our method surpasses current deep learning models.",1
"Rapid urbanization, increasing integration of distributed renewable energy resources, energy storage, and electric vehicles introduce new challenges for the power grid. In the US, buildings represent about 70% of the total electricity demand and demand response has the potential for reducing peaks of electricity by about 20%. Unlocking this potential requires control systems that operate on distributed systems, ideally data-driven and model-free. For this, reinforcement learning (RL) algorithms have gained increased interest in the past years. However, research in RL for demand response has been lacking the level of standardization that propelled the enormous progress in RL research in the computer science community. To remedy this, we created CityLearn, an OpenAI Gym Environment which allows researchers to implement, share, replicate, and compare their implementations of RL for demand response. Here, we discuss this environment and The CityLearn Challenge, a RL competition we organized to propel further progress in this field.",0
"The power grid faces new challenges due to the rapid urbanization, integration of distributed renewable energy resources, energy storage, and electric vehicles. Buildings in the US account for 70% of the total electricity demand, and demand response can potentially reduce peak electricity consumption by 20%. However, unlocking this potential requires control systems that operate on distributed systems and are data-driven and model-free. Reinforcement learning (RL) algorithms have gained interest in recent years, but RL research for demand response lacks standardization. To address this issue, we developed CityLearn, an OpenAI Gym Environment that enables researchers to implement, share, replicate, and compare their RL implementations for demand response. In this article, we present CityLearn and The CityLearn Challenge, an RL competition we organized to encourage further progress in this area.",1
"Since the publication of the original Transformer architecture (Vaswani et al. 2017), Transformers revolutionized the field of Natural Language Processing. This, mainly due to their ability to understand timely dependencies better than competing RNN-based architectures. Surprisingly, this architecture change does not affect the field of Reinforcement Learning (RL), even though RNNs are quite popular in RL, and time dependencies are very common in RL. Recently, Parisotto et al. 2019) conducted the first promising research of Transformers in RL. To support the findings of this work, this paper seeks to provide an additional example of a Transformer-based RL method. Specifically, the goal is a simple Transformer-based Deep Q-Learning method that is stable over several environments. Due to the unstable nature of Transformers and RL, an extensive method search was conducted to arrive at a final method that leverages developments around Transformers as well as Q-learning. The proposed method can match the performance of classic Q-learning on control environments while showing potential on some selected Atari benchmarks. Furthermore, it was critically evaluated to give additional insights into the relation between Transformers and RL.",0
"After the release of the original Transformer architecture (Vaswani et al. 2017), it transformed the field of Natural Language Processing by surpassing RNN-based architectures in their ability to comprehend timely dependencies. However, despite the popularity of RNNs in Reinforcement Learning (RL) and the prevalence of time dependencies, Transformers have not affected RL. Recently, Parisotto et al. (2019) conducted promising research on Transformers in RL. This paper aims to provide an additional example of a Transformer-based RL method, specifically a stable Deep Q-Learning method that works across multiple environments. The proposed method combines developments in Transformers and Q-learning and was critically evaluated to gain insights into the relationship between Transformers and RL. Despite the instability of Transformers and RL, the proposed method can match classic Q-learning's performance on control environments while showing potential on selected Atari benchmarks.",1
"The reinforcement learning (RL) framework formalizes the notion of learning with interactions. Many real-world problems have large state-spaces and/or action-spaces such as in Go, StarCraft, protein folding, and robotics or are non-Markovian, which cause significant challenges to RL algorithms. In this work we address the large action-space problem by sequentializing actions, which can reduce the action-space size significantly, even down to two actions at the expense of an increased planning horizon. We provide explicit and exact constructions and equivalence proofs for all quantities of interest for arbitrary history-based processes. In the case of MDPs, this could help RL algorithms that bootstrap. In this work we show how action-binarization in the non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds. ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a fixed-sized non-Markovian state-space with the help of a surrogate Markovian process. On the upside, ESA enjoys similar optimality guarantees as Markovian models do. But a downside is that the size of the aggregated state-space becomes exponential in the size of the action-space. In this work, we patch this issue by binarizing the action-space. We provide an upper bound on the number of states of this binarized ESA that is logarithmic in the original action-space size, a double-exponential improvement.",0
"The framework of reinforcement learning (RL) formalizes the concept of learning through interactions, but RL algorithms face significant challenges when dealing with real-world problems that have large state or action spaces, are non-Markovian, or involve tasks such as Go, StarCraft, protein folding, and robotics. To address the problem of a large action space, we propose sequentializing actions, which can reduce the size of the action space significantly, even down to two actions, albeit with an increased planning horizon. We provide explicit and exact constructions and equivalence proofs for all quantities of interest in arbitrary history-based processes. For MDPs, this approach could benefit RL algorithms that bootstrap. Furthermore, we demonstrate how action-binarization can improve Extreme State Aggregation (ESA) bounds in non-MDP problems. ESA allows transforming any (non-MDP, non-ergodic, history-based) RL problem into a fixed-sized non-Markovian state space with a surrogate Markovian process. However, the size of the aggregated state space becomes exponential in the size of the action space. By binarizing the action space, we provide an upper bound on the number of states of this binarized ESA that is logarithmic in the original action space size, resulting in a double-exponential improvement.",1
"Learning complicated value functions in high dimensional state space by function approximation is a challenging task, partially due to that the max-operator used in temporal difference updates can theoretically cause instability for most linear or non-linear approximation schemes. Mellowmax is a recently proposed differentiable and non-expansion softmax operator that allows a convergent behavior in learning and planning. Unfortunately, the performance bound for the fixed point it converges to remains unclear, and in practice, its parameter is sensitive to various domains and has to be tuned case by case. Finally, the Mellowmax operator may suffer from oversmoothing as it ignores the probability being taken for each action when aggregating them. In this paper, we address all the above issues with an enhanced Mellowmax operator, named SM2 (Soft Mellowmax). Particularly, the proposed operator is reliable, easy to implement, and has provable performance guarantee, while preserving all the advantages of Mellowmax. Furthermore, we show that our SM2 operator can be applied to the challenging multi-agent reinforcement learning scenarios, leading to stable value function approximation and state of the art performance.",0
"Function approximation for complicated value functions in high dimensional state space poses a challenge due to the instability that the max-operator used in temporal difference updates can theoretically cause for linear or non-linear approximation schemes. A recent differentiable and non-expansion softmax operator called Mellowmax has been proposed to allow for convergent behavior in learning and planning. However, the performance bound for the fixed point it converges to is unclear, and its parameter is sensitive to various domains and requires tuning. Additionally, the Mellowmax operator may suffer from oversmoothing as it ignores the probability of each action when aggregating them. To address these issues, we introduce an enhanced Mellowmax operator called SM2 (Soft Mellowmax) which is reliable, easy to implement, and has a provable performance guarantee while maintaining the advantages of Mellowmax. We demonstrate that SM2 can be applied to challenging multi-agent reinforcement learning scenarios, resulting in stable value function approximation and state of the art performance.",1
"Left atrial appendage (LAA) closure (LAAC) is a minimally invasive implant-based method to prevent cardiovascular stroke in patients with non-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT angiography plays a crucial role in choosing an appropriate LAAC implant size and a proper C-arm angulation. However, accurate orifice localization is hard because of the high anatomic variation of LAA, and unclear position and orientation of the orifice in available CT views. Deep localization models also yield high error in localizing the orifice in CT image because of the tiny structure of orifice compared to the vastness of CT image. In this paper, we propose a centerline depth-based reinforcement learning (RL) world for effective orifice localization in a small search space. In our scheme, an RL agent observes the centerline-to-surface distance and navigates through the LAA centerline to localize the orifice. Thus, the search space is significantly reduced facilitating improved localization. The proposed formulation could result in high localization accuracy comparing to the expert-annotations in 98 CT images. Moreover, the localization process takes about 8 seconds which is 18 times more efficient than the existing method. Therefore, this can be a useful aid to physicians during the preprocedural planning of LAAC.",0
"LAA closure is a minimally invasive technique that uses implants to prevent cardiovascular strokes in patients with non-valvular atrial fibrillation. The success of this procedure depends on the accurate localization of the LAA orifice, which is challenging due to the high anatomical variation of the LAA and unclear positioning in CT scans. Existing deep localization models also have a high error rate due to the small size of the orifice compared to the vastness of the CT image. In this study, we propose a centerline depth-based reinforcement learning (RL) world that reduces the search space for effective orifice localization. Our RL agent navigates through the LAA centerline and observes the centerline-to-surface distance to localize the orifice accurately. Our proposed method yields high localization accuracy and is 18 times more efficient than existing techniques, making it a valuable tool for physicians during preoperative planning of LAAC.",1
"Many real-world sequential decision-making problems involve critical systems with financial risks and human-life risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. We take the first steps towards ensuring safety, with high confidence, for smoothly-varying non-stationary decision problems. Our proposed method extends a type of safe algorithm, called a Seldonian algorithm, through a synthesis of model-free reinforcement learning with time-series analysis. Safety is ensured using sequential hypothesis testing of a policy's forecasted performance, and confidence intervals are obtained using wild bootstrap.",0
"Several sequential decision-making problems in the real world involve critical systems that pose financial and human-life risks. Although some methods have been proposed in the past for safe deployment, they assume that the underlying problem is stationary. However, many real-world problems are non-stationary. Assuming stationarity when stakes are high can be costly. We aim to ensure safety with high confidence levels in smoothly-varying non-stationary decision problems. Our proposed approach extends the Seldonian algorithm, a safe algorithm, through a combination of model-free reinforcement learning and time-series analysis. We ensure safety using sequential hypothesis testing of a policy's predicted performance, while confidence intervals are obtained using wild bootstrap.",1
"Deep reinforcement learning (RL) is computationally demanding and requires processing of many data points. Synchronous methods enjoy training stability while having lower data throughput. In contrast, asynchronous methods achieve high throughput but suffer from stability issues and lower sample efficiency due to `stale policies.' To combine the advantages of both methods we propose High-Throughput Synchronous Deep Reinforcement Learning (HTS-RL). In HTS-RL, we perform learning and rollouts concurrently, devise a system design which avoids `stale policies' and ensure that actors interact with environment replicas in an asynchronous manner while maintaining full determinism. We evaluate our approach on Atari games and the Google Research Football environment. Compared to synchronous baselines, HTS-RL is 2-6$\times$ faster. Compared to state-of-the-art asynchronous methods, HTS-RL has competitive throughput and consistently achieves higher average episode rewards.",0
"Deep reinforcement learning (RL) is a highly demanding task that involves processing a large volume of data points. Although synchronous methods offer stability during training, they have limited data throughput. Conversely, asynchronous methods provide high throughput, but they suffer from instability and lower sample efficiency due to ""stale policies."" To reap the benefits of both techniques, we propose High-Throughput Synchronous Deep Reinforcement Learning (HTS-RL). Our approach involves learning and rollouts concurrently while avoiding ""stale policies"" through a unique system design. Additionally, we ensure that actors interact with environment replicas asynchronously while maintaining full determinism. We evaluated our approach using Atari games and the Google Research Football environment. Compared to synchronous baselines, HTS-RL is 2-6 times faster. Moreover, HTS-RL has competitive throughput and consistently achieves higher average episode rewards than state-of-the-art asynchronous methods.",1
"Computer-aided design of molecules has the potential to disrupt the field of drug and material discovery. Machine learning, and deep learning, in particular, have been topics where the field has been developing at a rapid pace. Reinforcement learning is a particularly promising approach since it allows for molecular design without prior knowledge. However, the search space is vast and efficient exploration is desirable when using reinforcement learning agents. In this study, we propose an algorithm to aid efficient exploration. The algorithm is inspired by a concept known in the literature as curiosity. We show on three benchmarks that a curious agent finds better performing molecules. This indicates an exciting new research direction for reinforcement learning agents that can explore the chemical space out of their own motivation. This has the potential to eventually lead to unexpected new molecules that no human has thought about so far.",0
"The use of computer-assisted design in creating molecules has the ability to revolutionize the fields of drug and material discovery. The advancements in machine learning, particularly in deep learning, have led to rapid progress in the field. Reinforcement learning is a promising method as it allows for the creation of molecules without prior knowledge. However, due to the extensive search space, it is important to have efficient exploration for reinforcement learning agents. This study proposes an algorithm inspired by the concept of curiosity to aid in this exploration. The results of the study show that this curious agent is able to discover better performing molecules, suggesting a new direction for future research. This could ultimately lead to the discovery of new molecules that have not been previously considered by humans.",1
"Over recent years, deep reinforcement learning has shown strong successes in complex single-agent tasks, and more recently this approach has also been applied to multi-agent domains. In this paper, we propose a novel approach, called MAGNet, to multi-agent reinforcement learning that utilizes a relevance graph representation of the environment obtained by a self-attention mechanism, and a message-generation technique. We applied our MAGnet approach to the synthetic predator-prey multi-agent environment and the Pommerman game and the results show that it significantly outperforms state-of-the-art MARL solutions, including Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic Policy Gradient (MADDPG), and QMIX",0
"Recently, deep reinforcement learning has achieved significant success in complex single-agent tasks and has been extended to multi-agent domains. This paper introduces a new approach to multi-agent reinforcement learning, named MAGNet, which employs a relevance graph representation of the environment obtained by a self-attention mechanism and a message-generation technique. We evaluate the MAGNet approach in synthetic predator-prey multi-agent environment and the Pommerman game, and demonstrate its superiority over existing MARL methods, such as Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic Policy Gradient (MADDPG), and QMIX.",1
"Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems. Code is released in https://github.com/MathPhysSim/FERMI_RL_Paper.",0
"The potential of reinforcement learning in accelerator controls is significant. This study aims to demonstrate the practical application of this approach in solving problems related to accelerator physics. Although model-free reinforcement learning has succeeded in various fields, it still lacks sample efficiency, which can be addressed through model-based methods. We compare the effectiveness of model-based and model-free reinforcement learning in optimizing intensity on the FERMI FEL system. Our findings show that the model-based method has higher representational power and sample efficiency, while the model-free method has slightly better asymptotic performance. The model-based algorithm uses an uncertainty-aware model in a DYNA-style implementation, while the model-free algorithm is based on deep Q-learning. Both algorithms are designed to be robust to noise commonly found in accelerator control problems. The code for this study is available on https://github.com/MathPhysSim/FERMI_RL_Paper.",1
"We study the task of embodied visual active learning, where an agent is set to explore a 3d environment with the goal to acquire visual scene understanding by actively selecting views for which to request annotation. While accurate on some benchmarks, today's deep visual recognition pipelines tend to not generalize well in certain real-world scenarios, or for unusual viewpoints. Robotic perception, in turn, requires the capability to refine the recognition capabilities for the conditions where the mobile system operates, including cluttered indoor environments or poor illumination. This motivates the proposed task, where an agent is placed in a novel environment with the objective of improving its visual recognition capability. To study embodied visual active learning, we develop a battery of agents - both learnt and pre-specified - and with different levels of knowledge of the environment. The agents are equipped with a semantic segmentation network and seek to acquire informative views, move and explore in order to propagate annotations in the neighbourhood of those views, then refine the underlying segmentation network by online retraining. The trainable method uses deep reinforcement learning with a reward function that balances two competing objectives: task performance, represented as visual recognition accuracy, which requires exploring the environment, and the necessary amount of annotated data requested during active exploration. We extensively evaluate the proposed models using the photorealistic Matterport3D simulator and show that a fully learnt method outperforms comparable pre-specified counterparts, even when requesting fewer annotations.",0
"The focus of our research is on embodied visual active learning, which involves an agent navigating a 3D environment to gain understanding of visual scenes by selecting views for annotation. Despite their success in some areas, current deep visual recognition pipelines struggle to generalize in certain real-world situations and from unusual angles. Robotic perception requires recognition capabilities that work in the conditions encountered by the mobile system, such as cluttered indoor environments or poor lighting. Our research aims to improve visual recognition by placing an agent in a new environment and tasking it with refining its capabilities. We created a range of agents, both learnt and pre-specified, that vary in their knowledge of the environment. These agents are equipped with a semantic segmentation network and use online retraining to refine it by seeking informative views, exploring the area, and propagating annotations. To balance the competing objectives of task performance and annotated data requested during active exploration, we used deep reinforcement learning with a reward function. We evaluated our models using the Matterport3D simulator and found that the fully learnt method outperformed pre-specified counterparts, even when requesting fewer annotations.",1
"Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that ""recalls"" items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.",0
"Despite achieving impressive outcomes in various domains, neural networks struggle with sequential learning as they often forget previous tasks when attempting to learn new ones. We present a solution to this problem in sequential reinforcement learning through a novel model that combines ideas from both continual learning in image classification and reinforcement learning. Our model implements a dual memory system that isolates continual learning from reinforcement learning, as well as a pseudo-rehearsal system that utilizes a deep generative network to ""recall"" representative items from past tasks. This approach enables our model to learn Atari 2600 games sequentially without experiencing catastrophic forgetting, and it consistently outperforms human-level performance across all three games. Notably, our model achieves this without demanding additional storage, storing raw data, or revisiting past tasks. In contrast, previous state-of-the-art solutions are considerably more susceptible to forgetting when it comes to these complex deep reinforcement learning tasks.",1
"Image captioning transforms complex visual information into abstract natural language for representation, which can help computers understanding the world quickly. However, due to the complexity of the real environment, it needs to identify key objects and realize their connections, and further generate natural language. The whole process involves a visual understanding module and a language generation module, which brings more challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has shown its important role in a variety of image recognition tasks. Besides, RNN plays an essential role in the image captioning task. We introduce a AutoCaption method to better design the decoder module of the image captioning where we use the NAS to design the decoder module called AutoRNN automatically. We use the reinforcement learning method based on shared parameters for automatic design the AutoRNN efficiently. The search space of the AutoCaption includes connections between the layers and the operations in layers both, and it can make AutoRNN express more architectures. In particular, RNN is equivalent to a subset of our search space. Experiments on the MSCOCO datasets show that our AutoCaption model can achieve better performance than traditional hand-design methods. Our AutoCaption obtains the best published CIDEr performance of 135.8% on COCO Karpathy test split. When further using ensemble technology, CIDEr is boosted up to 139.5%.",0
"The process of image captioning involves converting intricate visual information into abstract natural language, which aids computers in swiftly comprehending the world. However, due to the intricacy of actual environments, it is necessary to identify crucial objects and understand their associations to generate natural language. This process requires a visual understanding module and a language generation module, which pose more design challenges to deep neural networks than other tasks. Neural Architecture Search (NAS) has proven essential in numerous image recognition tasks, and RNN is also crucial in image captioning. To enhance the decoder module of image captioning, we propose the AutoCaption method, which employs NAS to automatically design the decoder module known as AutoRNN. We utilize reinforcement learning based on shared parameters to efficiently design AutoRNN. The search space for AutoCaption includes connections between layers and operations in layers, allowing for the expression of more architectures. Notably, RNN is a subset of our search space. Experiments on the MSCOCO datasets demonstrate that our AutoCaption model outperforms traditional hand-designed methods. Our AutoCaption achieves the best published CIDEr performance of 135.8% on COCO Karpathy test split and can boost CIDEr to 139.5% when utilizing ensemble technology.",1
"Reinforcement learning (RL) algorithms usually require a substantial amount of interaction data and perform well only for specific tasks in a fixed environment. In some scenarios such as healthcare, however, usually only few records are available for each patient, and patients may show different responses to the same treatment, impeding the application of current RL algorithms to learn optimal policies. To address the issues of mechanism heterogeneity and related data scarcity, we propose a data-efficient RL algorithm that exploits structural causal models (SCMs) to model the state dynamics, which are estimated by leveraging both commonalities and differences across subjects. The learned SCM enables us to counterfactually reason what would have happened had another treatment been taken. It helps avoid real (possibly risky) exploration and mitigates the issue that limited experiences lead to biased policies. We propose counterfactual RL algorithms to learn both population-level and individual-level policies. We show that counterfactual outcomes are identifiable under mild conditions and that Q- learning on the counterfactual-based augmented data set converges to the optimal value function. Experimental results on synthetic and real-world data demonstrate the efficacy of the proposed approach.",0
"Typically, reinforcement learning (RL) algorithms necessitate a significant amount of interaction data and are suitable only for specific tasks in a fixed environment. However, in certain situations, such as healthcare, there are usually only a few records available for each patient, and patients may have varying responses to the same treatment, making it difficult to apply current RL algorithms to learn optimal policies. To address the challenges of data scarcity and mechanism heterogeneity, we introduce a data-efficient RL algorithm that uses structural causal models (SCMs) to model the state dynamics. We estimate the SCMs by leveraging both the similarities and differences among subjects. The learned SCM enables us to reason counterfactually about what would have occurred if another treatment had been administered. This approach avoids real exploration, which can be risky, and mitigates the issue of biased policies due to limited experiences. We propose counterfactual RL algorithms to learn both population-level and individual-level policies. We demonstrate that counterfactual outcomes are identifiable under mild conditions and that Q-learning on the counterfactual-based augmented dataset converges to the optimal value function. Our experimental results on synthetic and real-world data show the effectiveness of our proposed method.",1
"Most of the existing deep reinforcement learning (RL) approaches for session-based recommendations either rely on costly online interactions with real users, or rely on potentially biased rule-based or data-driven user-behavior models for learning. In this work, we instead focus on learning recommendation policies in the pure batch or offline setting, i.e. learning policies solely from offline historical interaction logs or batch data generated from an unknown and sub-optimal behavior policy, without further access to data from the real-world or user-behavior models. We propose BCD4Rec: Batch-Constrained Distributional RL for Session-based Recommendations. BCD4Rec builds upon the recent advances in batch (offline) RL and distributional RL to learn from offline logs while dealing with the intrinsically stochastic nature of rewards from the users due to varied latent interest preferences (environments). We demonstrate that BCD4Rec significantly improves upon the behavior policy as well as strong RL and non-RL baselines in the batch setting in terms of standard performance metrics like Click Through Rates or Buy Rates. Other useful properties of BCD4Rec include: i. recommending items from the correct latent categories indicating better value estimates despite large action space (of the order of number of items), and ii. overcoming popularity bias in clicked or bought items typically present in the offline logs.",0
"Many current deep reinforcement learning (RL) methods for session-based recommendations require expensive online interactions with real users or rely on potentially biased rule-based or data-driven user-behavior models for learning. However, this study focuses on learning recommendation policies solely from offline historical interaction logs or batch data generated from an unknown and sub-optimal behavior policy. The proposed method, BCD4Rec, uses Batch-Constrained Distributional RL to handle the stochastic nature of rewards from users with varied latent interest preferences. BCD4Rec outperforms both RL and non-RL baselines in the batch setting, as measured by standard performance metrics like Click Through Rates or Buy Rates. Additionally, BCD4Rec recommends items from the correct latent categories, indicating better value estimates despite a large action space, and overcomes popularity bias in clicked or bought items present in offline logs.",1
"Recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis, through imitating motion capture data via deep reinforcement learning. However, results have mostly been demonstrated on imitating a single distinct motion pattern, and do not generalize to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To bridge this gap, we focus on one class of interactive tasks -- sitting onto a chair. We propose a hierarchical reinforcement learning framework which relies on a collection of subtask controllers trained to imitate simple, reusable mocap motions, and a meta controller trained to execute the subtasks properly to complete the main task. We experimentally demonstrate the strength of our approach over different non-hierarchical and hierarchical baselines. We also show that our approach can be applied to motion prediction given an image input. A supplementary video can be found at https://youtu.be/3CeN0OGz2cA.",0
"Impressive advancements have been made in physics-based character animation, particularly in human motion synthesis through deep reinforcement learning. Despite this progress, current methods are limited to imitating single motion patterns and cannot adapt to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To address this limitation, we focus on the interactive task of sitting onto a chair and propose a hierarchical reinforcement learning framework. Our approach includes subtask controllers that are trained to imitate simple, reusable mocap motions and a meta controller that executes the subtasks to complete the main task. We demonstrate the effectiveness of our approach through experiments, comparing it to non-hierarchical and hierarchical baselines. Furthermore, we show that our approach can also be applied to motion prediction from image inputs. A supplementary video is available at https://youtu.be/3CeN0OGz2cA.",1
"The evaluation of hyperparameters, neural architectures, or data augmentation policies becomes a critical model selection problem in advanced deep learning with a large hyperparameter search space. In this paper, we propose an efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the scenario of hyperparameter search evaluation. It evaluates the potential of hyperparameters by the sub-samples of observations and is theoretically proved to be optimal under the criterion of cumulative regret. We further combine SS with Bayesian Optimization and develop a novel hyperparameter optimization algorithm called BOSS. Empirical studies validate our theoretical arguments of SS and demonstrate the superior performance of BOSS on a number of applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).",0
"When dealing with advanced deep learning models that have a large hyperparameter search space, selecting the appropriate hyperparameters, neural architectures, and data augmentation policies is crucial. In this paper, we introduce a bandit-based algorithm called Sub-Sampling (SS) that efficiently and reliably evaluates hyperparameters for the purpose of model selection. SS assesses the potential of hyperparameters by analyzing sub-samples of observations and is proven to be optimal under the cumulative regret criterion. We then combine SS with Bayesian Optimization to create a novel hyperparameter optimization algorithm called BOSS. Our empirical studies confirm the theoretical validity of SS and demonstrate the superior performance of BOSS in several applications, including Neural Architecture Search (NAS), Data Augmentation (DA), Object Detection (OD), and Reinforcement Learning (RL).",1
"Most 3d human pose estimation methods assume that input -- be it images of a scene collected from one or several viewpoints, or from a video -- is given. Consequently, they focus on estimates leveraging prior knowledge and measurement by fusing information spatially and/or temporally, whenever available. In this paper we address the problem of an active observer with freedom to move and explore the scene spatially -- in `time-freeze' mode -- and/or temporally, by selecting informative viewpoints that improve its estimation accuracy. Towards this end, we introduce Pose-DRL, a fully trainable deep reinforcement learning-based active pose estimation architecture which learns to select appropriate views, in space and time, to feed an underlying monocular pose estimator. We evaluate our model using single- and multi-target estimators with strong result in both settings. Our system further learns automatic stopping conditions in time and transition functions to the next temporal processing step in videos. In extensive experiments with the Panoptic multi-view setup, and for complex scenes containing multiple people, we show that our model learns to select viewpoints that yield significantly more accurate pose estimates compared to strong multi-view baselines.",0
"The majority of 3d human pose estimation methods presume that the input, whether it be images from one or more viewpoints or video, is already available. Therefore, they focus on utilizing prior knowledge and measurement by integrating information spatially and/or temporally when feasible. Our paper addresses the issue of an active observer with the freedom to move and explore the scene in both space and time, resulting in improved estimation accuracy by selecting informative viewpoints. We have developed Pose-DRL, a fully trainable deep reinforcement learning-based active pose estimation architecture that learns to choose appropriate views in space and time to enhance the underlying monocular pose estimator. We have evaluated our model using both single- and multi-target estimators, with strong results in both scenarios. Additionally, our system learns automatic stopping conditions in time and transition functions to the next temporal processing step in videos. Through extensive experiments with the Panoptic multi-view setup, we have demonstrated that our model learns to choose viewpoints resulting in significantly more precise pose estimates compared to strong multi-view baselines, especially in complex scenes containing multiple people.",1
"Learning to produce efficient movement behaviour for humanoid robots from scratch is a hard problem, as has been illustrated by the ""Learning to run"" competition at NIPS 2017. The goal of this competition was to train a two-legged model of a humanoid body to run in a simulated race course with maximum speed. All submissions took a tabula rasa approach to reinforcement learning (RL) and were able to produce relatively fast, but not optimal running behaviour. In this paper, we demonstrate how data from videos of human running (e.g. taken from YouTube) can be used to shape the reward of the humanoid learning agent to speed up the learning and produce a better result. Specifically, we are using the positions of key body parts at regular time intervals to define a potential function for potential-based reward shaping (PBRS). Since PBRS does not change the optimal policy, this approach allows the RL agent to overcome sub-optimalities in the human movements that are shown in the videos.   We present experiments in which we combine selected techniques from the top ten approaches from the NIPS competition with further optimizations to create an high-performing agent as a baseline. We then demonstrate how video-based reward shaping improves the performance further, resulting in an RL agent that runs twice as fast as the baseline in 12 hours of training. We furthermore show that our approach can overcome sub-optimal running behaviour in videos, with the learned policy significantly outperforming that of the running agent from the video.",0
"Developing efficient movement behaviour for humanoid robots from scratch is a challenging task, as evidenced by the ""Learning to run"" competition held at NIPS 2017. The objective of this event was to train a two-legged humanoid model to run as fast as possible on a simulated race course. All entries utilized a blank slate approach to reinforcement learning (RL) and generated relatively quick but suboptimal running behaviour. This study proposes the use of human running videos sourced from YouTube to shape the reward of the humanoid learning agent and hasten learning while producing a superior outcome. Specifically, we employ a potential-based reward shaping (PBRS) technique using the periodic positions of significant body parts. This approach allows the RL agent to overcome inefficiencies in the human movements depicted in the videos without altering the optimal policy. We combine the top ten approaches utilized in the NIPS competition with additional optimisations to create a high-performing agent as a baseline. We demonstrate that video-based reward shaping enhances performance further, resulting in an RL agent that doubles the baseline's speed after only twelve hours of training. Our approach also overcomes suboptimal running behaviour in videos, with the learned policy surpassing that of the running agent in the video.",1
"Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.",0
"Communication of task requirements between humans is typically achieved through the use of natural language. It would be ideal if natural language could also be utilized to convey goals to autonomous machines, such as robots, to reduce difficulties in task specification. However, the process of comprehending and mapping natural language goals to sequences of actions and states presents a challenge. Previous research has struggled to generalize learned policies to new natural language goals and environments. In this study, we introduce a unique adversarial inverse reinforcement learning algorithm to acquire a language-dependent policy and reward function. To enhance the generalization of the learned policy and reward function, we employ a variational goal generator to relabel trajectories and extract diverse goals during training. Our algorithm surpasses multiple baselines by a considerable margin on a visual-based natural language instruction-following dataset (Room-2-Room), indicating a promising advancement in facilitating the use of natural language instructions to specify agent goals.",1
"Efficient exploration under sparse rewards remains a key challenge in deep reinforcement learning. To guide exploration, previous work makes extensive use of intrinsic reward (IR). There are many heuristics for IR, including visitation counts, curiosity, and state-difference. In this paper, we analyze the pros and cons of each method and propose the regulated difference of inverse visitation counts as a simple but effective criterion for IR. The criterion helps the agent explore Beyond the Boundary of explored regions and mitigates common issues in count-based methods, such as short-sightedness and detachment. The resulting method, BeBold, solves the 12 most challenging procedurally-generated tasks in MiniGrid with just 120M environment steps, without any curriculum learning. In comparison, the previous SoTA only solves 50% of the tasks. BeBold also achieves SoTA on multiple tasks in NetHack, a popular rogue-like game that contains more challenging procedurally-generated environments.",0
"Deep reinforcement learning faces a significant obstacle in achieving efficient exploration when rewards are sparse. To aid in exploration, previous approaches have utilized intrinsic reward (IR) with various heuristics such as visitation counts, curiosity, and state-difference. In this study, we evaluate the advantages and disadvantages of each method and propose the regulated difference of inverse visitation counts as a straightforward yet effective criterion for IR. This criterion enables the agent to explore uncharted territories, addressing common issues found in count-based techniques such as short-sightedness and detachment. Our proposed method, BeBold, successfully solves the 12 most challenging procedurally-generated tasks in MiniGrid with only 120M environment steps, without the need for curriculum learning. This achievement surpasses the previous state-of-the-art (SoTA), which only solved 50% of the tasks. Additionally, BeBold outperforms the SoTA on multiple tasks in NetHack, a popular rogue-like game with more demanding procedurally-generated environments.",1
"The spread of deep learning on embedded devices has prompted the development of numerous methods to optimise the deployment of deep neural networks (DNN). Works have mainly focused on: i) efficient DNN architectures, ii) network optimisation techniques such as pruning and quantisation, iii) optimised algorithms to speed up the execution of the most computational intensive layers and, iv) dedicated hardware to accelerate the data flow and computation. However, there is a lack of research on cross-level optimisation as the space of approaches becomes too large to test and obtain a globally optimised solution. Thus, leading to suboptimal deployment in terms of latency, accuracy, and memory. In this work, we first detail and analyse the methods to improve the deployment of DNNs across the different levels of software optimisation. Building on this knowledge, we present an automated exploration framework to ease the deployment of DNNs. The framework relies on a Reinforcement Learning search that, combined with a deep learning inference framework, automatically explores the design space and learns an optimised solution that speeds up the performance and reduces the memory on embedded CPU platforms. Thus, we present a set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU platforms achieving up to 4x improvement in performance and over 2x reduction in memory with negligible loss in accuracy with respect to the BLAS floating-point implementation.",0
"Numerous methods have been developed to optimize the deployment of deep neural networks (DNN) on embedded devices, due to the spread of deep learning. These methods have primarily focused on efficient DNN architectures, network optimization techniques, optimized algorithms, and dedicated hardware. However, optimizing across levels has not been thoroughly researched due to the vast space of approaches. Consequently, suboptimal deployment results in terms of accuracy, latency, and memory. This work details and analyzes methods to improve DNN deployment across various levels of software optimization. Additionally, an automated exploration framework based on Reinforcement Learning combined with a deep learning inference framework is presented. This framework automatically explores the design space, learns an optimized solution that boosts performance and memory while reducing memory on embedded CPU platforms. The results obtained on a range of Arm Cortex-A CPU platforms demonstrate up to 4x performance improvement and over 2x reduction in memory without significant accuracy loss compared to the BLAS floating-point implementation.",1
"Context, the embedding of previous collected trajectories, is a powerful construct for Meta-Reinforcement Learning (Meta-RL) algorithms. By conditioning on an effective context, Meta-RL policies can easily generalize to new tasks within a few adaptation steps. We argue that improving the quality of context involves answering two questions: 1. How to train a compact and sufficient encoder that can embed the task-specific information contained in prior trajectories? 2. How to collect informative trajectories of which the corresponding context reflects the specification of tasks? To this end, we propose a novel Meta-RL framework called CCM (Contrastive learning augmented Context-based Meta-RL). We first focus on the contrastive nature behind different tasks and leverage it to train a compact and sufficient context encoder. Further, we train a separate exploration policy and theoretically derive a new information-gain-based objective which aims to collect informative trajectories in a few steps. Empirically, we evaluate our approaches on common benchmarks as well as several complex sparse-reward environments. The experimental results show that CCM outperforms state-of-the-art algorithms by addressing previously mentioned problems respectively.",0
"The use of context, which involves incorporating previous trajectories, is a valuable tool for Meta-Reinforcement Learning (Meta-RL) algorithms. By relying on effective context, Meta-RL policies can easily adapt to new tasks in just a few steps. To enhance the quality of context, we must address two critical questions: 1. How do we train an encoder that can capture the task-specific information found in prior trajectories in a compact yet sufficient way? 2. How do we collect informative trajectories that reflect the task specifications? Our proposed Meta-RL framework, called CCM (Contrastive learning augmented Context-based Meta-RL), addresses these issues. We utilize the contrastive nature of different tasks to train a compact and effective context encoder, and we also train a separate exploration policy. Additionally, we derive a new objective based on information gain to collect informative trajectories in a few steps. Through experiments on various benchmarks, including complex sparse-reward environments, we show that CCM outperforms existing algorithms by addressing the aforementioned challenges.",1
"Success stories of applied machine learning can be traced back to the datasets and environments that were put forward as challenges for the community. The challenge that the community sets as a benchmark is usually the challenge that the community eventually solves. The ultimate challenge of reinforcement learning research is to train real agents to operate in the real environment, but until now there has not been a common real-world RL benchmark. In this work, we present a prototype real-world environment from OffWorld Gym -- a collection of real-world environments for reinforcement learning in robotics with free public remote access. Close integration into existing ecosystem allows the community to start using OffWorld Gym without any prior experience in robotics and takes away the burden of managing a physical robotics system, abstracting it under a familiar API. We introduce a navigation task, where a robot has to reach a visual beacon on an uneven terrain using only the camera input and provide baseline results in both the real environment and the simulated replica. To start training, visit https://gym.offworld.ai",0
"The success of applied machine learning can be attributed to the challenges presented by the community in terms of datasets and environments. These challenges serve as benchmarks, and as such, they are the problems that the community eventually solves. Currently, the main challenge in reinforcement learning research is to train real agents to operate in the real environment, and there is no common real-world RL benchmark available yet. This study introduces a prototype real-world environment from OffWorld Gym, which is a collection of real-world environments for reinforcement learning in robotics with free public remote access. The integration of OffWorld Gym into the existing ecosystem allows the community to use it without prior experience in robotics and without the burden of managing a physical robotics system. A navigation task is presented, where a robot must reach a visual beacon on an uneven terrain using only the camera input, and baseline results are provided in both the real environment and the simulated replica. To begin training, visit https://gym.offworld.ai.",1
"We present a machine learning framework and a new test bed for data mining from the Slurm Workload Manager for high-performance computing (HPC) clusters. The focus was to find a method for selecting features to support decisions: helping users decide whether to resubmit failed jobs with boosted CPU and memory allocations or migrate them to a computing cloud. This task was cast as both supervised classification and regression learning, specifically, sequential problem solving suitable for reinforcement learning. Selecting relevant features can improve training accuracy, reduce training time, and produce a more comprehensible model, with an intelligent system that can explain predictions and inferences. We present a supervised learning model trained on a Simple Linux Utility for Resource Management (Slurm) data set of HPC jobs using three different techniques for selecting features: linear regression, lasso, and ridge regression. Our data set represented both HPC jobs that failed and those that succeeded, so our model was reliable, less likely to overfit, and generalizable. Our model achieved an R^2 of 95\% with 99\% accuracy. We identified five predictors for both CPU and memory properties.",0
"A new test bed and machine learning framework for data mining from the Slurm Workload Manager for high-performance computing (HPC) clusters is presented. The aim is to select features to support decision-making, specifically, whether to resubmit failed jobs with boosted CPU and memory allocations or migrate them to a computing cloud. The task is approached through both supervised classification and regression learning, utilizing sequential problem solving suitable for reinforcement learning. Selecting relevant features can improve training accuracy, reduce training time, and produce a more comprehensible model, with an intelligent system that can explain predictions and inferences. The supervised learning model trained on a data set of HPC jobs from the Simple Linux Utility for Resource Management (Slurm) using three different techniques for selecting features, namely linear regression, lasso, and ridge regression. The data set contains both HPC jobs that failed and those that succeeded, ensuring the model is reliable, less likely to overfit, and generalizable. The model achieves a 95\% R^2 with 99\% accuracy, identifying five predictors for both CPU and memory properties.",1
"A characteristic of reinforcement learning is the ability to develop unforeseen strategies when solving problems. While such strategies sometimes yield superior performance, they may also result in undesired or even dangerous behavior. In industrial scenarios, a system's behavior also needs to be predictable and lie within defined ranges. To enable the agents to learn (how) to align with a given specification, this paper proposes to explicitly transfer functional and non-functional requirements into shaped rewards. Experiments are carried out on the smart factory, a multi-agent environment modeling an industrial lot-size-one production facility, with up to eight agents and different multi-agent reinforcement learning algorithms. Results indicate that compliance with functional and non-functional constraints can be achieved by the proposed approach.",0
"Reinforcement learning has the capability to generate unanticipated strategies for problem-solving. Although these strategies may sometimes produce better outcomes, they could also lead to unwanted or hazardous actions. In industrial settings, a system's actions must be predictable and remain within predetermined boundaries. This paper suggests that to teach the agents to adhere to a specific specification, functional and non-functional requirements should be transformed into shaped rewards. The experiments conducted on the smart factory, a multi-agent environment that simulates a production facility, involve up to eight agents and various multi-agent reinforcement learning algorithms. The findings demonstrate that the proposed methodology can accomplish compliance with functional and non-functional limitations.",1
"In this work, we study vision-based end-to-end reinforcement learning on vehicle control problems, such as lane following and collision avoidance. Our controller policy is able to control a small-scale robot to follow the right-hand lane of a real two-lane road, while its training was solely carried out in a simulation. Our model, realized by a simple, convolutional network, only relies on images of a forward-facing monocular camera and generates continuous actions that directly control the vehicle. To train this policy we used Proximal Policy Optimization, and to achieve the generalization capability required for real performance we used domain randomization. We carried out thorough analysis of the trained policy, by measuring multiple performance metrics and comparing these to baselines that rely on other methods. To assess the quality of the simulation-to-reality transfer learning process and the performance of the controller in the real world, we measured simple metrics on a real track and compared these with results from a matching simulation. Further analysis was carried out by visualizing salient object maps.",0
The focus of our research is on using visual-based end-to-end reinforcement learning to resolve vehicle control issues such as avoiding collisions and staying in the correct lane. We developed a controller policy that could maneuver a small robot to adhere to the right-hand lane of a genuine two-lane road. The policy was trained exclusively in a simulation and relied only on images from a forward-facing monocular camera. The policy generated continuous actions that directly controlled the vehicle's movement. We applied Proximal Policy Optimization to train the policy and utilized domain randomization to achieve the generalization capability necessary for successful real-world performance. We compared multiple performance metrics of our trained policy with baselines that used alternative methods. We evaluated the simulation-to-reality transfer learning process and the controller's performance in the real world by measuring basic metrics on a real track and comparing them with matching simulation results. We also analyzed salient object maps to gain further insight into our findings.,1
"Reinforcement Learning offers tools to optimize policies based on the data obtained from the real system subject to the policy. While the potential of Reinforcement Learning is well understood, many critical aspects still need to be tackled. One crucial aspect is the issue of safety and stability. Recent publications suggest the use of Nonlinear Model Predictive Control techniques in combination with Reinforcement Learning as a viable and theoretically justified approach to tackle these problems. In particular, it has been suggested that robust MPC allows for making formal stability and safety claims in the context of Reinforcement Learning. However, a formal theory detailing how safety and stability can be enforced through the parameter updates delivered by the Reinforcement Learning tools is still lacking. This paper addresses this gap. The theory is developed for the generic robust MPC case, and further detailed in the robust tube-based linear MPC case, where the theory is fairly easy to deploy in practice.",0
"Reinforcement Learning utilizes data from real systems to optimize policies, though there are still crucial challenges to overcome. Safety and stability are of utmost importance, and recent research proposes combining Nonlinear Model Predictive Control techniques with Reinforcement Learning to address these issues. Robust MPC has been suggested as a means of formally ensuring stability and safety within Reinforcement Learning. However, a formal theory for enforcing safety and stability through Reinforcement Learning parameter updates is currently lacking. This paper aims to fill this gap by developing a theory for the generic robust MPC case, with a focus on the more practical application of the robust tube-based linear MPC case.",1
"Deep Q Network (DQN) is a very successful algorithm, yet the inherent problem of reinforcement learning, i.e. the exploit-explore balance, remains. In this work, we introduce entropy regularization into DQN and propose SQN. We find that the backup equation of soft Q learning can enjoy the corrective feedback if we view the soft backup as policy improvement in the form of Q, instead of policy evaluation. We show that Soft Q Learning with Corrective Feedback (SQL-CF) underlies the on-plicy nature of SQL and the equivalence of SQL and Soft Policy Gradient (SPG). With these insights, we propose an on-policy version of deep Q learning algorithm, i.e. Q On-Policy (QOP). We experiment with QOP on a self-play environment called Google Research Football (GRF). The QOP algorithm exhibits great stability and efficiency in training GRF agents.",0
"Although the Deep Q Network (DQN) algorithm is widely successful, the problem of balancing exploitation and exploration in reinforcement learning persists. To address this, we add entropy regularization to DQN and introduce the Soft Q Network (SQN). By treating soft backup as policy improvement instead of policy evaluation, the backup equation of soft Q learning can receive corrective feedback. We demonstrate that Soft Q Learning with Corrective Feedback (SQL-CF) is on-policy and equivalent to Soft Policy Gradient (SPG). Building on these findings, we propose an on-policy version of DQN called Q On-Policy (QOP) and test it on the Google Research Football (GRF) self-play environment. Our experiments show that QOP is highly stable and efficient in training GRF agents.",1
"Despite the increasing interest in multi-agent reinforcement learning (MARL) in multiple communities, understanding its theoretical foundation has long been recognized as a challenging problem. In this work, we address this problem by providing a finite-sample analysis for decentralized batch MARL with networked agents. Specifically, we consider two decentralized MARL settings, where teams of agents are connected by time-varying communication networks, and either collaborate or compete in a zero-sum game setting, without any central controller. These settings cover many conventional MARL settings in the literature. For both settings, we develop batch MARL algorithms that can be implemented in a decentralized fashion, and quantify the finite-sample errors of the estimated action-value functions. Our error analysis captures how the function class, the number of samples within each iteration, and the number of iterations determine the statistical accuracy of the proposed algorithms. Our results, compared to the finite-sample bounds for single-agent RL, involve additional error terms caused by decentralized computation, which is inherent in our decentralized MARL setting. This work appears to be the first finite-sample analysis for batch MARL, a step towards rigorous theoretical understanding of general MARL algorithms in the finite-sample regime.",0
"Although multi-agent reinforcement learning (MARL) has garnered interest from multiple communities, its theoretical foundation has proven to be a difficult problem. This study aims to address this challenge by conducting a finite-sample analysis of decentralized batch MARL with networked agents. The research focuses on two decentralized MARL scenarios where teams of agents collaborate or compete in a zero-sum game setting without a central controller. The team develops batch MARL algorithms that can be implemented in a decentralized manner and measures the finite-sample errors of the estimated action-value functions. The study examines how the function class, number of samples per iteration, and iterations impact the statistical accuracy of the proposed algorithms. The team notes that the decentralized computation inherent in their decentralized MARL setting leads to additional error terms compared to the finite-sample bounds for single-agent RL. This research is the first to conduct a finite-sample analysis for batch MARL and contributes to a more thorough theoretical understanding of general MARL algorithms in the finite-sample regime.",1
"In large-scale problems, standard reinforcement learning algorithms suffer from slow learning speed. In this paper, we follow the framework of using subspaces to tackle this problem. We propose a free-energy minimization framework for selecting the subspaces and integrate the policy of the state-space into the subspaces. Our proposed free-energy minimization framework rests upon Thompson sampling policy and behavioral policy of subspaces and the state-space. It is therefore applicable to a variety of tasks, discrete or continuous state space, model-free and model-based tasks. Through a set of experiments, we show that this general framework highly improves the learning speed. We also provide a convergence proof.",0
"When dealing with large-scale problems, traditional reinforcement learning algorithms may experience sluggish learning. To address this issue, we present an approach that employs subspaces. Our method involves using a free-energy minimization framework that selects the subspaces and integrates the state-space policy. The proposed framework utilizes a combination of Thompson sampling policy and behavioral policy in both the subspaces and state-space. As such, it can be applied to different tasks, including those with discrete or continuous state space, and model-free or model-based tasks. We conducted experiments to demonstrate that this framework enhances learning speed significantly. Additionally, we provide evidence of convergence.",1
"A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original ""feature map"" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.",0
"The kernel mean embedding, also known as the Hilbert space embedding of a distribution, has become a valuable tool for machine learning and inference. This approach involves mapping distributions into a reproducing kernel Hilbert space (RKHS), which allows for the use of kernel methods with probability measures. This method is an extension of the original ""feature map"" concept used in support vector machines (SVMs) and other kernel methods. While initially associated with these methods, it has since been applied to a variety of fields, including probabilistic modeling, statistical inference, causal discovery, and deep learning. This survey aims to provide a comprehensive review of the current state of research and recent advances in this area, as well as identify the most challenging issues and open problems that could lead to new research directions. The survey begins with an overview of the RKHS and positive definite kernels, followed by a detailed discussion of the Hilbert space embedding of marginal and conditional distributions, including theoretical guarantees and various applications. The embedding of distributions allows for the use of RKHS methods with probability measures, enabling applications such as kernel two-sample testing, independent testing, and learning on distributional data. The conditional mean embedding allows for non-parametric sum, product, and Bayes' rules, which are commonly used in graphical models, probabilistic inference, and reinforcement learning. Additionally, the survey explores the relationships between this framework and other related areas before concluding with suggestions for future research directions.",1
"In mobile health (mHealth) smart devices deliver behavioral treatments repeatedly over time to a user with the goal of helping the user adopt and maintain healthy behaviors. Reinforcement learning appears ideal for learning how to optimally make these sequential treatment decisions. However, significant challenges must be overcome before reinforcement learning can be effectively deployed in a mobile healthcare setting. In this work we are concerned with the following challenges: 1) individuals who are in the same context can exhibit differential response to treatments 2) only a limited amount of data is available for learning on any one individual, and 3) non-stationary responses to treatment. To address these challenges we generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling. IntelligentPooling learns personalized treatment policies thus addressing challenge one. To address the second challenge, IntelligentPooling updates each user's degree of personalization while making use of available data on other users to speed up learning. Lastly, IntelligentPooling allows responsivity to vary as a function of a user's time since beginning treatment, thus addressing challenge three. We show that IntelligentPooling achieves an average of 26% lower regret than state-of-the-art. We demonstrate the promise of this approach and its ability to learn from even a small group of users in a live clinical trial.",0
"The aim of mobile health (mHealth) is to assist users in adopting and maintaining healthy behaviors through repeated delivery of behavioral treatments via smart devices. Reinforcement learning is an ideal approach for making sequential treatment decisions, but there are significant challenges that must be overcome for its effective deployment in a mobile healthcare setting. These challenges include differential response to treatments among individuals in the same context, limited data availability for learning on a single individual, and non-stationary responses to treatment. This paper presents IntelligentPooling, a Thompson-Sampling bandit algorithm that addresses these challenges. IntelligentPooling learns personalized treatment policies to address challenge one, updates each user's degree of personalization while utilizing available data on other users to speed up learning to address challenge two, and allows responsivity to vary as a function of a user's time since beginning treatment to address challenge three. IntelligentPooling achieves an average of 26% lower regret than state-of-the-art and demonstrates its ability to learn from even a small group of users in a live clinical trial.",1
"In offline reinforcement learning (RL) agents are trained using a logged dataset. It appears to be the most natural route to attack real-life applications because in domains such as healthcare and robotics interactions with the environment are either expensive or unethical. Training agents usually requires reward functions, but unfortunately, rewards are seldom available in practice and their engineering is challenging and laborious. To overcome this, we investigate reward learning under the constraint of minimizing human reward annotations. We consider two types of supervision: timestep annotations and demonstrations. We propose semi-supervised learning algorithms that learn from limited annotations and incorporate unlabelled data. In our experiments with a simulated robotic arm, we greatly improve upon behavioural cloning and closely approach the performance achieved with ground truth rewards. We further investigate the relationship between the quality of the reward model and the final policies. We notice, for example, that the reward models do not need to be perfect to result in useful policies.",0
"The use of a logged dataset is common in offline reinforcement learning (RL) to train agents, especially in fields like healthcare and robotics where interactions with the environment can be costly or unethical. However, obtaining reward functions for training agents can be challenging and time-consuming. To address this, we explore reward learning approaches that minimize the need for human reward annotations. We examine two types of supervision - timestep annotations and demonstrations - and propose semi-supervised learning algorithms that utilize both labeled and unlabeled data. Our experiments with a simulated robotic arm demonstrate significant improvements in behavioral cloning and near-equivalent performance to using ground truth rewards. Interestingly, we also discover that reward models do not need to be perfect to produce effective policies.",1
"While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.",0
"Despite the significant advancements made in machine learning systems, they are still limited in their specialization and inability to apply acquired knowledge to new and similar tasks. This study explores a basic reinforcement learning problem and emphasizes the importance of developing policies that incorporate appropriate invariances for generalization to diverse settings. Three different methods for policy generalization are evaluated: data augmentation, meta-learning, and adversarial training. The data augmentation method is found to be successful, while the potential of meta-learning and adversarial learning as alternative task-agnostic approaches is also examined.",1
"Molecule optimization is a fundamental task for accelerating drug discovery, with the goal of generating new valid molecules that maximize multiple drug properties while maintaining similarity to the input molecule. Existing generative models and reinforcement learning approaches made initial success, but still face difficulties in simultaneously optimizing multiple drug properties. To address such challenges, we propose the MultI-constraint MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule as an initial guess and sample molecules from the target distribution. MIMOSA first pretrains two property agnostic graph neural networks (GNNs) for molecule topology and substructure-type prediction, where a substructure can be either atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and employs three basic substructure operations (add, replace, delete) to generate new molecules and associated weights. The weights can encode multiple constraints including similarity and drug property constraints, upon which we select promising molecules for next iteration. MIMOSA enables flexible encoding of multiple property- and similarity-constraints and can efficiently generate new molecules that satisfy various property constraints and achieved up to 49.6% relative improvement over the best baseline in terms of success rate.",0
"The optimization of molecules is crucial in expediting drug discovery. The objective is to produce novel and valid molecules that optimize multiple drug properties while maintaining similarity to the input compound. Although existing generative models and reinforcement learning techniques have shown some promise, they still encounter challenges in simultaneously optimizing multiple drug properties. To overcome these challenges, we present the MultI-constraint MOlecule SAmpling (MIMOSA) approach, which is a sampling framework that utilizes the input molecule as an initial estimate and samples molecules from the target distribution. MIMOSA employs two property-agnostic graph neural networks (GNNs) for molecule topology and substructure-type prediction, where substructures can either be atoms or single rings. For each iteration, MIMOSA uses the GNNs' predictions and employs basic substructure operations (add, replace, delete) to generate new molecules and associated weights. These weights can encode multiple constraints, including similarity and drug property constraints, and we select promising molecules for the next iteration. MIMOSA allows for flexible encoding of multiple property and similarity constraints and can efficiently generate new molecules that satisfy various property constraints. Our results show a relative improvement of up to 49.6% over the best baseline in terms of success rate.",1
"Actor-critic methods, a type of model-free reinforcement learning (RL), have achieved state-of-the-art performances in many real-world domains in continuous control. Despite their success, the wide-scale deployment of these models is still a far cry. The main problems in these actor-critic methods are inefficient exploration and sub-optimal policies. Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3), two cutting edge such algorithms, suffer from these issues. SAC effectively addressed the problems of sample complexity and convergence brittleness to hyper-parameters and thus outperformed all state-of-the-art algorithms including TD3 in harder tasks, whereas TD3 produced moderate results in all environments. SAC suffers from inefficient exploration owing to the Gaussian nature of its policy which causes borderline performance in simpler tasks. In this paper, we introduce Opportunistic Actor-Critic (OPAC), a novel model-free deep RL algorithm that employs better exploration policy and lesser variance. OPAC combines some of the most powerful features of TD3 and SAC and aims to optimize a stochastic policy in an off-policy way. For calculating the target Q-values, instead of two critics, OPAC uses three critics and based on the environment complexity, opportunistically chooses how the target Q-value is computed from the critics' evaluation. We have systematically evaluated the algorithm on MuJoCo environments where it achieves state-of-the-art performance and outperforms or at least equals the performance of TD3 and SAC.",0
"In many continuous control real-world applications, actor-critic methods, a type of model-free reinforcement learning, have attained exceptional results. Nevertheless, their widespread utilization remains elusive due to their inefficient exploration and sub-optimal policies. Both SAC and TD3, cutting edge algorithms, suffer from these issues, with SAC being more effective in addressing sample complexity and convergence brittleness to hyper-parameters, surpassing TD3 in more challenging tasks. On the other hand, TD3 produces moderate results in all environments. However, SAC's Gaussian nature of its policy results in inefficient exploration, leading to borderline performance in simpler tasks. To overcome these challenges, we introduce OPAC, a novel deep RL algorithm that utilizes better exploration policies and less variance by combining powerful features of SAC and TD3. OPAC optimizes a stochastic policy in an off-policy manner and utilizes three critics to calculate the target Q-values. Based on environment complexity, it opportunistically chooses how to compute the target Q-value from the critics' evaluation. Our systematic evaluation of OPAC on MuJoCo environments shows that it achieves state-of-the-art performance, outperforming or at least equaling the performance of TD3 and SAC.",1
"Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling.   To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences.   Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (<2%) with up to 50x faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.",0
"The use of embedded systems has become more widespread in both industrial and consumer applications due to the emergence of Cyber-Physical Systems and the Internet of Things. These systems are required to meet strict constraints, and consequently, embedded software must optimize multiple objectives simultaneously, including energy consumption, execution time, and code size. While compilers provide optimization phases to improve these metrics, selecting and ordering them typically requires expert knowledge and depends on various factors. State-of-the-art optimizers are limited in optimizing one metric at a time and require time-consuming adaptation through dynamic profiling for different targets. To solve these issues, we propose the MLComp methodology, where optimization phases are sequenced by a Reinforcement Learning-based policy. This methodology is supported by Machine Learning-based analytical models that enable quick performance estimation and reduce the time spent on dynamic profiling. Our framework automatically tests different Machine Learning models to select the best-fitting one and utilizes a trained Performance Estimator model to efficiently design Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Our Performance Estimator model achieves a lower relative error (<2%) with up to 50x faster training time than state-of-the-art estimation models across multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. Additionally, the Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.",1
"In recent years, Reinforcement Learning (RL) has seen increasing popularity in research and popular culture. However, skepticism still surrounds the practicality of RL in modern video game development. In this paper, we demonstrate by example that RL can be a great tool for Artificial Intelligence (AI) design in modern, non-trivial video games. We present our RL system for Ubisoft's Roller Champions, a 3v3 Competitive Multiplayer Sports Game played on an oval-shaped skating arena. Our system is designed to keep up with agile, fast-paced development, taking 1--4 days to train a new model following gameplay changes. The AIs are adapted for various game modes, including a 2v2 mode, a Training with Bots mode, in addition to the Classic game mode where they replace players who have disconnected. We observe that the AIs develop sophisticated co-ordinated strategies, and can aid in balancing the game as an added bonus. Please see the accompanying video at https://vimeo.com/466780171 (password: rollerRWRL2020) for examples.",0
"Reinforcement Learning (RL) has gained popularity in both research and popular culture, but doubts remain about its practicality in modern video game development. This paper aims to illustrate the usefulness of RL in designing Artificial Intelligence (AI) for complex video games. We showcase our RL system for Ubisoft's Roller Champions, a multiplayer sports game played on a skating arena. Our system is designed to adapt to fast-paced development, taking only 1-4 days to train a new model after gameplay changes. The AIs are customized for various game modes, including a 2v2 mode, Training with Bots mode, and Classic mode, where they replace disconnected players. Our observations show that the AIs develop advanced coordinated strategies, which can also help to balance the game. For examples, please refer to the accompanying video at https://vimeo.com/466780171 (password: rollerRWRL2020).",1
"This paper discusses an Enhanced Model-Agnostic Meta-Learning (E-MAML) algorithm that generates fast convergence of the policy function from a small number of training examples when applied to new learning tasks. Built on top of Model-Agnostic Meta-Learning (MAML), E-MAML maintains a set of policy parameters learned in the environment for previous tasks. We apply E-MAML to developing reinforcement learning (RL)-based online fault tolerant control schemes for dynamic systems. The enhancement is applied when a new fault occurs, to re-initialize the parameters of a new RL policy that achieves faster adaption with a small number of samples of system behavior with the new fault. This replaces the random task sampling step in MAML. Instead, it exploits the extant previously generated experiences of the controller. The enhancement is sampled to maximally span the parameter space to facilitate adaption to the new fault. We demonstrate the performance of our approach combining E-MAML with proximal policy optimization (PPO) on the well-known cart pole example, and then on the fuel transfer system of an aircraft.",0
"In this paper, the focus is on the Enhanced Model-Agnostic Meta-Learning (E-MAML) algorithm, which enables quick convergence of the policy function with minimal training examples for new learning tasks. E-MAML builds on the Model-Agnostic Meta-Learning (MAML) approach and retains the policy parameters learned in previous tasks. The paper applies E-MAML to create reinforcement learning (RL)-based fault-tolerant control schemes for dynamic systems. The enhancement is applied when a new fault occurs, initializing the parameters of a new RL policy that adapts quickly with a limited number of system behavior samples with the new fault. To achieve this, the paper replaces the random task sampling step in MAML and utilizes previously generated controller experiences. The enhancement covers the parameter space to facilitate adaptation to the new fault. The paper demonstrates the effectiveness of the approach by combining E-MAML with proximal policy optimization (PPO) on the cart pole example and the fuel transfer system of an airplane.",1
"In a wide variety of sequential decision making problems, it can be important to estimate the impact of rare events in order to minimize risk exposure. A popular risk measure is the conditional value-at-risk (CVaR), which is commonly estimated by averaging observations that occur beyond a quantile at a given confidence level. When this confidence level is very high, this estimation method can exhibit high variance due to the limited number of samples above the corresponding quantile. To mitigate this problem, extreme value theory can be used to derive an estimator for the CVaR that uses extrapolation beyond available samples. This estimator requires the selection of a threshold parameter to work well, which is a difficult challenge that has been widely studied in the extreme value theory literature. In this paper, we present an estimation procedure for the CVaR that combines extreme value theory and a recently introduced method of automated threshold selection by \cite{bader2018automated}. Under appropriate conditions, we estimate the tail risk using a generalized Pareto distribution. We compare empirically this estimation procedure with the commonly used method of sample averaging, and show an improvement in performance for some distributions. We finally show how the estimation procedure can be used in reinforcement learning by applying our method to the multi-arm bandit problem where the goal is to avoid catastrophic risk.",0
"In certain situations where decisions are made sequentially, it is important to assess the impact of infrequent events to minimize exposure to risk. The conditional value-at-risk (CVaR) is a well-known measure of risk, often estimated by averaging observations beyond a predetermined quantile at a specific confidence level. However, when the confidence level is very high, this estimation method may have high variance due to the limited number of samples beyond the corresponding quantile. To address this issue, extreme value theory can be applied to create an estimator for the CVaR that extrapolates beyond the available data. This method requires choosing a threshold parameter, which has been a challenging task extensively studied in extreme value theory literature. In this paper, we propose an estimation procedure for the CVaR that combines extreme value theory with an automated threshold selection method introduced by Bader (2018). Under appropriate conditions, we use a generalized Pareto distribution to estimate tail risk. We compare our approach to the commonly used sample averaging method and show improved performance for some distributions. Finally, we demonstrate how our estimation procedure can be applied to reinforcement learning by addressing the multi-arm bandit problem to avoid catastrophic risk.",1
"Language creates a compact representation of the world and allows the description of unlimited situations and objectives through compositionality. While these characterizations may foster instructing, conditioning or structuring interactive agent behavior, it remains an open-problem to correctly relate language understanding and reinforcement learning in even simple instruction following scenarios. This joint learning problem is alleviated through expert demonstrations, auxiliary losses, or neural inductive biases. In this paper, we propose an orthogonal approach called Hindsight Generation for Experience Replay (HIGhER) that extends the Hindsight Experience Replay (HER) approach to the language-conditioned policy setting. Whenever the agent does not fulfill its instruction, HIGhER learns to output a new directive that matches the agent trajectory, and it relabels the episode with a positive reward. To do so, HIGhER learns to map a state into an instruction by using past successful trajectories, which removes the need to have external expert interventions to relabel episodes as in vanilla HER. We show the efficiency of our approach in the BabyAI environment, and demonstrate how it complements other instruction following methods.",0
"The use of language enables a concise representation of the world and enables the description of countless situations and objectives through compositionality. While this can aid in instructing, conditioning, or structuring the behavior of interactive agents, effectively connecting language understanding and reinforcement learning in even simple instruction-following scenarios remains a challenge. To address this, expert demonstrations, auxiliary losses, or neural inductive biases are often employed. This paper introduces an alternative solution called Hindsight Generation for Experience Replay (HIGhER), which builds on the Hindsight Experience Replay (HER) method for language-conditioned policy settings. HIGhER learns to generate new directives that match the agent's path when it fails to follow the given instruction, and rewards the episode accordingly. It achieves this by mapping a state to an instruction using past successful trajectories, eliminating the need for external expert guidance. The efficiency of this approach is demonstrated in the BabyAI environment, and its potential to complement other instruction-following methods is highlighted.",1
"Exploration is essential for reinforcement learning (RL). To face the challenges of exploration, we consider a reward-free RL framework that completely separates exploration from exploitation and brings new challenges for exploration algorithms. In the exploration phase, the agent learns an exploratory policy by interacting with a reward-free environment and collects a dataset of transitions by executing the policy. In the planning phase, the agent computes a good policy for any reward function based on the dataset without further interacting with the environment. This framework is suitable for the meta RL setting where there are many reward functions of interest. In the exploration phase, we propose to maximize the Renyi entropy over the state-action space and justify this objective theoretically. The success of using Renyi entropy as the objective results from its encouragement to explore the hard-to-reach state-actions. We further deduce a policy gradient formulation for this objective and design a practical exploration algorithm that can deal with complex environments. In the planning phase, we solve for good policies given arbitrary reward functions using a batch RL algorithm. Empirically, we show that our exploration algorithm is effective and sample efficient, and results in superior policies for arbitrary reward functions in the planning phase.",0
"Reinforcement learning (RL) requires exploration, which poses challenges that can be addressed through a reward-free RL framework. This framework separates exploration from exploitation and presents new obstacles for exploration algorithms. During the exploration phase, the agent learns an exploratory policy by interacting with a reward-free environment and acquires a dataset of transitions. In the planning phase, the agent computes a good policy for any reward function based on the dataset without further interacting with the environment. This framework is ideal for the meta RL setting, which involves many reward functions of interest. To maximize exploration in the exploration phase, we propose Renyi entropy as the objective and explain its theoretical justification. Renyi entropy encourages exploration of difficult-to-reach state-actions and can be optimized using a policy gradient formulation, resulting in a practical exploration algorithm that works in complex environments. The planning phase involves solving for good policies given arbitrary reward functions using a batch RL algorithm. Empirically, our exploration algorithm is shown to be effective and sample-efficient, leading to superior policies for arbitrary reward functions in the planning phase.",1
"Transfer reinforcement learning (RL) aims at improving the learning efficiency of an agent by exploiting knowledge from other source agents trained on relevant tasks. However, it remains challenging to transfer knowledge between different environmental dynamics without having access to the source environments. In this work, we explore a new challenge in transfer RL, where only a set of source policies collected under diverse unknown dynamics is available for learning a target task efficiently. To address this problem, the proposed approach, MULTI-source POLicy AggRegation (MULTIPOLAR), comprises two key techniques. We learn to aggregate the actions provided by the source policies adaptively to maximize the target task performance. Meanwhile, we learn an auxiliary network that predicts residuals around the aggregated actions, which ensures the target policy's expressiveness even when some of the source policies perform poorly. We demonstrated the effectiveness of MULTIPOLAR through an extensive experimental evaluation across six simulated environments ranging from classic control problems to challenging robotics simulations, under both continuous and discrete action spaces. The demo videos and code are available on the project webpage: https://omron-sinicx.github.io/multipolar/.",0
"The objective of transfer reinforcement learning (RL) is to enhance an agent's learning efficiency by utilizing knowledge from other agents trained on related tasks. However, it continues to be difficult to transfer knowledge between different environmental dynamics when access to the source environments is not available. This study presents a novel challenge in transfer RL, where only a set of source policies gathered under varied unknown dynamics is accessible for effectively learning a target task. To tackle this problem, the proposed method, MULTI-source POLicy AggRegation (MULTIPOLAR), involves two key techniques. One is to learn to adaptively aggregate the actions provided by the source policies to maximize the target task performance. The other is to learn an auxiliary network that predicts residuals around the aggregated actions, which ensures that the target policy remains expressive even when some of the source policies perform poorly. The effectiveness of MULTIPOLAR was demonstrated through a comprehensive experimental evaluation across six simulated environments, spanning from classic control problems to challenging robotics simulations, and encompassing both continuous and discrete action spaces. The project webpage, https://omron-sinicx.github.io/multipolar/, contains demo videos and code.",1
Safety validation is important during the development of safety-critical autonomous systems but can require significant computational effort. Existing algorithms often start from scratch each time the system under test changes. We apply transfer learning to improve the efficiency of reinforcement learning based safety validation algorithms when applied to related systems. Knowledge from previous safety validation tasks is encoded through the action value function and transferred to future tasks with a learned set of attention weights. Including a learned state and action value transformation for each source task can improve performance even when systems have substantially different failure modes. We conduct experiments on safety validation tasks in gridworld and autonomous driving scenarios. We show that transfer learning can improve the initial and final performance of validation algorithms and reduce the number of training steps.,0
"During the development of safety-critical autonomous systems, safety validation is crucial but can be computationally intensive. Currently available algorithms start from scratch each time a system under test changes. To increase the efficiency of reinforcement learning-based safety validation algorithms when applied to related systems, we utilize transfer learning. The action value function is used to encode knowledge from previous safety validation tasks, which is then transferred to future tasks using a learned set of attention weights. Additionally, including a learned state and action value transformation for each source task can enhance performance even when systems have different failure modes. We conducted experiments on safety validation tasks in gridworld and autonomous driving scenarios and demonstrated that transfer learning can enhance the initial and final performance of validation algorithms while reducing the number of training steps required.",1
"Linear fixed point equations in Hilbert spaces arise in a variety of settings, including reinforcement learning, and computational methods for solving differential and integral equations. We study methods that use a collection of random observations to compute approximate solutions by searching over a known low-dimensional subspace of the Hilbert space. First, we prove an instance-dependent upper bound on the mean-squared error for a linear stochastic approximation scheme that exploits Polyak--Ruppert averaging. This bound consists of two terms: an approximation error term with an instance-dependent approximation factor, and a statistical error term that captures the instance-specific complexity of the noise when projected onto the low-dimensional subspace. Using information theoretic methods, we also establish lower bounds showing that both of these terms cannot be improved, again in an instance-dependent sense. A concrete consequence of our characterization is that the optimal approximation factor in this problem can be much larger than a universal constant. We show how our results precisely characterize the error of a class of temporal difference learning methods for the policy evaluation problem with linear function approximation, establishing their optimality.",0
"Linear fixed point equations in Hilbert spaces are present in various fields, such as reinforcement learning and computational techniques for solving differential and integral equations. Our research focuses on approaches that utilize a set of random observations to calculate approximate solutions by exploring a known low-dimensional subspace of the Hilbert space. We first demonstrate an instance-specific upper limit on the mean-squared error for a linear stochastic approximation scheme that uses Polyak-Ruppert averaging. This limit comprises two parts: an approximation error term with an instance-dependent approximation factor, and a statistical error term that reflects the individual complexity of the noise when projected onto the low-dimensional subspace. Furthermore, we employ information theoretic techniques to establish lower bounds that indicate that both of these terms cannot be improved in an instance-dependent manner. As a result of our findings, we conclude that the optimal approximation factor in this problem can be significantly greater than a universal constant. We demonstrate how our results accurately describe the error of a set of temporal difference learning methods for the policy evaluation problem with linear function approximation, demonstrating their optimality.",1
"With the continuous development of machine learning technology, major e-commerce platforms have launched recommendation systems based on it to serve a large number of customers with different needs more efficiently. Compared with traditional supervised learning, reinforcement learning can better capture the user's state transition in the decision-making process, and consider a series of user actions, not just the static characteristics of the user at a certain moment. In theory, it will have a long-term perspective, producing a more effective recommendation. The special requirements of reinforcement learning for data make it need to rely on an offline virtual system for training. Our project mainly establishes a virtual user environment for offline training. At the same time, we tried to improve a reinforcement learning algorithm based on bi-clustering to expand the action space and recommended path space of the recommendation agent.",0
"E-commerce platforms have introduced recommendation systems based on machine learning technology to efficiently cater to a large customer base with diverse needs. Reinforcement learning is a superior alternative to traditional supervised learning as it can better comprehend the user's decision-making process by considering their actions over time, rather than just their static attributes. This results in more effective long-term recommendations. However, reinforcement learning requires an offline virtual system for training due to specific data requirements. Our project focuses on creating such a virtual user environment for offline training and enhancing a reinforcement learning algorithm based on bi-clustering to broaden the recommendation agent's action and recommended path spaces.",1
"We explore the use of deep reinforcement learning to provide strategies for long term scheduling of hydropower production. We consider a use-case where the aim is to optimise the yearly revenue given week-by-week inflows to the reservoir and electricity prices. The challenge is to decide between immediate water release at the spot price of electricity and storing the water for later power production at an unknown price, given constraints on the system. We successfully train a soft actor-critic algorithm on a simplified scenario with historical data from the Nordic power market. The presented model is not ready to substitute traditional optimisation tools but demonstrates the complementary potential of reinforcement learning in the data-rich field of hydropower scheduling.",0
"The objective of our study is to investigate how deep reinforcement learning can be utilized to develop strategies for hydropower production scheduling over an extended period. Our focus is on a specific scenario where the aim is to maximize the annual revenue based on weekly inflows to the reservoir and electricity prices. The dilemma lies in determining when to release water at the current electricity price and when to store it for future production when the price is unknown, while also accounting for system limitations. Utilizing historical data from the Nordic power market, we have successfully trained a soft actor-critic algorithm on a simplified scenario. Although our model cannot replace traditional optimization tools, it highlights the potential of reinforcement learning as a complementary approach in the data-rich field of hydropower scheduling.",1
"Stock portfolio optimization is the process of constant re-distribution of money to a pool of various stocks. In this paper, we will formulate the problem such that we can apply Reinforcement Learning for the task properly. To maintain a realistic assumption about the market, we will incorporate transaction cost and risk factor into the state as well. On top of that, we will apply various state-of-the-art Deep Reinforcement Learning algorithms for comparison. Since the action space is continuous, the realistic formulation were tested under a family of state-of-the-art continuous policy gradients algorithms: Deep Deterministic Policy Gradient (DDPG), Generalized Deterministic Policy Gradient (GDPG) and Proximal Policy Optimization (PPO), where the former two perform much better than the last one. Next, we will present the end-to-end solution for the task with Minimum Variance Portfolio Theory for stock subset selection, and Wavelet Transform for extracting multi-frequency data pattern. Observations and hypothesis were discussed about the results, as well as possible future research directions.1",0
"The process of redistributing funds among various stocks in a portfolio is known as stock portfolio optimization. This paper aims to formulate the problem in a way that allows for the proper application of Reinforcement Learning. To ensure a realistic representation of the market, the state will also consider the transaction cost and risk factor. Additionally, we will compare various state-of-the-art Deep Reinforcement Learning algorithms since the action space is continuous. We tested the realistic formulation using Deep Deterministic Policy Gradient (DDPG), Generalized Deterministic Policy Gradient (GDPG), and Proximal Policy Optimization (PPO), with the former two proving to be more successful than the latter. We will then present an end-to-end solution for the task, incorporating Minimum Variance Portfolio Theory for stock subset selection and Wavelet Transform for multi-frequency data pattern extraction. Finally, we will discuss observations, hypotheses, and potential future research directions.",1
"There have been attempts in reinforcement learning to exploit a priori knowledge about the structure of the system. This paper proposes a hybrid reinforcement learning controller which dynamically interpolates a model-based linear controller and an arbitrary differentiable policy. The linear controller is designed based on local linearised model knowledge, and stabilises the system in a neighbourhood about an operating point. The coefficients of interpolation between the two controllers are determined by a scaled distance function measuring the distance between the current state and the operating point. The overall hybrid controller is proven to maintain the stability guarantee around the neighborhood of the operating point and still possess the universal function approximation property of the arbitrary non-linear policy. Learning has been done on both model-based (PILCO) and model-free (DDPG) frameworks. Simulation experiments performed in OpenAI gym demonstrate stability and robustness of the proposed hybrid controller. This paper thus introduces a principled method allowing for the direct importing of control methodology into reinforcement learning.",0
Reinforcement learning has made efforts to utilize prior knowledge of the system structure. This study presents a hybrid reinforcement learning controller that combines a model-based linear controller and a differentiable policy. The linear controller is based on local linearized model knowledge and stabilizes the system in a neighborhood around an operating point. Interpolation coefficients between the two controllers are determined by a scaled distance function that measures the distance between the current state and the operating point. The hybrid controller maintains stability in the neighborhood of the operating point while possessing the universal function approximation property of the arbitrary non-linear policy. Learning was conducted on both model-based (PILCO) and model-free (DDPG) frameworks. Simulation experiments in OpenAI gym demonstrate the stability and robustness of the proposed hybrid controller. This paper presents a systematic approach for incorporating control methodology into reinforcement learning.,1
"Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",0
"To enhance the movement of vehicles on freeways, ramp metering has become a widely-adopted method that employs traffic signals to regulate the flow of vehicles from on-ramps. Previous studies have relied on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies, to update signal timings in real-time. However, traffic cameras, which cover larger areas and provide more detailed traffic information, have become increasingly prevalent on road networks. This study proposes a deep reinforcement learning (DRL) method to investigate the potential of traffic video data in improving ramp metering efficiency. The proposed approach uses traffic video frames as inputs and learns optimal control strategies directly from high-dimensional visual inputs. A real-world case study reveals that the proposed DRL method results in lower travel times in the mainline, shorter vehicle queues at the on-ramp, and higher traffic flows downstream of the merging area compared to a state-of-the-practice method. These findings suggest that the proposed method can extract valuable information from video data to improve ramp metering controls.",1
"In reinforcement learning, domain randomisation is an increasingly popular technique for learning more general policies that are robust to domain-shifts at deployment. However, naively aggregating information from randomised domains may lead to high variance in gradient estimation and unstable learning process. To address this issue, we present a peer-to-peer online distillation strategy for RL termed P2PDRL, where multiple workers are each assigned to a different environment, and exchange knowledge through mutual regularisation based on Kullback-Leibler divergence. Our experiments on continuous control tasks show that P2PDRL enables robust learning across a wider randomisation distribution than baselines, and more robust generalisation to new environments at testing.",0
"The technique of domain randomisation is gaining popularity in reinforcement learning for developing policies that can handle shifts in domains during deployment. However, simply pooling information from randomised domains can result in unstable learning and high variance in gradient estimation. To tackle this issue, we introduce P2PDRL, a peer-to-peer online distillation approach for reinforcement learning. Under this strategy, multiple workers are assigned to different environments and exchange knowledge through mutual regularisation based on Kullback-Leibler divergence. Our experiments on continuous control tasks indicate that P2PDRL enables more robust learning across a wider randomisation distribution than baselines, leading to more resilient generalisation to new environments during testing.",1
"We propose a method for incorporating object interaction and human body dynamics into the task of 3D ego-pose estimation using a head-mounted camera. We use a kinematics model of the human body to represent the entire range of human motion, and a dynamics model of the body to interact with objects inside a physics simulator. By bringing together object modeling, kinematics modeling, and dynamics modeling in a reinforcement learning (RL) framework, we enable object-aware 3D ego-pose estimation. We devise several representational innovations through the design of the state and action space to incorporate 3D scene context and improve pose estimation quality. We also construct a fine-tuning step to correct the drift and refine the estimated human-object interaction. This is the first work to estimate a physically valid 3D full-body interaction sequence with objects (e.g., chairs, boxes, obstacles) from egocentric videos. Experiments with both controlled and in-the-wild settings show that our method can successfully extract an object-conditioned 3D ego-pose sequence that is consistent with the laws of physics.",0
"Our proposal introduces a technique to enhance 3D ego-pose estimation using a head-mounted camera by incorporating human body dynamics and object interaction. We utilize a human body kinematics model to represent the full range of human motion and a body dynamics model to interact with objects in a physics simulator. Our approach combines object modeling, kinematics modeling, and dynamics modeling within a reinforcement learning (RL) framework to enable object-aware 3D ego-pose estimation. To improve pose estimation accuracy, we introduce several innovative representational methods and a fine-tuning step that corrects drift and refines human-object interaction estimations. Our work is the first to estimate a physically valid 3D full-body interaction sequence with objects from egocentric videos. Our experiments, conducted in both controlled and in-the-wild settings, demonstrate that our method can accurately extract an object-conditioned 3D ego-pose sequence that adheres to the laws of physics.",1
"We consider the problem of learning a set of probability distributions from the empirical Bellman dynamics in distributional reinforcement learning (RL), a class of state-of-the-art methods that estimate the distribution, as opposed to only the expectation, of the total return. We formulate a method that learns a finite set of statistics from each return distribution via neural networks, as in (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). Existing distributional RL methods however constrain the learned statistics to \emph{predefined} functional forms of the return distribution which is both restrictive in representation and difficult in maintaining the predefined statistics. Instead, we learn \emph{unrestricted} statistics, i.e., deterministic (pseudo-)samples, of the return distribution by leveraging a technique from hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simpler objective amenable to backpropagation. Our method can be interpreted as implicitly matching all orders of moments between a return distribution and its Bellman target. We establish sufficient conditions for the contraction of the distributional Bellman operator and provide finite-sample analysis for the deterministic samples in distribution approximation. Experiments on the suite of Atari games show that our method outperforms the standard distributional RL baselines and sets a new record in the Atari games for non-distributed agents.",0
"The problem we are addressing pertains to learning a group of probability distributions from empirical Bellman dynamics in distributional reinforcement learning. This particular category of methods estimates the entire return distribution, instead of just its expectation. Our approach involves utilizing neural networks to learn a limited number of statistics from each return distribution, which is similar to the method used in (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). However, existing distributional RL methods have a drawback, as they limit the learned statistics to predefined functional forms of the return distribution, which poses difficulties in representation and maintaining the predefined statistics. To overcome this, we learn unrestricted statistics, or deterministic (pseudo-)samples, of the return distribution by using maximum mean discrepancy (MMD), a technique from hypothesis testing. This leads to a simpler objective that is suitable for backpropagation. Our method implicitly matches all orders of moments between a return distribution and its Bellman target. We also establish sufficient conditions for the contraction of the distributional Bellman operator and provide finite-sample analysis for the deterministic samples in distribution approximation. We conducted experiments on the suite of Atari games, which showed that our method outperformed the standard distributional RL baselines and set a new record in the Atari games for non-distributed agents.",1
"Recently, model-free reinforcement learning has attracted research attention due to its simplicity, memory and computation efficiency, and the flexibility to combine with function approximation. In this paper, we propose Exploration Enhanced Q-learning (EE-QL), a model-free algorithm for infinite-horizon average-reward Markov Decision Processes (MDPs) that achieves regret bound of $O(\sqrt{T})$ for the general class of weakly communicating MDPs, where $T$ is the number of interactions. EE-QL assumes that an online concentrating approximation of the optimal average reward is available. This is the first model-free learning algorithm that achieves $O(\sqrt T)$ regret without the ergodic assumption, and matches the lower bound in terms of $T$ except for logarithmic factors. Experiments show that the proposed algorithm performs as well as the best known model-based algorithms.",0
"Research attention has been focused on model-free reinforcement learning due to its memory and computation efficiency, simplicity, and ability to combine with function approximation. This paper introduces Exploration Enhanced Q-learning (EE-QL), a model-free algorithm for infinite-horizon average-reward Markov Decision Processes (MDPs). EE-QL achieves a regret bound of $O(\sqrt{T})$ for weakly communicating MDPs, where $T$ is the number of interactions. The algorithm assumes the availability of an online concentrating approximation of the optimal average reward. EE-QL is the first model-free learning algorithm that achieves $O(\sqrt T)$ regret without the ergodic assumption and matches the lower bound in terms of $T$ except for logarithmic factors. Experimental results demonstrate that EE-QL performs comparably to the best-known model-based algorithms.",1
"Effective training of advanced ML models requires large amounts of labeled data, which is often scarce in scientific problems given the substantial human labor and material cost to collect labeled data. This poses a challenge on determining when and where we should deploy measuring instruments (e.g., in-situ sensors) to collect labeled data efficiently. This problem differs from traditional pool-based active learning settings in that the labeling decisions have to be made immediately after we observe the input data that come in a time series. In this paper, we develop a real-time active learning method that uses the spatial and temporal contextual information to select representative query samples in a reinforcement learning framework. To reduce the need for large training data, we further propose to transfer the policy learned from simulation data which is generated by existing physics-based models. We demonstrate the effectiveness of the proposed method by predicting streamflow and water temperature in the Delaware River Basin given a limited budget for collecting labeled data. We further study the spatial and temporal distribution of selected samples to verify the ability of this method in selecting informative samples over space and time.",0
"The efficient training of advanced ML models requires a substantial amount of labeled data, which is often scarce in scientific problems due to the high cost of collecting such data. This presents a challenge in determining the optimal deployment of measuring instruments, such as in-situ sensors, to collect labeled data in a timely and efficient manner. Unlike traditional pool-based active learning settings, labeling decisions must be made immediately after input data is observed in a time series. To address this challenge, we propose a real-time active learning method that utilizes spatial and temporal contextual information to select representative query samples in a reinforcement learning framework. To reduce the need for large training data, we suggest transferring the policy learned from simulation data generated by existing physics-based models. To demonstrate the effectiveness of our proposed method, we apply it to predict streamflow and water temperature in the Delaware River Basin, given a limited budget for collecting labeled data. We also examine the spatial and temporal distribution of selected samples to confirm the method's ability to choose informative samples across space and time.",1
"Model-based reinforcement learning (MBRL) methods have shown strong sample efficiency and performance across a variety of tasks, including when faced with high-dimensional visual observations. These methods learn to predict the environment dynamics and expected reward from interaction and use this predictive model to plan and perform the task. However, MBRL methods vary in their fundamental design choices, and there is no strong consensus in the literature on how these design decisions affect performance. In this paper, we study a number of design decisions for the predictive model in visual MBRL algorithms, focusing specifically on methods that use a predictive model for planning. We find that a range of design decisions that are often considered crucial, such as the use of latent spaces, have little effect on task performance. A big exception to this finding is that predicting future observations (i.e., images) leads to significant task performance improvement compared to only predicting rewards. We also empirically find that image prediction accuracy, somewhat surprisingly, correlates more strongly with downstream task performance than reward prediction accuracy. We show how this phenomenon is related to exploration and how some of the lower-scoring models on standard benchmarks (that require exploration) will perform the same as the best-performing models when trained on the same training data. Simultaneously, in the absence of exploration, models that fit the data better usually perform better on the downstream task as well, but surprisingly, these are often not the same models that perform the best when learning and exploring from scratch. These findings suggest that performance and exploration place important and potentially contradictory requirements on the model.",0
"MBRL methods are effective in various tasks, including those with high-dimensional visual observations, due to their sample efficiency and performance. These methods rely on a predictive model to plan and execute tasks by learning the environment dynamics and expected reward through interaction. However, there is no consensus in the literature on the best design choices for MBRL methods. This paper examines design decisions for the predictive model in visual MBRL algorithms that use a predictive model for planning. The study finds that some commonly considered crucial design decisions, such as the use of latent spaces, have little effect on task performance. However, predicting future observations leads to significant task performance improvement compared to only predicting rewards. Furthermore, the accuracy of image prediction correlates more strongly with downstream task performance than reward prediction accuracy. The study also shows that exploration and fitting the data better are important but potentially contradictory requirements for the model's performance.",1
"Humans learn all their life long. They accumulate knowledge from a sequence of learning experiences and remember the essential concepts without forgetting what they have learned previously. Artificial neural networks struggle to learn similarly. They often rely on data rigorously preprocessed to learn solutions to specific problems such as classification or regression. In particular, they forget their past learning experiences if trained on new ones. Therefore, artificial neural networks are often inept to deal with real-life settings such as an autonomous-robot that has to learn on-line to adapt to new situations and overcome new problems without forgetting its past learning-experiences. Continual learning (CL) is a branch of machine learning addressing this type of problem. Continual algorithms are designed to accumulate and improve knowledge in a curriculum of learning-experiences without forgetting. In this thesis, we propose to explore continual algorithms with replay processes. Replay processes gather together rehearsal methods and generative replay methods. Generative Replay consists of regenerating past learning experiences with a generative model to remember them. Rehearsal consists of saving a core-set of samples from past learning experiences to rehearse them later. The replay processes make possible a compromise between optimizing the current learning objective and the past ones enabling learning without forgetting in sequences of tasks settings. We show that they are very promising methods for continual learning. Notably, they enable the re-evaluation of past data with new knowledge and the confrontation of data from different learning-experiences. We demonstrate their ability to learn continually through unsupervised learning, supervised learning and reinforcement learning tasks.",0
"Throughout their lives, humans are constantly learning and retaining knowledge from various experiences. In contrast, artificial neural networks face challenges in learning similarly, as they often rely on strictly preprocessed data to solve specific problems such as classification and regression. Additionally, these networks struggle to retain past learning experiences when presented with new ones, making them ill-suited for real-life applications like autonomous robotics that require continual learning and adaptation. Continual learning (CL) is a subfield of machine learning that addresses this problem by designing algorithms that can accumulate and improve knowledge without forgetting past experiences. In this thesis, we propose exploring continual algorithms that utilize replay processes, which include rehearsal and generative replay methods. These processes enable a compromise between optimizing current and past learning objectives, allowing for continual learning without forgetting in sequences of tasks. We demonstrate the potential of these methods in unsupervised, supervised, and reinforcement learning tasks, highlighting their ability to re-evaluate past data with new knowledge and confront data from different learning experiences.",1
"The generative adversarial imitation learning (GAIL) has provided an adversarial learning framework for imitating expert policy from demonstrations in high-dimensional continuous tasks. However, almost all GAIL and its extensions only design a kind of reward function of logarithmic form in the adversarial training strategy with the Jensen-Shannon (JS) divergence for all complex environments. The fixed logarithmic type of reward function may be difficult to solve all complex tasks, and the vanishing gradients problem caused by the JS divergence will harm the adversarial learning process. In this paper, we propose a new algorithm named Wasserstein Distance guided Adversarial Imitation Learning (WDAIL) for promoting the performance of imitation learning (IL). There are three improvements in our method: (a) introducing the Wasserstein distance to obtain more appropriate measure in the adversarial training process, (b) using proximal policy optimization (PPO) in the reinforcement learning stage which is much simpler to implement and makes the algorithm more efficient, and (c) exploring different reward function shapes to suit different tasks for improving the performance. The experiment results show that the learning procedure remains remarkably stable, and achieves significant performance in the complex continuous control tasks of MuJoCo.",0
"GAIL has established an adversarial learning framework for mimicking expert policy in high-dimensional continuous tasks through demonstrations. However, the current GAIL and its extensions only use a logarithmic reward function in adversarial training with the Jensen-Shannon divergence for all complex environments. This fixed reward function may not be effective for solving all complex tasks, and the JS divergence can cause vanishing gradients that hinder the adversarial learning process. To address these issues, we present a new algorithm called Wasserstein Distance guided Adversarial Imitation Learning (WDAIL) to enhance imitation learning (IL) performance. Our approach includes three key improvements: (a) employing the Wasserstein distance to obtain a more appropriate measure in the adversarial training process, (b) using proximal policy optimization (PPO) in the reinforcement learning stage to simplify implementation and improve efficiency, and (c) exploring various reward function shapes to suit different tasks and enhance performance. Our experimental results demonstrate that the learning process is highly stable and achieves significant performance in complex continuous control tasks of MuJoCo.",1
"Deep Learning has become interestingly popular in computer vision, mostly attaining near or above human-level performance in various vision tasks. But recent work has also demonstrated that these deep neural networks are very vulnerable to adversarial examples (adversarial examples - inputs to a model which are naturally similar to original data but fools the model in classifying it into a wrong class). Humans are very robust against such perturbations; one possible reason could be that humans do not learn to classify based on an error between ""target label"" and ""predicted label"" but possibly due to reinforcements that they receive on their predictions. In this work, we proposed a novel method to train deep learning models on an image classification task. We used a reward-based optimization function, similar to the vanilla policy gradient method used in reinforcement learning, to train our model instead of conventional cross-entropy loss. An empirical evaluation on the cifar10 dataset showed that our method learns a more robust classifier than the same model architecture trained using cross-entropy loss function (on adversarial training). At the same time, our method shows a better generalization with the difference in test accuracy and train accuracy $< 2\%$ for most of the time compared to the cross-entropy one, whose difference most of the time remains $> 2\%$.",0
"Computer vision has seen a rise in popularity of Deep Learning due to its ability to perform as well as or surpass human-level performance in vision tasks. However, recent studies have revealed that these neural networks are susceptible to adversarial examples, which are inputs that appear similar to the original data but mislead the model into making incorrect classifications. Unlike humans, who are resistant to such disturbances, these models rely on discrepancies between predicted and target labels for classification. Our research proposes an innovative approach to training deep learning models in image classification tasks, using a reward-based optimization function akin to the vanilla policy gradient method in reinforcement learning, instead of the conventional cross-entropy loss function. Our method was tested on the cifar10 dataset and demonstrated superior robustness and generalization capability, with test accuracy and train accuracy varying by less than 2% compared to the cross-entropy method.",1
"Humans can robustly localize themselves without a map after they get lost following prominent visual cues or landmarks. In this work, we aim at endowing autonomous agents the same ability. Such ability is important in robotics applications yet very challenging when an agent is exposed to partially calibrated environments, where camera images with accurate 6 Degree-of-Freedom pose labels only cover part of the scene. To address the above challenge, we explore using Reinforcement Learning to search for a policy to generate intelligent motions so as to actively localize the agent given visual information in partially calibrated environments. Our core contribution is to formulate the active visual localization problem as a Partially Observable Markov Decision Process and propose an algorithmic framework based on Deep Reinforcement Learning to solve it. We further propose an indoor scene dataset ACR-6, which consists of both synthetic and real data and simulates challenging scenarios for active visual localization. We benchmark our algorithm against handcrafted baselines for localization and demonstrate that our approach significantly outperforms them on localization success rate.",0
"After becoming lost, humans can rely on prominent visual cues or landmarks to efficiently locate themselves without a map. Our objective is to replicate this ability in autonomous agents, a critical skill for robotics applications. However, this is a complex task, particularly in partially calibrated environments where accurate 6 Degree-of-Freedom pose labels are only available for certain areas in the scene. To overcome this challenge, we utilize Reinforcement Learning to develop a policy for generating intelligent movements, enabling the agent to actively locate itself using visual information. Our main contribution involves formulating the active visual localization problem as a Partially Observable Markov Decision Process and creating an algorithmic framework based on Deep Reinforcement Learning to solve it. We introduce a challenging indoor scene dataset, ACR-6, which includes both real and synthetic data to simulate complex scenarios for active visual localization. We compared our algorithm to handcrafted baselines for localization and found that our approach outperformed them significantly in terms of localization success rate.",1
"With deep reinforcement learning (RL) methods achieving results that exceed human capabilities in games, robotics, and simulated environments, continued scaling of RL training is crucial to its deployment in solving complex real-world problems. However, improving the performance scalability and power efficiency of RL training through understanding the architectural implications of CPU-GPU systems remains an open problem. In this work we investigate and improve the performance and power efficiency of distributed RL training on CPU-GPU systems by approaching the problem not solely from the GPU microarchitecture perspective but following a holistic system-level analysis approach. We quantify the overall hardware utilization on a state-of-the-art distributed RL training framework and empirically identify the bottlenecks caused by GPU microarchitectural, algorithmic, and system-level design choices. We show that the GPU microarchitecture itself is well-balanced for state-of-the-art RL frameworks, but further investigation reveals that the number of actors running the environment interactions and the amount of hardware resources available to them are the primary performance and power efficiency limiters. To this end, we introduce a new system design metric, CPU/GPU ratio, and show how to find the optimal balance between CPU and GPU resources when designing scalable and efficient CPU-GPU systems for RL training.",0
"Continued expansion of deep reinforcement learning (RL) training is necessary for solving complex real-world problems, as RL methods have surpassed human capabilities in games, robotics, and simulated environments. However, enhancing the scalability and power efficiency of RL training by understanding the CPU-GPU system's architectural implications is still an unresolved issue. In this study, we examine and improve the performance and power efficiency of distributed RL training on CPU-GPU systems by taking a holistic system-level analysis approach instead of solely considering the GPU microarchitecture perspective. We assess the hardware utilization of a cutting-edge distributed RL training framework and identify the bottlenecks resulting from GPU microarchitectural, algorithmic, and system-level design choices. We demonstrate that the GPU microarchitecture is well-balanced for state-of-the-art RL frameworks, but the primary performance and power efficiency limiters are the number of actors operating the environment interactions and the hardware resources available to them. As a result, we present a new system design metric, the CPU/GPU ratio, and explain how to determine the optimal balance between CPU and GPU resources when creating scalable and efficient CPU-GPU systems for RL training.",1
"Thompson sampling (TS) has emerged as a robust technique for contextual bandit problems. However, TS requires posterior inference and optimization for action generation, prohibiting its use in many internet applications where latency and ease of deployment are of concern. We propose a novel imitation-learning-based algorithm that distills a TS policy into an explicit policy representation by performing posterior inference and optimization offline. The explicit policy representation enables fast online decision-making and easy deployment in mobile and server-based environments. Our algorithm iteratively performs offline batch updates to the TS policy and learns a new imitation policy. Since we update the TS policy with observations collected under the imitation policy, our algorithm emulates an off-policy version of TS. Our imitation algorithm guarantees Bayes regret comparable to TS, up to the sum of single-step imitation errors. We show these imitation errors can be made arbitrarily small when unlabeled contexts are cheaply available, which is the case for most large-scale internet applications. Empirically, we show that our imitation policy achieves comparable regret to TS, while reducing decision-time latency by over an order of magnitude.",0
"Although Thompson sampling (TS) is a reliable technique for contextual bandit problems, it is not suitable for use in many internet applications due to its requirement for posterior inference and optimization for action generation, which can cause latency and deployment difficulties. To address this issue, we propose a new imitation-learning-based algorithm that transforms a TS policy into an explicit policy representation by performing posterior inference and optimization offline. This allows for quick online decision-making and easy deployment on mobile and server-based platforms. Our algorithm updates the TS policy iteratively using observations collected under the imitation policy, thus emulating an off-policy version of TS. We guarantee that our imitation algorithm has Bayes regret that is comparable to TS, up to the sum of single-step imitation errors. We demonstrate that these imitation errors can be minimized when unlabeled contexts are cost-effective, which is typical in most large-scale internet applications. Our empirical results indicate that our imitation policy has comparable regret to TS while lowering decision-time latency by over an order of magnitude.",1
"While reinforcement learning algorithms can learn effective policies for complex tasks, these policies are often brittle to even minor task variations, especially when variations are not explicitly provided during training. One natural approach to this problem is to train agents with manually specified variation in the training task or environment. However, this may be infeasible in practical situations, either because making perturbations is not possible, or because it is unclear how to choose suitable perturbation strategies without sacrificing performance. The key insight of this work is that learning diverse behaviors for accomplishing a task can directly lead to behavior that generalizes to varying environments, without needing to perform explicit perturbations during training. By identifying multiple solutions for the task in a single environment during training, our approach can generalize to new situations by abandoning solutions that are no longer effective and adopting those that are. We theoretically characterize a robustness set of environments that arises from our algorithm and empirically find that our diversity-driven approach can extrapolate to various changes in the environment and task.",0
"Despite the fact that reinforcement learning algorithms can effectively learn policies for complex tasks, these policies are often inflexible when it comes to minor variations in the task, especially if those variations were not explicitly provided during training. One solution to this issue is to train agents with varied training tasks or environments, but this may not be feasible in practical situations due to the difficulty of creating suitable perturbations and the potential loss of performance. This study proposes a novel approach: by training agents to develop diverse behaviors for achieving a task, they can directly develop behavior that is adaptable to changing environments, without requiring explicit perturbations during training. Our approach identifies multiple solutions for the task in a single environment during training, allowing for adaptation to new situations by discarding ineffective solutions and adopting effective ones. We have theoretically characterized a robustness set of environments that results from our algorithm, and our empirical findings indicate that our diversity-driven approach can successfully extrapolate to various changes in the environment and task.",1
"Causal models bring many benefits to decision-making systems (or agents) by making them interpretable, sample-efficient, and robust to changes in the input distribution. However, spurious correlations can lead to wrong causal models and predictions. We consider the problem of inferring a causal model of a reinforcement learning environment and we propose a method to deal with spurious correlations. Specifically, our method designs a reward function that incentivizes an agent to do an intervention to find errors in the causal model. The data obtained from doing the intervention is used to improve the causal model. We propose several intervention design methods and compare them. The experimental results in a grid-world environment show that our approach leads to better causal models compared to baselines: learning the model on data from a random policy or a policy trained on the environment's reward. The main contribution consists of methods to design interventions to resolve spurious correlations.",0
"The utilization of causal models in decision-making systems or agents provides numerous benefits, such as interpretability, sample efficiency, and robustness to changes in input distribution. However, incorrect causal models and predictions can result from spurious correlations. This study focuses on developing a method to address spurious correlations when inferring a causal model in a reinforcement learning environment. The proposed approach involves creating a reward function that motivates the agent to intervene and identify errors in the causal model. The data collected from these interventions are then utilized to improve the causal model. The study suggests various intervention design methods and compares them. Results from experiments conducted in a grid-world environment indicate that the proposed approach leads to better causal models when compared to baselines, such as learning the model from data obtained from a random policy or a policy trained on the environment's reward. The primary contribution of this study is the development of methods for designing interventions that can resolve spurious correlations.",1
"Lithium-Ion (Li-I) batteries have recently become pervasive and are used in many physical assets. To enable a good prediction of the end of discharge of batteries, detailed electrochemical Li-I battery models have been developed. Their parameters are typically calibrated before they are taken into operation and are typically not re-calibrated during operation. However, since battery performance is affected by aging, the reality gap between the computational battery models and the real physical systems leads to inaccurate predictions. A supervised machine learning algorithm would require an extensive representative training dataset mapping the observation to the ground truth calibration parameters. This may be infeasible for many practical applications. In this paper, we implement a Reinforcement Learning-based framework for reliably and efficiently inferring calibration parameters of battery models. The framework enables real-time inference of the computational model parameters in order to compensate the reality-gap from the observations. Most importantly, the proposed methodology does not need any labeled data samples, (samples of observations and the ground truth calibration parameters). Furthermore, the framework does not require any information on the underlying physical model. The experimental results demonstrate that the proposed methodology is capable of inferring the model parameters with high accuracy and high robustness. While the achieved results are comparable to those obtained with supervised machine learning, they do not rely on the ground truth information during training.",0
"Li-Ion batteries are widely used in physical assets, and accurate prediction of their end of discharge is crucial. To achieve this, detailed electrochemical models are developed, calibrated before operation, and not re-calibrated during use. However, aging affects battery performance, leading to inaccurate predictions due to the reality gap between computational models and real systems. Supervised machine learning requires extensive training data mapping observations to ground truth calibration parameters, which is often impractical. This paper proposes a Reinforcement Learning-based framework that infers calibration parameters in real-time without labeled data or knowledge of the underlying physical model. The methodology achieves high accuracy and robustness, comparable to supervised machine learning, but without relying on ground truth information during training.",1
"Dams impact downstream river dynamics through flow regulation and disruption of upstream-downstream linkages. However, current dam operation is far from satisfactory due to the inability to respond the complicated and uncertain dynamics of the upstream-downstream system and various usages of the reservoir. Even further, the unsatisfactory dam operation can cause floods in downstream areas. Therefore, we leverage reinforcement learning (RL) methods to compute efficient dam operation guidelines in this work. Specifically, we build offline simulators with real data and different mathematical models for the upstream inflow, i.e., generalized least square (GLS) and dynamic linear model (DLM), then use the simulator to train the state-of-the-art RL algorithms, including DDPG, TD3 and SAC. Experiments show that the simulator with DLM can efficiently model the inflow dynamics in the upstream and the dam operation policies trained by RL algorithms significantly outperform the human-generated policy.",0
"The functioning of dams can have an impact on the dynamics of downstream rivers by regulating flow and disrupting connections between upstream and downstream areas. However, the current operation of dams is not satisfactory because it cannot respond adequately to the complex and uncertain dynamics of the system or the various uses of the reservoir. Additionally, suboptimal dam operation can cause flooding in downstream regions. Therefore, this study employs reinforcement learning (RL) techniques to determine effective dam operation guidelines. To achieve this, offline simulators are created using actual data and diverse mathematical models for the upstream inflow, such as generalized least square (GLS) and dynamic linear model (DLM). The simulators are then utilized to train cutting-edge RL algorithms like DDPG, TD3, and SAC. The findings reveal that the DLM simulator can effectively model the upstream inflow dynamics, and the dam operation policies established by RL algorithms outperform those generated by humans.",1
"Recent advances in Deep Reinforcement Learning (DRL) have largely focused on improving the performance of agents with the aim of replacing humans in known and well-defined environments. The use of these techniques as a game design tool for video game production, where the aim is instead to create Non-Player Character (NPC) behaviors, has received relatively little attention until recently. Turn-based strategy games like Roguelikes, for example, present unique challenges to DRL. In particular, the categorical nature of their complex game state, composed of many entities with different attributes, requires agents able to learn how to compare and prioritize these entities. Moreover, this complexity often leads to agents that overfit to states seen during training and that are unable to generalize in the face of design changes made during development. In this paper we propose two network architectures which, when combined with a \emph{procedural loot generation} system, are able to better handle complex categorical state spaces and to mitigate the need for retraining forced by design decisions. The first is based on a dense embedding of the categorical input space that abstracts the discrete observation model and renders trained agents more able to generalize. The second proposed architecture is more general and is based on a Transformer network able to reason relationally about input and input attributes. Our experimental evaluation demonstrates that new agents have better adaptation capacity with respect to a baseline architecture, making this framework more robust to dynamic gameplay changes during development. Based on the results shown in this paper, we believe that these solutions represent a step forward towards making DRL more accessible to the gaming industry.",0
"The recent advancements in Deep Reinforcement Learning (DRL) have mainly concentrated on enhancing the efficiency of agents to substitute humans in recognizable and well-defined surroundings. Nonetheless, the use of these techniques in the game design sector to create Non-Player Character (NPC) behaviors has not been given much consideration until recently. DRL poses unique challenges in turn-based strategy games such as Roguelikes, owing to the complicated game state that consists of multiple entities with distinct characteristics. Consequently, the agents need to learn how to compare and prioritize these entities. The complexity of the game state often results in agents that overfit to states seen during training and are unable to generalize in the face of design modifications made during development. In this paper, we put forward two network architectures that, when combined with a \emph{procedural loot generation} system, are capable of better handling complex categorical state spaces and reduce the need for retraining forced by design decisions. The first architecture is based on a dense embedding of the categorical input space that abstracts the discrete observation model and enables trained agents to generalize better. The second architecture is more comprehensive and is based on a Transformer network that can reason relationally about input and input attributes. Our experimental evaluation shows that the newly developed agents have better adaptation capacity compared to the baseline architecture, making this framework more resilient to dynamic gameplay changes during development. Based on the results presented in this paper, we believe that these solutions represent a significant milestone towards making DRL more attainable to the gaming industry.",1
"Multi-task learning is a very challenging problem in reinforcement learning. While training multiple tasks jointly allow the policies to share parameters across different tasks, the optimization problem becomes non-trivial: It remains unclear what parameters in the network should be reused across tasks, and how the gradients from different tasks may interfere with each other. Thus, instead of naively sharing parameters across tasks, we introduce an explicit modularization technique on policy representation to alleviate this optimization issue. Given a base policy network, we design a routing network which estimates different routing strategies to reconfigure the base network for each task. Instead of directly selecting routes for each task, our task-specific policy uses a method called soft modularization to softly combine all the possible routes, which makes it suitable for sequential tasks. We experiment with various robotics manipulation tasks in simulation and show our method improves both sample efficiency and performance over strong baselines by a large margin.",0
"Reinforcement learning faces a significant challenge when dealing with multi-task learning. Although training multiple tasks concurrently enables policies to share parameters, the optimization problem becomes complex. It is unclear which parameters in the network should be reused across tasks and how gradients from different tasks may interfere with each other. To address this issue, we propose an explicit modularization technique on policy representation instead of naively sharing parameters. Our approach involves designing a routing network that estimates different routing strategies to reconfigure the base network for each task. Instead of directly selecting routes, our task-specific policy utilizes soft modularization to softly combine all possible routes, making it optimal for sequential tasks. We apply our approach to various robotics manipulation tasks in simulation and demonstrate improved sample efficiency and performance compared to strong baselines.",1
"Recent domain adaptation methods have demonstrated impressive improvement on unsupervised domain adaptation problems. However, in the semi-supervised domain adaptation (SSDA) setting where the target domain has a few labeled instances available, these methods can fail to improve performance. Inspired by the effectiveness of pseudo-labels in domain adaptation, we propose a reinforcement learning based selective pseudo-labeling method for semi-supervised domain adaptation. It is difficult for conventional pseudo-labeling methods to balance the correctness and representativeness of pseudo-labeled data. To address this limitation, we develop a deep Q-learning model to select both accurate and representative pseudo-labeled instances. Moreover, motivated by large margin loss's capacity on learning discriminative features with little data, we further propose a novel target margin loss for our base model training to improve its discriminability. Our proposed method is evaluated on several benchmark datasets for SSDA, and demonstrates superior performance to all the comparison methods.",0
"Unsupervised domain adaptation problems have shown remarkable progress with recent domain adaptation methods. However, these methods may not enhance performance in the semi-supervised domain adaptation (SSDA) setting, where there are only a few labeled instances available in the target domain. Hence, we introduce a reinforcement learning based selective pseudo-labeling method for SSDA, inspired by the efficacy of pseudo-labels in domain adaptation. Conventional pseudo-labeling methods struggle to balance the accuracy and representativeness of pseudo-labeled data. To overcome this limitation, we employ a deep Q-learning model that can select both precise and representative pseudo-labeled instances. Additionally, we suggest a new target margin loss for training our base model, which enhances its ability to learn discriminative features with little data. We evaluate our method on various benchmark datasets for SSDA, and it outperforms all the comparison methods.",1
"Cooperative perception plays a vital role in extending a vehicle's sensing range beyond its line-of-sight. However, exchanging raw sensory data under limited communication resources is infeasible. Towards enabling an efficient cooperative perception, vehicles need to address the following fundamental question: What sensory data needs to be shared?, at which resolution?, and with which vehicles? To answer this question, in this paper, a novel framework is proposed to allow reinforcement learning (RL)-based vehicular association, resource block (RB) allocation, and content selection of cooperative perception messages (CPMs) by utilizing a quadtree-based point cloud compression mechanism. Furthermore, a federated RL approach is introduced in order to speed up the training process across vehicles. Simulation results show the ability of the RL agents to efficiently learn the vehicles' association, RB allocation, and message content selection while maximizing vehicles' satisfaction in terms of the received sensory information. The results also show that federated RL improves the training process, where better policies can be achieved within the same amount of time compared to the non-federated approach.",0
"The significance of cooperative perception in expanding a vehicle's sensing range beyond its line-of-sight is unquestionable. However, in situations where communication resources are limited, exchanging raw sensory data becomes impractical. In order to facilitate effective cooperative perception, vehicles must tackle important questions such as: What sensory data should be shared, at what resolution, and with which vehicles? This paper proposes a unique framework that leverages reinforcement learning-based vehicular association, resource block allocation, and content selection of cooperative perception messages. This is achieved through the use of a quadtree-based point cloud compression mechanism. Additionally, a federated RL approach is introduced to accelerate the training process across vehicles. Simulations demonstrate the ability of RL agents to efficiently learn vehicular association, RB allocation, and message content selection while optimizing the satisfaction of vehicles in terms of received sensory information. The results also exhibit that the federated RL approach enhances the training process by enabling better policies to be achieved within the same time frame as the non-federated approach.",1
"We present a mean-variance policy iteration (MVPI) framework for risk-averse control in a discounted infinite horizon MDP optimizing the variance of a per-step reward random variable. MVPI enjoys great flexibility in that any policy evaluation method and risk-neutral control method can be dropped in for risk-averse control off the shelf, in both on- and off-policy settings. This flexibility reduces the gap between risk-neutral control and risk-averse control and is achieved by working on a novel augmented MDP directly. We propose risk-averse TD3 as an example instantiating MVPI, which outperforms vanilla TD3 and many previous risk-averse control methods in challenging Mujoco robot simulation tasks under a risk-aware performance metric. This risk-averse TD3 is the first to introduce deterministic policies and off-policy learning into risk-averse reinforcement learning, both of which are key to the performance boost we show in Mujoco domains.",0
"A framework called mean-variance policy iteration (MVPI) is introduced for risk-averse control in a discounted infinite horizon MDP, wherein the objective is to optimize the variance of a per-step reward random variable. MVPI provides flexibility by allowing any policy evaluation method and risk-neutral control method to be used for risk-averse control in on- and off-policy settings. This flexibility is achieved by working on a novel augmented MDP directly, which reduces the gap between risk-neutral control and risk-averse control. An example of MVPI is risk-averse TD3, which outperforms vanilla TD3 and many previous risk-averse control methods in challenging Mujoco robot simulation tasks under a risk-aware performance metric. Risk-averse TD3 is the first to use deterministic policies and off-policy learning in risk-averse reinforcement learning, resulting in improved performance in Mujoco domains.",1
"Model-free reinforcement learning (RL), in particular Q-learning is widely used to learn optimal policies for a variety of planning and control problems. However, when the underlying state-transition dynamics are stochastic and high-dimensional, Q-learning requires a large amount of data and incurs a prohibitively high computational cost. In this paper, we introduce Hamiltonian Q-Learning, a data efficient modification of the Q-learning approach, which adopts an importance-sampling based technique for computing the Q function. To exploit stochastic structure of the state-transition dynamics, we employ Hamiltonian Monte Carlo to update Q function estimates by approximating the expected future rewards using Q values associated with a subset of next states. Further, to exploit the latent low-rank structure of the dynamic system, Hamiltonian Q-Learning uses a matrix completion algorithm to reconstruct the updated Q function from Q value updates over a much smaller subset of state-action pairs. By providing an efficient way to apply Q-learning in stochastic, high-dimensional problems, the proposed approach broadens the scope of RL algorithms for real-world applications, including classical control tasks and environmental monitoring.",0
"Q-learning, a model-free reinforcement learning approach, has become popular for learning optimal policies in planning and control problems. However, it faces challenges when dealing with stochastic and high-dimensional state-transition dynamics, requiring vast amounts of data and incurring high computational costs. This paper presents Hamiltonian Q-Learning, a data-efficient modification that utilizes an importance-sampling based technique for computing the Q function. The approach leverages Hamiltonian Monte Carlo to update Q function estimates by approximating expected future rewards using Q values associated with a subset of next states. Additionally, it uses a matrix completion algorithm to reconstruct the updated Q function from Q value updates over a much smaller subset of state-action pairs, exploiting the dynamic system's latent low-rank structure. By offering an efficient way to apply Q-learning in stochastic, high-dimensional problems, this approach broadens the scope of RL algorithms for practical applications such as classical control tasks and environmental monitoring.",1
"Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.",0
"Value Iteration Networks (VINs) have become a popular approach to incorporating planning algorithms into deep reinforcement learning, resulting in improved performance on tasks that require long-range reasoning and an understanding of environment dynamics. However, this method has several limitations: the model does not receive any incentive to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed to be fixed and known. To address these limitations, we propose eXecuted Latent Value Iteration Networks (XLVINs), which utilize recent advancements in contrastive self-supervised learning, graph representation learning, and neural algorithmic reasoning. These XLVINs can successfully apply VIN-style models to general environments, overcoming the limitations of the original VINs. XLVINs perform equally well as VIN-like models in situations where the underlying MDP is discrete, fixed, and known, and significantly outperform model-free baselines across three general MDP setups.",1
"We humans can impeccably search for a target object, given its name only, even in an unseen environment. We argue that this ability is largely due to three main reasons: the incorporation of prior knowledge (or experience), the adaptation of it to the new environment using the observed visual cues and most importantly optimistically searching without giving up early. This is currently missing in the state-of-the-art visual navigation methods based on Reinforcement Learning (RL). In this paper, we propose to use externally learned prior knowledge of the relative object locations and integrate it into our model by constructing a neural graph. In order to efficiently incorporate the graph without increasing the state-space complexity, we propose our Graph-based Value Estimation (GVE) module. GVE provides a more accurate baseline for estimating the Advantage function in actor-critic RL algorithm. This results in reduced value estimation error and, consequently, convergence to a more optimal policy. Through empirical studies, we show that our agent, dubbed as the optimistic agent, has a more realistic estimate of the state value during a navigation episode which leads to a higher success rate. Our extensive ablation studies show the efficacy of our simple method which achieves the state-of-the-art results measured by the conventional visual navigation metrics, e.g. Success Rate (SR) and Success weighted by Path Length (SPL), in AI2THOR environment.",0
"The ability of humans to search for a target object simply by knowing its name, even in an unfamiliar environment, is due to three main factors: prior knowledge, adaptation to new surroundings using visual cues, and persistent searching. This skill is lacking in current Reinforcement Learning (RL) visual navigation methods. To address this, we propose using externally learned prior knowledge of relative object locations and integrating it into our model through a neural graph. Our Graph-based Value Estimation (GVE) module efficiently incorporates the graph without increasing state-space complexity. This leads to a more accurate baseline for estimating the Advantage function in actor-critic RL algorithm, resulting in reduced value estimation error and convergence to a more optimal policy. Our optimistic agent has a more realistic estimate of state value during navigation episodes, leading to a higher success rate. Our simple method achieves state-of-the-art results measured by conventional visual navigation metrics such as Success Rate and Success weighted by Path Length in AI2THOR environment, as shown through empirical and ablation studies.",1
"Well-established optimization-based methods can guarantee an optimal trajectory for a short optimization horizon, typically no longer than a few seconds. As a result, choosing the optimal trajectory for this short horizon may still result in a sub-optimal long-term solution. At the same time, the resulting short-term trajectories allow for effective, comfortable and provable safe maneuvers in a dynamic traffic environment. In this work, we address the question of how to ensure an optimal long-term driving strategy, while keeping the benefits of classical trajectory planning. We introduce a Reinforcement Learning based approach that coupled with a trajectory planner, learns an optimal long-term decision-making strategy for driving on highways. By online generating locally optimal maneuvers as actions, we balance between the infinite low-level continuous action space, and the limited flexibility of a fixed number of predefined standard lane-change actions. We evaluated our method on realistic scenarios in the open-source traffic simulator SUMO and were able to achieve better performance than the 4 benchmark approaches we compared against, including a random action selecting agent, greedy agent, high-level, discrete actions agent and an IDM-based SUMO-controlled agent.",0
"Optimization-based techniques have been established as reliable methods to secure the best possible trajectory for a brief optimization horizon, usually lasting just a few seconds. However, opting for an optimal trajectory within this limited horizon could lead to a sub-optimal long-term solution. Nonetheless, it's crucial to note that these short-term trajectories enable smooth, safe, and effective maneuvers in a dynamic traffic environment. To tackle this challenge, we present a Reinforcement Learning-driven strategy, which, when combined with a trajectory planner, learns a long-term decision-making approach for highway driving. By developing locally optimal maneuvers online, we strike a balance between the infinite low-level continuous action space and the restricted flexibility of a predetermined number of standard lane-change actions. In realistic scenarios simulated using the open-source traffic simulator SUMO, our technique outperformed four benchmark approaches, including a random action selector, greedy agent, high-level, discrete actions agent, and IDM-based SUMO-controlled agent.",1
"Despite its promise, reinforcement learning's real-world adoption has been hampered by the need for costly exploration to learn a good policy. Imitation learning (IL) mitigates this shortcoming by using an oracle policy during training as a bootstrap to accelerate the learning process. However, in many practical situations, the learner has access to multiple suboptimal oracles, which may provide conflicting advice in a state. The existing IL literature provides a limited treatment of such scenarios. Whereas in the single-oracle case, the return of the oracle's policy provides an obvious benchmark for the learner to compete against, neither such a benchmark nor principled ways of outperforming it are known for the multi-oracle setting. In this paper, we propose the state-wise maximum of the oracle policies' values as a natural baseline to resolve conflicting advice from multiple oracles. Using a reduction of policy optimization to online learning, we introduce a novel IL algorithm MAMBA, which can provably learn a policy competitive with this benchmark. In particular, MAMBA optimizes policies by using a gradient estimator in the style of generalized advantage estimation (GAE). Our theoretical analysis shows that this design makes MAMBA robust and enables it to outperform the oracle policies by a larger margin than the IL state of the art, even in the single-oracle case. In an evaluation against standard policy gradient with GAE and AggreVaTe(D), we showcase MAMBA's ability to leverage demonstrations both from a single and from multiple weak oracles, and significantly speed up policy optimization.",0
"Reinforcement learning has not been widely adopted in real-world applications due to the expensive exploration required to learn an effective policy. Imitation learning addresses this issue by using an oracle policy to accelerate the learning process, but it may become challenging when multiple suboptimal oracles are available, leading to conflicting advice in some states. The existing literature on imitation learning has limited coverage of such scenarios, as there are no benchmark methods or principled ways to outperform them in the multi-oracle setting. This paper proposes the state-wise maximum of oracle policies' values as a baseline to resolve conflicting advice from multiple oracles. The authors introduce a novel imitation learning algorithm, MAMBA, which uses a gradient estimator similar to generalized advantage estimation (GAE) to optimize policies. The authors' theoretical analysis shows that MAMBA is robust and can outperform oracle policies by a larger margin than the state-of-the-art imitation learning method, even in the single-oracle case. The evaluation of MAMBA against standard policy gradient with GAE and AggreVaTe(D) demonstrates its ability to leverage demonstrations from a single or multiple weak oracles to significantly accelerate policy optimization.",1
"Analog IC design relies on human experts to search for parameters that satisfy circuit specifications with their experience and intuitions, which is highly labor intensive, time consuming and suboptimal. Machine learning is a promising tool to automate this process. However, supervised learning is difficult for this task due to the low availability of training data: 1) Circuit simulation is slow, thus generating large-scale dataset is time-consuming; 2) Most circuit designs are propitiatory IPs within individual IC companies, making it expensive to collect large-scale datasets. We propose Learning to Design Circuits (L2DC) to leverage reinforcement learning that learns to efficiently generate new circuits data and to optimize circuits. We fix the schematic, and optimize the parameters of the transistors automatically by training an RL agent with no prior knowledge about optimizing circuits. After iteratively getting observations, generating a new set of transistor parameters, getting a reward, and adjusting the model, L2DC is able to optimize circuits. We evaluate L2DC on two transimpedance amplifiers. Trained for a day, our RL agent can achieve comparable or better performance than human experts trained for a quarter. It first learns to meet hard-constraints (eg. gain, bandwidth), and then learns to optimize good-to-have targets (eg. area, power). Compared with grid search-aided human design, L2DC can achieve $\mathbf{250}\boldsymbol{\times}$ higher sample efficiency with comparable performance. Under the same runtime constraint, the performance of L2DC is also better than Bayesian Optimization.",0
"The process of designing Analog ICs is currently done manually by human experts, which is a time-consuming and labor-intensive task. This method is suboptimal and relies heavily on the experience and intuition of the experts. To overcome these challenges, machine learning has emerged as a promising tool for automating the process. However, the low availability of training data makes supervised learning difficult. This is due to the time-consuming nature of circuit simulation and the proprietary nature of most circuit designs. To address this issue, we propose Learning to Design Circuits (L2DC) which leverages reinforcement learning to efficiently generate new circuit data and optimize circuits. By fixing the schematic and optimizing transistor parameters automatically, L2DC can optimize circuits without prior knowledge. Our evaluation on two transimpedance amplifiers shows that L2DC achieves comparable or better performance than human experts trained for a quarter of the time. L2DC first learns to meet hard-constraints and then optimizes good-to-have targets. Compared to grid search-aided human design, L2DC achieves 250 times higher sample efficiency with comparable performance. Additionally, L2DC outperforms Bayesian Optimization under the same runtime constraint.",1
"In this paper, the problem of the trajectory design for a group of energy-constrained drones operating in dynamic wireless network environments is studied. In the considered model, a team of drone base stations (DBSs) is dispatched to cooperatively serve clusters of ground users that have dynamic and unpredictable uplink access demands. In this scenario, the DBSs must cooperatively navigate in the considered area to maximize coverage of the dynamic requests of the ground users. This trajectory design problem is posed as an optimization framework whose goal is to find optimal trajectories that maximize the fraction of users served by all DBSs. To find an optimal solution for this non-convex optimization problem under unpredictable environments, a value decomposition based reinforcement learning (VDRL) solution coupled with a meta-training mechanism is proposed. This algorithm allows the DBSs to dynamically learn their trajectories while generalizing their learning to unseen environments. Analytical results show that, the proposed VD-RL algorithm is guaranteed to converge to a local optimal solution of the non-convex optimization problem. Simulation results show that, even without meta-training, the proposed VD-RL algorithm can achieve a 53.2% improvement of the service coverage and a 30.6% improvement in terms of the convergence speed, compared to baseline multi-agent algorithms. Meanwhile, the use of meta-learning improves the convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must deal with a previously unseen task.",0
"This paper investigates the issue of designing trajectories for a group of energy-limited drones operating in dynamic wireless network conditions. The model considers a team of drone base stations (DBSs) serving clusters of ground users with unpredictable uplink access requirements. To maximize user coverage, the DBSs must navigate cooperatively in the area. The paper proposes an optimization framework for this trajectory design problem, aiming to find optimal trajectories that serve the maximum fraction of users. To solve this non-convex optimization problem under unpredictable environments, the paper proposes a value decomposition-based reinforcement learning (VDRL) solution with a meta-training mechanism. This algorithm allows the DBSs to learn their trajectories dynamically and generalize their learning to unseen environments. Analytical results show that the proposed VD-RL algorithm can converge to a local optimal solution of the non-convex optimization problem. Simulation results demonstrate that the proposed VD-RL algorithm can achieve a significant improvement in service coverage and convergence speed compared to baseline multi-agent algorithms, and the use of meta-learning further improves the convergence speed when the DBSs encounter a previously unseen task.",1
"We introduce a new reinforcement learning approach combining a planning quasi-metric (PQM) that estimates the number of steps required to go from any state to another, with task-specific ""aimers"" that compute a target state to reach a given goal. This decomposition allows the sharing across tasks of a task-agnostic model of the quasi-metric that captures the environment's dynamics and can be learned in a dense and unsupervised manner. We achieve multiple-fold training speed-up compared to recently published methods on the standard bit-flip problem and in the MuJoCo robotic arm simulator.",0
"Our novel reinforcement learning method employs a planning quasi-metric (PQM) to assess the number of steps necessary to travel from one state to another, as well as task-specific ""aimers"" that determine a target state for a given objective. By breaking down the approach in this manner, we can use a task-agnostic model of the quasi-metric, which captures the dynamics of the environment and can be learned in an unsupervised and dense manner, for multiple tasks. We achieved significantly faster training times compared to recent methods on both the bit-flip problem and the MuJoCo robotic arm simulator.",1
"The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decisions individually at each timestep in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed the structure of a dynamical system into deep neural network-based policies by reparameterizing action spaces via second-order differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where actions represent the raw control space. The embedded structure allows end-to-end policy learning for both reinforcement and imitation learning setups. We show that NDPs outperform the prior state-of-the-art in terms of either efficiency or performance across several robotic control tasks for both imitation and reinforcement learning setups. Project video and code are available at https://shikharbahl.github.io/neural-dynamic-policies/",0
"The current approach to sensorimotor control, whether through imitation or reinforcement learning, involves training policies directly in raw action spaces like torque, joint angle, or end-effector position. This method forces the agent to make decisions at each timestep during training, which limits scalability for continuous, high-dimensional, and long-horizon tasks. In classical robotics, however, dynamical systems have been used as a policy representation to learn robot behaviors through demonstrations. Although these techniques lack the flexibility and generalizability provided by deep learning or reinforcement learning, they have not been fully explored in these settings. This study aims to bridge this gap by integrating the structure of a dynamical system into deep neural network-based policies through reparameterizing action spaces using second-order differential equations. The resulting Neural Dynamic Policies (NDPs) predict in trajectory distribution space, offering end-to-end policy learning for both reinforcement and imitation learning setups. Results show that NDPs outperform prior state-of-the-art methods in terms of efficiency and performance across several robotic control tasks for both imitation and reinforcement learning setups. The project video and code are available at https://shikharbahl.github.io/neural-dynamic-policies/.",1
"Deep Reinforcement Learning achieves very good results in domains where reward functions can be manually engineered. At the same time, there is growing interest within the community in using games based on Procedurally Content Generation (PCG) as benchmark environments since this type of environment is perfect for studying overfitting and generalization of agents under domain shift. Inverse Reinforcement Learning (IRL) can instead extrapolate reward functions from expert demonstrations, with good results even on high-dimensional problems, however there are no examples of applying these techniques to procedurally-generated environments. This is mostly due to the number of demonstrations needed to find a good reward model. We propose a technique based on Adversarial Inverse Reinforcement Learning which can significantly decrease the need for expert demonstrations in PCG games. Through the use of an environment with a limited set of initial seed levels, plus some modifications to stabilize training, we show that our approach, DE-AIRL, is demonstration-efficient and still able to extrapolate reward functions which generalize to the fully procedural domain. We demonstrate the effectiveness of our technique on two procedural environments, MiniGrid and DeepCrawl, for a variety of tasks.",0
"Deep Reinforcement Learning has proven successful in domains where reward functions have been manually engineered. However, the community is increasingly interested in using Procedurally Content Generated (PCG) games as benchmark environments to study overfitting and generalization of agents under domain shift. Inverse Reinforcement Learning (IRL) can extrapolate reward functions from expert demonstrations, even on high-dimensional problems. However, these techniques have not been applied to PCG environments due to the large number of demonstrations required to find a suitable reward model. To address this issue, we propose a technique based on Adversarial Inverse Reinforcement Learning called DE-AIRL. This approach significantly decreases the need for expert demonstrations in PCG games by utilizing an environment with a limited set of initial seed levels and some modifications to stabilize training. Our results demonstrate that DE-AIRL is demonstration-efficient and can extrapolate reward functions that generalize to the fully procedural domain. We showcase the effectiveness of our technique on two procedural environments, MiniGrid and DeepCrawl, for various tasks.",1
"Reinforcement learning (RL) has shown great promise in optimizing long-term user interest in recommender systems. However, existing RL-based recommendation methods need a large number of interactions for each user to learn a robust recommendation policy. The challenge becomes more critical when recommending to new users who have a limited number of interactions. To that end, in this paper, we address the cold-start challenge in the RL-based recommender systems by proposing a meta-level model-based reinforcement learning approach for fast user adaptation. In our approach, we learn to infer each user's preference with a user context variable that enables recommendation systems to better adapt to new users with few interactions. To improve adaptation efficiency, we learn to recover the user policy and reward from only a few interactions via an inverse reinforcement learning method to assist a meta-level recommendation agent. Moreover, we model the interaction relationship between the user model and recommendation agent from an information-theoretic perspective. Empirical results show the effectiveness of the proposed method when adapting to new users with only a single interaction sequence. We further provide a theoretical analysis of the recommendation performance bound.",0
"Recommender systems have shown promise in optimizing user interest in the long term, with reinforcement learning (RL) being a popular approach. However, existing RL-based methods require many interactions to learn a robust recommendation policy, which proves challenging when recommending to new users with limited interactions. To tackle this issue, we propose a meta-level model-based RL approach that utilizes a user context variable to better adapt to new users with few interactions. Our method recovers user policy and reward from a few interactions using inverse RL and models the interaction relationship between user model and recommendation agent from an information-theoretic perspective. Empirical results show the effectiveness of our method in adapting to new users with only a single interaction sequence, with a theoretical analysis of the recommendation performance bound provided.",1
