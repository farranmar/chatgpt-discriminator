"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"In recent years, graph neural networks have emerged as powerful tools for modeling complex relationships among elements within graphs. These models typically rely on attention mechanisms that allow nodes to selectively focus on relevant information from neighboring nodes in order to generate accurate representations of their local environments. Despite significant advances in these methods, they remain limited by the inherent lack of interaction between different levels of representation learned at each node. This paper proposes Multi-Level Attention Pooling (MLAP), which addresses this challenge by introducing a novel mechanism for integrating multi-scale context into graph neural network processing. MLAP enables the efficient learning and exploitation of nonlinear interactions across multiple levels of abstraction in both global and local graph domains, significantly improving the ability of GNNs to capture rich information encoded in the topology. Our method achieves state-of-the-art performance on diverse benchmark datasets including citation networks, social networks, bioinformatics problems, and semantic reasoning tasks. By unifying graph representations through multi-level attentional pooling, we demonstrate the effectiveness of our approach for capturing relational information present in many real-world applications involving graphs.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"This paper compares decision forests and deep networks as models that are commonly used in machine learning applications, particularly when working with small sample sizes. Both types of models have proven effective in many cases, but there are differences in their structure and approach to processing data. Our analysis shows that while both decision forests and deep networks can yield accurate predictions even at low sample sizes, decision forests may perform better than some types of deep neural network architectures due to their unique strengths such as interpretability and robustness to noisy inputs. We examine how these factors play into performance outcomes using simulations based on real datasets. Ultimately, our findings suggest that practitioners should consider carefully which type of model is most appropriate given their specific needs and available resources rather than relying solely on either one.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"""Power Up: Enhancing Graph Convolutional Networks through Graph Powering""  Graph convolutional networks (GCNs) have emerged as powerful tools for analyzing graph data, such as social networks or molecular structures. However, GCNs can struggle with overfitting or underfitting, particularly on datasets with varying densities or scales. To address these limitations, we propose a novel approach called Graph Powering that transforms the input graphs into regularized power graphs before applying standard GCN models. By adding additional edges based on node degrees and global properties like clustering coefficients, our method helps regulate the flow of information across nodes while preserving local structure. We evaluate our approach using several benchmark datasets and show significantly improved performance compared to state-of-the-art methods, including higher accuracy rates and better robustness against changes in graph density. Our findings highlight the effectiveness of Graph Powering for enhancing GCN capabilities and further advancing the analysis of complex networked data.",1
"With the increasing popularity of Graph Neural Networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only blackbox access to the trained model is granted. To build defenses, differential privacy has emerged as a mechanism to disguise the sensitive data in training datasets. Following the strategy of Private Aggregation of Teacher Ensembles (PATE), recent methods leverage a large ensemble of teacher models. These teachers are trained on disjoint subsets of private data and are employed to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets may destroy the structural information and adversely affect accuracy. We propose a new graph-specific scheme of releasing a student GNN, which avoids splitting private training data altogether. The student GNN is trained using public data, partly labeled privately using the teacher GNN models trained exclusively for each query node. We theoretically analyze our approach in the R\`{e}nyi differential privacy framework and provide privacy guarantees. Besides, we show the solid experimental performance of our method compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available.",0
"This paper presents a method for releasing graph neural networks (GNNs) trained on sensitive data while preserving privacy through differential privacy guarantees. GNNs have shown great promise in many applications due to their ability to capture complex relationships among data points in large graphs. However, these models often require access to sensitive information that must remain private. Our approach leverages recent advances in GNN algorithms combined with novel techniques in differential privacy to achieve both accuracy and privacy. We demonstrate our methods using real-world datasets and show that they can effectively protect users' privacy while maintaining high levels of model performance.",1
"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to the state-of-the-art white and black-box models.",0
"In recent years, artificial intelligence (AI) has seen many advancements in complex pattern classification tasks due to improvements in deep learning algorithms. However, these methods often lack transparency and interpretability, which limits their use in safety-critical applications such as medical diagnosis and autonomous driving. This study proposes a novel framework called ""Recurrence-aware Long-term Cognitive Network"" (R-LCGN), addressing both explainability and high performance in recurrent neural networks. R-LCGN integrates temporal attention into Recurrent Highway Networks by utilizing external memory modules that store key patterns from past time steps. Our experimental results on several benchmark datasets demonstrate significant improvements over state-of-the-art models, particularly in terms of accuracy, interpretability, and stability. Notably, we showcase how our method can identify and focus on informative features during the course of processing sequential data. These properties make R-LCGN highly suitable for real-world applications where explaining model behavior is crucial. The proposed approach enhances human trustworthiness in AI systems and contributes to sustainable development in various domains. We anticipate that this research will pave the way toward more robust and reliable AI solutions and ultimately improve societyâ€™s perception and acceptance of advanced machine learning techniques.",1
"Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training, however, in practice, graph-structured data is usually formed in a streaming fashion, so that learning a graph continuously is often necessary. In this paper, we aim to bridge GNN to lifelong learning by converting a graph problem to a regular learning problem, so that GNN is able to inherit the lifelong learning techniques developed for convolutional neural networks (CNNs). To this end, we propose a new graph topology based on feature cross-correlation, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in human action recognition with distributed streaming signals for wearable devices.",0
"Lifelong graph learning refers to machine learning approaches that can effectively learn from vast amounts of data throughout an individual's lifetime. These methods involve adapting models over time to accommodate new information and refine existing knowledge. In practice, this requires developing algorithms capable of maintaining accuracy while handling changing datasets and evolving requirements. One promising technique involves using graph neural networks (GNNs), which have shown success in tasks such as node classification, link prediction, and semantic segmentation. However, challenges remain in designing GNNs able to learn continuously without degradation. This work proposes novel lifelong graph learning architectures based on dynamic memory replay techniques inspired by human cognition. Experiments demonstrate improved performance compared to state-of-the-art methods across diverse domains, including social network analysis, recommendation systems, and scientific collaboration networks. Overall, these results provide insights into advancing intelligent agents equipped with advanced learning capabilities under real-world conditions.",1
"Deep learning models, such as convolutional neural networks, have long been applied to image and multi-media tasks, particularly those with structured data. More recently, there has been more attention to unstructured data that can be represented via graphs. These types of data are often found in health and medicine, social networks, and research data repositories. Graph convolutional neural networks have recently gained attention in the field of deep learning that takes advantage of graph-based data representation with automatic feature extraction via convolutions. Given the popularity of these methods in a wide range of applications, robust uncertainty quantification is vital. This remains a challenge for large models and unstructured datasets. Bayesian inference provides a principled approach to uncertainty quantification of model parameters for deep learning models. Although Bayesian inference has been used extensively elsewhere, its application to deep learning remains limited due to the computational requirements of the Markov Chain Monte Carlo (MCMC) methods. Recent advances in parallel computing and advanced proposal schemes in MCMC sampling methods has opened the path for Bayesian deep learning. In this paper, we present Bayesian graph convolutional neural networks that employ tempered MCMC sampling with Langevin-gradient proposal distribution implemented via parallel computing. Our results show that the proposed method can provide accuracy similar to advanced optimisers while providing uncertainty quantification for key benchmark problems.",0
"Graphs are ubiquitous data structures that arise naturally in numerous applications such as computer vision, natural language processing, social network analysis, bioinformatics, and many others. In recent years, there has been significant interest in using deep learning methods, particularly convolutional neural networks (CNN), on graphs due to their ability to effectively capture complex spatial dependencies. However, designing efficient CNN architectures for graph-structured data remains challenging due to the high computational cost associated with encoding node features, which makes scalability a major issue. This work proposes a new approach based on Bayesian inference and tempered Markov chain Monte Carlo (MCMC) simulation to learn graph CNN models efficiently. We first describe how to apply graph CNNs to solve regression tasks by defining a set of linear layers operating directly over the graphs without ever reshaping them into flat vector representations. Then, we present our novel temperature scaling algorithm, which can guide MCMC sampling towards areas where the target distribution peaks more strongly. This leads to a faster convergence rate while maintaining low autocorrelation among samples. Our method leverages these advancements to enable fast approximate posterior inference during model training. Experimental results demonstrate that our proposed technique achieves state-of-the-art performance across multiple benchmark datasets while significantly reducing computation time compared to previous approaches. Overall, our findings highlight the potential benefits of combining Bayesian inference with tempered MCMC simulation to design robust graph CNN models for real-world applications.",1
"A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.",0
"One of the primary challenges faced by graph convolutional networks (GCN) is performance degradation. This issue can arise due to several factors such as the increasing size of graphs, insufficient model capacity, noise in data, overfitting, etc. Addressing these issues is crucial for achieving accurate predictions and reliable results. In our study, we aimed at understanding the root causes of performance degeneration in GCN models. We analyzed how different network architectures respond to changes in input parameters, particularly graph size, signal-to-noise ratio, and parameter initialization strategies. Our findings indicate that simple modifications to existing methods like early stopping techniques can lead to significant improvements in performance. Additionally, we examined the effectiveness of state-of-the-art regularization techniques like weight decay, dropout, and data augmentations on mitigating overfitting. We observed considerable stability enhancements through these approaches, which led to substantial accuracy gains for small datasets with high noise levels. Ultimately, our research offers valuable insights into designing better GCN models capable of handling complex problems in machine learning and other related fields. By addressing the shortcomings of traditional methods and proposing innovative solutions, we hope to inspire further advances in this rapidly evolving field.",1
"Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.",0
"In this work we analyze the current state of graph transformer architectures based on global self attention mechanisms. We first discuss how these models have been applied to graphs in recent years, highlighting their strengths but also identifying limitations in terms of scalability and parallelization. Then, we introduce our proposal: edge augmentation as a novel approach that allows us to significantly improve performance while making use of less computing resources than previously required. Our experiments on several benchmark datasets show that edge augmentation can lead to better results without increasing computational complexity, by exploiting more effectively local context within each node's neighborhood. With these findings, we aim to contribute towards establishing solid foundations for future research in graph transformers.",1
"Link prediction is one of the key problems for graph-structured data. With the advancement of graph neural networks, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) have been proposed to learn graph embeddings in an unsupervised way. It has been shown that these methods are effective for link prediction tasks. However, they do not work well in link predictions when a node whose degree is zero (i.g., isolated node) is involved. We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero regardless of their content features. In this paper, we propose a novel Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization to derive better embeddings for isolated nodes. We show that our VGNAEs outperform the existing state-of-the-art models for link prediction tasks. The code is available at https://github.com/SeongJinAhn/VGNAE.",0
"This paper proposes a novel deep learning architecture called variational graph normalized autoencoder (VGNA), which combines principles from graph theory with traditional neural network architectures. By applying spectral graph embedding techniques, we can learn low dimensional latent representations that better capture complex relationships among data points. These learned embeddings allow us to perform more robust downstream tasks such as anomaly detection and clustering. We evaluate VGNA on benchmark datasets against state-of-the-art methods and demonstrate significant improvements in performance across several tasks. Our approach allows for flexible incorporation of domain knowledge through custom graphs, providing a powerful toolkit for researchers working with large high-dimensional datasets. In summary, our work offers new perspectives on leveraging graph structure within traditional neural networks, significantly advancing the fields of representation learning and anomaly detection.",1
